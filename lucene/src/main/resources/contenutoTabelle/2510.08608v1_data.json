{
    "S1.T1": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 1: Comparison of existing culture-related benchmark datasets with MMA-Asia (Ours). ,  and  represent the the text, image and speech modalities, respectively. “MLA” and “MDA” denote “multilingual alignment” and “multimodal alignment”, respectively.",
        "body": "Benchmark\n\n\n\n\nMLA\n\n\n\n\nMDA\n\n\n\n\nRI\n\n\n\n\n# of\nCtries\n\n\n\n\n# of\nLangs\n\n\n\n\nModality\n\n\n\n\nMulti-step\nreasoning\n\n\n\n\nQuestion forms\n\n\n\n\nTotal\nsamples\n\n\n\n\nTotal\ndomains\n\n\n\n\n\n\n\n\nSeaEval\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n4\n\n\n\n\n1\n\n\n\n\n\n\n\n\n✓\n\n\n\n\nDiverse\n\n\n\n\n415\n\n\n\n\n-\n\n\n\n\n\n\nCLIcK\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n\n\n\n\n-\n\n\n\n\nDiverse\n\n\n\n\n1,995\n\n\n\n\n11\n\n\n\n\n\n\nBLEnD\n\n\n\n\n✓\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n16\n\n\n\n\n13\n\n\n\n\n\n\n\n\n-\n\n\n\n\nFixed\n\n\n\n\n52,557\n\n\n\n\n6\n\n\n\n\n\n\nCulturalbench\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n45\n\n\n\n\n1\n\n\n\n\n\n\n\n\n-\n\n\n\n\nFixed\n\n\n\n\n1,696\n\n\n\n\n17\n\n\n\n\n\n\nCVQA\n\n\n\n\n✓\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n30\n\n\n\n\n31\n\n\n\n\n, \n\n\n\n\n-\n\n\n\n\nFixed\n\n\n\n\n10,000\n\n\n\n\n10\n\n\n\n\n\n\nCulturalbenchVQA\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n11\n\n\n\n\n1\n\n\n\n\n, \n\n\n\n\n-\n\n\n\n\nDiverse\n\n\n\n\n2,378\n\n\n\n\n5\n\n\n\n\n\n\nALM-bench\n\n\n\n\n✓\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n73\n\n\n\n\n100\n\n\n\n\n, \n\n\n\n\n-\n\n\n\n\nDiverse\n\n\n\n\n22,763\n\n\n\n\n19\n\n\n\n\n\n\nMd3\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n3\n\n\n\n\n1\n\n\n\n\n\n\n\n\n-\n\n\n\n\nDiverse\n\n\n\n\n3,689\n\n\n\n\n2\n\n\n\n\n\n\nMULTI-AUDIOJAIL\n\n\n\n\n✓\n\n\n\n\n✗\n\n\n\n\n✗\n\n\n\n\n6\n\n\n\n\n6\n\n\n\n\n\n\n\n\n-\n\n\n\n\nDiverse\n\n\n\n\n102,720\n\n\n\n\n-\n\n\n\n\n\n\nOurs\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n8\n\n\n\n\n10\n\n\n\n\n, , \n\n\n\n\n✓\n\n\n\n\nDiverse\n\n\n\n\n27,000\n\n\n\n\n9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Benchmark</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">MLA</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">MDA</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">RI</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\"># of</span>\n<span class=\"ltx_p\">Ctries</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\"># of</span>\n<span class=\"ltx_p\">Langs</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Modality</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">Multi-step</span>\n<span class=\"ltx_p\">reasoning</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Question forms</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">Total</span>\n<span class=\"ltx_p\">samples</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_center ltx_align_top\">\n<span class=\"ltx_p\">Total</span>\n<span class=\"ltx_p\">domains</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">SeaEval</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g7\" src=\"text.png\" width=\"13\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">415</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">CLIcK</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g8\" src=\"text.png\" width=\"13\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1,995</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">11</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">BLEnD</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">16</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">13</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g9\" src=\"text.png\" width=\"13\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">Fixed</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">52,557</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Culturalbench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g10\" src=\"text.png\" width=\"13\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Fixed</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1,696</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">17</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">CVQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">31</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g11\" src=\"text.png\" width=\"13\"/>\n<span class=\"ltx_p ltx_align_center\">, <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"14\" id=\"S1.T1.g12\" src=\"image.png\" width=\"14\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">Fixed</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">10,000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">10</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">CulturalbenchVQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g13\" src=\"text.png\" width=\"13\"/>\n<span class=\"ltx_p ltx_align_center\">, <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"14\" id=\"S1.T1.g14\" src=\"image.png\" width=\"14\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">2,378</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">ALM-bench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">73</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">100</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g15\" src=\"text.png\" width=\"13\"/>\n<span class=\"ltx_p ltx_align_center\">, <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"14\" id=\"S1.T1.g16\" src=\"image.png\" width=\"14\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">22,763</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">19</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Md3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"14\" id=\"S1.T1.g17\" src=\"speech.png\" width=\"14\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">3,689</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p ltx_align_center\">2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">MULTI-AUDIOJAIL</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">&#10007;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"14\" id=\"S1.T1.g18\" src=\"speech.png\" width=\"14\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">102,720</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFF5F5;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:68.3pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">Ours</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:22.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"13\" id=\"S1.T1.g19\" src=\"text.png\" width=\"13\"/>\n<span class=\"ltx_p ltx_align_center\">, <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"14\" id=\"S1.T1.g20\" src=\"image.png\" width=\"14\"/>, <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"14\" id=\"S1.T1.g21\" src=\"speech.png\" width=\"14\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:39.8pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#00FF00;\">&#10003;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">Diverse</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:34.1pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">27,000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_b\" style=\"width:28.5pt;padding:-2.5pt 2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFF5F5;\">\n<span class=\"ltx_p ltx_align_center\">9</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reasoning",
            "domains",
            "culturalbench",
            "md3",
            "text",
            "fixed",
            "ours",
            "diverse",
            "modalities",
            "modality",
            "datasets",
            "“mla”",
            "“multilingual",
            "langs",
            "mla",
            "benchmark",
            "blend",
            "speech",
            "culturalbenchvqa",
            "ctries",
            "culturerelated",
            "denote",
            "alignment”",
            "existing",
            "mmaasia",
            "seaeval",
            "multiaudiojail",
            "image",
            "almbench",
            "question",
            "samples",
            "represent",
            "“mda”",
            "click",
            "cvqa",
            "forms",
            "multistep",
            "total",
            "“multimodal",
            "respectively",
            "comparison",
            "mda"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmaasia",
                    "reasoning",
                    "text",
                    "multistep",
                    "modalities",
                    "image",
                    "question",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "reasoning",
                    "text",
                    "modalities",
                    "datasets",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "mmaasia",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmaasia",
                    "reasoning",
                    "multistep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Aligned tri-modal, multilingual benchmark.</span> We release <span class=\"ltx_text ltx_font_bold\">MMA-ASIA</span> with 27,000 multilingual multimodal questions authored by in-country experts across 8 countries and 10 languages.</p>\n\n",
                "matched_terms": [
                    "mmaasia",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech datasets reveal substantial accent-related biases <cite class=\"ltx_cite ltx_citemacro_citep\">(Eisenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib13\" title=\"\">2023</a>; Roh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib26\" title=\"\">2025</a>)</cite>, and perturbations can drastically change outcomes. However, they rarely evaluate cultural knowledge directly or analyze trimodal alignment and consistency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "mmaasia",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "mmaasia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "reasoning",
                    "multistep",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "existing",
                    "mmaasia",
                    "question",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "mmaasia",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "forms",
                    "modalities",
                    "modality",
                    "represent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "modalities",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "culturerelated",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "text",
                    "culturerelated",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "image",
                    "denote"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "multistep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmaasia",
                    "reasoning",
                    "text",
                    "multistep",
                    "image",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "mmaasia",
                    "reasoning",
                    "multistep",
                    "modality",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "multistep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> illustrates examples of deconstructing multi-step reasoning questions into single-fact verification subquestions. We enumerate all single-fact subquestions embedded in each multi-step item to assess the model&#8217;s generalization.</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collect culture-related textual facts from the public internet and filter out any content containing racism or hate speech. Our &#8220;knowledge points&#8221; are a few sentences manually summarized by annotators, and all questions are authored from scratch, so no infringement issues are involved.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "culturerelated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtained voice samples from individuals outside the data team for speech synthesis, without disclosing any personal information. Consent was obtained prior to recording, and the audio is used solely for research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samples"
                ]
            }
        ]
    },
    "A1.T2": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 2: Data Annotator Demographics and Skills",
        "body": "Professional\n\n\nBackground",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Professional</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Background</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "skills",
            "demographics",
            "annotator",
            "background",
            "data",
            "professional"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n"
        ],
        "contextual_paragraphs": []
    },
    "A1.T3": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 3: Data Reviewer Demographics and Skills",
        "body": "Local Lang.\n\n\nProf.",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Local Lang.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Prof.</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "lang",
            "skills",
            "local",
            "demographics",
            "reviewer",
            "prof",
            "data"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "local",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "local",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "local",
                    "data"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 4: Local Languages by Country",
        "body": "English, Chinese,\n\n\nMalay, Tamil",
        "html_code": "<table class=\"ltx_tabular ltx_align_top\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">English, Chinese,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Malay, Tamil</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "malay",
            "english",
            "chinese",
            "country",
            "languages",
            "local",
            "tamil"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An in-country expert team curated each national subset. All team members were native speakers of the local language and proficient in English. Annotators had lived in the respective country for more than ten years, ensuring deep familiarity with local cultural contexts. Detailed annotator information is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS1\" title=\"A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>. Before annotation began, we held project-wide briefings to explain the scope and requirements. We also distributed a detailed English annotation guideline (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS3\" title=\"A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "local",
                    "english",
                    "country"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese",
                    "languages",
                    "malay",
                    "tamil"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "tamil",
                    "local",
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "local",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "english",
                    "country"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "english",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "local",
                    "english",
                    "languages",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese",
                    "languages",
                    "malay",
                    "tamil"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "local",
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "local",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "local",
                    "english",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese",
                    "languages",
                    "malay",
                    "tamil"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "english",
                    "languages"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 5: An example of multi-step reasoning question",
        "body": "Question\n\n\n\n\nAn ancient tower became famous due to a poem by the Tang Dynasty poet Cui Hao.\nIn which period was it proposed to be constructed with iron materials?\n(A) 16th year of Guangxu, Qing Dynasty  (B) 1st year of Jiaqing, Qing Dynasty  (C) 7th year of Tongzhi, Qing Dynasty  (D) 8th year of the Republic of China\n\n\n\n\n\n\nReasoning Decomposition\n\n\n\n\nStep 1: Identify the tower.\nThe poem is Cui Hao’s “Yellow Crane Tower,” so the tower is Yellow Crane Tower.\nStep 2: Recall historical events.\nIn the 10th year of Guangxu (1884) the tower was destroyed by fire.\nIn the 16th year of Guangxu (1890), Zhang Zhidong (Governor-General of Hubei and Hunan) first proposed rebuilding the tower using iron materials.\nStep 3: Match with the options.\nA (1890): Correct, matches the historical fact.\nB (1796): Incorrect, too early.\nC (1868): Incorrect, before the fire.\nD (1919): Incorrect, after the Qing Dynasty and not the first proposal.\nFinal Answer: A. 16th year of Guangxu, Qing Dynasty.",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">Question</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">An ancient tower became famous due to a poem by the Tang Dynasty poet Cui Hao.\nIn which period was it proposed to be constructed with iron materials?\n<span class=\"ltx_text ltx_font_italic\">(A) 16th year of Guangxu, Qing Dynasty &#8195;(B) 1st year of Jiaqing, Qing Dynasty &#8195;(C) 7th year of Tongzhi, Qing Dynasty &#8195;(D) 8th year of the Republic of China</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">Reasoning Decomposition</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Identify the tower.</span>\nThe poem is Cui Hao&#8217;s &#8220;Yellow Crane Tower,&#8221; so the tower is <em class=\"ltx_emph ltx_font_italic\">Yellow Crane Tower</em>.</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Recall historical events.</span>\nIn the 10th year of Guangxu (1884) the tower was destroyed by fire.\nIn the 16th year of Guangxu (1890), Zhang Zhidong (Governor-General of Hubei and Hunan) first proposed rebuilding the tower using iron materials.</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Match with the options.</span>\nA (1890): <span class=\"ltx_text ltx_font_bold\">Correct</span>, matches the historical fact.\nB (1796): Incorrect, too early.\nC (1868): Incorrect, before the fire.\nD (1919): Incorrect, after the Qing Dynasty and not the first proposal.</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Final Answer:</span> <span class=\"ltx_text ltx_font_bold\">A. 16th year of Guangxu, Qing Dynasty</span>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "fact",
            "governorgeneral",
            "reasoning",
            "answer",
            "after",
            "step",
            "hunan",
            "zhang",
            "cui",
            "hubei",
            "dynasty",
            "proposed",
            "16th",
            "qing",
            "crane",
            "first",
            "destroyed",
            "example",
            "7th",
            "poem",
            "which",
            "jiaqing",
            "year",
            "guangxu",
            "fire",
            "tower",
            "not",
            "period",
            "10th",
            "due",
            "matches",
            "match",
            "correct",
            "before",
            "events",
            "zhidong",
            "china",
            "8th",
            "iron",
            "decomposition",
            "tower”",
            "famous",
            "question",
            "poet",
            "“yellow",
            "tongzhi",
            "yellow",
            "too",
            "hao’s",
            "became",
            "rebuilding",
            "recall",
            "hao",
            "1st",
            "identify",
            "tang",
            "final",
            "ancient",
            "materials",
            "multistep",
            "proposal",
            "options",
            "constructed",
            "republic",
            "early",
            "incorrect",
            "historical"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "correct",
                    "multistep",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "which",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "step",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "materials",
                    "china",
                    "proposed",
                    "constructed",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both the text-only and multimodal tracks, teams generated at least 56 keywords or short phrases per category as <em class=\"ltx_emph ltx_font_italic\">cultural prompts</em>. If a category could not supply enough prompts, the shortfall was filled using prompts from other categories. These prompts were designed to capture both diversity and geographic breadth, reducing the risk of homogenizing Asian cultures. Using these prompts, team members retrieved relevant texts and images from the web and extracted short passages to serve as the basis for question authoring. When source content was ambiguous, we cross-checked with multiple references to ensure authenticity. All images were obtained from Creative Commons (CC)-licensed resources (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "recall",
                    "reasoning",
                    "materials",
                    "multistep",
                    "not",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "after",
                    "options",
                    "answer",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "question",
                    "matches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "due",
                    "example",
                    "china"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "reasoning",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "not",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "tower",
                    "answer",
                    "identify"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After lightweight smoothing and normalization of M(<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>), the resulting <math alttext=\"\\hat{M}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>M</mi><mo>^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\hat{M}^{(\\ell)}</annotation></semantics></math> then undergoes cross-layer aggregation and bilinear interpolation upsampling on the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> blocks at the end of the visual tower, as defined by Eqs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E4\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E5\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, to the original image resolution. For more robust results, we specifically choose the last 3 blocks and do cross-layer aggregation by averaging them.</p>\n\n",
                "matched_terms": [
                    "tower",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "which",
                    "reasoning",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "example",
                    "reasoning",
                    "which",
                    "answer",
                    "multistep",
                    "not",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "step",
                    "reasoning",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "correct",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Integration Failure:</span> 2.6% (Claude), 10.7%(Qwen) of cases failed at the final synthesis step, despite all sub-questions being answered correctly.</p>\n\n",
                "matched_terms": [
                    "step",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "after",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">History</span>:\nMajor historical periods and events, notable figures and heritage sites, and how historical memory shapes contemporary society and culture. May also include colonial/independence histories and cultural change driven by migration and war.</p>\n\n",
                "matched_terms": [
                    "events",
                    "historical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "options",
                    "answer",
                    "not",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "not",
                    "before"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "after",
                    "reasoning",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "example",
                    "reasoning",
                    "zhang",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "answer",
                    "final",
                    "multistep",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> illustrates examples of deconstructing multi-step reasoning questions into single-fact verification subquestions. We enumerate all single-fact subquestions embedded in each multi-step item to assess the model&#8217;s generalization.</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use only images under Creative Commons licenses and strictly for research purposes. We apply an automated face-blurring tool &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib25\" title=\"\">2023</a>)</cite> to protect privacy, followed by manual review to catch any misses or false positives. Please note that faces of historical figures or cartoon characters are not masked. Our images do not contain pornography, violence, or other harmful content.</p>\n\n",
                "matched_terms": [
                    "not",
                    "historical"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 6: Models used for speech data generation",
        "body": "Language\n\n\nModel\n\n\n\n\n\n\nEnglish with accent, Korean, Japanese, Chinese\n\n\nCosyVoice2-0.5B (Du et al., 2025)\n\n\n\n\nEnglish without accent\n\n\nCosyVoice-300M (Du et al., 2024)\n\n\n\n\nVietnamese, Tamil, Mongolian, Malay\n\n\nInternally developed TTS systems\n\n\n\n\nIndonesian\n\n\nIndonesian TTS (Kim et al., 2021)\n\n\n\n\nHindi\n\n\nAI4Bharat Indic-TTS (Sankar et al., 2024)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">English with accent, Korean, Japanese, Chinese</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">English without accent</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">CosyVoice-300M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib11\" title=\"\">2024</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vietnamese, Tamil, Mongolian, Malay</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Internally developed TTS systems</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Indonesian</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Indonesian TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib20\" title=\"\">2021</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Hindi</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">AI4Bharat Indic-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sankar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib28\" title=\"\">2024</a>)</cite></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "hindi",
            "accent",
            "cosyvoice205b",
            "malay",
            "sankar",
            "tamil",
            "used",
            "korean",
            "speech",
            "mongolian",
            "indictts",
            "english",
            "tts",
            "internally",
            "generation",
            "language",
            "japanese",
            "model",
            "cosyvoice300m",
            "developed",
            "ai4bharat",
            "without",
            "chinese",
            "systems",
            "vietnamese",
            "kim",
            "models",
            "data",
            "indonesian"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "model",
                    "models",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "models",
                    "english",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An in-country expert team curated each national subset. All team members were native speakers of the local language and proficient in English. Annotators had lived in the respective country for more than ten years, ensuring deep familiarity with local cultural contexts. Detailed annotator information is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS1\" title=\"A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>. Before annotation began, we held project-wide briefings to explain the scope and requirements. We also distributed a detailed English annotation guideline (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS3\" title=\"A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "language",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "hindi",
                    "english",
                    "chinese",
                    "malay",
                    "tamil"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tamil",
                    "used",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "tts",
                    "accent",
                    "english",
                    "data",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "models",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "english",
                    "data",
                    "vietnamese",
                    "korean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "language",
                    "japanese",
                    "model",
                    "english",
                    "chinese",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "mongolian",
                    "language",
                    "hindi",
                    "models",
                    "english",
                    "chinese",
                    "malay",
                    "tamil",
                    "korean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "model",
                    "models",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "used",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obviously, the transfer and generalization of cultural knowledge pose a significant challenge for language models, irrespective of their scale. Claude corrects 35.3% of initial errors through stepwise decomposition-reintegration evaluation, showing broad knowledge coverage but limited cultural generalization. Qwen&#8217;s performance is constrained by knowledge gaps and poor transferability. While 29.2% of multi-step problems have all sub-questions answered correctly, 10.7% fail at reintegration, indicating weaker cultural generalization capabilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "english",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\">Note: NLP: Natural Language Processing; SP: Speech Processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "accent",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mongolian",
                    "generation",
                    "japanese",
                    "systems",
                    "hindi",
                    "accent",
                    "english",
                    "tts",
                    "chinese",
                    "used",
                    "malay",
                    "tamil",
                    "vietnamese",
                    "korean",
                    "indonesian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio files were standardized in WAV format with a 16 kHz sampling rate. And our speech generation uses two input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "language",
                    "used",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "japanese",
                    "model",
                    "english",
                    "indonesian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "used",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtained voice samples from individuals outside the data team for speech synthesis, without disclosing any personal information. Consent was obtained prior to recording, and the audio is used solely for research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "used",
                    "without"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 7: English prompts for different modals.",
        "body": "Modal\n\n\n\n\nPrompt Template\n\n\n\n\n\n\n\n\nText-Only/Rephrase\n\n\n\n\nPlease answer the following culture-related question.\\n{question}\\n{options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.\n\n\n\n\n\n\nVQA\n\n\n\n\nBased on the image, please answer the following culture-related question.\\n{question}\\n{options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.\n\n\n\n\n\n\nSpeech question & text options\n\n\n\n\nThis is a culture-related question.\\n Based on the question mentioned in this audio, please choose the correct answers from the following provided options. {options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.\n\n\n\n\n\n\nSpeech question & options\n\n\n\n\nThis is a culture-related question. Based on the question and options mentioned in this audio, please choose the correct options. This is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_align_left ltx_font_bold\">Modal</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Template</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Text-Only/Rephrase</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Please answer the following culture-related question.\\n{question}\\n{options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">VQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Based on the image, please answer the following culture-related question.\\n{question}\\n{options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Speech question &amp; text options</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">This is a culture-related question.\\n Based on the question mentioned in this audio, please choose the correct answers from the following provided options. {options}\\nThis is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Speech question &amp; options</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">This is a culture-related question. Based on the question and options mentioned in this audio, please choose the correct options. This is a multiple-choice question. Please first return all possible option letters, then explain your choice in English.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "answer",
            "option",
            "text",
            "mentioned",
            "please",
            "vqa",
            "first",
            "speech",
            "choose",
            "all",
            "english",
            "possible",
            "answers",
            "template",
            "culturerelated",
            "from",
            "following",
            "questionn",
            "return",
            "choice",
            "correct",
            "questionnquestionnoptionsnthis",
            "multiplechoice",
            "letters",
            "textonlyrephrase",
            "image",
            "then",
            "prompts",
            "question",
            "optionsnthis",
            "prompt",
            "modal",
            "your",
            "modals",
            "different",
            "provided",
            "explain",
            "options",
            "audio",
            "based"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "correct",
                    "text",
                    "answers",
                    "multiplechoice",
                    "image",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "correct",
                    "text",
                    "answers",
                    "from",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "english",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual VQA datasets use community-sourced images and questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Nayak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib23\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>)</cite>, revealing vision-language gaps and language sensitivity. Yet they typically lack text/speech-parallel versions of identical items, making it hard to isolate whether failures stem from cultural knowledge, visual grounding, or language handling.</p>\n\n",
                "matched_terms": [
                    "from",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "english",
                    "answers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "image",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An in-country expert team curated each national subset. All team members were native speakers of the local language and proficient in English. Annotators had lived in the respective country for more than ten years, ensuring deep familiarity with local cultural contexts. Detailed annotator information is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS1\" title=\"A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>. Before annotation began, we held project-wide briefings to explain the scope and requirements. We also distributed a detailed English annotation guideline (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS3\" title=\"A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "all",
                    "english",
                    "explain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "possible",
                    "all",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both the text-only and multimodal tracks, teams generated at least 56 keywords or short phrases per category as <em class=\"ltx_emph ltx_font_italic\">cultural prompts</em>. If a category could not supply enough prompts, the shortfall was filled using prompts from other categories. These prompts were designed to capture both diversity and geographic breadth, reducing the risk of homogenizing Asian cultures. Using these prompts, team members retrieved relevant texts and images from the web and extracted short passages to serve as the basis for question authoring. When source content was ambiguous, we cross-checked with multiple references to ensure authenticity. All images were obtained from Creative Commons (CC)-licensed resources (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "all",
                    "from",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "question",
                    "all",
                    "english",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "answer",
                    "correct",
                    "text",
                    "english",
                    "options",
                    "vqa",
                    "question",
                    "audio",
                    "provided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "all",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "answer",
                    "english",
                    "options",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "all",
                    "template",
                    "prompts",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "following",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "english",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "english",
                    "vqa",
                    "then"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "culturerelated",
                    "english",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "option",
                    "correct",
                    "all",
                    "english",
                    "answers",
                    "from",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "answer",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "question",
                    "all",
                    "answers",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "all",
                    "english",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "english",
                    "answers",
                    "different",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "image",
                    "culturerelated",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "answer",
                    "mentioned",
                    "image",
                    "vqa",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After lightweight smoothing and normalization of M(<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>), the resulting <math alttext=\"\\hat{M}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>M</mi><mo>^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\hat{M}^{(\\ell)}</annotation></semantics></math> then undergoes cross-layer aggregation and bilinear interpolation upsampling on the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p5.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> blocks at the end of the visual tower, as defined by Eqs. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E4\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E5\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, to the original image resolution. For more robust results, we specifically choose the last 3 blocks and do cross-layer aggregation by averaging them.</p>\n\n",
                "matched_terms": [
                    "choose",
                    "image",
                    "then"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "question",
                    "mentioned",
                    "image",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "correct",
                    "answers",
                    "from",
                    "image",
                    "vqa",
                    "then"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal models demonstrate significant inconsistency in cultural awareness across modalities, indicating flawed cultural knowledge transfer. In VQA, this deficiency stems from two core issues: &#8220;<span class=\"ltx_text ltx_font_italic\">selective attention pitfall</span>&#8221; where models over-focus on text-prompted subjects while missing key visual cues, and visual token-induced &#8220;<span class=\"ltx_text ltx_font_italic\">reasoning hallucinations</span>&#8221;.</p>\n\n",
                "matched_terms": [
                    "from",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "all",
                    "from",
                    "then",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "answers",
                    "english",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "all",
                    "answer",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "english",
                    "different",
                    "from",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "all",
                    "english",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "correct",
                    "option",
                    "all",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before generating the English audio, we standardized the input text through normalization of numbers and symbols, handling of abbreviations and special terms, and sentence segmentation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "different",
                    "from",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "text",
                    "options",
                    "audio",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio files were standardized in WAV format with a 16 kHz sampling rate. And our speech generation uses two input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Only the question stem from the <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item;</p>\n\n",
                "matched_terms": [
                    "question",
                    "from",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options",
                    "answer",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "speech",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "answer",
                    "text",
                    "options",
                    "vqa",
                    "question",
                    "provided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "prompt",
                    "text",
                    "based",
                    "from",
                    "image",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "then",
                    "image",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the question in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the completed responses under different generation modes are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T14\" title=\"Table 14 &#8227; A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "different",
                    "provided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "all",
                    "from",
                    "then",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "text",
                    "all",
                    "answers",
                    "mentioned",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "english",
                    "answers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collect culture-related textual facts from the public internet and filter out any content containing racism or hate speech. Our &#8220;knowledge points&#8221; are a few sentences manually summarized by annotators, and all questions are authored from scratch, so no infringement issues are involved.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "from",
                    "culturerelated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtained voice samples from individuals outside the data team for speech synthesis, without disclosing any personal information. Consent was obtained prior to recording, and the audio is used solely for research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "audio"
                ]
            }
        ]
    },
    "A1.T8": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 8: Prompts Used with Closed-Source APIs for Translation, Answer Extraction, and Answer–Reference Consistency Checking",
        "body": "Task\n\n\nPrompt\n\n\n\n\n\n\nTranslation\n\n\nTranslate the following sentence into English. \\n{Input sentence}\\nThis is a multiple-choice question in the cultural domain of {Country}. Pay particular attention to the terms in the input and use their official translations; if no official translation exists, you may use transliteration. Ensure accuracy, faithfulness, and fluency. Return only the translation; do not include any additional hints or analysis.\n\n\n\n\nLLM-as-Judge\n\n\nYou are an expert evaluator. Your task is to determine if the ’Model Answer’ correctly and completely incorporates the information from the ’Knowledge Point’.\nKnowledge Point:\\n{knowledge_point}\\nModel Answer\\n{model_answer}\\n\\n.\nIf the Model Answer correctly and completely includes the information from the Knowledge Point, or if the model’s response aligns with the Knowledge Point’s content and viewpoint. And the Model Answer doesn’t contain any factual error, Answer ’yes’. Otherwise Answer ’no’. Please only return ’yes’ or ’no’.\n\n\n\n\nAnswer Extraction\n\n\nThis is a model’s response to a multiple-choice question. First, understand the text, then extract the model’s chosen options, returning only the option letters (e.g., A, B, C). Do not include the option content. Output the result in a format like [A, B]. If the response does not contain any final choice, return NA.",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Prompt</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Translation</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Translate the following sentence into English. \\n{Input sentence}\\nThis is a multiple-choice question in the cultural domain of {Country}. Pay particular attention to the terms in the input and use their official translations; if no official translation exists, you may use transliteration. Ensure accuracy, faithfulness, and fluency. Return only the translation; do not include any additional hints or analysis.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLM-as-Judge</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">You are an expert evaluator. Your task is to determine if the &#8217;Model Answer&#8217; correctly and completely incorporates the information from the &#8217;Knowledge Point&#8217;.\nKnowledge Point:\\n{knowledge_point}\\nModel Answer\\n{model_answer}\\n\\n.\nIf the Model Answer correctly and completely includes the information from the Knowledge Point, or if the model&#8217;s response aligns with the Knowledge Point&#8217;s content and viewpoint. And the Model Answer doesn&#8217;t contain any factual error, Answer &#8217;yes&#8217;. Otherwise Answer &#8217;no&#8217;. Please only return &#8217;yes&#8217; or &#8217;no&#8217;.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Answer Extraction</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">This is a model&#8217;s response to a multiple-choice question. First, understand the text, then extract the model&#8217;s chosen options, returning only the option letters (e.g., A, B, C). Do not include the option content. Output the result in a format like [A, B]. If the response does not contain any final choice, return NA.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "completely",
            "doesn’t",
            "knowledge",
            "extract",
            "information",
            "faithfulness",
            "translation",
            "analysis",
            "pay",
            "include",
            "llmasjudge",
            "translate",
            "cultural",
            "closedsource",
            "fluency",
            "letters",
            "then",
            "apis",
            "’no’",
            "final",
            "factual",
            "official",
            "pointnknowledgepointnmodel",
            "format",
            "aligns",
            "translations",
            "contain",
            "first",
            "sentencenthis",
            "returning",
            "ninput",
            "following",
            "determine",
            "multiplechoice",
            "result",
            "model’s",
            "prompts",
            "question",
            "consistency",
            "transliteration",
            "only",
            "expert",
            "ensure",
            "’yes’",
            "point",
            "point’s",
            "answer",
            "option",
            "text",
            "sentence",
            "output",
            "input",
            "please",
            "incorporates",
            "additional",
            "response",
            "chosen",
            "you",
            "from",
            "not",
            "choice",
            "country",
            "does",
            "into",
            "accuracy",
            "correctly",
            "prompt",
            "otherwise",
            "your",
            "’knowledge",
            "’model",
            "options",
            "their",
            "error",
            "task",
            "evaluator",
            "answer–reference",
            "content",
            "used",
            "like",
            "viewpoint",
            "hints",
            "english",
            "checking",
            "answer’",
            "extraction",
            "understand",
            "return",
            "answernmodelanswernn",
            "model",
            "exists",
            "domain",
            "terms",
            "point’",
            "particular",
            "attention",
            "any",
            "use",
            "includes"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cultural",
                    "text",
                    "input",
                    "multiplechoice",
                    "checking",
                    "attention",
                    "analysis",
                    "consistency",
                    "question",
                    "knowledge",
                    "used",
                    "their",
                    "ensure",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "their",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "cultural",
                    "text",
                    "result",
                    "from",
                    "consistency",
                    "question",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "english",
                    "consistency",
                    "prompts",
                    "question",
                    "knowledge",
                    "aligns"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "english",
                    "from",
                    "use",
                    "consistency",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Five-axis evaluation protocol.</span> We formalize cultural awareness <em class=\"ltx_emph ltx_font_italic\">consistency</em> (cross-modal/cross-lingual) and <em class=\"ltx_emph ltx_font_italic\">grounding</em> with negative controls and ablations, plus generalization tests under held-out themes/countries. We provide reference implementations and CI-tested evaluation scripts.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent benchmarks assess culture-specific knowledge via MCQs <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib19\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Susanto et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib30\" title=\"\">2025</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>)</cite>, consistently showing (i) performance gaps favoring English/high-resource settings and (ii) sensitivity to formatting. However, most lack aligned multimodal counterparts to test cross-modal cultural understanding.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual VQA datasets use community-sourced images and questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Nayak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib23\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>)</cite>, revealing vision-language gaps and language sensitivity. Yet they typically lack text/speech-parallel versions of identical items, making it hard to isolate whether failures stem from cultural knowledge, visual grounding, or language handling.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "from",
                    "use",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech datasets reveal substantial accent-related biases <cite class=\"ltx_cite ltx_citemacro_citep\">(Eisenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib13\" title=\"\">2023</a>; Roh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib26\" title=\"\">2025</a>)</cite>, and perturbations can drastically change outcomes. However, they rarely evaluate cultural knowledge directly or analyze trimodal alignment and consistency.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "consistency",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQ performance may reflect shortcuts rather than grounded reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>. Benchmarks seldom include negative controls or report cross-lingual/cross-modal consistency&#8212;essential for distinguishing cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "from",
                    "include",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "text",
                    "english",
                    "consistency",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "text",
                    "from",
                    "country",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An in-country expert team curated each national subset. All team members were native speakers of the local language and proficient in English. Annotators had lived in the respective country for more than ten years, ensuring deep familiarity with local cultural contexts. Detailed annotator information is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS1\" title=\"A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>. Before annotation began, we held project-wide briefings to explain the scope and requirements. We also distributed a detailed English annotation guideline (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS3\" title=\"A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "english",
                    "country",
                    "expert",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "includes",
                    "official",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both the text-only and multimodal tracks, teams generated at least 56 keywords or short phrases per category as <em class=\"ltx_emph ltx_font_italic\">cultural prompts</em>. If a category could not supply enough prompts, the shortfall was filled using prompts from other categories. These prompts were designed to capture both diversity and geographic breadth, reducing the risk of homogenizing Asian cultures. Using these prompts, team members retrieved relevant texts and images from the web and extracted short passages to serve as the basis for question authoring. When source content was ambiguous, we cross-checked with multiple references to ensure authenticity. All images were obtained from Creative Commons (CC)-licensed resources (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "from",
                    "content",
                    "not",
                    "prompts",
                    "question",
                    "ensure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "cultural",
                    "english",
                    "from",
                    "not",
                    "official",
                    "question",
                    "knowledge",
                    "transliteration",
                    "used",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "text",
                    "english",
                    "options",
                    "into",
                    "question",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "answer",
                    "question",
                    "knowledge",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "answer",
                    "cultural",
                    "model’s",
                    "question",
                    "knowledge",
                    "used",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "final",
                    "their",
                    "english",
                    "fluency",
                    "options",
                    "country",
                    "knowledge",
                    "accuracy",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "otherwise",
                    "closedsource",
                    "include",
                    "prompts",
                    "question",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "closedsource",
                    "consistency",
                    "following",
                    "knowledge",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "english",
                    "content",
                    "country",
                    "model’s",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "model",
                    "cultural",
                    "english",
                    "prompts",
                    "transliteration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "text",
                    "their",
                    "english",
                    "analysis",
                    "content",
                    "then",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "english",
                    "from",
                    "analysis",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "choice",
                    "their",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "option",
                    "accuracy",
                    "english",
                    "from",
                    "model’s",
                    "use",
                    "consistency",
                    "contain",
                    "llmasjudge",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results suggest that MCQs alone do not reliably measure cultural understanding (especially\nfor open-source models) and should be augmented with culture-grounded verification,\nincluding knowledge-point checks and rationale assessment.</p>\n\n",
                "matched_terms": [
                    "not",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "model",
                    "answer",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "question",
                    "any",
                    "answer",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "english",
                    "consistency",
                    "not",
                    "does",
                    "knowledge",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From these observations, we observe that the consistency between languages depends on the data resources and cultural exposure. Resource asymmetry degrades consistency, whereas cultural prominence helps. Consistency decays nonlinearly as more languages are considered, and strong pairwise agreement does not guarantee multi-language coherence. Visual cues can narrow gaps in certain settings, but are insufficient to overcome the structural limitations of low-resource languages.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "from",
                    "does",
                    "not",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cultural",
                    "output",
                    "english",
                    "analysis",
                    "consistency",
                    "knowledge",
                    "additional",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "input",
                    "from",
                    "analysis",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "answer",
                    "model",
                    "cultural",
                    "attention",
                    "use",
                    "prompts",
                    "only",
                    "extract"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "question",
                    "attention",
                    "following",
                    "additional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "model",
                    "from",
                    "content",
                    "model’s",
                    "then",
                    "correctly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal models demonstrate significant inconsistency in cultural awareness across modalities, indicating flawed cultural knowledge transfer. In VQA, this deficiency stems from two core issues: &#8220;<span class=\"ltx_text ltx_font_italic\">selective attention pitfall</span>&#8221; where models over-focus on text-prompted subjects while missing key visual cues, and visual token-induced &#8220;<span class=\"ltx_text ltx_font_italic\">reasoning hallucinations</span>&#8221;.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "from",
                    "attention",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "model",
                    "cultural",
                    "factual",
                    "from",
                    "not",
                    "then",
                    "into",
                    "question",
                    "knowledge",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "answer",
                    "closedsource",
                    "english",
                    "model’s",
                    "into",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Integration Failure:</span> 2.6% (Claude), 10.7%(Qwen) of cases failed at the final synthesis step, despite all sub-questions being answered correctly.</p>\n\n",
                "matched_terms": [
                    "correctly",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obviously, the transfer and generalization of cultural knowledge pose a significant challenge for language models, irrespective of their scale. Claude corrects 35.3% of initial errors through stepwise decomposition-reintegration evaluation, showing broad knowledge coverage but limited cultural generalization. Qwen&#8217;s performance is constrained by knowledge gaps and poor transferability. While 29.2% of multi-step problems have all sub-questions answered correctly, 10.7% fail at reintegration, indicating weaker cultural generalization capabilities.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "their",
                    "correctly",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "text",
                    "attention",
                    "analysis",
                    "consistency",
                    "include",
                    "use",
                    "into",
                    "knowledge",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "english",
                    "from",
                    "country",
                    "information",
                    "their",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "cultural",
                    "english",
                    "analysis",
                    "from",
                    "country",
                    "question",
                    "knowledge",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">History</span>:\nMajor historical periods and events, notable figures and heritage sites, and how historical memory shapes contemporary society and culture. May also include colonial/independence histories and cultural change driven by migration and war.</p>\n\n",
                "matched_terms": [
                    "include",
                    "cultural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "used",
                    "includes",
                    "cultural",
                    "official"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "option",
                    "cultural",
                    "from",
                    "content",
                    "country",
                    "consistency",
                    "knowledge",
                    "only",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before generating the English audio, we standardized the input text through normalization of numbers and symbols, handling of abbreviations and special terms, and sentence segmentation.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "text",
                    "sentence",
                    "english",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "english",
                    "ensure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "text",
                    "input",
                    "fluency",
                    "options",
                    "not",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio files were standardized in WAV format with a 16 kHz sampling rate. And our speech generation uses two input types.</p>\n\n",
                "matched_terms": [
                    "input",
                    "format"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Only the question stem from the <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item;</p>\n\n",
                "matched_terms": [
                    "question",
                    "only",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "text",
                    "only",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "closedsource",
                    "output",
                    "input",
                    "any",
                    "model’s",
                    "not",
                    "used",
                    "ensure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "text",
                    "options",
                    "question",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "text",
                    "from",
                    "content",
                    "use",
                    "only",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "input",
                    "into",
                    "attention",
                    "content",
                    "model’s",
                    "then",
                    "information",
                    "question",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cultural",
                    "text",
                    "their",
                    "english",
                    "input",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "model",
                    "answer",
                    "final",
                    "from",
                    "model’s",
                    "then",
                    "question",
                    "only",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the binary nature of the task and the observed human&#8211;model agreement rates (98%, 98%, 96%), the conservative lower-bound on inter-model agreement is <span class=\"ltx_text ltx_font_bold\">96%</span>. We therefore conclude that a single, top-performing judge model suffices for our setting, and cross-model adjudication is unnecessary.</p>\n\n",
                "matched_terms": [
                    "model",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "from",
                    "closedsource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "cultural",
                    "text",
                    "attention",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> illustrates examples of deconstructing multi-step reasoning questions into single-fact verification subquestions. We enumerate all single-fact subquestions embedded in each multi-step item to assess the model&#8217;s generalization.</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "translate",
                    "model",
                    "english",
                    "use",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collect culture-related textual facts from the public internet and filter out any content containing racism or hate speech. Our &#8220;knowledge points&#8221; are a few sentences manually summarized by annotators, and all questions are authored from scratch, so no infringement issues are involved.</p>\n\n",
                "matched_terms": [
                    "from",
                    "content",
                    "any"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use only images under Creative Commons licenses and strictly for research purposes. We apply an automated face-blurring tool &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib25\" title=\"\">2023</a>)</cite> to protect privacy, followed by manual review to catch any misses or false positives. Please note that faces of historical figures or cartoon characters are not masked. Our images do not contain pornography, violence, or other harmful content.</p>\n\n",
                "matched_terms": [
                    "any",
                    "content",
                    "not",
                    "please",
                    "use",
                    "only",
                    "contain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We obtained voice samples from individuals outside the data team for speech synthesis, without disclosing any personal information. Consent was obtained prior to recording, and the audio is used solely for research.</p>\n\n",
                "matched_terms": [
                    "from",
                    "used",
                    "any",
                    "information"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 9: Text-only modality performance with exact numbers, measured by Accuracy (%): the number of items where the model’s choice exactly matches the correct option, divided by 500. “-” means “not support”. The better-performing result among different languages within the same country is bolded.",
        "body": "Model\nKR-en\nMN-mn\nMN-en\nSG-zh\nSG-en\nSG-ms\nSG-ta\nVN-vi\nVN-en\n\n\n\n\nGemini 2.5 Flash\n65.2\n58.2\n70.0\n34.0\n43.4\n39.0\n31.4\n73.6\n73.0\n\n\nClaude Sonnet 4\n66.0\n59.4\n71.0\n29.8\n41.8\n37.8\n31.6\n74.4\n72.2\n\n\nGPT-4o\n68.8\n55.4\n69.4\n22.4\n47.6\n39.4\n33.2\n74.8\n71.8\n\n\nGLM-4-9B-chat\n42.8\n23.0\n52.6\n20.2\n24.8\n18.4\n19.4\n47.6\n51.0\n\n\nMistral-8B-Instruct\n40.8\n13.0\n50.8\n17.2\n25.2\n22.4\n15.4\n41.8\n48.6\n\n\nBaichuan-M2-32B\n51.6\n16.8\n61.6\n26.6\n33.6\n28.8\n19.4\n67.6\n62.8\n\n\nQwen3-30B-A3B\n49.8\n44.6\n65.0\n26.4\n29.2\n23.6\n22.8\n57.8\n65.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">KR-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-mn</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ms</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">65.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">58.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">70.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">34.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">39.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">31.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">73.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">73.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Claude Sonnet 4</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">66.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">29.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">41.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">31.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">74.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">72.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">68.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">55.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">69.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">47.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">39.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">33.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">74.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">71.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">GLM-4-9B-chat</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">42.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">23.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">24.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">18.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">19.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">47.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Mistral-8B-Instruct</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">40.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">13.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">17.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">25.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">41.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">48.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Baichuan-M2-32B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">51.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">61.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">33.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">19.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">67.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">62.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Qwen3-30B-A3B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">44.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">26.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">29.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">23.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">22.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">57.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "betterperforming",
            "option",
            "textonly",
            "vnen",
            "qwen330ba3b",
            "exactly",
            "modality",
            "gpt4o",
            "same",
            "glm49bchat",
            "means",
            "sgta",
            "numbers",
            "gemini",
            "claude",
            "within",
            "sonnet",
            "where",
            "divided",
            "support”",
            "“not",
            "mistral8binstruct",
            "exact",
            "choice",
            "matches",
            "bolded",
            "measured",
            "correct",
            "model",
            "mnmn",
            "mnen",
            "vnvi",
            "result",
            "flash",
            "country",
            "model’s",
            "number",
            "performance",
            "accuracy",
            "kren",
            "baichuanm232b",
            "items",
            "among",
            "different",
            "sgzh",
            "sgms",
            "languages",
            "sgen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
            "<p class=\"ltx_p\">The exact data corresponding to the bar chart in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "correct",
                    "within",
                    "result",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "languages",
                    "where",
                    "gpt4o",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "among",
                    "within",
                    "modality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "among",
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "textonly",
                    "languages",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "items",
                    "textonly",
                    "correct",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "textonly",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "country",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "performance",
                    "matches",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "modality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "among",
                    "languages",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "textonly",
                    "within",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "matches",
                    "option",
                    "correct",
                    "textonly",
                    "model’s",
                    "claude",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "within",
                    "languages",
                    "model’s",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "textonly",
                    "languages",
                    "claude",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "model",
                    "textonly",
                    "within",
                    "different",
                    "languages",
                    "same",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "textonly",
                    "where",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "sonnet",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obviously, the transfer and generalization of cultural knowledge pose a significant challenge for language models, irrespective of their scale. Claude corrects 35.3% of initial errors through stepwise decomposition-reintegration evaluation, showing broad knowledge coverage but limited cultural generalization. Qwen&#8217;s performance is constrained by knowledge gaps and poor transferability. While 29.2% of multi-step problems have all sub-questions answered correctly, 10.7% fail at reintegration, indicating weaker cultural generalization capabilities.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "accuracy",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "different",
                    "within",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages",
                    "modality",
                    "country"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "correct",
                    "option",
                    "textonly",
                    "country",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "textonly",
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "model",
                    "claude",
                    "sonnet",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "within",
                    "among",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "items",
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "sonnet",
                    "model",
                    "gpt4o"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 10: VQA modality performance with exact numbers, measured by Accuracy (%): the number of items where the model’s choice exactly matches the correct option, divided by 500. “-” means “not support”. The better-performing result among different languages within the same country is bolded.",
        "body": "Model\nKR-en\nMN-mn\nMN-en\nSG-zh\nSG-en\nSG-ms\nSG-ta\nVN-vi\nVN-en\n\n\n\n\nGemini 2.5 Flash\n72.8\n41.2\n49.2\n53.0\n62.2\n59.8\n60.8\n76.6\n65.4\n\n\nClaude Sonnet 4\n74.4\n35.0\n49.0\n34.6\n57.2\n55.2\n49.8\n73.2\n67.4\n\n\nGPT-4o\n65.8\n41.4\n53.0\n30.2\n70.6\n68.4\n62.4\n75.8\n63.8\n\n\nQwen2.5-VL-32B\n67.0\n20.6\n45.2\n40.8\n52.8\n45.6\n22.6\n65.0\n64.6\n\n\nLlama-3.2-11B-Vision\n47.8\n5.8\n21.6\n32.2\n39.4\n35.0\n15.0\n48.6\n49.6\n\n\nKimi-VL-A3B-instruct\n56.0\n-\n-\n38.6\n43.6\n-\n-\n-\n50.0\n\n\nQwen2.5-Omni-7B\n49.2\n3.2\n36.6\n30.0\n43.4\n38.6\n14.2\n59.8\n58.2\n\n\nDeepseek-VL-Small\n30.0\n-\n-\n16.4\n27.2\n-\n-\n-\n41.6\n\n\nGLM-4.1V-9B-Thinking\n65.4\n28.0\n39.4\n44.8\n52.8\n41.8\n34.8\n59.4\n59.8\n\n\nInternVL-Chat-V1-5\n56.6\n-\n-\n28.0\n44.8\n-\n-\n-\n54.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">KR-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-mn</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ms</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">72.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">62.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">76.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Claude Sonnet 4</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">74.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">57.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">73.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">67.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">65.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">53.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">70.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">75.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">63.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Qwen2.5-VL-32B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">67.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">45.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">64.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Llama-3.2-11B-Vision</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">47.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">21.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">39.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">48.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Kimi-VL-A3B-instruct</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Qwen2.5-Omni-7B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">36.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">14.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">59.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Deepseek-VL-Small</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">41.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GLM-4.1V-9B-Thinking</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">39.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">44.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">59.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">InternVL-Chat-V1-5</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">44.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">54.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "betterperforming",
            "option",
            "deepseekvlsmall",
            "vnen",
            "exactly",
            "modality",
            "vqa",
            "gpt4o",
            "same",
            "means",
            "sgta",
            "numbers",
            "gemini",
            "claude",
            "internvlchatv15",
            "within",
            "sonnet",
            "where",
            "glm41v9bthinking",
            "divided",
            "support”",
            "“not",
            "exact",
            "choice",
            "matches",
            "bolded",
            "measured",
            "correct",
            "model",
            "mnmn",
            "mnen",
            "vnvi",
            "kimivla3binstruct",
            "result",
            "flash",
            "country",
            "model’s",
            "qwen25vl32b",
            "number",
            "performance",
            "accuracy",
            "kren",
            "llama3211bvision",
            "items",
            "among",
            "qwen25omni7b",
            "different",
            "sgzh",
            "sgms",
            "languages",
            "sgen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
            "<p class=\"ltx_p\">The exact data corresponding to the bar chart in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "correct",
                    "within",
                    "result",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "where",
                    "gpt4o",
                    "languages",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual VQA datasets use community-sourced images and questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Nayak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib23\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>)</cite>, revealing vision-language gaps and language sensitivity. Yet they typically lack text/speech-parallel versions of identical items, making it hard to isolate whether failures stem from cultural knowledge, visual grounding, or language handling.</p>\n\n",
                "matched_terms": [
                    "items",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "among",
                    "within",
                    "modality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "among",
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "languages",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "items",
                    "vqa",
                    "correct",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "country",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "performance",
                    "matches",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "modality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "among",
                    "languages",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "within",
                    "vqa",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "matches",
                    "option",
                    "correct",
                    "vqa",
                    "model’s",
                    "claude",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "within",
                    "languages",
                    "model’s",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "vqa",
                    "languages",
                    "claude",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "model",
                    "within",
                    "different",
                    "vqa",
                    "languages",
                    "same",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "model",
                    "vqa",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "within",
                    "model",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "where",
                    "model’s",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal models demonstrate significant inconsistency in cultural awareness across modalities, indicating flawed cultural knowledge transfer. In VQA, this deficiency stems from two core issues: &#8220;<span class=\"ltx_text ltx_font_italic\">selective attention pitfall</span>&#8221; where models over-focus on text-prompted subjects while missing key visual cues, and visual token-induced &#8220;<span class=\"ltx_text ltx_font_italic\">reasoning hallucinations</span>&#8221;.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "sonnet",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obviously, the transfer and generalization of cultural knowledge pose a significant challenge for language models, irrespective of their scale. Claude corrects 35.3% of initial errors through stepwise decomposition-reintegration evaluation, showing broad knowledge coverage but limited cultural generalization. Qwen&#8217;s performance is constrained by knowledge gaps and poor transferability. While 29.2% of multi-step problems have all sub-questions answered correctly, 10.7% fail at reintegration, indicating weaker cultural generalization capabilities.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "accuracy",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "different",
                    "within",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages",
                    "modality",
                    "country"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "same",
                    "country",
                    "correct",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "qwen25omni7b",
                    "kimivla3binstruct",
                    "deepseekvlsmall",
                    "internvlchatv15",
                    "glm41v9bthinking",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "model",
                    "claude",
                    "sonnet",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "among",
                    "within",
                    "vqa",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "items",
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "sonnet",
                    "model",
                    "gpt4o"
                ]
            }
        ]
    },
    "A1.T11": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 11: Rephrase VQA (Text-Only) modality performance with exact numbers, measured by Accuracy (%): the number of items where the model’s choice exactly matches the correct option, divided by 500. “-” means “not support”. The better-performing result among different languages within the same country is bolded.",
        "body": "Model\nKR-en\nMN-mn\nMN-en\nSG-zh\nSG-en\nSG-ms\nSG-ta\nVN-vi\nVN-en\n\n\n\n\nGemini 2.5 Flash\n76.4\n47.8\n49.2\n54.6\n63.6\n59.6\n52.4\n76.0\n71.6\n\n\nClaude Sonnet 4\n77.0\n38.8\n56.0\n46.0\n65.2\n61.0\n51.8\n76.0\n73.6\n\n\nGPT-4o\n75.2\n43.2\n59.8\n30.2\n68.2\n66.4\n53.6\n79.4\n72.8\n\n\nQwen2.5-VL-32B\n72.0\n19.0\n56.2\n36.4\n57.6\n50.8\n25.4\n69.2\n70.0\n\n\nLlama-3.2-11B-Vision\n54.6\n10.2\n29.0\n25.4\n45.2\n40.2\n20.0\n39.4\n51.2\n\n\nKimi-VL-A3B-Instruct\n53.8\n-\n-\n40.4\n36.6\n-\n-\n-\n40.0\n\n\nQwen2.5-Omni-7B\n60.6\n2.6\n43.4\n21.4\n43.6\n29.4\n9.8\n58.4\n60.0\n\n\nDeepseek-VL-Small\n58.8\n-\n-\n25.2\n39.2\n-\n-\n-\n58.4\n\n\nGLM-4.1V-9B-Thinking\n67.0\n26.6\n43.8\n42.0\n51.0\n40.4\n34.8\n66.2\n62.8\n\n\nInternVL-Chat-V1.5\n48.6\n-\n-\n36.0\n41.6\n-\n-\n-\n46.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">KR-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-mn</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">MN-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ms</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">47.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">54.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">52.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">76.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">71.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Claude Sonnet 4</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">77.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">38.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">46.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">51.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">76.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">73.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">75.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">59.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">68.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">66.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">53.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">79.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Qwen2.5-VL-32B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">72.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">57.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">70.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Llama-3.2-11B-Vision</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">54.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">29.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">45.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">39.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Kimi-VL-A3B-Instruct</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">53.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">40.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">40.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Qwen2.5-Omni-7B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">60.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">60.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Deepseek-VL-Small</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">58.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">39.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">58.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GLM-4.1V-9B-Thinking</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">67.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">43.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">40.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">InternVL-Chat-V1.5</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">41.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">46.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "betterperforming",
            "option",
            "deepseekvlsmall",
            "textonly",
            "vnen",
            "exactly",
            "modality",
            "vqa",
            "gpt4o",
            "same",
            "means",
            "sgta",
            "numbers",
            "gemini",
            "claude",
            "internvlchatv15",
            "within",
            "sonnet",
            "where",
            "glm41v9bthinking",
            "divided",
            "support”",
            "“not",
            "exact",
            "choice",
            "matches",
            "bolded",
            "measured",
            "correct",
            "model",
            "mnmn",
            "rephrase",
            "mnen",
            "kimivla3binstruct",
            "result",
            "flash",
            "country",
            "model’s",
            "qwen25vl32b",
            "number",
            "performance",
            "accuracy",
            "kren",
            "llama3211bvision",
            "items",
            "among",
            "qwen25omni7b",
            "different",
            "sgzh",
            "vnvi",
            "sgms",
            "languages",
            "sgen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
            "<p class=\"ltx_p\">The exact data corresponding to the bar chart in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "correct",
                    "within",
                    "result",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "languages",
                    "where",
                    "gpt4o",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual VQA datasets use community-sourced images and questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Nayak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib23\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>)</cite>, revealing vision-language gaps and language sensitivity. Yet they typically lack text/speech-parallel versions of identical items, making it hard to isolate whether failures stem from cultural knowledge, visual grounding, or language handling.</p>\n\n",
                "matched_terms": [
                    "items",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "among",
                    "within",
                    "modality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "among",
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "textonly",
                    "languages",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "items",
                    "correct",
                    "textonly",
                    "where",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "textonly",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "country",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "performance",
                    "matches",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "modality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "among",
                    "languages",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "textonly",
                    "within",
                    "vqa",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "matches",
                    "option",
                    "correct",
                    "textonly",
                    "vqa",
                    "model’s",
                    "claude",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "within",
                    "languages",
                    "model’s",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "textonly",
                    "vqa",
                    "languages",
                    "claude",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "model",
                    "textonly",
                    "within",
                    "different",
                    "vqa",
                    "languages",
                    "same",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "model",
                    "vqa",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "within",
                    "model",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "textonly",
                    "rephrase",
                    "where",
                    "vqa",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal models demonstrate significant inconsistency in cultural awareness across modalities, indicating flawed cultural knowledge transfer. In VQA, this deficiency stems from two core issues: &#8220;<span class=\"ltx_text ltx_font_italic\">selective attention pitfall</span>&#8221; where models over-focus on text-prompted subjects while missing key visual cues, and visual token-induced &#8220;<span class=\"ltx_text ltx_font_italic\">reasoning hallucinations</span>&#8221;.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "sonnet",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Successful Correction:</span> 35.3% (Claude), 18.5% (Qwen) of problems were resolved, with all sub-questions and the final contextualized answer being correct.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obviously, the transfer and generalization of cultural knowledge pose a significant challenge for language models, irrespective of their scale. Claude corrects 35.3% of initial errors through stepwise decomposition-reintegration evaluation, showing broad knowledge coverage but limited cultural generalization. Qwen&#8217;s performance is constrained by knowledge gaps and poor transferability. While 29.2% of multi-step problems have all sub-questions answered correctly, 10.7% fail at reintegration, indicating weaker cultural generalization capabilities.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "accuracy",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "different",
                    "within",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "languages",
                    "modality",
                    "country"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "correct",
                    "option",
                    "textonly",
                    "country",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Only the question stem from the <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item;</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "rephrase",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "rephrase",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "qwen25omni7b",
                    "kimivla3binstruct",
                    "deepseekvlsmall",
                    "internvlchatv15",
                    "glm41v9bthinking",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "vqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "textonly",
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "model",
                    "claude",
                    "sonnet",
                    "flash",
                    "model’s",
                    "gpt4o",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "among",
                    "textonly",
                    "within",
                    "rephrase",
                    "vqa",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "items",
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, LLMs are used for three purposes: (1) we use GPT-4o and Claude Sonnet 4 to translate created datasets to the English version; (2) we use GPT-4o to evaluate whether model responses match the reference answers in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS6\" title=\"4.6 Cultural Knowledge Generalization &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.6</span></a>; and (3) we use GPT-4o to polish the manuscript.</p>\n\n",
                "matched_terms": [
                    "claude",
                    "sonnet",
                    "model",
                    "gpt4o"
                ]
            }
        ]
    },
    "A1.T12": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 12: Speech modality (speech question & text options) performance with exact numbers, measured by Accuracy (%): the number of items where the model’s choice exactly matches the correct option, divided by 500. “-” means “not support”. The better-performing result among different languages within the same country is bolded.",
        "body": "Model\nSG-zh\nSG-en\nSG-en\nSG-ms\nSG-ta\nVN-vi\nVN-en\nVN-en\n\n\n\n-\nAcc\nNoAcc\n-\n-\n-\nAcc\nNoAcc\n\n\n\n\nQwen2.5-Omni-7B\n39.4\n39.4\n37.0\n-\n-\n-\n54.6\n52.0\n\n\nGemini 2.5 Flash\n47.0\n55.2\n54.4\n41.4\n39.6\n60.6\n68.6\n67.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ms</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NoAcc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NoAcc</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Qwen2.5-Omni-7B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">39.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">39.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">54.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">52.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">47.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">55.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">54.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">41.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">39.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">60.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">68.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">67.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "betterperforming",
            "option",
            "text",
            "vnen",
            "exactly",
            "modality",
            "same",
            "noacc",
            "means",
            "sgta",
            "numbers",
            "speech",
            "gemini",
            "within",
            "where",
            "divided",
            "support”",
            "“not",
            "acc",
            "exact",
            "choice",
            "matches",
            "bolded",
            "measured",
            "correct",
            "model",
            "vnvi",
            "result",
            "flash",
            "country",
            "model’s",
            "question",
            "number",
            "performance",
            "accuracy",
            "items",
            "among",
            "qwen25omni7b",
            "different",
            "sgzh",
            "options",
            "sgms",
            "languages",
            "sgen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
            "<p class=\"ltx_p\">The exact data corresponding to the bar chart in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "correct",
                    "text",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "correct",
                    "text",
                    "within",
                    "result",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "where",
                    "languages",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "items",
                    "among",
                    "text",
                    "within",
                    "modality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "languages",
                    "country",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "among",
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "question",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "items",
                    "correct",
                    "text",
                    "where",
                    "options",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "model’s",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "options",
                    "country",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "matches",
                    "question",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "modality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "among",
                    "languages",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "text",
                    "within",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "matches",
                    "correct",
                    "option",
                    "model’s",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "within",
                    "where",
                    "model’s",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "number",
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "matches",
                    "model",
                    "within",
                    "different",
                    "languages",
                    "same",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s",
                    "correct",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "languages",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "within",
                    "different",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "modality",
                    "country",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "correct",
                    "option",
                    "country",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before generating the English audio, we standardized the input text through normalization of numbers and symbols, handling of abbreviations and special terms, and sentence segmentation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "numbers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "text",
                    "options"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "qwen25omni7b",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "model",
                    "flash",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "options",
                    "question",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "text",
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the question in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the completed responses under different generation modes are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T14\" title=\"Table 14 &#8227; A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "model",
                    "question",
                    "flash",
                    "model’s",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "among",
                    "within",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "text",
                    "items",
                    "model",
                    "model’s"
                ]
            }
        ]
    },
    "A1.T13": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 13: Speech modality (speech question & options) performance with exact numbers, measured by Accuracy (%): the number of items where the model’s choice exactly matches the correct option, divided by 500. “-” means “not support”. The better-performing result among different languages within the same country is bolded.",
        "body": "Model\nSG-zh\nSG-en\nSG-en\nSG-ms\nSG-ta\nVN-vi\nVN-en\nVN-en\n\n\n\n-\nAcc\nNoAcc\n-\n-\n-\nAcc\nNoAcc\n\n\n\n\nQwen2.5-Omni-7B\n27.0\n26.0\n23.6\n-\n-\n-\n39.2\n37.0\n\n\nGemini 2.5 Flash\n28.8\n42.2\n37.6\n16.0\n15.8\n52.0\n46.2\n45.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ms</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">SG-ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VN-en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NoAcc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NoAcc</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Qwen2.5-Omni-7B</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">26.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">23.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">39.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">28.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">42.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">46.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">45.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "betterperforming",
            "option",
            "vnen",
            "exactly",
            "modality",
            "same",
            "noacc",
            "means",
            "sgta",
            "numbers",
            "speech",
            "gemini",
            "within",
            "where",
            "divided",
            "support”",
            "“not",
            "acc",
            "exact",
            "choice",
            "matches",
            "bolded",
            "measured",
            "correct",
            "model",
            "vnvi",
            "result",
            "flash",
            "country",
            "model’s",
            "question",
            "number",
            "performance",
            "accuracy",
            "items",
            "among",
            "qwen25omni7b",
            "different",
            "sgzh",
            "options",
            "sgms",
            "languages",
            "sgen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "correct",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language and vision&#8211;language models are being increasingly deployed across various cultures and languages. Yet, their behavior remains uneven; performance is strongest in high-resource, Western contexts and degrades in non-Western settings, particularly across Asia <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib7\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Vayani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib37\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>)</cite>. As multimodal, multilingual models proliferate <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib5\" title=\"\">2024a</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib2\" title=\"\">2023</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib18\" title=\"\">2023</a>; OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, evaluating whether they hold <em class=\"ltx_emph ltx_font_italic\">consistent</em> cultural interpretations across languages and modalities is both important and underexplored.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "model",
                    "correct",
                    "within",
                    "result",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "where",
                    "languages",
                    "performance",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "among",
                    "items",
                    "within",
                    "modality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MMA-ASIA was collaboratively constructed by research teams from eight countries: China, Singapore, Japan, South Korea, Mongolia, Vietnam, Indonesia, and India. The pipeline comprised five stages: (i) annotator selection, (ii) selection of representative cultural themes and languages, (iii) collection of text and image materials, (iv) question authoring and annotation by country sub-teams, and (v) human review and revision for quality and cultural representativeness. For the definition of cultural themes, we followed the framework proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Adilazuarda et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib1\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "among",
                    "number",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "question",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "items",
                    "correct",
                    "where",
                    "options",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "model’s",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each country team completed a draft, in-country linguists conducted quality reviews. The review covered: ambiguity in wording, accuracy of English translations, clarity and fluency of the speech data, completeness of knowledge points, and appropriateness of answer options. Teams revised their subsets based on this feedback, yielding the final high-quality release.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "options",
                    "country",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate existing LLMs on the MMA-ASIA benchmark. Unless stated otherwise, all runs are <em class=\"ltx_emph ltx_font_italic\">zero-shot</em> with a unified prompt template whose language matches the question language (prompts and experimental settings are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS6\" title=\"A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>). We report results along five dimensions and, for each, analyze the factors that drive performance. We access three closed-source models: GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib24\" title=\"\">2024</a>)</cite>, Claude-Sonnet-4, and Gemini&#160;2.5&#160;Pro&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib31\" title=\"\">2025a</a>)</cite>, and eleven open-source multilingual or multimodal models, including the Qwen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>; Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib34\" title=\"\">2025b</a>)</cite>, LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, and GLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(GLM et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib14\" title=\"\">2024</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite> families. Models and tasks are detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS5\" title=\"A.5 Model Selection &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>. For models without multilingual support, we report English-only scores for comparability. For speech evaluation, we include only models that accept <em class=\"ltx_emph ltx_font_italic\">speech tokens</em> directly; models that require intermediate automatic speech recognition (ASR) are excluded.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "matches",
                    "question",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes accuracy on MMA-ASIA. Nearly all state-of-the-art models score below 80% and most below 50%, highlighting the benchmark&#8217;s difficulty. Closed-source models outperform open-source models; even the strongest open-source family (Qwen) trails the closed-source average by more than 10 percentage points. Performance varies with (i) the resource level of the language and (ii) the evaluation modality. The following subsections analyze: (1) cultural-awareness disparities across countries and languages, (2) cross-lingual and cross-modal consistency, (3) cultural-awareness grounding, and (4) cultural knowledge generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "languages",
                    "modality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate country effects, we compare each model&#8217;s scores across countries within a fixed modality and, for each country, retain the model&#8217;s <em class=\"ltx_emph ltx_font_italic\">best</em> score over its available languages (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T9\" title=\"Table 9 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T10\" title=\"Table 10 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T11\" title=\"Table 11 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). For example, in the text-only setting, GPT-4o scores 71.4% in Korean and 68.8% in English; we use the precision in English to represent the awareness of Korean culture in GPT-4o, to avoid confounding cultural competence with language proficiency. Models show higher awareness for Korean culture on average (63.98% across 4 modalities), plausibly due to global diffusion and richer data availability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>); Dal&#160;Yong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Remarkably, Vietnamese culture (62.96%) is on par with Korean, likely reflecting Vietnam&#8217;s high social-media penetration (79.8% of the population) and thus large volumes of user-generated content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DataReportal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib10\" title=\"\">2025</a>)</cite>. In contrast, China and India exhibit larger gaps, consistent with multilayered cultural forms and greater regional heterogeneity. Mongolia trails further, consistent with low-resource language settings and sparser training corpora.</p>\n\n",
                "matched_terms": [
                    "within",
                    "modality",
                    "model’s",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">English prompts often outperform low-resource languages, reflecting the breadth of English corpora and limited cross-lingual transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib16\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib45\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib44\" title=\"\">a</a>)</cite>. This advantage diminishes or reverses for medium/high-resource local languages (e.g., Chinese, Japanese), where culture-specific terms and proper names are well represented locally but rare in English corpora, hurting retrieval and grounding. For example, &#8220;&#20044;&#25252; (Wuhu),&#8221; a lineage among Uyghur ancestral groups, lacks a standard English equivalent; transliteration is rare and ambiguous in English data. Thus, when the model is competent in the relevant local language, using that language can yield better cultural grounding than English.</p>\n\n",
                "matched_terms": [
                    "among",
                    "languages",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a consistent ordering: <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em>. Data availability follows the same order (text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> image&#8211;text <math alttext=\"\\gg\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mo>&#8811;</mo><annotation encoding=\"application/x-tex\">\\gg</annotation></semantics></math> raw speech). Speech adds uncertainty (noise, homophony), and many architectures encode modalities separately and then fuse downstream, introducing alignment/compression losses that widen gaps. Interestingly, in Speech, Qwen and Gemini outperform their standard English baselines in 6 and 5 country-specific cultural settings, respectively (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Accents appear to serve as a prior cue for specific cultures, enhancing the models&#8217; accuracy on corresponding tasks. We attribute this to the co-occurrence of accents and their related cultural content within the data (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS10\" title=\"A.10 Analysis of Speech as a Cultural Prior &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.10</span></a> for detailed analysis).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "within",
                    "same",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the foregoing analysis, it is evident that LLMs in Asian cultural contexts also display cultural and modality biases shaped by data distributions; furthermore, given the limited effectiveness of cross-lingual cultural knowledge transfer, English cannot be assumed to perform reliably better on culture-related tasks. In contrast, in the speech modality, accents, often treated as noise, paradoxically serve as effective cultural cues that activate relevant context and improve performance. In speech, accents, typically considered noise, actually serve as effective cultural cues that activate relevant context and improve performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCQs are convenient but can be solved via shortcuts (e.g., option elimination) rather than grounded knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Myung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>; Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib27\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. We adopt two measures: (i) <span class=\"ltx_text ltx_font_bold\">Retained background knowledge:</span> Each item accompanied by itsemphknowledge points (supporting evidence). (ii) <span class=\"ltx_text ltx_font_bold\">Explainable responses:</span> During testing, models must provide a textual rationale for their choice.</p>\n\n",
                "matched_terms": [
                    "choice",
                    "option"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "matches",
                    "correct",
                    "option",
                    "model’s",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "within",
                    "where",
                    "model’s",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf1\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a>, contemporary multilingual LLMs exhibit weak cross-lingual consistency on culturally grounded tasks in both text-only and VQA settings. The weakness is most evident for language pairs with large resource gaps. For Mongolian culture, the disparity between Mongolian and English yields only 65.2% consistency for Claude, while all open-source models remain below 50%. By contrast, Korean culture shows higher consistency, plausibly reflecting the global diffusion of contemporary Korean media and the resulting multilingual exposure to related knowledge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib17\" title=\"\">2024</a>; Dal&#160;Yong, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib9\" title=\"\">2018</a>)</cite>. Consistency also declines sharply as the number of evaluated languages increases. For Singapore-related items, when Chinese, English, Tamil, and Malay are assessed jointly, the maximum consistency does not exceed 45% (Gemini on VQA), despite relatively high pairwise values of 60.60% (EN&#8211;TA), 64.20% (EN&#8211;MS), and 55.20% (EN&#8211;ZH). In some culturally challenging cases, visual context can partially bridge languages: for Indian culture, GLM-4.1 achieves 44.20% cross-lingual consistency on Hindi VQA, which is 13.2 points higher than its rephrased text-only counterpart, although both remain low.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "number",
                    "items",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "matches",
                    "model",
                    "within",
                    "different",
                    "languages",
                    "same",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We specifically isolate instances where the model succeeded with pure text input but failed in the VQA context. Apart from the most common errors arising from a lack of understanding of culture-related image contexts, our analysis reveals two additional predominant categories of errors:</p>\n\n",
                "matched_terms": [
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The pitfall of prompt-guided selective attention.</span> Models often tend to focus predominantly on explicitly mentioned objects in prompts, whereas cultural VQA requires a more nuanced ability to identify culture-specific visual cues within images. To validate whether the models&#8217; focuses are truly culture-specific, we extract visual evidence using answer-conditioned multi-layer Grad-CAM <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib29\" title=\"\">2019</a>)</cite>, interpolating and mapping the resulting heatmaps back to the original image for visualization. Specifically, given an image <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, a textual prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an answer <math alttext=\"y_{a:b}=(y_{a},\\dots,y_{b})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>a</mi></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">y_{a:b}=(y_{a},\\dots,y_{b})</annotation></semantics></math> autoregressively produced by a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m4\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, we define an answer-conditioned objective on the log-likelihood of the answer tokens as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E2\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Here we use the token-sum objective. For memory efficiency, only the visual tower is set to require gradients. We denote the forward activation of the <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>-th visual block as <math alttext=\"A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>C</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A(\\ell)\\in\\mathbb{R}^{C\\times H\\times W}</annotation></semantics></math>, the gradient of the block is <math alttext=\"\\mathbf{G}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m7\" intent=\":literal\"><semantics><msup><mi>&#119814;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{G}^{(\\ell)}</annotation></semantics></math>. A per-layer Grad-CAM is built via channelwise inner product followed by <math alttext=\"\\operatorname{ReLU}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m8\" intent=\":literal\"><semantics><mi>ReLU</mi><annotation encoding=\"application/x-tex\">\\operatorname{ReLU}</annotation></semantics></math> (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E3\" title=\"In 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s",
                    "correct",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speech",
                    "accuracy",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our annotation team comprises members from eight different countries. All team members are native speakers of their local languages and proficient in English, with professional backgrounds in natural language processing or speech processing. Each annotator has lived in the respective country for more than ten years. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T2\" title=\"Table 2 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the annotators&#8217; details and professional backgrounds; to protect privacy, we replace personal names with numeric identifiers within each country. After each team completes the first round of annotations, in-country linguistic experts conduct a data review. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T3\" title=\"Table 3 &#8227; A.1 Annotator Demographic &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> lists the language experts&#8217; information; likewise, we anonymize personal names.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "within",
                    "different",
                    "country",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "items",
                    "modality",
                    "country",
                    "languages",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language/Ethnicity</span>:\nCode-switching (where applicable); official and commonly used languages; dialects and accent features; writing systems and naming conventions; multi-ethnic compositions and cultural practices. Also includes politeness strategies in language and norms of cross-group communication.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The content of the guideline distributed to annotators is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F11\" title=\"Figure 11 &#8227; A.3 Annotation Guideline &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. To minimize heuristic cues arising from non-cultural knowledge, we add a consistency constraint on distractors: they must belong to the same category as the correct option and closely resemble it in observable attributes and semantic representation. We also encourage each team to uncover cultural elements unique to their own country, rather than focusing only on widely known aspects. For the Language category, if code-switching is prevalent in the annotators&#8217; country, we strongly encourage including such language-assessment examples in the Text-Only portion of the dataset. All content involving racism or hate speech is prohibited from inclusion in our dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "correct",
                    "option",
                    "country",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "options"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate nine vision&#8211;language (and omni) models on image and text: Qwen2.5-VL-32B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib3\" title=\"\">2025</a>)</cite>, Llama-3.2-11B-Vision-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib36\" title=\"\">2023</a>)</cite>, Kimi-VL-A3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib32\" title=\"\">2025a</a>)</cite>, DeepSeek-VL-Small&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib40\" title=\"\">2024</a>)</cite>, GLM-4.1V-9B-Thinking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib35\" title=\"\">2025c</a>)</cite>, InternVL-Chat-V1-5&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib6\" title=\"\">2024b</a>)</cite>, and Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib41\" title=\"\">2025</a>)</cite>.\nKimi-VL-A3B-Instruct, DeepSeek-VL-Small, and InternVL-Chat-V1-5 are evaluated in English only; Qwen2.5-VL-32B-Instruct, Llama-3.2-11B-Vision-Instruct, and GLM-4.1V-9B-Thinking are evaluated in multiple languages.\nQwen2.5-Omni-7B is evaluated across image, text, and speech in a multilingual setting.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qwen25omni7b",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All evaluations in this work are conducted in the zero-shot setting, using single-turn inference for each model on an NVIDIA H100 80G. For image inputs that exceed a model&#8217;s maximum allowable resolution, we proportionally downscale the image until it is under 1m&#771;egapixel before testing. We decode with greedy search (no sampling; <span class=\"ltx_text ltx_font_typewriter\">do_sample=false</span>, <span class=\"ltx_text ltx_font_typewriter\">num_beams=1</span>), so temperature/top-<em class=\"ltx_emph ltx_font_italic\">p</em>/top-<em class=\"ltx_emph ltx_font_italic\">k</em> are not used; the maximum output length is set to 2048 tokens to ensure reproducibility. GPT-4o and Gemini 2.5 Flash are accessed via OpenRouter API platform, and Claude via the Anthropic API. For all closed-source models, we set <span class=\"ltx_text ltx_font_typewriter\">temperature=0</span> to minimize randomness and improve reproducibility. Our speech inputs are no longer than 30 seconds and sampled at 16 kHz&#8212;well below Gemini 2.5 Flash&#8217;s maximum speech-input duration and Qwen2.5-Omni-7B&#8217;s maximum input token limit. So we do not perform any input-length processing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "model",
                    "flash",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "within",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the question in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the completed responses under different generation modes are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T14\" title=\"Table 14 &#8227; A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to images and text, speech input introduces greater uncertainty through environmental noise, homophony, and accents&#8212;with accents closely tied to cultural context. Our research reveals that accents function beyond mere noise. Testing synthetic speech in standard English versus multiple national accents, we found Qwen and Gemini outperformed their standard English baselines in 6 and 5 country-specific cultural settings (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F2\" title=\"Figure 2 &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), respectively. Notably, Qwen achieved 2.8% and 3.6% accuracy gains for Indonesian and Japanese accents (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T12\" title=\"Table 12 &#8227; A.7 Performance of LLMs on MMA-Asia across modalities &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>). We attribute this to systematic co-occurrence of accented English with country-specific entities and contexts in training corpora, enabling accents to serve as cultural and lexical priors during inference. Our findings demonstrate that accents can function as valuable cultural cues rather than simply noise sources for model exploitation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gemini",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "items",
                    "model",
                    "question",
                    "flash",
                    "model’s",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results for LLMs&#8217; Rationale Unfaithfulness Rates (RUR) across Rephrase VQA and Speech are shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F13\" title=\"Figure 13 &#8227; A.12 Rationale Unfaithfulness Rates Across Rephrase VQA and Spoken QA &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe patterns consistent with the Text-Only and VQA modalities: closed-source models generally have lower RURs than open-source models, though they still fall within the 5%&#8211;20% range. Among open-source models, Llama shows a markedly higher RUR on non-Spanish languages than on Spanish, which we attribute to linguistic bias stemming from the disproportionately large share of Spanish in Llama&#8217;s training data relative to other languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "among",
                    "within",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F14\" title=\"Figure 14 &#8227; A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> shows attention heatmaps over image regions when the model produces incorrect answers. In Case 2, the model concentrates on the subject&#8217;s clothing and surrounding scene but overlooks the footwear beside the person and the key text in the upper-left corner that identifies the subject. Similarly, in Figure 3 the model focuses excessively on attire and playing posture while ignoring the crucial detail that the instrument&#8217;s body is wrapped in snakeskin. All of these support our finding: the model&#8217;s excessive focus on items mentioned in the prompt causes it to overlook critical cultural details.</p>\n\n",
                "matched_terms": [
                    "items",
                    "model",
                    "model’s"
                ]
            }
        ]
    },
    "A1.T14": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 14: Model outputs across different modes for the same question.",
        "body": "Vision-ablated\n\n\nPrefix Replay",
        "html_code": "<table class=\"ltx_tabular ltx_align_top\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Vision-ablated</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prefix Replay</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "prefix",
            "model",
            "across",
            "replay",
            "different",
            "outputs",
            "modes",
            "question",
            "same",
            "visionablated"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the question in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the completed responses under different generation modes are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T14\" title=\"Table 14 &#8227; A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "prefix",
                    "model",
                    "across",
                    "replay",
                    "question",
                    "visionablated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extensive baselines and analyses.</span> We report zero-shot baselines for 14 model families (multilingual LLMs and VLMs), including common-support subsets, and diagnose failure modes by modality, language, and reasoning step count.</p>\n\n",
                "matched_terms": [
                    "modes",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing datasets evaluate cultural understanding within a single modality or language at a time, without tightly <em class=\"ltx_emph ltx_font_italic\">aligning</em> instances across modalities and languages, and no built-in <em class=\"ltx_emph ltx_font_italic\">grounding controls</em>. MMA-ASIA addresses these gaps by: (i) providing semantically aligned tri-modal items (text, image+question, speech) in parallel local-language and English versions; (ii) adopting <em class=\"ltx_emph ltx_font_italic\">cross-modal</em> and <em class=\"ltx_emph ltx_font_italic\">cross-lingual</em> consistency as prime metrics; and (iii) integrating targeted ablations and negative controls to test whether answers rely on the intended cultural signal rather than shortcuts. This design enables clearer attribution of failure modes: knowledge vs. language vs. modality, and more reliable measurement of cultural awareness in multimodal, multilingual models. We summarize key differences among representative datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "modes",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "question",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "outputs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal cultural-awareness consistency evaluates whether a model gives the same output for semantically equivalent queries presented in different modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F4.sf2\" title=\"In Figure 4 &#8227; 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> shows that, across the eight Asian countries, the pattern largely matches the cross-lingual case: averaged over models, cross-lingual consistency is 48%, and cross-modal consistency rarely exceeds 67%. This gap indicates asymmetric transfer of cultural knowledge across modalities. Under low-resource language settings, almost all models struggle to maintain stable cross-modal answers. Within the same national context, medium- to high-resource local languages typically yield higher cross-modal consistency than English. To examine the observed ordering <em class=\"ltx_emph ltx_font_italic\">text-only</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">VQA</em> <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> <em class=\"ltx_emph ltx_font_italic\">spoken QA</em> (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS2\" title=\"4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we conduct a detailed error analysis with Qwen2.5-VL-32B-Instruct. Because spoken QA adds additional complexities (noise, accents, intonation), our analysis in this section focuses on VQA versus text-only performance; we leave a fuller study of speech to future work.</p>\n\n",
                "matched_terms": [
                    "different",
                    "same",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "replay",
                    "prefix",
                    "model",
                    "visionablated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "modes",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transportation</span>:\nTransportation across historical periods and the evolution of vehicles, along with regional differences in modes of transport. May also cover landmark transit systems and commuting culture.</p>\n\n",
                "matched_terms": [
                    "modes",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality speech synthesis, we employed CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib12\" title=\"\">2025</a>)</cite> for English audio generation. This tool supports voice cloning from sampled speakers, producing speech that preserves the timbre and accent of the reference voice. We collected representative recordings from native speakers across eight countries to capture diverse accents for speech synthesis. For standard English, we adopted CosyVoice&#8217;s built-in default English voice (English female voice). For non-English languages, CosyVoice was also used to generate Chinese, Japanese, and Korean audio. In addition, we employed in-house high-quality TTS systems built by different speech processing teams for Vietnamese, Tamil, Mongolian, and Malay, while the Coqui-ai TTS toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coqui.ai, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib8\" title=\"\">2025</a>)</cite> was used for Indonesian and Hindi.</p>\n\n",
                "matched_terms": [
                    "different",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T6\" title=\"Table 6 &#8227; A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the models used for speech generation across different languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T7\" title=\"Table 7 &#8227; Prompt templates for evaluation tasks across modalities. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the English prompts used in our evaluations across different modalities. When the query switches to another language, the corresponding translated version of the prompt will be used to ensure input-language consistency. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T8\" title=\"Table 8 &#8227; Experiments setting. &#8227; A.6 Prompt Templates and Experimental Settings &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents the prompts used to invoke closed-source model APIs for translation, answer-consistency evaluation, and answer extraction.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "question",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that visual content increases\nreasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. To validate our hypothesis, we propose a &#8220;Vision-ablated Prefix Replay&#8221; (VPR) method. This method enables a model with parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to first describe the image contents based on image <math alttext=\"\\mathbf{x}^{\\text{img}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>img</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{img}}</annotation></semantics></math> and text prompt <math alttext=\"\\mathbf{x}^{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119857;</mi><mtext>text</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{x}^{\\text{text}}</annotation></semantics></math>. After this initial description, we structurally ablate the visual condition and fix the prefix <math alttext=\"\\hat{\\mathbf{S}}_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119826;</mi><mo>^</mo></mover><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{S}}_{1:n}</annotation></semantics></math> for subsequent reasoning generation. This evaluates the marginal contribution of visual conditions to reasoning. Specifically, assuming the model completes image description within the first <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> tokens, we remove visual conditions starting from the <math alttext=\"(n+1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n+1)</annotation></semantics></math>-th token and use only the text prompt and generated tokens as prefix. The joint probability distribution of the subsequent sequence <math alttext=\"\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS9.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mrow><mi/><mo>&gt;</mo><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{&gt;n}=(s_{n+1},\\dots,s_{T})</annotation></semantics></math> can be expressed as:\n    </p>\n\n",
                "matched_terms": [
                    "prefix",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "prefix",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "question",
                    "same",
                    "model"
                ]
            }
        ]
    },
    "A1.T15": {
        "source_file": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation",
        "caption": "Table 15: Sub-question decomposition of multi-step reasoning questions.",
        "body": "Multi-Step Question\n\n\n\n\nSub-Question\n\n\nAnswer for Sub-Question\n\n\n\n\n\n\nWhich of the following buildings belongs to the same ethnic characteristic architecture as the stilted building? A. Tulou B. Moxiaolou C. Diaofang D. Yaodong\n\n\n\n\nWhat ethnic group is associated with the stilted building?\n\n\nThe Tujia ethnic group.\n\n\n\n\nWhat ethnic group is associated with the Tulou?\n\n\nHakka\n\n\n\n\nWhat ethnic group is associated with the Moxiaolou?\n\n\nThe Tujia ethnic group.\n\n\n\n\n…\n\n\n…",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:119.5pt;\"><span class=\"ltx_text ltx_font_bold\">Multi-Step Question</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Sub-Question</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Answer for Sub-Question</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" rowspan=\"7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:119.5pt;\">Which of the following buildings belongs to the same ethnic characteristic architecture as the stilted building? A. Tulou B. Moxiaolou C. Diaofang D. Yaodong</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">What ethnic group is associated with the stilted building?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">The Tujia ethnic group.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">What ethnic group is associated with the Tulou?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Hakka</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">What ethnic group is associated with the Moxiaolou?</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">The Tujia ethnic group.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">&#8230;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#8230;</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reasoning",
            "answer",
            "belongs",
            "architecture",
            "questions",
            "same",
            "yaodong",
            "stilted",
            "what",
            "which",
            "subquestion",
            "hakka",
            "following",
            "associated",
            "group",
            "tujia",
            "building",
            "decomposition",
            "question",
            "ethnic",
            "buildings",
            "diaofang",
            "multistep",
            "characteristic",
            "tulou",
            "moxiaolou"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Balepur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>; Molfese et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite> suggests that scaling increases LLMs&#8217; factual memory but not genuine logical generalization. To distinguish whether cultural multi-step reasoning errors stem from knowledge gaps or generalization deficits, we conduct a deconstruction study. We decompose each question into atomic sub-questions testing single knowledge points, which models answer first (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> for an example). We then evaluate the original question under two conditions: (i) with in-context &#8220;sub-question &#8594; model answer&#8221; pairs, and (ii) from scratch. If models solve all subquestions but fail the original question from scratch, they possess the knowledge but cannot transfer it, indicating generalization deficits. If they err on sub-questions, failure likely reflects missing culture-specific knowledge.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T15\" title=\"Table 15 &#8227; A.14 Sub-question Decomposition &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> illustrates examples of deconstructing multi-step reasoning questions into single-fact verification subquestions. We enumerate all single-fact subquestions embedded in each multi-step item to assess the model&#8217;s generalization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs&#8217; cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79% require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures &#8211; (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects &#8220;shortcut learning&#8221; by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "multistep",
                    "building",
                    "questions",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate (i) cultural awareness consistency, defined as the extent to which a model gives stable answers to semantically equivalent inputs when the representation (text, image+question, or spoken question) or the language changes; (ii) cultural awareness grounding, defined as whether correct answers rely on appropriate cultural signals rather than exploitable shortcuts; and (iii) cultural awareness generalization, defined as whether a model that has access to the relevant cultural knowledge can perform the required reasoning within those cultural contexts <cite class=\"ltx_cite ltx_citemacro_cite\">Balepur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib4\" title=\"\">2024</a>); Molfese et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib21\" title=\"\">2025</a>); Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib43\" title=\"\">2023</a>)</cite>.\nNevertheless, conducting such evaluations presents significant challenges. Existing culture-centric datasets (e.g., <cite class=\"ltx_cite ltx_citemacro_cite\">Myung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib22\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>) frequently suffer from two key limitations: (i) insufficient alignment of instances across modalities, (ii) inadequate representation of low-resource Asian languages. Furthermore, evaluation processes are easily hacked through memorization or elimination in multiple choice questions (MCQs), which bypass the genuine reasoning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib39\" title=\"\">2025</a>; Hartmann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib15\" title=\"\">2023</a>)</cite>. As a result, we still lack a principled way to separate actual cultural competence from artifacts.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "which",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this research gap, we introduce <span class=\"ltx_text ltx_font_bold\">MMA-ASIA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">We will release the data, splits, prompts, decoding settings, and per-item metadata (e.g., knowledge points, reasoning tags) for benchmarking, reproducing, and future extensions.</span></span></span></span></span>, an explainable evaluation framework for Asian cultural knowledge. MMA-ASIA aligns tri-modal items (textual question, image+question, and Text-to-Speech (TTS)-spoken question) with identical semantics and provides parallel local-language and English versions authored by native experts across 8 countries and 10 languages to make a comprehensive evaluation. The framework measures five axes: (1) cultural awareness disparity, (2) cross-modal consistency, (3) cross-lingual consistency, (4) cultural knowledge generalization under held-out regimes, and (5) grounding validation via targeted ablations and negative controls.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using MMA-ASIA, we evaluate 15 multilingual and multimodal LLMs(e.g., GPT-4o, Qwen, Llama). We find that (i) accuracy drops markedly in low-resource Asian languages compared to English, (ii) cross-modal consistency lags text-only performance, indicating incomplete transfer from language to vision and speech, and (iii) grounding controls reduce a non-trivial fraction of apparent &#8220;wins,&#8221; revealing shortcut use. We also analyze multi-step, culture-specific reasoning errors and where visual or linguistic cues fail to connect.\nWe summarize our contributions as follows:</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through collaborative discussions, we finalized 9 cultural themes, <em class=\"ltx_emph ltx_font_italic\">Daily life habits/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical location and climate, Education, Fashion/Clothing and Language/Race</em>. We balanced the number of questions across themes as much as possible. Each national subset includes the country&#8217;s official language(s) and English. For India, we selected Hindi as the representative language due to its large speaker base among the 22 official languages. For Singapore, we included all four official languages: English, Chinese, Malay, and Tamil. In total, MMA-ASIA covers ten languages; full details are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "buildings",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Teams used collected materials to create multi-choice QA data, requiring at least 60% of questions to involve multi-step cultural reasoning. Multi-step reasoning questions require sequential derivation and/or synthesis from multiple independent knowledge components, not just single-fact recall or paraphrase (detailed examples in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS2\" title=\"A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>). Question templates are not fixed for preserving variety in question styles. Each national subset has two components: a <em class=\"ltx_emph ltx_font_italic\">multimodal</em> and a <em class=\"ltx_emph ltx_font_italic\">text-only</em> component. All QA data were authored in official local languages and translated to English using Claude 4 for Tamil and GPT-4o for other languages.\nAll translations underwent manual verification; mistranslations were corrected, and for terms lacking standard English equivalents, we applied phonetic transliteration or adopted the locally prevalent rendering.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "multistep",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Component.</span>\nAnnotators created VQA items where the correct answer requires visual understanding. Each VQA question was rephrased into a semantically-equivalent text-only MCQ. The answer options and the correct answer were kept unchanged. We provided both the original and rephrased items in English and the local language. We also generated speech inputs by converting the text to audio using high-quality TTS systems (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS4\" title=\"A.4 Details of the TTS Tool and Procedure for Building the Speech Data &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a> provides TTS toolkit and speech data building details). For Spoken QA, we considered two configurations: (i) converting only the question stem to speech while keeping textual options, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to reduce ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; configuration across five evaluation dimensions. Results for the fully spoken setting (spoken stem and options) on the test set are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS8\" title=\"A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.8</span></a> for reference.\nTo reflect accent effects, we produced English speech in both accent-neutral and locally accented versions.</p>\n\n",
                "matched_terms": [
                    "question",
                    "building",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only Component.</span>\nThis component contains questions that were not suitable for pairing with an image or are inherently text-based. We applied the same requirement that at least 60% of questions involve multi-step reasoning. All questions were created in multiple languages.</p>\n\n",
                "matched_terms": [
                    "same",
                    "reasoning",
                    "multistep",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators additionally identified the requisite <em class=\"ltx_emph ltx_font_italic\">knowledge points</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these denote the minimal information necessary to arrive at the correct answer, usually summarized in a few concise sentences. These knowledge points are included with the dataset and support the evaluation of whether model explanations reflect culturally authentic reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each question, annotators identified the requisite knowledge points, the minimal information needed for the correct answer, typically summarized in a few sentences (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S2.F1\" title=\"Figure 1 &#8227; How is MMA-ASIA different from others? &#8227; 2 Related Work &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). These knowledge points are used for model&#8217;s cultural awareness grounding validation (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.SS3\" title=\"4.3 Cultural Awareness Grounding Validation &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a LLM-as-Judge approach to verify whether, <em class=\"ltx_emph ltx_font_italic\">given a correct answer</em>, the model&#8217;s explanation matches the item&#8217;s knowledge points. To reduce variability across judges, we require explanations in English. The human consistency checks, the multimodel consistency evaluation, and all LLM-as-Judge parameter settings are described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS11\" title=\"A.11 Consistency Analysis and Hyperparameters Setting for LLM-as-Judge &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.11</span></a>. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F3\" title=\"Figure 3 &#8227; Across modalities (holding language fixed). &#8227; 4.2 cultural Awareness Disparity &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports the <em class=\"ltx_emph ltx_font_italic\">Rationale Unfaithfulness Rate</em> (RUR) for text-only and VQA items, defined as the proportion of correct answers whose explanations contradict or omit the required knowledge. Despite strong overall accuracy, proprietary models (Claude, GPT, and Gemini) still show RUR values between 5% and 20%. The issue is more pronounced for open-source models: Llama-3.2-11B-Vision-Instruct and Qwen3-30B-A3B-Thinking have the highest RUR, with Llama&#8217;s rate particularly elevated on non-English inputs. Qwen3-30B-A3B-Thinking often produces long explanations that contain hallucinations and sometimes derives the correct option from premises that contradict the ground truth, indicating reliance on generic heuristics rather than culturally grounded reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual cultural-awareness consistency is defined as the degree to which a model gives consistent outputs to semantically equivalent prompts posed in different languages, irrespective of answer correctness, which can be expressed as Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.E1\" title=\"In 4.4 Cultural-Awareness Consistency Across Languages &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib38\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">L_{i}</annotation></semantics></math> denotes the set of languages available for question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"m_{i}=|L_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>L</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{i}=|L_{i}|</annotation></semantics></math> is its cardinality; <math alttext=\"S\\subseteq L_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8838;</mo><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">S\\subseteq L_{i}</annotation></semantics></math> with <math alttext=\"|S|=s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">|S|=s</annotation></semantics></math> denotes any size-<math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> language subset; <math alttext=\"a_{i}^{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{\\ell}</annotation></semantics></math> is the model&#8217;s answer to question <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> when prompted in language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathbf{1}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m10\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(\\cdot)</annotation></semantics></math> is the indicator function; and <math alttext=\"\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m11\" intent=\":literal\"><semantics><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>a</mi><mi>i</mi><mi mathvariant=\"normal\">&#8467;</mi></msubsup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\bigl|\\{a_{i}^{\\ell}:\\ell\\in S\\}\\bigr|=1</annotation></semantics></math> asserts that all answers within <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m12\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> are identical. If <math alttext=\"m_{i}&lt;s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m13\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">m_{i}&lt;s</annotation></semantics></math>, there are no valid subsets and item <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m14\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> contributes zero.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phenomenon is clearly revealed by the heatmaps in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, showing how the model focuses on subjects explicitly mentioned in the prompt when answering questions. However, this selective attention can unfortunately lead the model to overlook other critically important local details within the image, consequently resulting in erroneous inferences.\nIn Case 1 (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F5\" title=\"Figure 5 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), when the model is presented with the question &#8220;Which of the following figures is the master of the person shown in the picture?&#8221; in Chinese, its attention is predominantly drawn to the figure on the left side of the image, who is explicitly referenced in the prompt. However, the model overlooks the Howling Celestial Dog in the upper-right corner of the image, a crucial clue for identifying Erlang Shen. Additional attention-visualization examples and analyses are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS13\" title=\"A.13 Attention heatmaps for incorrect model predictions &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.13</span></a>.</p>\n\n",
                "matched_terms": [
                    "following",
                    "question",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image tokens contribute to reasoning hallucinations.</span> We find that visual content increases reasoning hallucinations in models compared to text-only QA, despite accurate image recognition capabilities. In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, while the baseline model correctly identifies &#8220;Guan Yu&#8221;, it still produces multiple reasoning hallucinations (highlighted in green). However, text-only Rephrase VQA queries show no such hallucinations (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>), indicating that reasoning errors likely originate from image tokens and suggest modality-specific bias in multimodal reasoning. To test this hypothesis, we propose Vision-ablated Prefix Replay (VPR), which generates image descriptions then removes visual conditions while maintaining fixed prefixes for subsequent reasoning (details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.SS9\" title=\"A.9 Vision-ablated Prefix Replay &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">A.9</span></a>). In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#S4.F6\" title=\"Figure 6 &#8227; 4.5 Cultural-Awareness Consistency Across Modalities &#8227; 4 Experiments &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, VPR conditions the model&#8217;s reasoning on the generated caption &#8216;The image depicts a statue of Guan Yu&#8217; while discarding visual tokens, eliminating hallucinations and producing the correct answer. Furthermore, we select 50 questions where VQA answers contained hallucinations but corresponding text-only queries were correct. VPR eliminates hallucinations and produced correct answers for 19 of these cases (38%), supporting our hypothesis.</p>\n\n",
                "matched_terms": [
                    "which",
                    "reasoning",
                    "answer",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the failure modes of Claude Sonnet 4 (closed-source) and Qwen3-30B-A3B-Thinking (open-source) on multi-step reasoning tasks in English. Our experiment uses problems that each model initially failed (232 for Claude, 271 for Qwen), decomposing each into 3-8 sub-questions with ground-truth answers. An LLM (GPT-4o) is used to judge the correctness of the model&#8217;s answer to each step. The results for Claude and Qwen are as follows:</p>\n\n",
                "matched_terms": [
                    "multistep",
                    "reasoning",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sub-question Failure:</span> 62.1% (Claude), 70.8% (Qwen) cases failed at the initial sub-question reasoning stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "subquestion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced MMA-ASIA, a tri-modal (text, image, speech), multilingual benchmark and framework for evaluating cultural awareness in LLMs across 8 Asian countries and 10 languages. Our contributions include an aligned, human-curated dataset with substantial multi-step reasoning, a five-dimensional protocol that measures accuracy, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity, and analysis tools that reveal shortcut use. Results show persistent data-driven cultural bias, uneven cross-lingual transfer, and fragile multimodal reasoning (selective visual attention and image-induced hallucinations). At the same time, accented speech can act as a useful cultural cue. Models also struggle to integrate known facts into multi-step reasoning, indicating a generalization bottleneck. We argue for consistency- and grounding-aware evaluation, as well as methods that strengthen cross-modal alignment and broaden high-quality coverage in low-resource languages. MMA-ASIA provides data, protocols, and baselines to track progress toward culturally reliable multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "same",
                    "reasoning",
                    "multistep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under the MMA-ASIA framework, the dataset covers 8 countries and 10 languages, with each country&#8217;s split presented in both English and its local language, totaling 27,000 questions. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T4\" title=\"Table 4 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the countries included in our dataset and their corresponding local language(s). Over 79% of all items are multi-step cultural reasoning questions. We define a multi-step cultural reasoning item as one whose solution requires sequential derivation and/or synthesis from at least two independent knowledge components, rather than mere recall or paraphrase of a single cultural fact. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T5\" title=\"Table 5 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents an example multi-step question with its analysis. The proportion of multi-step items by country and modality is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F8\" title=\"Figure 8 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The dataset spans nine categories&#8212;<em class=\"ltx_emph ltx_font_italic\">Daily Life/Culture, Food/Cuisine, Transportation, Buildings, History, Geographical Location &amp; Climate, Education, Fashion/Clothing, and Language/Ethnicity</em>&#8212;with per-country category distributions summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F7\" title=\"Figure 7 &#8227; A.2 Data Statistics &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "buildings",
                    "multistep",
                    "questions",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Buildings</span>:\nTraditional and modern architectural styles; religious and public buildings; housing forms and materials; city skylines and the preservation of historic districts. Can also address symbolic meanings in architecture and region-specific structural features.</p>\n\n",
                "matched_terms": [
                    "buildings",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each generated audio sample was individually verified. When errors occurred&#8212;such as inappropriate pauses, missing segments, or mispronunciations&#8212;we first adjusted the input text and re-synthesized the audio, as TTS systems are often highly sensitive to textual variations. If repeated corrections still failed, we resorted to manual re-recording. Unlike in other language tasks, our requirement here was not fluency or naturalness, but rather clear articulation of the questions and answer options.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire <span class=\"ltx_text ltx_font_italic\">Rephrase VQA (text-only)</span> item, including the question and its answer options.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We considered two configurations when constructing the TTS-Spoken QA dataset: (i) converting only the question stem to speech while keeping the answer options as text, and (ii) converting both the stem and the options to speech. To preserve comparability with VQA under controlled variables and to minimize ambiguity introduced by fully spoken options, our main experiments adopt the &#8220;spoken stem + textual options&#8221; setting across five evaluation dimensions. Results for the fully spoken setting (spoken stem and spoken options) on the test set are provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.F12\" title=\"Figure 12 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#A1.T13\" title=\"Table 13 &#8227; A.8 Results for fully spoken question and answering &#8227; Appendix A Appendix &#8227; MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> for reference. We find that converting both the question and options to speech leads to a significant performance drop compared with the &#8220;spoken question + textual options&#8221; configuration, indicating that spoken options introduce greater uncertainty than the spoken question itself. This warrants further investigation in future work.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has explored related ideas. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08608v1#bib.bib42\" title=\"\">2025</a>)</cite> attempt to suppress visual leakage by blocking the attention paths to image-token positions during decoding. However, this cannot fully eliminate the influence of visual content: in a causal language model, the information of earlier image tokens is encoded into subsequent question tokens, so residual visual information remains even when attention to image tokens is blocked. In contrast, our method first elicits a textual description of the image, then removes the visual input and recomputes the representations of the prefix tokens, thereby purging visual information and enabling a more precise assessment of the image content&#8217;s contribution to the model&#8217;s reasoning process.</p>\n\n",
                "matched_terms": [
                    "question",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sampled 50 items from the dataset, each comprising a multi-step reasoning question and its decomposed sub-questions. Three annotators independently judged whether the model&#8217;s answer was semantically consistent with the gold answer for each (binary: <em class=\"ltx_emph ltx_font_italic\">yes</em>/<em class=\"ltx_emph ltx_font_italic\">no</em>). For each sub-question, the human judgment was determined by majority vote. We then queried <span class=\"ltx_text ltx_font_bold\">Claude Sonnet&#160;4</span>, <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>, and <span class=\"ltx_text ltx_font_bold\">Gemini&#160;2.5&#160;Flash</span> via API to obtain their judgments on the same items. An item was counted as <em class=\"ltx_emph ltx_font_italic\">consistent</em> for a model only if the model&#8217;s judgments for all sub-questions and the final question matched the human judgments. Results showed human&#8211;model agreement of <span class=\"ltx_text ltx_font_bold\">98%</span> for GPT-4o, <span class=\"ltx_text ltx_font_bold\">98%</span> for Claude, and <span class=\"ltx_text ltx_font_bold\">96%</span> for Gemini. Considering cost, we selected <span class=\"ltx_text ltx_font_bold\">GPT-4o</span> as the primary judge.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "answer",
                    "subquestion",
                    "question",
                    "multistep",
                    "same"
                ]
            }
        ]
    }
}