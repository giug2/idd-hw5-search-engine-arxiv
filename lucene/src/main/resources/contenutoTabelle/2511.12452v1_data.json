{
    "S4.T1": {
        "caption": "Table 1: Captioning evaluating results for our base model and multilingual variant, aggregated across all languages.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BERTScore F<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium\">1</span></sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">xFlickrCO</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama-3.2-11B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.777</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">MultilingualCap</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.819</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">XM3600</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama-3.2-11B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.792</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">MultilingualCap</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.822</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "xflickrco",
            "languages",
            "across",
            "xm3600",
            "variant",
            "base",
            "captioning",
            "evaluating",
            "multilingual",
            "model",
            "multilingualcap",
            "all",
            "bertscore",
            "results",
            "aggregated",
            "dataset",
            "llama3211b",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "multilingual",
                    "results",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, dense captioning has gained increasing attention as an effective strategy to enhance model performance. Dense captions are detailed textual descriptions that capture a richer set of visual elements. For example, PixelProse is a dataset of dense image captions with high quality and fidelity <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>]</cite>. Also, increasing caption density has been shown to improve vision-language models&#8217; compositional reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doveh2023densealignedcaptionsdac</span>]</cite>. Pyramid-XL generates point-language dense captions used to finetune the model, resulting in significantly improved 3D object generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2025gpt4pointunifiedframeworkpointlanguage</span>]</cite>. However, these synthetic captioning methods have limitations, such as hallucinations <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025lowhallucinationsyntheticcaptionslargescale</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "multilingual",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "multilingual",
                    "dataset",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, while these benchmarks are essential for assessment, there remains a critical need for high-quality training data that supports rich, fine-grained descriptions for foundational model training. Our work solved this problem and supports visual grounding by pointing .</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "multilingual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PointLLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite>, a multi-modal large language model designed to understand 3D objects represented as point clouds. Unlike traditional models that rely on 2D images and struggle with issues like depth ambiguity and occlusions, PointLLM directly processes 3D geometric and appearance data. PointLLM is exclusively trained on 3D objects, can handle scene-level point clouds, but it exhibits only limited capability for captioning, let alone complex tasks. The PointLLM paper highlights that effectively handling scene-level point clouds remains constrained by the lack of high-quality annotated data. Precisely addressing this gap, our work provides the essential scene-level high-quality data.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first briefly introduce the dense captioning platform. Then, we show the details of our multilingual dense captioning dataset for multicultural images and 3D assets.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "multilingual",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply HOLODECK 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>]</cite> to generate the 3D scenes. Then, we convert 3D scene models from GLB format to colored point clouds for further model training. All points are transformed to world coordinates, and the output is stored as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> array (XYZ + RGB), where <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=8{,}192</annotation></semantics></math> points per scene.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following experiments, we fine-tune VLMs on our MLDC-MC dataset to demonstrate its effectiveness in improving multilingual generation, visual grounding, and multicultural alignment capabilities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "multilingual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "xflickrco",
                    "languages",
                    "across",
                    "base",
                    "multilingual",
                    "model",
                    "multilingualcap",
                    "all",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a two-stage fine-tuning strategy to adapt the Qwen2-VL-7B-Instruct vision-language model for multilingual multicultural dense captioning tasks.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "multilingual",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "multilingual",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Visual Grounding by Pointing.</span> The second stage fine-tunes the Stage 1 checkpoint on MLDC-MC Part B with point-level annotations. This stage focuses on refining the model&#8217;s ability to generate precise, localized descriptions that correspond to specific regions of interest in the input images. The sequential training strategy allows the model to first acquire general multilingual capabilities before specializing in the target dense captioning task.</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "multilingual",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate that our dataset can enhance the multicultural capability of VLMs, we first examine its effectiveness on aligning with Chinese cultural contexts.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "all",
                    "results",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We partition the MLDC-3D dataset to ensure no data leakage between training and testing. Our dataset comprises 898 unique scenes, which are split into 798 training scenes and 100 test scenes, following a scene-balanced strategy. Specifically, we ensured that two scenes from each of the 50 scene subcategories were reserved for the test set. This results in 16,485 training samples (combining 7,854 open-ended QA and 8,631 multiple-choice QA) and 2,085 test samples (combining 990 open-ended QA and 1,095 multiple-choice QA), maintaining an approximate 8:1 train-test ratio.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance disparity across categories suggests that: (1)&#160;the model successfully captures coarse-grained scene-level features but requires further refinement for fine-grained spatial relationship understanding and reasoning, and (2)&#160;incorporating external knowledge or multi-modal reasoning may be necessary for tasks demanding common-sense inference. The largest category (distance relations, 3,288 training samples), showing suboptimal performance, indicates a critical area for model improvement. These results establish a strong baseline for 3D scene understanding and reasoning using point cloud-based LLMs, and highlight promising directions for future architectural and training enhancements.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "across",
                    "multilingual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "languages",
                    "multilingual",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "our"
                ]
            }
        ]
    },
    "S10.T2": {
        "caption": "Table 2: \nNumber of annotations and median length for our collected captions across all languages in the MLDC-MC part A. For Chinese, Japanese, and Thai, we report a median word count of 1 because these languages do not use whitespace to delimit words; the more informative statistic is the character count in the third row. Please refer to the Appendix for the statistics of MLDC-MC Part B.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ko</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ru</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">hi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ja</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">es</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">no</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">de</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">th</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ur</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Annotation Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2777</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2427</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">378</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">149</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">107</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Median Word Count</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">157</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">104</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">110</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">169</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">85</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">134</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">135</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">122</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Median Character Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">222</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">820</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">410</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">677</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">741</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">639</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">309</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">603</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">517</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">717</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">754</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">832</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">318</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "word",
            "thai",
            "annotation",
            "length",
            "words",
            "row",
            "not",
            "our",
            "for",
            "collected",
            "annotations",
            "whitespace",
            "informative",
            "appendix",
            "statistics",
            "character",
            "third",
            "across",
            "count",
            "delimit",
            "statistic",
            "refer",
            "report",
            "mldcmc",
            "number",
            "languages",
            "please",
            "use",
            "part",
            "all",
            "more",
            "because",
            "japanese",
            "chinese",
            "captions",
            "median"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "annotations",
                    "annotation",
                    "more",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "our",
                    "collected",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "annotation",
                    "mldcmc",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "our",
                    "annotations",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "chinese",
                    "collected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "use",
                    "not",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "collected",
                    "part",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
                "matched_terms": [
                    "please",
                    "across",
                    "collected",
                    "refer",
                    "statistics",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "across",
                    "annotation",
                    "more",
                    "all",
                    "because",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following experiments, we fine-tune VLMs on our MLDC-MC dataset to demonstrate its effectiveness in improving multilingual generation, visual grounding, and multicultural alignment capabilities.</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "report",
                    "use",
                    "part",
                    "mldcmc",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "all",
                    "chinese",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Visual Grounding by Pointing.</span> The second stage fine-tunes the Stage 1 checkpoint on MLDC-MC Part B with point-level annotations. This stage focuses on refining the model&#8217;s ability to generate precise, localized descriptions that correspond to specific regions of interest in the input images. The sequential training strategy allows the model to first acquire general multilingual capabilities before specializing in the target dense captioning task.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The point coordinates are extracted from MLDC-MC Part B where each marker point is represented as a tuple <math alttext=\"(n,x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n,x,y)</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the object name and <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> are normalized coordinates in percentage space <math alttext=\"[0,100]^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msup><annotation encoding=\"application/x-tex\">[0,100]^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "annotations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate that our dataset can enhance the multicultural capability of VLMs, we first examine its effectiveness on aligning with Chinese cultural contexts.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "part",
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cultural Description Richness:</span> whether the generated captions describe cultural elements in a detailed, diverse, and contextually informative manner.</p>\n\n",
                "matched_terms": [
                    "informative",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "more",
                    "all",
                    "mldcmc",
                    "chinese",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "use",
                    "words",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "across",
                    "annotation",
                    "more",
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "all",
                    "statistics",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "not",
                    "collected",
                    "our"
                ]
            }
        ]
    },
    "S10.T3": {
        "caption": "Table 3: \nNumber of annotations and median length for our collected captions across all languages in the MLDC-MC part B. For Chinese, Japanese, and Thai, we report a median word count of 1 because these languages do not use whitespace to delimit words; the more informative statistic is the character count in the third row.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ur</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ar</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ro</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">gu</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">pt</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">te</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">th</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Annotation Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Median Word Count</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">166</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">133</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">129</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">110</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:144%;\">1</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:144%;\">&#8224;</span></sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Median Character Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">540</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">298</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">436</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">512</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">586</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">278</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">827</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">813</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "word",
            "thai",
            "annotation",
            "length",
            "words",
            "row",
            "not",
            "our",
            "for",
            "collected",
            "annotations",
            "whitespace",
            "informative",
            "character",
            "third",
            "across",
            "count",
            "delimit",
            "statistic",
            "report",
            "mldcmc",
            "number",
            "languages",
            "use",
            "part",
            "all",
            "more",
            "because",
            "japanese",
            "chinese",
            "captions",
            "median"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "annotations",
                    "annotation",
                    "more",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "our",
                    "collected",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "annotation",
                    "mldcmc",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "our",
                    "annotations",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "chinese",
                    "collected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "use",
                    "not",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "collected",
                    "annotation",
                    "more",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "collected",
                    "part",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
                "matched_terms": [
                    "across",
                    "collected",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "across",
                    "annotation",
                    "more",
                    "all",
                    "because",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following experiments, we fine-tune VLMs on our MLDC-MC dataset to demonstrate its effectiveness in improving multilingual generation, visual grounding, and multicultural alignment capabilities.</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "report",
                    "use",
                    "part",
                    "mldcmc",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "all",
                    "chinese",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Visual Grounding by Pointing.</span> The second stage fine-tunes the Stage 1 checkpoint on MLDC-MC Part B with point-level annotations. This stage focuses on refining the model&#8217;s ability to generate precise, localized descriptions that correspond to specific regions of interest in the input images. The sequential training strategy allows the model to first acquire general multilingual capabilities before specializing in the target dense captioning task.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The point coordinates are extracted from MLDC-MC Part B where each marker point is represented as a tuple <math alttext=\"(n,x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n,x,y)</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the object name and <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> are normalized coordinates in percentage space <math alttext=\"[0,100]^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msup><annotation encoding=\"application/x-tex\">[0,100]^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "annotations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate that our dataset can enhance the multicultural capability of VLMs, we first examine its effectiveness on aligning with Chinese cultural contexts.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "part",
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cultural Description Richness:</span> whether the generated captions describe cultural elements in a detailed, diverse, and contextually informative manner.</p>\n\n",
                "matched_terms": [
                    "informative",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "more",
                    "all",
                    "mldcmc",
                    "chinese",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "use",
                    "words",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "across",
                    "annotation",
                    "more",
                    "mldcmc",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "all",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "not",
                    "collected",
                    "our"
                ]
            }
        ]
    },
    "S10.T4": {
        "caption": "Table 4: \nAnnotation prompts for MLDC-MC.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">MLDC-MC Part A</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">MLDC-MC Part B</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What is the image at first glance?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What is your initial impression of the image? Describe what you see.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What are the objects and their counts?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What text content, if any, is present in the image?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What does the text say?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Are there any subtle details or nuances that stand out to you?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What are the positions of the objects?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What elements or features are present in the background?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What subtle details are noticeable?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Does this image evoke any emotions?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What is in the background?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Can you identify a specific country, region, or community this image likely comes from?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What is the style and color?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Is this object, activity, or setting known by different names or represented differently in other regions or dialects?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Is there any contextual information such as location that might help to understand the image?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Are there any common misconceptions about the contents of this image?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Is there anything that can be inferred from the image?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">&#8212;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Is this image culturally distinct? If yes, please explain why you think this image is culturally distinct.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">&#8212;</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "counts",
            "does",
            "say",
            "prompts",
            "their",
            "region",
            "details",
            "likely",
            "information",
            "activity",
            "understand",
            "color",
            "such",
            "from",
            "first",
            "elements",
            "community",
            "stand",
            "names",
            "other",
            "positions",
            "location",
            "you",
            "annotation",
            "describe",
            "distinct",
            "comes",
            "background",
            "think",
            "country",
            "image",
            "text",
            "about",
            "identify",
            "please",
            "evoke",
            "explain",
            "misconceptions",
            "specific",
            "might",
            "there",
            "differently",
            "what",
            "content",
            "represented",
            "initial",
            "emotions",
            "setting",
            "help",
            "noticeable",
            "object",
            "any",
            "see",
            "known",
            "mldcmc",
            "common",
            "anything",
            "features",
            "contextual",
            "nuances",
            "subtle",
            "different",
            "regions",
            "present",
            "culturally",
            "out",
            "style",
            "yes",
            "your",
            "contents",
            "impression",
            "dialects",
            "glance",
            "why",
            "part",
            "objects",
            "inferred"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "object",
                    "regions",
                    "features",
                    "present",
                    "content",
                    "culturally",
                    "their",
                    "annotation",
                    "such",
                    "details",
                    "from",
                    "there",
                    "objects",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, dense captioning has gained increasing attention as an effective strategy to enhance model performance. Dense captions are detailed textual descriptions that capture a richer set of visual elements. For example, PixelProse is a dataset of dense image captions with high quality and fidelity <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>]</cite>. Also, increasing caption density has been shown to improve vision-language models&#8217; compositional reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doveh2023densealignedcaptionsdac</span>]</cite>. Pyramid-XL generates point-language dense captions used to finetune the model, resulting in significantly improved 3D object generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2025gpt4pointunifiedframeworkpointlanguage</span>]</cite>. However, these synthetic captioning methods have limitations, such as hallucinations <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025lowhallucinationsyntheticcaptionslargescale</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "such",
                    "object",
                    "image",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-annotated data can effectively mitigate the problem. Several studies have published datasets or benchmarks of human-annotated dense captioning images <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">urbanek2024pictureworth77text</span>]</cite>. However, due to limitations in annotation interfaces (<em class=\"ltx_emph ltx_font_italic\">e.g</em>. reliance on typing and limited interaction modalities), there remains significant room for quality and efficiency improvement. Recent work like COTALK <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen-etal-2025-chain</span>]</cite> demonstrates that speech-based annotation significantly improves efficiency, achieving a 40% speedup over typing-based annotation.</p>\n\n",
                "matched_terms": [
                    "there",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "from",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "culturally",
                    "their",
                    "annotation",
                    "specific",
                    "mldcmc",
                    "nuances",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "object",
                    "from",
                    "first",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "culturally",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PointLLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite>, a multi-modal large language model designed to understand 3D objects represented as point clouds. Unlike traditional models that rely on 2D images and struggle with issues like depth ambiguity and occlusions, PointLLM directly processes 3D geometric and appearance data. PointLLM is exclusively trained on 3D objects, can handle scene-level point clouds, but it exhibits only limited capability for captioning, let alone complex tasks. The PointLLM paper highlights that effectively handling scene-level point clouds remains constrained by the lack of high-quality annotated data. Precisely addressing this gap, our work provides the essential scene-level high-quality data.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "represented",
                    "understand"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first briefly introduce the dense captioning platform. Then, we show the details of our multilingual dense captioning dataset for multicultural images and 3D assets.</p>\n\n",
                "matched_terms": [
                    "details",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "their",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "object",
                    "identify",
                    "annotation",
                    "from",
                    "might",
                    "objects",
                    "image",
                    "text",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "distinct",
                    "does",
                    "culturally",
                    "background",
                    "annotation",
                    "details",
                    "from",
                    "image",
                    "different",
                    "about",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "different",
                    "distinct",
                    "from",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Images have been captioned by both native and non-native speakers. We hypothesize that dense captions created by native speakers include a lot of cultural information that would be missing in non-native captions. The pointing coordinates on the image are recorded as percentage values, retained to two decimal places.</p>\n\n",
                "matched_terms": [
                    "image",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
                "matched_terms": [
                    "please",
                    "distinct",
                    "prompts",
                    "details",
                    "objects",
                    "describe",
                    "about",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "identify",
                    "from",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "image",
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "other",
                    "culturally",
                    "such"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "first",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Visual Grounding by Pointing.</span> The second stage fine-tunes the Stage 1 checkpoint on MLDC-MC Part B with point-level annotations. This stage focuses on refining the model&#8217;s ability to generate precise, localized descriptions that correspond to specific regions of interest in the input images. The sequential training strategy allows the model to first acquire general multilingual capabilities before specializing in the target dense captioning task.</p>\n\n",
                "matched_terms": [
                    "regions",
                    "part",
                    "specific",
                    "mldcmc",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The point coordinates are extracted from MLDC-MC Part B where each marker point is represented as a tuple <math alttext=\"(n,x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n,x,y)</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the object name and <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> are normalized coordinates in percentage space <math alttext=\"[0,100]^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msup><annotation encoding=\"application/x-tex\">[0,100]^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "represented",
                    "part",
                    "from",
                    "mldcmc",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "prompts",
                    "you"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This text-based encoding approach enables the vision-language model to naturally learn the association between visual regions and their corresponding spatial coordinates through supervised fine-tuning, without requiring architectural modifications to handle coordinate outputs separately.</p>\n\n",
                "matched_terms": [
                    "regions",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spatial Accuracy:</span> measures how precisely the annotated points are localized on the objects mentioned in the point names.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "names"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Object Coverage Completeness:</span> assesses whether the annotated points adequately cover the objects in the images.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, evaluators are presented with the image and the outputs from the model before and after training. They are asked to determine, for each evaluation dimension, which version performs better or if they are equivalent. The final result for each image is determined by taking the majority vote among the evaluators. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F5\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" title=\"Figure 11 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> compares the test set outputs of Qwen2-VL-7B-Instruct (after two-stage training) and vanilla Qwen3-VL-8B-Instruct. It is evident that the fine-tuned Qwen2 outperforms the more advanced Qwen3. We also evaluated the vanilla Qwen2-VL-7B-Instruct model on this pointing task using the same prompts; however, it was unable to follow the keypoint-listing instruction or produce any coordinate outputs.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "image",
                    "from",
                    "any"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cultural Element Recognition:</span> whether the model correctly identifies Chinese cultural elements present in the image.</p>\n\n",
                "matched_terms": [
                    "image",
                    "present",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cultural Description Richness:</span> whether the generated captions describe cultural elements in a detailed, diverse, and contextually informative manner.</p>\n\n",
                "matched_terms": [
                    "describe",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "identify",
                    "culturally",
                    "their",
                    "mldcmc",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "distinct",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "represented",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "from",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "other",
                    "text",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "distinct",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "mldcmc",
                    "setting",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We apply HOLODECK 2.0 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> to generate the 3D scenes. The process begins with a natural language description which is processed by a Vision-Language Model (VLM) to generate a 2D reference image for style and then extract quality-controlled, individual 2D images for each object. These 2D images are fed into 3D generative models (such as Hunyuan3D 2.1) to efficiently create high-quality 3D assets. Finally, the VLM infers spatial constraints from the text and image, which are iteratively applied by a Depth-First-Search (DFS) solver to achieve a semantically coherent and physically plausible 3D layout.</span>\n</p>\n\n",
                "matched_terms": [
                    "style",
                    "such",
                    "from",
                    "object",
                    "text",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "objects",
                    "color",
                    "from",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "their",
                    "about",
                    "from",
                    "information"
                ]
            }
        ]
    },
    "S10.T5": {
        "caption": "Table 5:  chrF++ scores for each model across all languages in the two evaluation datasets. Note that chrF++ is not necessarily comparable across languages.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Lang</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Llama-3.2-11B-Vision-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">MultilingualCap</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">xFlickrCO</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">de</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.911</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">7.219</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">es</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">9.531</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">12.069</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">id</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">8.616</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">10.133</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">ja</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">2.237</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">4.922</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">ru</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">0.645</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">13.144</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">tr</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.890</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.423</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">zh</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">0.382</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">2.225</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">XM3600</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">ar</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">1.080</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">4.683</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">bn</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">6.363</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.024</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">cs</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">7.553</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.900</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">da</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.958</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">8.432</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">de</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.187</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">7.378</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">el</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">3.691</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">3.846</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">es</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.514</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">7.690</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">fa</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">3.931</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">6.637</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">fi</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.157</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">7.955</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">fil</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">10.736</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">15.606</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">fr</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">8.370</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.187</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">fuj</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">1.764</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">2.782</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">he</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.745</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">7.264</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">hi</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">1.218</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.825</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">hr</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">7.115</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.183</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">huo</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">3.472</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">4.807</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">id</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">10.386</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">13.187</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">it</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">10.798</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">13.091</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">ja</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">3.034</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">3.314</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">mi</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">3.127</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">3.958</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">nl</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">4.195</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">6.105</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">no</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.885</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">6.411</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">pl</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.889</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.090</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">pt</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.719</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">9.556</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">quz</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">3.954</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">2.599</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">ro</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">8.048</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.177</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">ru</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">0.357</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.708</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">sv</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">5.817</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.239</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">sw</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">10.895</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">9.986</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">te</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">8.987</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">8.403</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">th</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">6.403</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">9.721</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">tr</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">4.649</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">5.387</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">uk</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.336</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">10.506</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">vi</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">5.942</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">13.542</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:144%;\">zh</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:144%;\">0.123</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">4.826</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "xflickrco",
            "multilingualcap",
            "evaluation",
            "each",
            "xm3600",
            "comparable",
            "not",
            "two",
            "lang",
            "chrf",
            "scores",
            "fil",
            "llama3211bvisioninstruct",
            "across",
            "necessarily",
            "note",
            "huo",
            "datasets",
            "languages",
            "quz",
            "fuj",
            "model",
            "all",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "dataset",
                    "datasets",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, dense captioning has gained increasing attention as an effective strategy to enhance model performance. Dense captions are detailed textual descriptions that capture a richer set of visual elements. For example, PixelProse is a dataset of dense image captions with high quality and fidelity <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>]</cite>. Also, increasing caption density has been shown to improve vision-language models&#8217; compositional reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doveh2023densealignedcaptionsdac</span>]</cite>. Pyramid-XL generates point-language dense captions used to finetune the model, resulting in significantly improved 3D object generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2025gpt4pointunifiedframeworkpointlanguage</span>]</cite>. However, these synthetic captioning methods have limitations, such as hallucinations <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025lowhallucinationsyntheticcaptionslargescale</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "not",
                    "each",
                    "languages",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply HOLODECK 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>]</cite> to generate the 3D scenes. Then, we convert 3D scene models from GLB format to colored point clouds for further model training. All points are transformed to world coordinates, and the output is stored as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> array (XYZ + RGB), where <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=8{,}192</annotation></semantics></math> points per scene.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "each",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "xflickrco",
                    "languages",
                    "across",
                    "multilingualcap",
                    "model",
                    "all",
                    "chrf",
                    "dataset",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, evaluators are presented with the image and the outputs from the model before and after training. They are asked to determine, for each evaluation dimension, which version performs better or if they are equivalent. The final result for each image is determined by taking the majority vote among the evaluators. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F5\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" title=\"Figure 11 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> compares the test set outputs of Qwen2-VL-7B-Instruct (after two-stage training) and vanilla Qwen3-VL-8B-Instruct. It is evident that the fine-tuned Qwen2 outperforms the more advanced Qwen3. We also evaluated the vanilla Qwen2-VL-7B-Instruct model on this pointing task using the same prompts; however, it was unable to follow the keypoint-listing instruction or produce any coordinate outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "all",
                    "evaluation",
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We partition the MLDC-3D dataset to ensure no data leakage between training and testing. Our dataset comprises 898 unique scenes, which are split into 798 training scenes and 100 test scenes, following a scene-balanced strategy. Specifically, we ensured that two scenes from each of the 50 scene subcategories were reserved for the test set. This results in 16,485 training samples (combining 7,854 open-ended QA and 8,631 multiple-choice QA) and 2,085 test samples (combining 990 open-ended QA and 1,095 multiple-choice QA), maintaining an approximate 8:1 train-test ratio.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model demonstrates strong performance on two tasks: scene classification achieves 80.37% accuracy, and size comparison reaches 76.62%, indicating effective learning of global scene semantics and spatial scale relationships. Medium-difficulty tasks, including localization (57.29%) and object presence recognition (54.63%), show reasonable but improvable performance. Notably, the model struggles with tasks requiring complex spatial understanding or reasoning: distance reasoning achieves only 42.28% accuracy, while anomaly detection, which requires common-sense reasoning and scene context understanding, proves most challenging at 32.71%, only marginally above random chance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance disparity across categories suggests that: (1)&#160;the model successfully captures coarse-grained scene-level features but requires further refinement for fine-grained spatial relationship understanding and reasoning, and (2)&#160;incorporating external knowledge or multi-modal reasoning may be necessary for tasks demanding common-sense inference. The largest category (distance relations, 3,288 training samples), showing suboptimal performance, indicates a critical area for model improvement. These results establish a strong baseline for 3D scene understanding and reasoning using point cloud-based LLMs, and highlight promising directions for future architectural and training enhancements.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We apply HOLODECK 2.0 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> to generate the 3D scenes. The process begins with a natural language description which is processed by a Vision-Language Model (VLM) to generate a 2D reference image for style and then extract quality-controlled, individual 2D images for each object. These 2D images are fed into 3D generative models (such as Hunyuan3D 2.1) to efficiently create high-quality 3D assets. Finally, the VLM infers spatial constraints from the text and image, which are iteratively applied by a Depth-First-Search (DFS) solver to achieve a semantically coherent and physically plausible 3D layout.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "not"
                ]
            }
        ]
    },
    "S10.T6": {
        "caption": "Table 6: \nNumber of annotations and median length for our collected captions across the top 10 languages by sample count in the MLDC-3D. For each scene annotation, we concatenate the object-level and scene-level captions to calculate the length. The dataset contains 26 languages. For Chinese, we report a median word count of 1 because these languages do not use whitespace to delimit words; the more informative statistic is the character count in the third row.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">hi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ko</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">pt</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">id</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ru</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ur</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">es</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Annotation Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1004</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">736</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Median Word Count</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">855</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8224;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">661</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">449</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">785</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">670</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">776</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">383</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">758</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">506</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Median Character Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3568</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1210</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2252</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1385</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2586</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3011</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4483</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2189</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2351</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2266</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "word",
            "annotation",
            "length",
            "each",
            "words",
            "row",
            "not",
            "our",
            "for",
            "collected",
            "annotations",
            "whitespace",
            "informative",
            "character",
            "third",
            "scene",
            "concatenate",
            "across",
            "count",
            "delimit",
            "scenelevel",
            "top",
            "statistic",
            "report",
            "mldc3d",
            "number",
            "languages",
            "objectlevel",
            "use",
            "more",
            "because",
            "calculate",
            "dataset",
            "chinese",
            "contains",
            "captions",
            "median"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "annotations",
                    "annotation",
                    "more",
                    "scene",
                    "dataset",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, dense captioning has gained increasing attention as an effective strategy to enhance model performance. Dense captions are detailed textual descriptions that capture a richer set of visual elements. For example, PixelProse is a dataset of dense image captions with high quality and fidelity <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>]</cite>. Also, increasing caption density has been shown to improve vision-language models&#8217; compositional reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doveh2023densealignedcaptionsdac</span>]</cite>. Pyramid-XL generates point-language dense captions used to finetune the model, resulting in significantly improved 3D object generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2025gpt4pointunifiedframeworkpointlanguage</span>]</cite>. However, these synthetic captioning methods have limitations, such as hallucinations <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025lowhallucinationsyntheticcaptionslargescale</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "our",
                    "collected",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "scenelevel",
                    "annotations",
                    "mldc3d",
                    "annotation",
                    "dataset",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "objectlevel",
                    "annotations",
                    "annotation",
                    "each",
                    "scene",
                    "dataset",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multilingual and Multicultural Image Captioning.</span>\nMost vision-language datasets for image description remain heavily English-centric <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2025multilingualdiversityimprovesvisionlanguage</span>]</cite>. Recent years have seen efforts to broaden this scope: for example, the CVLUE benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang-etal-2024-cvlue</span>]</cite> introduces large-scale Chinese vision&#8211;language data, enabling systematic study of bilingual capabilities in modern vision&#8211;language models. Similarly, the CVQA benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite> provides a culturally diverse multilingual VQA dataset with 10k human-annotated questions from 30 countries covering 31 languages, collected by native speakers to ensure broad cultural coverage.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "chinese",
                    "collected",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "scene",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PointLLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite>, a multi-modal large language model designed to understand 3D objects represented as point clouds. Unlike traditional models that rely on 2D images and struggle with issues like depth ambiguity and occlusions, PointLLM directly processes 3D geometric and appearance data. PointLLM is exclusively trained on 3D objects, can handle scene-level point clouds, but it exhibits only limited capability for captioning, let alone complex tasks. The PointLLM paper highlights that effectively handling scene-level point clouds remains constrained by the lack of high-quality annotated data. Precisely addressing this gap, our work provides the essential scene-level high-quality data.</p>\n\n",
                "matched_terms": [
                    "scenelevel",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first briefly introduce the dense captioning platform. Then, we show the details of our multilingual dense captioning dataset for multicultural images and 3D assets.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "each",
                    "scene",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "scenelevel",
                    "annotations",
                    "use",
                    "annotation",
                    "scene",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "collected",
                    "annotation",
                    "more",
                    "each",
                    "contains",
                    "dataset",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "collected",
                    "dataset",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "across",
                    "scenelevel",
                    "annotation",
                    "more",
                    "each",
                    "because",
                    "scene",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following experiments, we fine-tune VLMs on our MLDC-MC dataset to demonstrate its effectiveness in improving multilingual generation, visual grounding, and multicultural alignment capabilities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "report",
                    "use",
                    "contains",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "dataset",
                    "chinese",
                    "not",
                    "captions",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Multilingual Image-Text Alignment.</span> In the first stage, we pre-train the model on MLDC-MC Part A to establish robust cross-modal alignment between visual inputs and textual descriptions across multiple languages. This stage enables the model to enhance foundational multilingual vision-language understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, evaluators are presented with the image and the outputs from the model before and after training. They are asked to determine, for each evaluation dimension, which version performs better or if they are equivalent. The final result for each image is determined by taking the majority vote among the evaluators. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F5\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" title=\"Figure 11 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> compares the test set outputs of Qwen2-VL-7B-Instruct (after two-stage training) and vanilla Qwen3-VL-8B-Instruct. It is evident that the fine-tuned Qwen2 outperforms the more advanced Qwen3. We also evaluated the vanilla Qwen2-VL-7B-Instruct model on this pointing task using the same prompts; however, it was unable to follow the keypoint-listing instruction or produce any coordinate outputs.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "more",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate that our dataset can enhance the multicultural capability of VLMs, we first examine its effectiveness on aligning with Chinese cultural contexts.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chinese",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chinese",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cultural Description Richness:</span> whether the generated captions describe cultural elements in a detailed, diverse, and contextually informative manner.</p>\n\n",
                "matched_terms": [
                    "informative",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "more",
                    "dataset",
                    "chinese",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scenelevel",
                    "mldc3d",
                    "scene",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "scenelevel",
                    "each",
                    "scene",
                    "dataset",
                    "contains",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "scenelevel",
                    "each",
                    "scene",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We partition the MLDC-3D dataset to ensure no data leakage between training and testing. Our dataset comprises 898 unique scenes, which are split into 798 training scenes and 100 test scenes, following a scene-balanced strategy. Specifically, we ensured that two scenes from each of the 50 scene subcategories were reserved for the test set. This results in 16,485 training samples (combining 7,854 open-ended QA and 8,631 multiple-choice QA) and 2,085 test samples (combining 990 open-ended QA and 1,095 multiple-choice QA), maintaining an approximate 8:1 train-test ratio.</p>\n\n",
                "matched_terms": [
                    "mldc3d",
                    "each",
                    "scene",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "scenelevel",
                    "use",
                    "scene",
                    "words",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scenelevel",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance disparity across categories suggests that: (1)&#160;the model successfully captures coarse-grained scene-level features but requires further refinement for fine-grained spatial relationship understanding and reasoning, and (2)&#160;incorporating external knowledge or multi-modal reasoning may be necessary for tasks demanding common-sense inference. The largest category (distance relations, 3,288 training samples), showing suboptimal performance, indicates a critical area for model improvement. These results establish a strong baseline for 3D scene understanding and reasoning using point cloud-based LLMs, and highlight promising directions for future architectural and training enhancements.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scenelevel",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "across",
                    "mldc3d",
                    "annotation",
                    "more",
                    "scene",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "captions",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "languages",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "not",
                    "collected",
                    "our"
                ]
            }
        ]
    },
    "S10.T7": {
        "caption": "Table 7: \nAnnotation prompts MLDC-3D.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">MLDC-3D</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What type of room or space is this? What is the primary function or purpose of this scene?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Count and identify all the objects visible in the scene. (Note: Similar objects can be grouped together even if they differ in style, e.g., you can say &#8221;two chairs&#8221; for chairs of different styles. However, please describe objects separately even if they were combined as individual objects earlier, e.g., count &#8221;bowl&#8221; and &#8221;spoon&#8221; separately rather than as &#8221;bowl with spoon&#8221;.)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">Which objects in this scene naturally work together or form functional groups? For each group, explain how someone would naturally use these objects in order. e.g. &#8221;A person would first turn on the lamp, then sit down on chairand open the book.&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">What unreasonable aspects can you find in the scene? If there are none, please state so.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">After removing the ground plane from the scene, which object is positioned in the center of the scene?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">If you are standing at the center object you mentioned, describe 1) what objects you would see. Use position words such as in front of, to the right, etc. 2) measuring from the closest point of each object, which objects would be closest/farthest to you. 3) which objects are larger/smaller than the center object. (Format: If I am standing at [object] and facing [object], I would see &#8230;)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">After removing the ground plane from the scene, what objects are located at the corners of the scene?</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">If you are standing at one of the corner objects you mentioned, describe 1) what objects you would see. Use position words such as in front of, to the right, etc. 2) measuring from the closest point of each object, which objects would be closest/farthest to you. 3) which objects are larger/smaller than the center object. (Format: If I am standing at [object] and facing [object], I would see &#8230;)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:455.2pt;\"><span class=\"ltx_text\" style=\"font-size:144%;\">If you are in the scene, are there any objects that are completely or partially hidden from certain viewing angles? Describe the situation in detail. (Format: If I stand/sit/kneel/&#8230; at [object], facing [object], I can not see &#8230;, because &#8230;)</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "aspects",
            "say",
            "bowl",
            "hidden",
            "prompts",
            "group",
            "separately",
            "each",
            "words",
            "bowl",
            "primary",
            "chairand",
            "such",
            "from",
            "certain",
            "how",
            "first",
            "facing",
            "positioned",
            "position",
            "corner",
            "spoon",
            "would",
            "state",
            "visible",
            "none",
            "earlier",
            "use",
            "standing",
            "space",
            "because",
            "measuring",
            "standsitkneel",
            "plane",
            "ground",
            "person",
            "they",
            "format",
            "type",
            "you",
            "rather",
            "even",
            "viewing",
            "together",
            "annotation",
            "differ",
            "center",
            "mentioned",
            "describe",
            "not",
            "open",
            "someone",
            "after",
            "partially",
            "count",
            "identify",
            "purpose",
            "grouped",
            "corners",
            "individual",
            "unreasonable",
            "please",
            "explain",
            "work",
            "there",
            "order",
            "combined",
            "what",
            "find",
            "down",
            "which",
            "etc",
            "object",
            "any",
            "naturally",
            "scene",
            "turn",
            "closest",
            "detail",
            "front",
            "however",
            "located",
            "removing",
            "see",
            "note",
            "situation",
            "right",
            "functional",
            "lamp",
            "point",
            "all",
            "function",
            "chairs",
            "different",
            "style",
            "closestfarthest",
            "sit",
            "spoon",
            "book",
            "completely",
            "than",
            "mldc3d",
            "styles",
            "two",
            "one",
            "largersmaller",
            "angles",
            "groups",
            "then",
            "form",
            "room",
            "chairs",
            "similar",
            "objects"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "which",
                    "annotation",
                    "such",
                    "from",
                    "there",
                    "scene",
                    "objects",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, dense captioning has gained increasing attention as an effective strategy to enhance model performance. Dense captions are detailed textual descriptions that capture a richer set of visual elements. For example, PixelProse is a dataset of dense image captions with high quality and fidelity <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>]</cite>. Also, increasing caption density has been shown to improve vision-language models&#8217; compositional reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doveh2023densealignedcaptionsdac</span>]</cite>. Pyramid-XL generates point-language dense captions used to finetune the model, resulting in significantly improved 3D object generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qi2025gpt4pointunifiedframeworkpointlanguage</span>]</cite>. However, these synthetic captioning methods have limitations, such as hallucinations <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singla2024pixelsproselargedataset</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025lowhallucinationsyntheticcaptionslargescale</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "however",
                    "such",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-annotated data can effectively mitigate the problem. Several studies have published datasets or benchmarks of human-annotated dense captioning images <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">urbanek2024pictureworth77text</span>]</cite>. However, due to limitations in annotation interfaces (<em class=\"ltx_emph ltx_font_italic\">e.g</em>. reliance on typing and limited interaction modalities), there remains significant room for quality and efficiency improvement. Recent work like COTALK <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen-etal-2025-chain</span>]</cite> demonstrates that speech-based annotation significantly improves efficiency, achieving a 40% speedup over typing-based annotation.</p>\n\n",
                "matched_terms": [
                    "however",
                    "room",
                    "work",
                    "annotation",
                    "there"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we publish an online dense captioning toolkit, which has a well-established mechanism to ensure the acquisition of high-quality human-annotated multimodal data. It represents a comprehensive pipeline for multimodal data acquisition, encompassing task design, task deployment, annotation gathering, and releasing the final, high-quality data. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S0.F1\" title=\"In DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates examples from our collected data.</p>\n\n",
                "matched_terms": [
                    "work",
                    "from",
                    "which",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research Molmo and its accompanying PixMo datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>]</cite> mark a significant step forward in developing state-of-the-art open-weight Vision-Language Models (VLMs). Their contribution is particularly exemplified by the adoption of an audio-driven annotation paradigm, which enables the collection of large-scale, high-quality 2D image data without relying on proprietary VLM distillation.\nHowever, PixMo has not released an open-source annotation platform, which limits the community&#8217;s ability to expand the dataset in specific domain. Our open-source online platform, DenseAnnotate, distinguishes itself by extending this dense captioning approach into fundamentally new and challenging domains. Crucially, while PixMo focuses on 2D imagery through separate tasks (PixMo-Cap for dense captioning and PixMo-Points for pointing), DenseAnnotate introduces a unified online annotation platform that enables the simultaneous collection of dense captions and pointing annotations within a single interface, and further extends dense captioning capabilities to complex 3D assets. By constructing our MLDC-3D dataset, we directly address the critical lack of scene-level high-quality data required for models dedicated to 3D understanding. Furthermore, DenseAnnotate places a specialized emphasis on capturing cultural nuances through our MLDC-MC dataset. This collection leverages native-language annotators to acquire detailed and culturally aligned descriptions, resulting in significantly longer captions, thus ensuring a depth of linguistic and cultural representation that complements existing open MLLMs efforts.</p>\n\n",
                "matched_terms": [
                    "however",
                    "open",
                    "which",
                    "mldc3d",
                    "annotation",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "point",
                    "together",
                    "annotation",
                    "from",
                    "each",
                    "scene",
                    "object",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, while these benchmarks are essential for assessment, there remains a critical need for high-quality training data that supports rich, fine-grained descriptions for foundational model training. Our work solved this problem and supports visual grounding by pointing .</p>\n\n",
                "matched_terms": [
                    "however",
                    "work",
                    "there"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "however",
                    "work",
                    "scene",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PointLLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite>, a multi-modal large language model designed to understand 3D objects represented as point clouds. Unlike traditional models that rely on 2D images and struggle with issues like depth ambiguity and occlusions, PointLLM directly processes 3D geometric and appearance data. PointLLM is exclusively trained on 3D objects, can handle scene-level point clouds, but it exhibits only limited capability for captioning, let alone complex tasks. The PointLLM paper highlights that effectively handling scene-level point clouds remains constrained by the lack of high-quality annotated data. Precisely addressing this gap, our work provides the essential scene-level high-quality data.</p>\n\n",
                "matched_terms": [
                    "work",
                    "objects",
                    "point"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we first briefly introduce the dense captioning platform. Then, we show the details of our multilingual dense captioning dataset for multicultural images and 3D assets.</p>\n\n",
                "matched_terms": [
                    "then",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "individual",
                    "each",
                    "scene",
                    "objects",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "then",
                    "identify",
                    "use",
                    "individual",
                    "annotation",
                    "one",
                    "from",
                    "scene",
                    "objects",
                    "object",
                    "not",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aimed to select\nimages containing culturally distinctive elements for human annotation. Approximately 20% of the images were sourced from MMID <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hewitt-etal-2018-learning</span>]</cite>, 20% from MS COCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2015microsoftcococaptionsdata</span>]</cite>, and about 60% from CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">romero2024cvqaculturallydiversemultilingualvisual</span>]</cite>; further details are provided in the Appendix.\nWe collected 6,220 distinct captions in 13 different languages. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> contains detailed statistics for each language. Our captions are significantly longer and more detailed than COCO captions; on average, our English captions are 820 characters long, compared to 52 for COCO. They are also human-annotated and contain fewer inaccuracies and hallucinations compared to ShareGPT4V. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F3\" title=\"In Annotation Workflow &#8227; 3.1 DenseAnnotate: A Dense Captioning Toolkit &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> contains an example of this with an image from the COCO dataset. The COCO caption is very short and fails to capture the majority of the detail in the image, while the ShareGPT4V caption is more detailed but contains major inaccuracies. For instance, it refers to a woman with a green bag, but the woman in the background does not have a green bag. It also claims that the person in the foreground is carrying a briefcase, which is also untrue. Our human-annotated caption is able to capture a large amount of the detail in the picture while also not containing factual inaccuracies.</p>\n\n",
                "matched_terms": [
                    "than",
                    "which",
                    "annotation",
                    "person",
                    "each",
                    "from",
                    "different",
                    "detail",
                    "not",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this part, our dataset is from Flickr. We collected 2,526 distinct captions in 18 different languages. To ensure diversity and cultural representation, we targeted images from India, Spain, and Korea. The search process relied on country-specific keywords (e.g., &#8220;street market India,&#8221; &#8220;cathedral Italy,&#8221; &#8220;festival Mexico&#8221;) to capture a wide range of cultural and everyday settings. Duplicates and low-quality images were removed, and only clear, contextually relevant samples were retained.</p>\n\n",
                "matched_terms": [
                    "different",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply HOLODECK 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>]</cite> to generate the 3D scenes. Then, we convert 3D scene models from GLB format to colored point clouds for further model training. All points are transformed to world coordinates, and the output is stored as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> array (XYZ + RGB), where <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=8{,}192</annotation></semantics></math> points per scene.</p>\n\n",
                "matched_terms": [
                    "then",
                    "format",
                    "point",
                    "all",
                    "from",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "identify",
                    "visible",
                    "which",
                    "annotation",
                    "all",
                    "each",
                    "because",
                    "from",
                    "scene",
                    "objects",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "use",
                    "down",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, almost all of our metrics, and almost all of the languages we benchmark, MultilingualCap outperforms the base Llama model by a significant margin. This is an indication that models trained on our dataset are capable of producing superior multilingual captions. This is true even for languages that are not in our training dataset such as Indonesian, although the improvement is less significant. We hypothesize that our model learns a general, underlying structure for generating multilingual, detailed and culturally relevant captions, enabling effective cross-lingual transfer to unseen languages. On the other hand, our model demonstrates the largest performance gains in languages where the available training data volume is highest: MultilingualCap improves by 482% over the baseline in Chinese and 1938% in Russian in ChrF++ score on the xFlickrCO dataset.</p>\n\n",
                "matched_terms": [
                    "not",
                    "such",
                    "even",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The point coordinates are extracted from MLDC-MC Part B where each marker point is represented as a tuple <math alttext=\"(n,x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n,x,y)</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the object name and <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> are normalized coordinates in percentage space <math alttext=\"[0,100]^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msup><annotation encoding=\"application/x-tex\">[0,100]^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "point",
                    "space",
                    "each",
                    "from",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The assistant response is constructed by directly concatenating the descriptive transcript with the formatted point annotations, <em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">a table with food\\n&lt;point&gt;65.20,63.90&lt;/point&gt; table; &lt;point&gt;52.60,58.60&lt;/point&gt; food;</span>. The human instruction explicitly prompts the model to follow this format: &#8220;list keypoints as <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;x,y&lt;/point&gt; name</span> (percent coordinates) for all objects you are describing,&#8221; enabling the model to learn this structured output format through supervised fine-tuning.</p>\n\n",
                "matched_terms": [
                    "format",
                    "you",
                    "prompts",
                    "point",
                    "all",
                    "objects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This text-based encoding approach enables the vision-language model to naturally learn the association between visual regions and their corresponding spatial coordinates through supervised fine-tuning, without requiring architectural modifications to handle coordinate outputs separately.</p>\n\n",
                "matched_terms": [
                    "naturally",
                    "separately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Point&#8211;Caption Consistency:</span> evaluates how well the point names align with the entities mentioned in the caption, reflecting semantic correspondence.</p>\n\n",
                "matched_terms": [
                    "how",
                    "point",
                    "mentioned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spatial Accuracy:</span> measures how precisely the annotated points are localized on the objects mentioned in the point names.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "point",
                    "how",
                    "mentioned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Object Coverage Completeness:</span> assesses whether the annotated points adequately cover the objects in the images.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, evaluators are presented with the image and the outputs from the model before and after training. They are asked to determine, for each evaluation dimension, which version performs better or if they are equivalent. The final result for each image is determined by taking the majority vote among the evaluators. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F5\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" title=\"Figure 11 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> compares the test set outputs of Qwen2-VL-7B-Instruct (after two-stage training) and vanilla Qwen3-VL-8B-Instruct. It is evident that the fine-tuned Qwen2 outperforms the more advanced Qwen3. We also evaluated the vanilla Qwen2-VL-7B-Instruct model on this pointing task using the same prompts; however, it was unable to follow the keypoint-listing instruction or produce any coordinate outputs.</p>\n\n",
                "matched_terms": [
                    "however",
                    "any",
                    "prompts",
                    "which",
                    "after",
                    "each",
                    "from",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tuned Qwen2-VL-7B-Instruct using only Chinese image-text pairs, which Chinese people label. Our MLDC-MC part A dataset includes a total of 631 Chinese-culture-related images annotated in Chinese, with 600 images used for training and 31 for testing. We compared the outputs of the Qwen model before and after training. To assess the cultural alignment quality, we recruited three native Chinese speakers to conduct a human evaluation along three dimensions:</p>\n\n",
                "matched_terms": [
                    "which",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation process is similar to the one in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.SS2\" title=\"4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Sec.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F6\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the human evaluation results comparing the fine-tuned Qwen model with the original version. The fine-tuned model consistently outperforms the baseline across all three dimensions. Specifically, it achieves higher accuracy in recognizing Chinese cultural elements, indicating that the model has learned to better identify culturally significant visual cues. The improvement in cultural element interpretation further suggests that the model not only detects these elements but also understands their underlying cultural meanings. Moreover, the fine-tuned model demonstrates a substantial advantage in cultural description richness, showing its ability to generate more detailed and contextually enriched cultural descriptions. Overall, these results verify that fine-tuning on MLDC-MC effectively enhances the model&#8217;s cultural alignment and descriptive capability in Chinese cultural contexts, and further indicate that the proposed dataset has the potential to enhance the multicultural capability of VLMs.</p>\n\n",
                "matched_terms": [
                    "identify",
                    "all",
                    "one",
                    "similar",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "work",
                    "point",
                    "mldc3d",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "format",
                    "point",
                    "each",
                    "from",
                    "scene",
                    "objects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "format",
                    "point",
                    "each",
                    "from",
                    "scene",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We partition the MLDC-3D dataset to ensure no data leakage between training and testing. Our dataset comprises 898 unique scenes, which are split into 798 training scenes and 100 test scenes, following a scene-balanced strategy. Specifically, we ensured that two scenes from each of the 50 scene subcategories were reserved for the test set. This results in 16,485 training samples (combining 7,854 open-ended QA and 8,631 multiple-choice QA) and 2,085 test samples (combining 990 open-ended QA and 1,095 multiple-choice QA), maintaining an approximate 8:1 train-test ratio.</p>\n\n",
                "matched_terms": [
                    "which",
                    "mldc3d",
                    "each",
                    "from",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "however",
                    "use",
                    "which",
                    "one",
                    "scene",
                    "words",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene-level PointLLM</span> We finetune <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> using 16,485 training samples. Our fine-tuned model generates clean, concise, and well-formatted responses and demonstrates a transformation from model collapse to functional task execution. The multiple-choice QA test set performance is shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S5.F7\" title=\"In 5.2 Experimental Results and Analysis &#8227; 5 Scene-level PointLLM &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Our fully fine-tuned model achieves an overall accuracy of 57.26%, substantially outperforming the baseline of 37.17%. Performance varies significantly across question types, revealing distinct strengths and weaknesses of the learned representations.</p>\n\n",
                "matched_terms": [
                    "functional",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model demonstrates strong performance on two tasks: scene classification achieves 80.37% accuracy, and size comparison reaches 76.62%, indicating effective learning of global scene semantics and spatial scale relationships. Medium-difficulty tasks, including localization (57.29%) and object presence recognition (54.63%), show reasonable but improvable performance. Notably, the model struggles with tasks requiring complex spatial understanding or reasoning: distance reasoning achieves only 42.28% accuracy, while anomaly detection, which requires common-sense reasoning and scene context understanding, proves most challenging at 32.71%, only marginally above random chance.</p>\n\n",
                "matched_terms": [
                    "object",
                    "which",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance disparity across categories suggests that: (1)&#160;the model successfully captures coarse-grained scene-level features but requires further refinement for fine-grained spatial relationship understanding and reasoning, and (2)&#160;incorporating external knowledge or multi-modal reasoning may be necessary for tasks demanding common-sense inference. The largest category (distance relations, 3,288 training samples), showing suboptimal performance, indicates a critical area for model improvement. These results establish a strong baseline for 3D scene understanding and reasoning using point cloud-based LLMs, and highlight promising directions for future architectural and training enhancements.</p>\n\n",
                "matched_terms": [
                    "point",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "mldc3d",
                    "scene",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "then",
                    "rather",
                    "than",
                    "which",
                    "annotation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-MC are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T2\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tabs.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T3\" style=\"font-size:144%;\" title=\"Table 3 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T4\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F10\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> gives an example of our MLDC-MC data, which shows our data captures the cultural information. The test result of multilingual generation capability is shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> demonstrates the performance of pointing and captioning after fine-tuning on our data.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "which",
                    "after",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We apply HOLODECK 2.0 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> to generate the 3D scenes. The process begins with a natural language description which is processed by a Vision-Language Model (VLM) to generate a 2D reference image for style and then extract quality-controlled, individual 2D images for each object. These 2D images are fed into 3D generative models (such as Hunyuan3D 2.1) to efficiently create high-quality 3D assets. Finally, the VLM infers spatial constraints from the text and image, which are iteratively applied by a Depth-First-Search (DFS) solver to achieve a semantically coherent and physically plausible 3D layout.</span>\n</p>\n\n",
                "matched_terms": [
                    "then",
                    "which",
                    "style",
                    "individual",
                    "such",
                    "each",
                    "from",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "then",
                    "format",
                    "point",
                    "after",
                    "all",
                    "position",
                    "from",
                    "each",
                    "scene",
                    "objects",
                    "plane",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">This study involves data collected from student participants at our institution. The data collection was conducted in accordance with our institution&#8217;s ethical guidelines but was not subject to a formal Institutional Review Board (IRB) approval process. All participants were informed about the purpose of the study and provided their consent for their data to be used for research purposes. The collected data contain no personally identifiable or sensitive information, and participation was entirely voluntary.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "not",
                    "purpose",
                    "from"
                ]
            }
        ]
    },
    "S10.T8": {
        "caption": "Table 8: \nThe indoor and outdoor 3D scenes we use in this paper. For each scene, we generated ten instances using the Holodeck 2.0 model.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Type</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Category</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Subcategories</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Home</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Bedroom</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Living Room</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Kitchen</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Dining Room</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Study Room</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Bathroom</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Children&#8217;s Room</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Balcony</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Work Space</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Office</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Meeting Room</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Library</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Study Hall</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Classroom</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Laboratory</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Recording Studio</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Hospital Ward</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Commercial Space</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Coffee Shop</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Restaurant</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Supermarket</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Bar</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Bookstore</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor Market</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Hair Salon</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Clinic</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Public Space</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Museum</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Church</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Theater</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Music Room</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Activity Room</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor Workshop</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Indoor Flower Exhibition Hall</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Gym</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Outdoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Nature</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Beach</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:144%;\">Forest Camping Site</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Garden</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Mountain Cabin</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Desert Oasis</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Lake Side</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Outdoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Urban Space</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Amusement Park</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">City Square</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Bus Terminal</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Rooftop Viewpoint</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">School Playground</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Pedestrian Street</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Outdoor</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:144%;\">Rural</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Farmland</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Ranch</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Fishing Village Dock</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:144%;\">Mountain Village</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_b\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_b\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">Marketplace</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:144%;\">Orchard</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "laboratory",
            "library",
            "studio",
            "playground",
            "living",
            "each",
            "rooftop",
            "activity",
            "childrens",
            "beach",
            "meeting",
            "desert",
            "clinic",
            "balcony",
            "rural",
            "hospital",
            "marketplace",
            "market",
            "ten",
            "use",
            "street",
            "space",
            "city",
            "fishing",
            "type",
            "bar",
            "theater",
            "nature",
            "recording",
            "instances",
            "bathroom",
            "coffee",
            "exhibition",
            "oasis",
            "camping",
            "hall",
            "work",
            "village",
            "indoor",
            "amusement",
            "study",
            "salon",
            "scene",
            "hair",
            "restaurant",
            "mountain",
            "bus",
            "ranch",
            "shop",
            "school",
            "dock",
            "ward",
            "lake",
            "cabin",
            "scenes",
            "subcategories",
            "museum",
            "orchard",
            "workshop",
            "model",
            "farmland",
            "outdoor",
            "paper",
            "park",
            "side",
            "gym",
            "commercial",
            "viewpoint",
            "site",
            "classroom",
            "public",
            "bedroom",
            "urban",
            "category",
            "forest",
            "supermarket",
            "home",
            "square",
            "pedestrian",
            "dining",
            "church",
            "holodeck",
            "office",
            "bookstore",
            "terminal",
            "generated",
            "room",
            "kitchen",
            "music",
            "flower",
            "garden"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To make the dataset diverse and robust, our design encompasses two primary types: indoor and outdoor, spanning across seven major categories (Home, Work Space, Commercial Space, Public Space, Nature, Urban Space, and Rural) and including 50 distinct scenes subcategories. Please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a> for details. In total, we collected the captions for 898 different 3D scenes and 7,460 different 3D objects, resulting in about 2,000 scene-level dense captions and 19,000 object-level dense captions. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> contains detailed statistics for each language. For each of the scenes, we ask annotators to describe objects one by one in detail, and then answer 9 questions according to the scene. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the prompts for annotators.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">The statistics of the languages in MLDC-3D are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T6\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The annotation prompts are shown in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T7\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T8\" style=\"font-size:144%;\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> shows all the 3D scenes types in the data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data.\nA key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image&#8217;s visual content.\nDense annotations, which provide longer and more comprehensive descriptions covering substantially more visual details, attributes, and relationships, are more valuable but remain scarce.\nTraditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation.\nIn this paper, we present <span class=\"ltx_text ltx_font_bold\">DenseAnnotate</span>, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets.\nAnnotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts.\nOur platform incorporates speech-to-text transcription and region-of-attention marking.\nTo demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over <span class=\"ltx_text ltx_font_bold\">1,000</span> annotators across two domains: culturally diverse images and 3D scenes.\nWe curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in <span class=\"ltx_text ltx_font_bold\">20 languages</span>, including <span class=\"ltx_text ltx_font_bold\">8,746</span> image captions, <span class=\"ltx_text ltx_font_bold\">2,000</span> scene captions, and <span class=\"ltx_text ltx_font_bold\">19,000</span> object captions.\nModels trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities.\nOur results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "paper",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-annotated data can effectively mitigate the problem. Several studies have published datasets or benchmarks of human-annotated dense captioning images <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">urbanek2024pictureworth77text</span>]</cite>. However, due to limitations in annotation interfaces (<em class=\"ltx_emph ltx_font_italic\">e.g</em>. reliance on typing and limited interaction modalities), there remains significant room for quality and efficiency improvement. Recent work like COTALK <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen-etal-2025-chain</span>]</cite> demonstrates that speech-based annotation significantly improves efficiency, achieving a 40% speedup over typing-based annotation.</p>\n\n",
                "matched_terms": [
                    "work",
                    "room"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To summarize, our contributions are three-fold: (1) We develop a unified multimodal annotation platform capable of handling both 2D images and 3D assets in a web-based interface; (2) To the best of our knowledge, we release the first human-annotated multilingual dataset of multicultural 2D imagery with dense captions aligned to pointing annotations, enabling improved multilingual multicultural captioning and region-level grounding; (3) We further construct the first human-annotated multilingual dataset that provides full-scene 3D dense captions together with independent object-level dense descriptions, where each object is physically separable from the scene and semantically grounded within it. This dataset supports 3D understanding and reasoning over point clouds.</p>\n\n",
                "matched_terms": [
                    "each",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, while these benchmarks are essential for assessment, there remains a critical need for high-quality training data that supports rich, fine-grained descriptions for foundational model training. Our work solved this problem and supports visual grounding by pointing .</p>\n\n",
                "matched_terms": [
                    "work",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3D Scene Captioning.</span>\nDescribing 3D environments in natural language is a very recent endeavor, and existing 3D captioning resources are scarce. Recent advances like MMScan <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lyu2025mmscanmultimodal3dscene</span>]</cite> and TOD3Cap <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jin2024tod3cap3ddensecaptioning</span>]</cite> utilize semi-automatic pipelines leveraging visual-language models and subsequent human refinement to generate large-scale 3D scene descriptions. However, these approaches are predominantly monolingual (English) and are intrinsically limited by the expressiveness and speed constraints of text-based inputs. In stark contrast, our work introduces an innovative audio-driven annotation platform, enabling the collection of rich, fine-grained, and multilingual captions while ensuring annotation quality by transcription summarization and the platform&#8217;s auto-check mechanism.</p>\n\n",
                "matched_terms": [
                    "work",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PointLLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite>, a multi-modal large language model designed to understand 3D objects represented as point clouds. Unlike traditional models that rely on 2D images and struggle with issues like depth ambiguity and occlusions, PointLLM directly processes 3D geometric and appearance data. PointLLM is exclusively trained on 3D objects, can handle scene-level point clouds, but it exhibits only limited capability for captioning, let alone complex tasks. The PointLLM paper highlights that effectively handling scene-level point clouds remains constrained by the lack of high-quality annotated data. Precisely addressing this gap, our work provides the essential scene-level high-quality data.</p>\n\n",
                "matched_terms": [
                    "work",
                    "model",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S2.F2\" title=\"In 2 Related Work &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our system&#8217;s three-stage workflow. In the first stage, unlabeled images or 3D data are combined with task-specific instructions to generate annotation-guiding questions. Task creators can either adopt the system&#8217;s suggested questions or input their own guidance for annotators. In the second stage, human annotators interact with the data through multiple operations&#8212;such as pointing and naming for 2D images, and zooming, panning, and freely rotating for 3D objects or scenes. In the latter case, annotators can also isolate and closely examine individual objects within the scene before producing their audio descriptions. To make the captions dense enough, we set the minimum recording times as follows: 60 seconds for images or 3D scenes, and 20 seconds for 3D objects. The recordings are transcribed into raw captions, and annotators can edit typos. We recruit multiple annotators to label each of the images or 3D assets. In the last stage, multiple raw captions are summarized using LLMs to produce high-quality, dense captions, thereby enhancing both the accuracy and richness of the annotations.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "each",
                    "scene",
                    "recording"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform enforces multiple quality control mechanisms to ensure annotation completeness and consistency. For 2D image tasks, the platform enforces that annotators mark at least five objects with native-language labels. For 3D scene tasks, the system implements a mandatory sequential workflow: individual object annotations (annotation step 1) must be completed first, then the system unlocks the scene-level recording interface (annotation step 2). This approach not only enables annotators to become familiar with the objects in the scene first, but also increases the density of the overall description. Temporal constraints enforce minimum recording durations (20 seconds for individual objects, 60 seconds for scenes or images) with automatic 3-minute cutoffs to balance annotation depth and annotator fatigue. Also, we developed a dual-transcript preservation mechanism that stores both the automatically generated transcription from OpenAI&#8217;s Whisper API and the user-edited version, enabling us to detect excessive discrepancies between them and identify cases where annotators might have copied the VLMs&#8217; output instead of recording manually. At last, we use GPT-4o to summarize multiple transcriptions into one final caption to improve the text quality of the transcriptions.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "use",
                    "generated",
                    "recording",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply HOLODECK 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>]</cite> to generate the 3D scenes. Then, we convert 3D scene models from GLB format to colored point clouds for further model training. All points are transformed to world coordinates, and the output is stored as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> array (XYZ + RGB), where <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=8{,}192</annotation></semantics></math> points per scene.</p>\n\n",
                "matched_terms": [
                    "holodeck",
                    "scenes",
                    "model",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the original dense captions, we go further to generate open-ended and multiple-choice question-answer pairs. Stage 1 uses GPT-4o to extract structured answers from multi-language transcripts. By aggregating multiple transcripts in the same language for each scene, we enable GPT-4o to extract answers effectively (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S3.F4\" title=\"In Part A: Captions-Only Dataset &#8227; 3.2 Multilingual Dense Captioning for Multicultural Images (MLDC-MC) &#8227; 3 DenseAnnotate &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, Open-Ended QA). Specifically, all OEQA categories (e.g., Anomaly Detection) are sourced from the scene-level dense captions (from annotation step 2), with the exception of the &#8217;Dense Description&#8217; category, which is derived from the individual-object captions (from annotation step 1). Because the answers are based on multiple transcripts, they are more comprehensive and reliable. Stage 2 converts OEQA pairs into multiple-choice questions using GPT-4o, leveraging question-specific strategies. For example, to ensure high-quality distractors, we sample them from cross-question data. This means that for options involving objects, we extract plausible distractors from the set of real objects identified in the scene via an Open-Ended QA &#8220;Count and identify all the objects visible in the scene&#8221;. This yields 9,726 multiple-choice question-answer pairs across 6 variants.</p>\n\n",
                "matched_terms": [
                    "category",
                    "each",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the pretrained Llama-3.2-11B-Vision-Instruct model as our base model and finetune it on our MLDC-MC Part A. We evaluate our finetuned model, MultilingualCap, on several captioning benchmarks to evaluate the quality of multilingual generation. We use XM3600 <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">thapliyal2022crossmodal3600massivelymultilingualmultimodal</span>]</cite> and xFlickrCO <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bugliarello2022igluebenchmarktransferlearning</span>]</cite>, which are both multilingual image captioning datasets. XM3600 contains human-annotated\ncaptions in 36 different languages and xFlickrCO has a mix of human-annotated and machine-translated captions in 8 different languages. We use standard textual similarity metric BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2020bertscoreevaluatingtextgeneration</span>]</cite>. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.T1\" title=\"In 4.1 Multilingual Generation Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the results. We also report the chrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">popovic-2015-chrf</span>]</cite> results broken down by language in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.T5\" title=\"In 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Tab.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The point coordinates are extracted from MLDC-MC Part B where each marker point is represented as a tuple <math alttext=\"(n,x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n,x,y)</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the object name and <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> are normalized coordinates in percentage space <math alttext=\"[0,100]^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">]</mo></mrow><mn>2</mn></msup><annotation encoding=\"application/x-tex\">[0,100]^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "space",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, evaluators are presented with the image and the outputs from the model before and after training. They are asked to determine, for each evaluation dimension, which version performs better or if they are equivalent. The final result for each image is determined by taking the majority vote among the evaluators. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S4.F5\" title=\"In 4.2 Fine-Grained Visual Grounding Capability &#8227; 4 Multicultural and Multilingual Imagery Caption Generation &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12452v1#S10.F11\" title=\"Figure 11 &#8227; 10 Ethics Statement &#8227; DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> compares the test set outputs of Qwen2-VL-7B-Instruct (after two-stage training) and vanilla Qwen3-VL-8B-Instruct. It is evident that the fine-tuned Qwen2 outperforms the more advanced Qwen3. We also evaluated the vanilla Qwen2-VL-7B-Instruct model on this pointing task using the same prompts; however, it was unable to follow the keypoint-listing instruction or produce any coordinate outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of our dataset on 3D scene understanding and reasoning, we conduct experiments using the PointLLM framework. While prior work has primarily focused on single-object point clouds, our MLDC-3D dataset introduces rich scene-level supervision across multiple question types. These experiments further validate that MLDC-3D complements existing 3D instruction-tuning resources and enables robust learning of global scene semantics and complex spatial relationships.</p>\n\n",
                "matched_terms": [
                    "work",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A distinct advantage of our data is the inclusion of complete 3D scenes, unlike the 2D images or videos commonly derived from them. Therefore, we chose PointLLM for its capability to directly process 3D objects. PointLLM proposes a novel two-stage training process, leveraging a large, automatically generated dataset of point-text instructions, enabling the model to accurately classify and caption 3D objects.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "model",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure compatibility with the PointLLM architecture, we maintain the same point cloud representation format as the original Objaverse-based dataset. Each scene is represented as an <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">N\\times 6</annotation></semantics></math> matrix stored in NumPy format (.npy). The key difference from the original PointLLM dataset lies in the semantic scope: while PointLLM uses single-object point clouds from Objaverse, our dataset contains multi-object scene-level point clouds where spatial relationships between objects are preserved through world coordinate transformations.</p>\n\n",
                "matched_terms": [
                    "each",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our scene-level instruction-following data adopts the Stage 2 conversation format from PointLLM. The dataset comprises two conversation types: <span class=\"ltx_text ltx_font_italic\">detailed_description</span> for comprehensive scene descriptions and <span class=\"ltx_text ltx_font_italic\">single_round</span> for question-answering tasks (including both open-ended questions and multiple-choice questions). Each sample follows the standard format with fields <span class=\"ltx_text ltx_font_typewriter\">object_id</span> (scene name), <span class=\"ltx_text ltx_font_typewriter\">conversation_type</span>, and <span class=\"ltx_text ltx_font_typewriter\">conversations</span> containing human-assistant dialogue pairs. The special token <span class=\"ltx_text ltx_font_typewriter\">&lt;point&gt;</span> is prepended to the first user query to indicate point cloud input. This format ensures seamless integration with PointLLM&#8217;s data loader.</p>\n\n",
                "matched_terms": [
                    "each",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We partition the MLDC-3D dataset to ensure no data leakage between training and testing. Our dataset comprises 898 unique scenes, which are split into 798 training scenes and 100 test scenes, following a scene-balanced strategy. Specifically, we ensured that two scenes from each of the 50 scene subcategories were reserved for the test set. This results in 16,485 training samples (combining 7,854 open-ended QA and 8,631 multiple-choice QA) and 2,085 test samples (combining 990 open-ended QA and 1,095 multiple-choice QA), maintaining an approximate 8:1 train-test ratio.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "each",
                    "subcategories",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance</span>\nTo establish a baseline, we first evaluate <span class=\"ltx_text ltx_markedasmath\">PointLLM_7B_v1.2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024pointllmempoweringlargelanguage</span>]</cite> using our multiple-choice QA. However, qualitative analysis of the generated outputs reveals a critical insight: the original model produces degenerate outputs consisting of repetitive tokens, special characters, and incoherent text fragments (<em class=\"ltx_emph ltx_font_italic\">e.g</em>., <span class=\"ltx_text ltx_font_typewriter\">Sent&#167;#H&#732;thexic</span>). In other words, the baseline model exhibits <span class=\"ltx_text ltx_font_bold\">complete generative failure</span> on our scene tasks. This finding suggests that the pre-trained model, despite being trained on 660K brief description data and 70K complex instruction data, lacks the training distribution alignment necessary for our scene-level tasks. This inherent limitation underscores the importance of our novel dataset, which is specifically designed to drive advancements in scene-level 3D understanding and complex reasoning. We train the model with one sixty-fourth of the training data to provide basic scene capacity and use it as the baseline.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model",
                    "scene",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model demonstrates strong performance on two tasks: scene classification achieves 80.37% accuracy, and size comparison reaches 76.62%, indicating effective learning of global scene semantics and spatial scale relationships. Medium-difficulty tasks, including localization (57.29%) and object presence recognition (54.63%), show reasonable but improvable performance. Notably, the model struggles with tasks requiring complex spatial understanding or reasoning: distance reasoning achieves only 42.28% accuracy, while anomaly detection, which requires common-sense reasoning and scene context understanding, proves most challenging at 32.71%, only marginally above random chance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance disparity across categories suggests that: (1)&#160;the model successfully captures coarse-grained scene-level features but requires further refinement for fine-grained spatial relationship understanding and reasoning, and (2)&#160;incorporating external knowledge or multi-modal reasoning may be necessary for tasks demanding common-sense inference. The largest category (distance relations, 3,288 training samples), showing suboptimal performance, indicates a critical area for model improvement. These results establish a strong baseline for 3D scene understanding and reasoning using point cloud-based LLMs, and highlight promising directions for future architectural and training enhancements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "category",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced DenseAnnotate, an audio-driven multimodal annotation platform that scales dense caption collection across both 2D imagery and 3D scenes. Our MLDC-MC and MLDC-3D datasets address critical gaps in multilingual, multicultural, and 3D scene data resources. Empirical evaluations demonstrate that models fine-tuned on these datasets achieve substantial gains. By integrating human expressiveness with scalable automation, DenseAnnotate offers a practical and extensible paradigm for next-generation vision&#8211;language research, setting a foundation for more inclusive multimodal intelligence.\n\n\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n</span></p>\n\n",
                "matched_terms": [
                    "scenes",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We developed a web-based multimodal annotation platform that unifies data collection workflows for both 2D images and interactive 3D scenes. We collect audio data rather than textual data from annotators, which has been shown to improve the quality and ease of collection of dense captions </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deitke2024molmopixmoopenweights</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. The system employs a three-tier architecture consisting of a React-based frontend for real-time annotation interfaces, a Flask backend integrated with Supabase Backend-as-a-Service for scalable data management, and a PostgreSQL relational database with row-level security policies ensuring multi-tenant data isolation. Our design prioritizes organizational scalability through a hierarchical model where administrators create tasks with customizable questions and instructions, which are then distributed to annotators based on demographic metadata and organizational membership. The architecture incorporates Babylon.js WebGL rendering engine for client-side 3D visualization, enabling annotators to interact with 3D scenes through intuitive camera controls (rotation, panning, zoom) without requiring specialized software installation.</span>\n</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We apply HOLODECK 2.0 </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:144%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bian2025holodeck20visionlanguageguided3d</span>\n    <span class=\"ltx_text\" style=\"font-size:144%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> to generate the 3D scenes. The process begins with a natural language description which is processed by a Vision-Language Model (VLM) to generate a 2D reference image for style and then extract quality-controlled, individual 2D images for each object. These 2D images are fed into 3D generative models (such as Hunyuan3D 2.1) to efficiently create high-quality 3D assets. Finally, the VLM infers spatial constraints from the text and image, which are iteratively applied by a Depth-First-Search (DFS) solver to achieve a semantically coherent and physically plausible 3D layout.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "holodeck",
                    "scenes",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:144%;\">We convert 3D scene models from GLB format to colored point clouds using Blender&#8217;s Python API for further model training. After importing the GLB file and triangulating all mesh objects (excluding the ground plane), we perform surface sampling using barycentric coordinates. For each sampled point on a triangular face, we generate random barycentric weights (</span>\n  <math alttext=\"r_{1},r_{2},r_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">1</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">2</mn>\n        </msub>\n        <mo mathsize=\"1.440em\">,</mo>\n        <msub>\n          <mi mathsize=\"1.440em\">r</mi>\n          <mn mathsize=\"1.440em\">3</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1},r_{2},r_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">) with the constraint that if </span>\n  <math alttext=\"r_{1}+r_{2}&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\">+</mo>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n        </mrow>\n        <mo mathsize=\"1.440em\">&gt;</mo>\n        <mn mathsize=\"1.440em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}+r_{2}&gt;1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, then </span>\n  <math alttext=\"r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">1</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"1.440em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"1.440em\">r</mi>\n            <mn mathsize=\"1.440em\">2</mn>\n          </msub>\n          <mo mathsize=\"1.440em\" stretchy=\"false\">&#8592;</mo>\n          <mrow>\n            <mn mathsize=\"1.440em\">1</mn>\n            <mo mathsize=\"1.440em\">&#8722;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r_{1}\\leftarrow 1-r_{1},r_{2}\\leftarrow 1-r_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">, and compute the position as </span>\n  <math alttext=\"p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">p</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">1</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">2</mn>\n            </msub>\n          </mrow>\n          <mo mathsize=\"1.440em\">+</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"1.440em\">r</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"1.440em\">v</mi>\n              <mn mathsize=\"1.440em\">3</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p=r_{1}v_{1}+r_{2}v_{2}+r_{3}v_{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\">. For color information, we interpolate UV coordinates using the same barycentric weights and perform nearest-neighbor sampling from the texture maps. All points are transformed to world coordinates, and the output is stored as an </span>\n  <math alttext=\"N\\times 6\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo lspace=\"0.222em\" mathsize=\"1.440em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"1.440em\">6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N\\times 6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> array (XYZ + RGB), where </span>\n  <math alttext=\"N=8{,}192\" class=\"ltx_Math\" display=\"inline\" id=\"S9.SS0.SSS0.Px1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"1.440em\">N</mi>\n        <mo mathsize=\"1.440em\">=</mo>\n        <mrow>\n          <mn mathsize=\"1.440em\">8</mn>\n          <mo mathsize=\"1.440em\">,</mo>\n          <mn mathsize=\"1.440em\">192</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N=8{,}192</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:144%;\"> points per scene.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "scene"
                ]
            }
        ]
    }
}