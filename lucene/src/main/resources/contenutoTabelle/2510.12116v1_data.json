{
    "S5.T1": {
        "caption": "Table 1: \nPerformance (%, ↑\\uparrow) on the sd-qa subset under different token-level intervention strategies.\n“Bottom3”: Only the three least-aligned tokens are modified; “All”: The entire alignment path is modified.\nFor each row, results that outperform the corresponding Speech Input are typeset in bold.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speech Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Angle Projection</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Length Normalization</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Bottom3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">All</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Bottom3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">All</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">38.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">38.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">37.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LoRA</td>\n<td class=\"ltx_td ltx_align_center\">47.92</td>\n<td class=\"ltx_td ltx_align_center\">38.88</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">40.51</span></td>\n<td class=\"ltx_td ltx_align_center\">38.88</td>\n<td class=\"ltx_td ltx_align_center\">38.16</td>\n<td class=\"ltx_td ltx_align_center\">31.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">49.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">47.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">42.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.96</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "bottom3",
            "strategy",
            "length",
            "each",
            "row",
            "modified",
            "projection",
            "intervention",
            "path",
            "corresponding",
            "strategies",
            "sdqa",
            "leastaligned",
            "“bottom3”",
            "input",
            "llama318b",
            "text",
            "lora",
            "performance",
            "alignment",
            "tokenlevel",
            "bold",
            "angle",
            "under",
            "results",
            "speech",
            "only",
            "normalization",
            "tokens",
            "entire",
            "↑uparrow",
            "qwen257b",
            "three",
            "model",
            "“all”",
            "all",
            "outperform",
            "subset",
            "typeset",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.T1\" title=\"Table 1 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Angle Projection yields improvements or maintains performance in 6 out of 8 intervention settings, demonstrating that increasing the angular similarity of token-level text and speech representations can enhance downstream outcomes. For LoRA fine-tuned models, applying angle projection to either the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> or <span class=\"ltx_text ltx_font_italic\">All</span> alignment-path tokens consistently improves results. Notably, intervening on only the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> tokens leads to more robust gains, with <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> improving by 7.52% and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> by 4.19%. In contrast, length normalization provides improvement in only one case, with performance declining in the remaining settings, indicating an overall detrimental effect on LSLM&#8217;s speech sequence modeling.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "strategies",
                    "tokenlevel",
                    "normalization",
                    "length",
                    "angle",
                    "speech",
                    "input",
                    "projection",
                    "text",
                    "tokens",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of Large Speech Language Models (LSLMs) has revolutionized human-computer interaction by enabling direct processing of both speech representations and text inputs, subsequently generating textual or spoken outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib1\" title=\"\">2024</a>)</cite>.\nCompared to traditional pipeline architectures that sequentially chain Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-To-Speech (TTS) components, end-to-end LSLMs offer significant advantages, including reduced latency, inherent error resilience, and more expressive speech synthesis capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies in LSLMs have focused on aligning speech modalities with text space through speech tokenizers <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> or encoder-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>); Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib4\" title=\"\">2023</a>)</cite>. These cross-modal alignment strategies aim to harness the linguistic capabilities of pretrained LLMs while integrating speech processing functionalities <cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "strategies",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, significant performance disparities persist between LSLMs and conventional pipeline models in semantic understanding tasks. Benchmark results from VoiceBench <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite> reveal a striking contrast: the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + LLaMA-3.1-8B</span> pipeline achieves 79.06 overall score, while its LSLM counterpart <span class=\"ltx_text ltx_font_italic\">LLaMA-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>)</cite> scores merely 37.51. This pattern continues in Uro-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib29\" title=\"\">2025</a>)</cite> evaluations, where the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + Qwen2-7B-Instruct</span> pipeline attains 78.13 overall score compared to <span class=\"ltx_text ltx_font_italic\">Freeze-Omni</span>&#8217;s <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib28\" title=\"\">2024b</a>)</cite> 48.28, despite both systems employing the same underlying LLM. Notably, while Uro-bench&#8217;s dependence on transcribed speech outputs might inherently favor pipeline architectures, the magnitude of these performance drops remains substantial and warrants investigation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llama318b",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Nevertheless, contemporary investigations into LSLMs remain largely confined to engineering practices, adopting unverified integrated solutions spanning training stages, dataset scales, parameter-efficient strategies, and multimodal objectives without systematic analysis of their individual contributions or synergistic effects <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>); Zhong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib36\" title=\"\">2024</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib20\" title=\"\">2025b</a>)</cite>. This practice of design-by-intuition raises critical concerns, as invalidated architectural choices may inadvertently exacerbate the modality alignment discrepancy.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we take the persistent performance gap between end-to-end LSLMs and traditional ASR+LLM pipeline systems as our starting point. We systematically reproduce and quantify this discrepancy across various LLM backbones and training strategies, and, for the first time, empirically reveal the underlying mechanisms behind the performance difference. Specifically, after speech-text alignment training, a clear and consistent performance gap exists between text and speech inputs within the same model.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "strategies",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight into the modality gap, we systematically analyze the similarity between speech and text representations at both sequence and token levels, aiming to reveal the mechanisms of speech-text alignment within LSLMs. At the sequence (coarse-grained) level, we observe that as representations propagate through deeper layers of the model, their cosine similarity increases steadily, reflecting progressive directional alignment. In parallel, the Euclidean distance between modalities also increases, indicating a divergence in magnitude that likely reflects modality-specific characteristics learned by the model. At the token (fine-grained) level, we find that the model develops a spontaneous monotonic alignment pattern between speech and text tokens, indicating consistent local correspondence across modalities.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, our study systematically examines the relationship between internal representation similarity and the modality gap exhibited on evaluation benchmarks. A clear linear correlation is observed at both the sequence and token levels, suggesting that the nature of internal cross-modal alignment is closely related to the performance disparity between speech and text inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "path",
                    "alignment",
                    "sdqa",
                    "length",
                    "angle",
                    "subset",
                    "speech",
                    "modified",
                    "projection",
                    "normalization",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are threefold: (1) We systematically identify that the primary source of the performance gap between LSLMs and pipeline systems lies in the modality difference between speech and text inputs. (2) We analyze internal representations and find that the modality gap is closely linked to the similarity between speech and text representations at both sequence and token levels. (3) We provide the first empirical evidence that targeted interventions on speech representations can improve speech input performance on challenging cases.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By focusing on understanding and revealing the mechanisms behind modality alignment, our work offers a deeper exploration of the factors influencing LSLM performance. This approach not only addresses the current performance discrepancy but also paves the way for future advancements in integrating speech modalities into LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "only",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The alignment between speech and textual modalities is crucial for the performance of LSLMs. Recent studies have explored five distinct methodologies for this task <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>. The latent space mapping approach, exemplified by <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SALMONN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib24\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">VITA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib11\" title=\"\">2024</a>)</cite>, uses a joint audio encoder-adapter architecture to directly project speech inputs into the LLM&#8217;s latent textual space. This paradigm effectively reduces computational overhead by compressing the audio sequence length via the audio adapter module. Meanwhile, it also preserves the LLM&#8217;s inherent reasoning capabilities and has demonstrated competitive performance across multiple benchmarks.</p>\n\n",
                "matched_terms": [
                    "length",
                    "speech",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> adopts modality chaining by discretizing speech into symbolic units and expanding the LLM&#8217;s vocabulary, while <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite> and <span class=\"ltx_text ltx_font_italic\">Moshi</span> <cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib7\" title=\"\">2024</a>)</cite> utilize interleaved text-speech tokens and parallel generation architectures, respectively. <span class=\"ltx_text ltx_font_italic\">SyncLLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Veluri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib26\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">IntrinsicVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib34\" title=\"\">2024b</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Align-SLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib18\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">OmniFlatten</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib33\" title=\"\">2024a</a>)</cite> pioneers direct speech-to-speech interaction without textual intermediates. Although significant progress has been made with these methodologies in existing research, their performance on audio processing tasks remains suboptimal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has systematically analyzed the modality gap phenomenon and shown that it persists across a wide range of multimodal models <cite class=\"ltx_cite ltx_citemacro_cite\">Liang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib17\" title=\"\">2022</a>)</cite>. This gap largely arises from the cone effect, where embeddings from different modalities are restricted to distinct subspaces, leading to misalignment and degraded cross-modal performance.</p>\n\n",
                "matched_terms": [
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech translation, the modality gap has also been a subject of investigation. Evidence suggests that this gap can emerge during the early phases of fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib12\" title=\"\">2023</a>)</cite>. Furthermore, it has been shown that the resulting misalignment between modalities leads to divergent predictions and degrades performance relative to text-only machine translation systems. This representational divergence was empirically quantified using the cosine similarity between speech and text embeddings, confirming a substantial gap <cite class=\"ltx_cite ltx_citemacro_cite\">Fang and Feng (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib9\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of comparing high-dimensional representations, Centered Kernel Alignment (CKA) was introduced as a robust similarity measure <cite class=\"ltx_cite ltx_citemacro_cite\">Kornblith et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib16\" title=\"\">2019</a>)</cite>. Subsequent work has shown that a simple, sample-wise cosine similarity can also effectively capture layer-wise similarity in transformer models, yielding results comparable to CKA with greater computational efficiency <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, the Wasserstein distance between paired speech and text embeddings has been used to measure cross-modal consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib19\" title=\"\">2025a</a>)</cite>. The Gramian Representation Alignment Measure (GRAM) is also designed to evaluate the alignment of multiple modalities simultaneously <cite class=\"ltx_cite ltx_citemacro_cite\">Cicchetti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib5\" title=\"\">2024</a>)</cite>.\nBoth methods have been integrated into training and effectively improve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates the performance degradation of LSLMs in processing speech inputs compared to their base models&#8217; performance on text inputs. Through comprehensive experiments conducted on multiple LLM backbones using both full-parameter and LoRA fine-tuning methods <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib13\" title=\"\">2021</a>)</cite>, we find that the primary contributor to this modality gap is the suboptimal alignment between textual and auditory modalities in LSLMs.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the architecture of the LSLM setup used in this study comprises three core components: a speech encoder, a speech adapter, and an LLM backbone.\nThe speech signal is first encoded by the speech encoder into a latent representation and then compressed by the speech adapter by a factor of <span class=\"ltx_text ltx_font_italic\">5</span> to reduce computational overhead.\nMeanwhile, text inputs are processed through a standard tokenization pipeline and embedded via the LLM&#8217;s embedding layer.\nThese speech and text embeddings are then concatenated to form a unified multimodal sequence that serves as input to the LLM backbone, enabling autoregressive generation of textual responses.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span>, a widely used ASR model, serves as the speech encoder. The speech adapter is implemented as a lightweight module with two fully connected layers. We conducted experiments with various LLM backbones, including <span class=\"ltx_text ltx_font_italic\">LLaMA3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">LLaMA3.1-8B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B-Instruct</span>, and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B-Instruct</span>. For brevity, henceforth we omit the suffix \"Instruct\" when referring to these model variants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset is constructed following the framework of <span class=\"ltx_text ltx_font_italic\">Ke-Speech-Chat</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>)</cite>, exclusively comprising single-turn dialogue samples. Each sample includes both speech and text instructions, as well as a text response. We refined the raw text using <span class=\"ltx_text ltx_font_italic\">Qwen2.5-72B-Instruct</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib30\" title=\"\">2024</a>); Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib25\" title=\"\">2024</a>)</cite>, aligning it with natural conversational patterns observed in real-world scenarios. Subsequently, a three-stage filtering mechanism was applied to purify the data, targeting safety, semantic clarity, and linguistic naturalness. The speech instruction-response pairs were synthesized using <span class=\"ltx_text ltx_font_italic\">CosyVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib8\" title=\"\">2024</a>)</cite>. Based on automated transcription via <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib22\" title=\"\">2022</a>)</cite>, speech samples exceeding a Word Error Rate (WER) threshold of 0.1 are discarded. Finally, Our training dataset contains 637,283 samples, with speech instructions totaling 1,604 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All LSLMs are trained for 2 epochs on our training dataset using the AdamW optimizer with a peak learning rate of 2e-5. For LoRA, we set <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, <math alttext=\"\\alpha=4.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>4.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=4.0</annotation></semantics></math>, and the dropout rate to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. All experiments were conducted on a distributed setup with 2 nodes, each equipped with 8 NVIDIA A100 GPUs. Training a single model requires approximately 384 GPU hours on this setup.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "each",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we adopt five subsets of the <span class=\"ltx_text ltx_font_italic\">VoiceBench</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8212;<span class=\"ltx_text ltx_font_typewriter\">AdvBench</span>, <span class=\"ltx_text ltx_font_typewriter\">IFEval</span>, <span class=\"ltx_text ltx_font_typewriter\">OBQA</span>, <span class=\"ltx_text ltx_font_typewriter\">MMSU</span>, and <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span>&#8212;yielding a total of 4,947 test samples after filtering.\nThese subsets collectively cover 93% of the full VoiceBench and are particularly suitable for robust evaluation as their metrics do not require additional LLMs, thereby minimizing variability.\nAll evaluations strictly adhere to the official VoiceBench evaluation protocol to ensure consistency and reproducibility.\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of LSLMs after full-parameter and LoRA fine-tuning on the 4 models introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.SS1\" title=\"3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, each tested under both speech and text modalities. Detailed results are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS1\" title=\"A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS2\" title=\"A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the performance of each model at epoch 2 on both text and speech inputs, alongside the corresponding base model&#8217;s text-only performance.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "under",
                    "each",
                    "results",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the data shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on average, LSLMs exhibit a 25% performance decline on speech inputs relative to their base models on text. This decline can be attributed to two factors: (1) fine-tuning&#8211;induced erosion of reasoning and generation capabilities, with an average drop of 8.79%; and (2) suboptimal speech&#8211;text alignment, with an average drop of 16.46%. Given the variety of model sizes and tuning methods evaluated, this trend appears general. This phenomenon indicates that the observed performance degradation stems primarily from the speech&#8211;text modality gap, and that bridging this gap is crucial to enhance LSLM speech processing.\nIndeed, this modality gap is not unique to our setup, as we observe a similar trend across diverse public LSLMs in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.SS1\" title=\"B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "under",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> speech-text query pairs <math alttext=\"\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}</annotation></semantics></math>, where <math alttext=\"x_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{s}</annotation></semantics></math> denotes the speech input and <math alttext=\"x_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{t}</annotation></semantics></math> its corresponding text transcription, we process each sample through the model as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F3\" title=\"Figure 3 &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "speech",
                    "each",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the speech modality, the input <math alttext=\"x^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">x^{s}</annotation></semantics></math> is encoded by the speech encoder and linear projector, resulting in an initial embedding sequence\n<math alttext=\"h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math> is the number of speech frames and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimension. This sequence, along with a system prompt, is fed into an <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>-layer model, yielding layer-wise representations\n<math alttext=\"h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>l</mi><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"l\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in\\{1,\\ldots,L\\}</annotation></semantics></math> indexes the model layer. Similarly, for the text modality, the input <math alttext=\"x^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">x^{t}</annotation></semantics></math> is tokenized and embedded, producing\n<math alttext=\"h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the length of the token sequence. The corresponding layer-wise representations are\n<math alttext=\"h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "length",
                    "speech",
                    "text",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the relationship between speech and text representations, we employ two similarity metrics, denoted in a unified manner as <math alttext=\"f^{(\\cdot)}(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f^{(\\cdot)}(x,y)</annotation></semantics></math>, where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> indicates the choice of metric (<math alttext=\"\\mathrm{cos}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>cos</mi><annotation encoding=\"application/x-tex\">\\mathrm{cos}</annotation></semantics></math>: cosine similarity, <math alttext=\"\\mathrm{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">d</mi><annotation encoding=\"application/x-tex\">\\mathrm{d}</annotation></semantics></math>: Euclidean distance):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{l,i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,i}^{s}</annotation></semantics></math> and <math alttext=\"h_{l,j}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>j</mi></mrow><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,j}^{t}</annotation></semantics></math> denote the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame or token embedding at layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> for the speech and text modalities, respectively. The global relationship between modalities at each layer is then assessed by computing <math alttext=\"f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>cos</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math> and <math alttext=\"f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">d</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "under",
                    "each",
                    "results",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several notable patterns are observed in the similarity dynamics.\nFor <span class=\"ltx_text ltx_font_bold\">cosine similarity</span>, all models demonstrate a consistent increase as the network depth grows, indicating progressively stronger alignment between speech and text representations in deeper layers.\nMoreover, later training checkpoints consistently yield higher similarity scores across all layers, reflecting improved cross-modal alignment as training advances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These trends suggest that deeper layers and extended training foster improved alignment in representational direction (cosine similarity), while preserving modality-specific distinctions in magnitude (Euclidean distance).\nThis alignment pattern could facilitate effective multimodal integration while preserving essential characteristics of each modality.</p>\n\n",
                "matched_terms": [
                    "each",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the relationship between representation similarity and downstream performance, we compute a scalar similarity score for each model by averaging similarity across layers:</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify the modality gap as the drop in benchmark scores between text and speech inputs, as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">M^{t}</annotation></semantics></math> and <math alttext=\"M^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">M^{s}</annotation></semantics></math> are overall benchmark scores obtained from text and speech inputs, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F5\" title=\"Figure 5 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the linear relationship between similarity and\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>.\nEach point corresponds to a model checkpoint, with the <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> value indicating the strength of correlation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "under",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings empirically validate the connection between internal cross-modal representations and performance-level modality gaps. The stronger correlations observed in LoRA-tuned models may stem from the constrained low-rank adaptation, which preserves the integrity of pretrained text representations while facilitating targeted speech-text alignment. In contrast, full fine-tuning grants more representational flexibility, potentially introducing overfitting that weakens this correlation.\nConsequently, representation similarity serves as a more reliable predictor of modality performance under LoRA than under full-parameter fine-tuning.</p>\n\n",
                "matched_terms": [
                    "full",
                    "alignment",
                    "under",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the coarse-grained sequence-level analysis in the previous section, this section focuses on token-level alignment patterns, examining <span class=\"ltx_text ltx_font_bold\">the role and contribution of each token in modality alignment</span>. We will begin with case studies, and subsequently introduce more detailed quantitative metrics to facilitate a finer-grained investigation. Through correlation analysis and intervention experiments, we explore the relationship between token-level alignment and downstream task performance, thereby further revealing the underlying speech-text alignment mechanisms in LSLMs.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "alignment",
                    "tokenlevel",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f^{(\\cdot)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(\\cdot)}</annotation></semantics></math> denotes the selected similarity metric.\nAcross all models and training paradigms, we consistently observed that <span class=\"ltx_text ltx_font_bold\">the token-wise similarity matrix typically exhibits extreme values along a nearly monotonic path</span>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F6\" title=\"Figure 6 &#8227; Observations &#8227; 5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, with the increase in text token index, there is a monotonic alignment path in the speech frame sequence along which the similarity (or distance) values are locally maximized (or minimized). This monotonic path does not strictly align with the main diagonal, but reflects the actual temporal alignment structure between speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "all",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically quantify this alignment pattern, for each text token <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>, we identify the index of the speech frame with maximal similarity or minimal distance as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "each",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process produces an alignment path between the text and speech sequences. To verify the presence of monotonicity in these alignments, we use the Spearman rank correlation coefficient between text token indices and their aligned speech frame indices as the evaluation metric. Detailed statistics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. At the final training epoch across all models, the average Spearman coefficient is 0.85 for cosine similarity and 0.70 for Euclidean distance. The proportion of tokens with perfectly identical alignment paths under both similarity measures is 0.59, indicating substantial consistency in the alignment results.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "all",
                    "under",
                    "results",
                    "speech",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The widespread emergence of this monotonic pattern suggests that the model not only aligns modalities globally, but also <span class=\"ltx_text ltx_font_bold\">spontaneously learns a soft, monotonic alignment between speech frames and text tokens at the token level</span>. Importantly, this alignment pattern emerges automatically in end-to-end speech-text alignment tasks, reflecting the model&#8217;s ability to capture and map the temporal structure of speech to the semantic structure of text in a robust manner.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "only",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the observed token-level alignment patterns, we propose the <span class=\"ltx_text ltx_font_bold\">Alignment Path Score</span> (APS) to quantify the strength of speech-text alignment at the token level. Specifically, APS is defined as:</p>\n\n",
                "matched_terms": [
                    "tokenlevel",
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the number of layers, <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the number of text tokens, and <math alttext=\"[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>A</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow><mrow><msubsup><mi>i</mi><mi>j</mi><mo>&#8727;</mo></msubsup><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}</annotation></semantics></math> represents the maximal similarity (or minimal distance) along the alignment path for each token.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "each",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically evaluate the relationship between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> defined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS3\" title=\"4.3 Correlation Between Representation Similarity and Modality Gap &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> on the LSLMs using a linear regression analysis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F7\" title=\"Figure 7 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LoRA-trained LSLMs yield higher <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values for both cosine (<math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math> vs. <math alttext=\"0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mn>0.75</mn><annotation encoding=\"application/x-tex\">0.75</annotation></semantics></math>) and Euclidean APS (<math alttext=\"0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mn>0.72</mn><annotation encoding=\"application/x-tex\">0.72</annotation></semantics></math> vs. <math alttext=\"0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mn>0.64</mn><annotation encoding=\"application/x-tex\">0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span>; <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math> vs. <math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>) compared to previous baselines, indicating stronger linear correlations with <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>. Under full-parameter finetuning, the correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> is similar to that of sequence-level metrics, with both showing low <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m11\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values. As previously suggested, the greater training noise and instability in this setting may limit the explanatory power of both sequence-level and token-level alignment metrics.</p>\n\n",
                "matched_terms": [
                    "tokenlevel",
                    "under",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results suggest that APS offers a more direct and sensitive measurement of the relationship between alignment quality and downstream performance. The stronger correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> highlights that fine-grained, token-level alignment is the key mechanism underlying LSLM speech understanding.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "tokenlevel",
                    "results",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in the previous section, our analyses revealed a strong correlation between the token-level alignment score (APS) and the modality gap in model performance (<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>). However, correlation does not necessarily imply causation. To further investigate whether the token-level alignment mechanism causally affects the speech understanding ability of LSLMs, we conducted a series of targeted intervention experiments.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "alignment",
                    "model",
                    "tokenlevel",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "bottom3",
                    "length",
                    "each",
                    "projection",
                    "intervention",
                    "path",
                    "corresponding",
                    "sdqa",
                    "llama318b",
                    "text",
                    "lora",
                    "alignment",
                    "angle",
                    "under",
                    "speech",
                    "normalization",
                    "tokens",
                    "qwen257b",
                    "three",
                    "all",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further case analysis shows that angular or length-based interventions on speech tokens can correct cases where the model fails on speech input but succeeds on the corresponding text. These corrections fall into two categories: (1) resolving semantic misunderstandings from misinterpreting spoken content, and (2) rectifying factual errors despite correct semantic parsing. Representative examples for both are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS4\" title=\"A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, highlighting the potential of token-level interventions to improve linguistic comprehension and factual consistency for spoken queries.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "tokenlevel",
                    "speech",
                    "input",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "path",
                    "alignment",
                    "model",
                    "tokenlevel",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalizability of Findings.</span>\nOur main experiments focus on a specific set of architectures and alignment frameworks. Although Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2\" title=\"Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> reports consistent phenomena across external LSLMs, broader validation on larger models and fundamentally novel alignment strategies remains necessary.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-hoc Nature of Interventions.</span>\nOur intervention strategies are applied post hoc at inference time and serve primarily as analytical probes of token-level alignment. An important direction is to integrate these insights into training to explicitly optimize cross-modal consistency and improve speech-input performance.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "alignment",
                    "strategies",
                    "tokenlevel",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T2\" title=\"Table 2 &#8227; A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the evaluation results of our models under different training paradigms and checkpoints. For each model and training strategy, we report the performance on both speech input and text input across multiple benchmark subsets, as well as their respective overall scores. Additionally, we provide the <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> metric, defined as the difference between the overall text input and speech input performance. This comprehensive comparison allows us to assess the alignment and robustness of various models and training approaches with respect to both input modalities.</p>\n\n",
                "matched_terms": [
                    "different",
                    "alignment",
                    "model",
                    "strategy",
                    "under",
                    "each",
                    "speech",
                    "results",
                    "input",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a comprehensive comparison with the end-to-end LSLMs analyzed in this work, we report the performance of traditional ASR+LLM pipeline systems. These systems utilize <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> as the ASR module, paired with the corresponding LLM backbones from our main experiments. All pipeline evaluations were conducted on the identical 4,947-sample VoiceBench test set and adhere strictly to the official protocol to ensure a fair comparison. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T3\" title=\"Table 3 &#8227; A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "performance",
                    "results",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the measurement of alignment path statistics, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS1\" title=\"5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. We report these statistics across different training stages, model scales, and training strategies. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we consider three metrics: (1) alignment path monotonicity based on the cosine similarity matrix, which reflects the degree of order in the alignment between text tokens and speech frames; (2) alignment path monotonicity based on the Euclidean distance matrix, defined in a similar manner but using Euclidean distances for alignment construction; and (3) token-level alignment path consistency, defined as the proportion of tokens whose aligned speech frame indices are identical under both similarity measures.</p>\n\n",
                "matched_terms": [
                    "path",
                    "different",
                    "alignment",
                    "strategies",
                    "three",
                    "model",
                    "tokenlevel",
                    "under",
                    "speech",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "strategies",
                    "model",
                    "tokenlevel",
                    "results",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents representative cases from the intervention experiments detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, with results compiled in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> through&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The interventions involved two primary strategies: <span class=\"ltx_text ltx_font_bold\">angle</span> projection, which aligns the direction of the speech representation with its corresponding text representation, and <span class=\"ltx_text ltx_font_bold\">length</span> normalization, which matches their vector norms. These strategies were applied either to the three tokens with the lowest alignment confidence (<span class=\"ltx_text ltx_font_bold\">bot3</span>) or to all tokens along the alignment path (<span class=\"ltx_text ltx_font_bold\">all</span>).</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "path",
                    "alignment",
                    "corresponding",
                    "strategies",
                    "three",
                    "normalization",
                    "length",
                    "angle",
                    "all",
                    "results",
                    "speech",
                    "projection",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis of these cases reveals two primary categories of error correction. The first category involves the resolution of semantic misunderstandings arising from the spoken input. For example, in Case 1 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), the model initially misinterprets the spoken entity \"Brittany\" as \"Britain,\" leading to an irrelevant answer. Similarly, Case 2 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T5\" title=\"Table 5 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) shows an erroneous entity recognition from the speech input. After applying interventions to key tokens along the alignment path, the model successfully realigns its semantic representation with the ground-truth text, thereby recovering the correct understanding and generating an accurate response.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "model",
                    "speech",
                    "input",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second category addresses factual errors that occur even when the initial semantic parsing of the spoken query is correct. In Case 3 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T6\" title=\"Table 6 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and Case 4 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>), the model demonstrates a correct understanding of the question&#8217;s topic but fails to produce factually accurate or complete answers. Our interventions, by modifying the representation direction or norm along the alignment path, also prove effective in these scenarios, guiding the model to generate factually correct responses consistent with the reference answers.</p>\n\n",
                "matched_terms": [
                    "path",
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Collectively, these case studies demonstrate that fine-grained interventions on the token alignment path, through either embedding direction or norm modification, consistently improve the model&#8217;s answer accuracy and robustness for spoken inputs. This effect is observed in correcting both semantic misinterpretations and factual knowledge errors, indicating that such interventions can enhance multimodal alignment and enable more reliable knowledge retrieval from speech input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "path",
                    "speech",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "corresponding",
                    "model",
                    "each",
                    "speech",
                    "text",
                    "tokens",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results, presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T8\" title=\"Table 8 &#8227; B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, consistently reveal a significant performance drop for speech inputs across all models. This corroborates our central thesis that the modality gap is a prevalent challenge, independent of the specific LSLM architecture or alignment strategy.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "strategy",
                    "all",
                    "results",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4\" title=\"4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we analyzed the sequence-level similarity dynamics between speech and text representations in <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span>. The results are visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.F9\" title=\"Figure 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The observed layer-wise similarity dynamics, characterized by an increase in cosine similarity and a concurrent upward trend in Euclidean distance with network depth, are highly analogous to the phenomena identified in our primary experiments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further evaluated the token-level alignment path monotonicity for <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> using the three metrics defined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T9\" title=\"Table 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, these results are highly consistent with the values obtained from the models in the primary experiments at their final training stages, detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Such consistency across disparate training paradigms provides strong evidence for the spontaneous emergence of a monotonic alignment path as a generalizable phenomenon.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment",
                    "three",
                    "tokenlevel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we replicated the token-level intervention experiments from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T10\" title=\"Table 10 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, applying angle projection to the least-aligned tokens (<span class=\"ltx_text ltx_font_italic\">Bottom3</span>) successfully improved performance on speech inputs. This result demonstrates that our proposed intervention strategy for mitigating the modality gap is also effective for models trained with a multi-stage approach.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "bottom3",
                    "strategy",
                    "tokenlevel",
                    "sdqa",
                    "leastaligned",
                    "angle",
                    "subset",
                    "speech",
                    "projection",
                    "tokens",
                    "performance"
                ]
            }
        ]
    },
    "A1.T2": {
        "caption": "Table 2: Comparison of Alignment Experiment Results: Speech and Text Input Performance Across Steps",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">Strategy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">Param</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">Steps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\">Speech Input (%, <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\">Text Input (%, <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T2.m3\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">AdvBench</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">IfEval</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">OBQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MMSU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">sd-qa</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Overall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">AdvBench</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">IfEval</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">OBQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MMSU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">sd-qa</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Overall</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"10\">Qwen2.5-1.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"5\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"5\">1.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">77.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">15.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">25.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">26.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">31.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">35.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">96.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">20.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">61.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">42.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">44.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">52.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">17.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">95.77</td>\n<td class=\"ltx_td ltx_align_center\">14.01</td>\n<td class=\"ltx_td ltx_align_center\">44.40</td>\n<td class=\"ltx_td ltx_align_center\">29.99</td>\n<td class=\"ltx_td ltx_align_center\">31.10</td>\n<td class=\"ltx_td ltx_align_center\">43.06</td>\n<td class=\"ltx_td ltx_align_center\">98.85</td>\n<td class=\"ltx_td ltx_align_center\">20.64</td>\n<td class=\"ltx_td ltx_align_center\">69.01</td>\n<td class=\"ltx_td ltx_align_center\">44.99</td>\n<td class=\"ltx_td ltx_align_center\">38.89</td>\n<td class=\"ltx_td ltx_align_center\">54.48</td>\n<td class=\"ltx_td ltx_align_center\">11.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">94.04</td>\n<td class=\"ltx_td ltx_align_center\">12.39</td>\n<td class=\"ltx_td ltx_align_center\">45.93</td>\n<td class=\"ltx_td ltx_align_center\">32.60</td>\n<td class=\"ltx_td ltx_align_center\">29.84</td>\n<td class=\"ltx_td ltx_align_center\">42.96</td>\n<td class=\"ltx_td ltx_align_center\">97.69</td>\n<td class=\"ltx_td ltx_align_center\">18.97</td>\n<td class=\"ltx_td ltx_align_center\">69.45</td>\n<td class=\"ltx_td ltx_align_center\">45.02</td>\n<td class=\"ltx_td ltx_align_center\">39.47</td>\n<td class=\"ltx_td ltx_align_center\">54.12</td>\n<td class=\"ltx_td ltx_align_center\">11.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">95.77</td>\n<td class=\"ltx_td ltx_align_center\">13.67</td>\n<td class=\"ltx_td ltx_align_center\">43.52</td>\n<td class=\"ltx_td ltx_align_center\">31.36</td>\n<td class=\"ltx_td ltx_align_center\">30.02</td>\n<td class=\"ltx_td ltx_align_center\">42.87</td>\n<td class=\"ltx_td ltx_align_center\">98.65</td>\n<td class=\"ltx_td ltx_align_center\">18.44</td>\n<td class=\"ltx_td ltx_align_center\">68.13</td>\n<td class=\"ltx_td ltx_align_center\">45.25</td>\n<td class=\"ltx_td ltx_align_center\">34.45</td>\n<td class=\"ltx_td ltx_align_center\">52.99</td>\n<td class=\"ltx_td ltx_align_center\">10.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">98.46</td>\n<td class=\"ltx_td ltx_align_center\">13.36</td>\n<td class=\"ltx_td ltx_align_center\">49.45</td>\n<td class=\"ltx_td ltx_align_center\">33.12</td>\n<td class=\"ltx_td ltx_align_center\">33.45</td>\n<td class=\"ltx_td ltx_align_center\">45.57</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">18.64</td>\n<td class=\"ltx_td ltx_align_center\">69.89</td>\n<td class=\"ltx_td ltx_align_center\">46.39</td>\n<td class=\"ltx_td ltx_align_center\">40.60</td>\n<td class=\"ltx_td ltx_align_center\">54.91</td>\n<td class=\"ltx_td ltx_align_center\">9.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">9M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">88.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">59.35</td>\n<td class=\"ltx_td ltx_align_center\">13.67</td>\n<td class=\"ltx_td ltx_align_center\">30.99</td>\n<td class=\"ltx_td ltx_align_center\">26.28</td>\n<td class=\"ltx_td ltx_align_center\">27.85</td>\n<td class=\"ltx_td ltx_align_center\">31.63</td>\n<td class=\"ltx_td ltx_align_center\">84.42</td>\n<td class=\"ltx_td ltx_align_center\">24.27</td>\n<td class=\"ltx_td ltx_align_center\">74.51</td>\n<td class=\"ltx_td ltx_align_center\">74.51</td>\n<td class=\"ltx_td ltx_align_center\">39.42</td>\n<td class=\"ltx_td ltx_align_center\">54.93</td>\n<td class=\"ltx_td ltx_align_center\">23.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">80.00</td>\n<td class=\"ltx_td ltx_align_center\">14.22</td>\n<td class=\"ltx_td ltx_align_center\">36.04</td>\n<td class=\"ltx_td ltx_align_center\">27.00</td>\n<td class=\"ltx_td ltx_align_center\">29.48</td>\n<td class=\"ltx_td ltx_align_center\">37.35</td>\n<td class=\"ltx_td ltx_align_center\">88.08</td>\n<td class=\"ltx_td ltx_align_center\">21.90</td>\n<td class=\"ltx_td ltx_align_center\">74.07</td>\n<td class=\"ltx_td ltx_align_center\">51.89</td>\n<td class=\"ltx_td ltx_align_center\">37.97</td>\n<td class=\"ltx_td ltx_align_center\">54.78</td>\n<td class=\"ltx_td ltx_align_center\">17.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">83.08</td>\n<td class=\"ltx_td ltx_align_center\">13.81</td>\n<td class=\"ltx_td ltx_align_center\">36.48</td>\n<td class=\"ltx_td ltx_align_center\">27.94</td>\n<td class=\"ltx_td ltx_align_center\">30.20</td>\n<td class=\"ltx_td ltx_align_center\">38.30</td>\n<td class=\"ltx_td ltx_align_center\">88.46</td>\n<td class=\"ltx_td ltx_align_center\">21.82</td>\n<td class=\"ltx_td ltx_align_center\">74.95</td>\n<td class=\"ltx_td ltx_align_center\">51.76</td>\n<td class=\"ltx_td ltx_align_center\">38.70</td>\n<td class=\"ltx_td ltx_align_center\">55.14</td>\n<td class=\"ltx_td ltx_align_center\">16.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">84.42</td>\n<td class=\"ltx_td ltx_align_center\">13.46</td>\n<td class=\"ltx_td ltx_align_center\">40.00</td>\n<td class=\"ltx_td ltx_align_center\">30.32</td>\n<td class=\"ltx_td ltx_align_center\">30.38</td>\n<td class=\"ltx_td ltx_align_center\">39.72</td>\n<td class=\"ltx_td ltx_align_center\">88.08</td>\n<td class=\"ltx_td ltx_align_center\">21.56</td>\n<td class=\"ltx_td ltx_align_center\">75.38</td>\n<td class=\"ltx_td ltx_align_center\">51.43</td>\n<td class=\"ltx_td ltx_align_center\">40.14</td>\n<td class=\"ltx_td ltx_align_center\">55.32</td>\n<td class=\"ltx_td ltx_align_center\">15.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"10\">Qwen2.5-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">96.54</td>\n<td class=\"ltx_td ltx_align_center\">16.14</td>\n<td class=\"ltx_td ltx_align_center\">54.29</td>\n<td class=\"ltx_td ltx_align_center\">33.96</td>\n<td class=\"ltx_td ltx_align_center\">33.82</td>\n<td class=\"ltx_td ltx_align_center\">46.95</td>\n<td class=\"ltx_td ltx_align_center\">99.62</td>\n<td class=\"ltx_td ltx_align_center\">23.29</td>\n<td class=\"ltx_td ltx_align_center\">78.68</td>\n<td class=\"ltx_td ltx_align_center\">58.36</td>\n<td class=\"ltx_td ltx_align_center\">41.23</td>\n<td class=\"ltx_td ltx_align_center\">60.23</td>\n<td class=\"ltx_td ltx_align_center\">13.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">94.23</td>\n<td class=\"ltx_td ltx_align_center\">18.60</td>\n<td class=\"ltx_td ltx_align_center\">57.80</td>\n<td class=\"ltx_td ltx_align_center\">36.92</td>\n<td class=\"ltx_td ltx_align_center\">36.35</td>\n<td class=\"ltx_td ltx_align_center\">48.78</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">26.55</td>\n<td class=\"ltx_td ltx_align_center\">79.56</td>\n<td class=\"ltx_td ltx_align_center\">59.17</td>\n<td class=\"ltx_td ltx_align_center\">45.03</td>\n<td class=\"ltx_td ltx_align_center\">61.91</td>\n<td class=\"ltx_td ltx_align_center\">13.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">97.31</td>\n<td class=\"ltx_td ltx_align_center\">16.58</td>\n<td class=\"ltx_td ltx_align_center\">58.68</td>\n<td class=\"ltx_td ltx_align_center\">35.88</td>\n<td class=\"ltx_td ltx_align_center\">32.19</td>\n<td class=\"ltx_td ltx_align_center\">48.13</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">23.80</td>\n<td class=\"ltx_td ltx_align_center\">75.16</td>\n<td class=\"ltx_td ltx_align_center\">58.65</td>\n<td class=\"ltx_td ltx_align_center\">41.77</td>\n<td class=\"ltx_td ltx_align_center\">59.69</td>\n<td class=\"ltx_td ltx_align_center\">11.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">98.85</td>\n<td class=\"ltx_td ltx_align_center\">15.84</td>\n<td class=\"ltx_td ltx_align_center\">58.02</td>\n<td class=\"ltx_td ltx_align_center\">35.56</td>\n<td class=\"ltx_td ltx_align_center\">33.45</td>\n<td class=\"ltx_td ltx_align_center\">48.34</td>\n<td class=\"ltx_td ltx_align_center\">100.0</td>\n<td class=\"ltx_td ltx_align_center\">24.39</td>\n<td class=\"ltx_td ltx_align_center\">71.87</td>\n<td class=\"ltx_td ltx_align_center\">56.25</td>\n<td class=\"ltx_td ltx_align_center\">43.58</td>\n<td class=\"ltx_td ltx_align_center\">59.22</td>\n<td class=\"ltx_td ltx_align_center\">10.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">20M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">99.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">92.12</td>\n<td class=\"ltx_td ltx_align_center\">16.92</td>\n<td class=\"ltx_td ltx_align_center\">42.64</td>\n<td class=\"ltx_td ltx_align_center\">30.03</td>\n<td class=\"ltx_td ltx_align_center\">35.62</td>\n<td class=\"ltx_td ltx_align_center\">43.47</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">55.38</td>\n<td class=\"ltx_td ltx_align_center\">88.13</td>\n<td class=\"ltx_td ltx_align_center\">68.41</td>\n<td class=\"ltx_td ltx_align_center\">47.56</td>\n<td class=\"ltx_td ltx_align_center\">71.74</td>\n<td class=\"ltx_td ltx_align_center\">28.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">94.81</td>\n<td class=\"ltx_td ltx_align_center\">22.10</td>\n<td class=\"ltx_td ltx_align_center\">48.13</td>\n<td class=\"ltx_td ltx_align_center\">34.16</td>\n<td class=\"ltx_td ltx_align_center\">37.07</td>\n<td class=\"ltx_td ltx_align_center\">47.25</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">50.79</td>\n<td class=\"ltx_td ltx_align_center\">87.25</td>\n<td class=\"ltx_td ltx_align_center\">67.86</td>\n<td class=\"ltx_td ltx_align_center\">48.10</td>\n<td class=\"ltx_td ltx_align_center\">70.61</td>\n<td class=\"ltx_td ltx_align_center\">23.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">95.19</td>\n<td class=\"ltx_td ltx_align_center\">22.63</td>\n<td class=\"ltx_td ltx_align_center\">49.45</td>\n<td class=\"ltx_td ltx_align_center\">35.56</td>\n<td class=\"ltx_td ltx_align_center\">38.52</td>\n<td class=\"ltx_td ltx_align_center\">48.27</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">49.71</td>\n<td class=\"ltx_td ltx_align_center\">88.13</td>\n<td class=\"ltx_td ltx_align_center\">67.79</td>\n<td class=\"ltx_td ltx_align_center\">48.28</td>\n<td class=\"ltx_td ltx_align_center\">70.59</td>\n<td class=\"ltx_td ltx_align_center\">22.32</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">96.92</td>\n<td class=\"ltx_td ltx_align_center\">25.42</td>\n<td class=\"ltx_td ltx_align_center\">56.26</td>\n<td class=\"ltx_td ltx_align_center\">37.67</td>\n<td class=\"ltx_td ltx_align_center\">38.88</td>\n<td class=\"ltx_td ltx_align_center\">51.03</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">48.19</td>\n<td class=\"ltx_td ltx_align_center\">87.69</td>\n<td class=\"ltx_td ltx_align_center\">67.24</td>\n<td class=\"ltx_td ltx_align_center\">47.92</td>\n<td class=\"ltx_td ltx_align_center\">70.02</td>\n<td class=\"ltx_td ltx_align_center\">18.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"10\">Llama3.2-3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">95.96</td>\n<td class=\"ltx_td ltx_align_center\">12.06</td>\n<td class=\"ltx_td ltx_align_center\">23.52</td>\n<td class=\"ltx_td ltx_align_center\">25.02</td>\n<td class=\"ltx_td ltx_align_center\">31.46</td>\n<td class=\"ltx_td ltx_align_center\">37.60</td>\n<td class=\"ltx_td ltx_align_center\">98.46</td>\n<td class=\"ltx_td ltx_align_center\">21.90</td>\n<td class=\"ltx_td ltx_align_center\">63.96</td>\n<td class=\"ltx_td ltx_align_center\">43.82</td>\n<td class=\"ltx_td ltx_align_center\">40.69</td>\n<td class=\"ltx_td ltx_align_center\">53.76</td>\n<td class=\"ltx_td ltx_align_center\">16.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">12.97</td>\n<td class=\"ltx_td ltx_align_center\">27.47</td>\n<td class=\"ltx_td ltx_align_center\">25.47</td>\n<td class=\"ltx_td ltx_align_center\">34.90</td>\n<td class=\"ltx_td ltx_align_center\">39.97</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">22.43</td>\n<td class=\"ltx_td ltx_align_center\">61.32</td>\n<td class=\"ltx_td ltx_align_center\">42.55</td>\n<td class=\"ltx_td ltx_align_center\">42.68</td>\n<td class=\"ltx_td ltx_align_center\">53.64</td>\n<td class=\"ltx_td ltx_align_center\">13.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">93.85</td>\n<td class=\"ltx_td ltx_align_center\">13.72</td>\n<td class=\"ltx_td ltx_align_center\">24.84</td>\n<td class=\"ltx_td ltx_align_center\">24.98</td>\n<td class=\"ltx_td ltx_align_center\">29.84</td>\n<td class=\"ltx_td ltx_align_center\">37.44</td>\n<td class=\"ltx_td ltx_align_center\">98.65</td>\n<td class=\"ltx_td ltx_align_center\">24.17</td>\n<td class=\"ltx_td ltx_align_center\">62.20</td>\n<td class=\"ltx_td ltx_align_center\">42.88</td>\n<td class=\"ltx_td ltx_align_center\">42.86</td>\n<td class=\"ltx_td ltx_align_center\">54.15</td>\n<td class=\"ltx_td ltx_align_center\">16.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">98.65</td>\n<td class=\"ltx_td ltx_align_center\">11.60</td>\n<td class=\"ltx_td ltx_align_center\">22.64</td>\n<td class=\"ltx_td ltx_align_center\">24.98</td>\n<td class=\"ltx_td ltx_align_center\">32.73</td>\n<td class=\"ltx_td ltx_align_center\">38.12</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">23.88</td>\n<td class=\"ltx_td ltx_align_center\">59.12</td>\n<td class=\"ltx_td ltx_align_center\">40.53</td>\n<td class=\"ltx_td ltx_align_center\">42.13</td>\n<td class=\"ltx_td ltx_align_center\">52.98</td>\n<td class=\"ltx_td ltx_align_center\">14.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">12M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">97.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">58.08</td>\n<td class=\"ltx_td ltx_align_center\">12.18</td>\n<td class=\"ltx_td ltx_align_center\">22.42</td>\n<td class=\"ltx_td ltx_align_center\">23.19</td>\n<td class=\"ltx_td ltx_align_center\">35.62</td>\n<td class=\"ltx_td ltx_align_center\">30.30</td>\n<td class=\"ltx_td ltx_align_center\">98.46</td>\n<td class=\"ltx_td ltx_align_center\">62.64</td>\n<td class=\"ltx_td ltx_align_center\">76.92</td>\n<td class=\"ltx_td ltx_align_center\">55.50</td>\n<td class=\"ltx_td ltx_align_center\">59.13</td>\n<td class=\"ltx_td ltx_align_center\">70.53</td>\n<td class=\"ltx_td ltx_align_center\">40.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">74.42</td>\n<td class=\"ltx_td ltx_align_center\">12.97</td>\n<td class=\"ltx_td ltx_align_center\">20.88</td>\n<td class=\"ltx_td ltx_align_center\">24.79</td>\n<td class=\"ltx_td ltx_align_center\">38.70</td>\n<td class=\"ltx_td ltx_align_center\">34.35</td>\n<td class=\"ltx_td ltx_align_center\">98.08</td>\n<td class=\"ltx_td ltx_align_center\">59.32</td>\n<td class=\"ltx_td ltx_align_center\">75.60</td>\n<td class=\"ltx_td ltx_align_center\">54.52</td>\n<td class=\"ltx_td ltx_align_center\">52.98</td>\n<td class=\"ltx_td ltx_align_center\">68.10</td>\n<td class=\"ltx_td ltx_align_center\">33.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">84.42</td>\n<td class=\"ltx_td ltx_align_center\">13.52</td>\n<td class=\"ltx_td ltx_align_center\">19.12</td>\n<td class=\"ltx_td ltx_align_center\">25.37</td>\n<td class=\"ltx_td ltx_align_center\">38.52</td>\n<td class=\"ltx_td ltx_align_center\">36.19</td>\n<td class=\"ltx_td ltx_align_center\">98.08</td>\n<td class=\"ltx_td ltx_align_center\">55.39</td>\n<td class=\"ltx_td ltx_align_center\">77.14</td>\n<td class=\"ltx_td ltx_align_center\">54.39</td>\n<td class=\"ltx_td ltx_align_center\">50.27</td>\n<td class=\"ltx_td ltx_align_center\">67.05</td>\n<td class=\"ltx_td ltx_align_center\">30.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">79.66</td>\n<td class=\"ltx_td ltx_align_center\">13.28</td>\n<td class=\"ltx_td ltx_align_center\">22.64</td>\n<td class=\"ltx_td ltx_align_center\">24.53</td>\n<td class=\"ltx_td ltx_align_center\">40.33</td>\n<td class=\"ltx_td ltx_align_center\">36.09</td>\n<td class=\"ltx_td ltx_align_center\">97.69</td>\n<td class=\"ltx_td ltx_align_center\">53.25</td>\n<td class=\"ltx_td ltx_align_center\">76.92</td>\n<td class=\"ltx_td ltx_align_center\">54.26</td>\n<td class=\"ltx_td ltx_align_center\">51.54</td>\n<td class=\"ltx_td ltx_align_center\">66.73</td>\n<td class=\"ltx_td ltx_align_center\">30.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"10\">Llama3.1-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">93.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">99.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">14.23</td>\n<td class=\"ltx_td ltx_align_center\">31.43</td>\n<td class=\"ltx_td ltx_align_center\">27.03</td>\n<td class=\"ltx_td ltx_align_center\">36.89</td>\n<td class=\"ltx_td ltx_align_center\">41.76</td>\n<td class=\"ltx_td ltx_align_center\">99.62</td>\n<td class=\"ltx_td ltx_align_center\">18.11</td>\n<td class=\"ltx_td ltx_align_center\">67.91</td>\n<td class=\"ltx_td ltx_align_center\">41.09</td>\n<td class=\"ltx_td ltx_align_center\">43.04</td>\n<td class=\"ltx_td ltx_align_center\">53.95</td>\n<td class=\"ltx_td ltx_align_center\">12.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">97.50</td>\n<td class=\"ltx_td ltx_align_center\">13.42</td>\n<td class=\"ltx_td ltx_align_center\">38.68</td>\n<td class=\"ltx_td ltx_align_center\">28.43</td>\n<td class=\"ltx_td ltx_align_center\">37.61</td>\n<td class=\"ltx_td ltx_align_center\">43.13</td>\n<td class=\"ltx_td ltx_align_center\">99.04</td>\n<td class=\"ltx_td ltx_align_center\">18.71</td>\n<td class=\"ltx_td ltx_align_center\">64.62</td>\n<td class=\"ltx_td ltx_align_center\">43.33</td>\n<td class=\"ltx_td ltx_align_center\">42.13</td>\n<td class=\"ltx_td ltx_align_center\">53.57</td>\n<td class=\"ltx_td ltx_align_center\">10.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">98.85</td>\n<td class=\"ltx_td ltx_align_center\">12.70</td>\n<td class=\"ltx_td ltx_align_center\">33.63</td>\n<td class=\"ltx_td ltx_align_center\">27.26</td>\n<td class=\"ltx_td ltx_align_center\">37.97</td>\n<td class=\"ltx_td ltx_align_center\">42.08</td>\n<td class=\"ltx_td ltx_align_center\">99.81</td>\n<td class=\"ltx_td ltx_align_center\">17.03</td>\n<td class=\"ltx_td ltx_align_center\">58.90</td>\n<td class=\"ltx_td ltx_align_center\">39.66</td>\n<td class=\"ltx_td ltx_align_center\">43.58</td>\n<td class=\"ltx_td ltx_align_center\">51.79</td>\n<td class=\"ltx_td ltx_align_center\">9.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">10,000</td>\n<td class=\"ltx_td ltx_align_center\">99.23</td>\n<td class=\"ltx_td ltx_align_center\">13.94</td>\n<td class=\"ltx_td ltx_align_center\">44.18</td>\n<td class=\"ltx_td ltx_align_center\">28.59</td>\n<td class=\"ltx_td ltx_align_center\">36.53</td>\n<td class=\"ltx_td ltx_align_center\">44.49</td>\n<td class=\"ltx_td ltx_align_center\">99.42</td>\n<td class=\"ltx_td ltx_align_center\">17.91</td>\n<td class=\"ltx_td ltx_align_center\">67.03</td>\n<td class=\"ltx_td ltx_align_center\">42.39</td>\n<td class=\"ltx_td ltx_align_center\">41.05</td>\n<td class=\"ltx_td ltx_align_center\">53.56</td>\n<td class=\"ltx_td ltx_align_center\">9.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"5\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"5\">20M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">85.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">90.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">81.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">4,000</td>\n<td class=\"ltx_td ltx_align_center\">94.23</td>\n<td class=\"ltx_td ltx_align_center\">17.43</td>\n<td class=\"ltx_td ltx_align_center\">36.04</td>\n<td class=\"ltx_td ltx_align_center\">30.68</td>\n<td class=\"ltx_td ltx_align_center\">43.94</td>\n<td class=\"ltx_td ltx_align_center\">44.47</td>\n<td class=\"ltx_td ltx_align_center\">99.42</td>\n<td class=\"ltx_td ltx_align_center\">63.58</td>\n<td class=\"ltx_td ltx_align_center\">81.54</td>\n<td class=\"ltx_td ltx_align_center\">64.57</td>\n<td class=\"ltx_td ltx_align_center\">54.61</td>\n<td class=\"ltx_td ltx_align_center\">72.75</td>\n<td class=\"ltx_td ltx_align_center\">28.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">6,000</td>\n<td class=\"ltx_td ltx_align_center\">93.46</td>\n<td class=\"ltx_td ltx_align_center\">19.16</td>\n<td class=\"ltx_td ltx_align_center\">41.10</td>\n<td class=\"ltx_td ltx_align_center\">31.10</td>\n<td class=\"ltx_td ltx_align_center\">45.57</td>\n<td class=\"ltx_td ltx_align_center\">46.08</td>\n<td class=\"ltx_td ltx_align_center\">99.42</td>\n<td class=\"ltx_td ltx_align_center\">61.10</td>\n<td class=\"ltx_td ltx_align_center\">81.10</td>\n<td class=\"ltx_td ltx_align_center\">64.18</td>\n<td class=\"ltx_td ltx_align_center\">54.79</td>\n<td class=\"ltx_td ltx_align_center\">72.12</td>\n<td class=\"ltx_td ltx_align_center\">26.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">8,000</td>\n<td class=\"ltx_td ltx_align_center\">97.12</td>\n<td class=\"ltx_td ltx_align_center\">19.36</td>\n<td class=\"ltx_td ltx_align_center\">43.74</td>\n<td class=\"ltx_td ltx_align_center\">32.37</td>\n<td class=\"ltx_td ltx_align_center\">43.58</td>\n<td class=\"ltx_td ltx_align_center\">47.23</td>\n<td class=\"ltx_td ltx_align_center\">99.42</td>\n<td class=\"ltx_td ltx_align_center\">62.75</td>\n<td class=\"ltx_td ltx_align_center\">81.10</td>\n<td class=\"ltx_td ltx_align_center\">64.02</td>\n<td class=\"ltx_td ltx_align_center\">54.61</td>\n<td class=\"ltx_td ltx_align_center\">72.38</td>\n<td class=\"ltx_td ltx_align_center\">25.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">10,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">96.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">21.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">32.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">99.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">59.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">81.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.33</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "15b",
            "full",
            "qwen2515b",
            "strategy",
            "overall",
            "param",
            "advbench",
            "↓downarrow",
            "obqa",
            "comparison",
            "sdqa",
            "mmsu",
            "input",
            "20m",
            "llama318b",
            "text",
            "lora",
            "performance",
            "across",
            "ifeval",
            "alignment",
            "experiment",
            "results",
            "speech",
            "llama323b",
            "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
            "↑uparrow",
            "qwen257b",
            "model",
            "steps",
            "12m"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T2\" title=\"Table 2 &#8227; A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the evaluation results of our models under different training paradigms and checkpoints. For each model and training strategy, we report the performance on both speech input and text input across multiple benchmark subsets, as well as their respective overall scores. Additionally, we provide the <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> metric, defined as the difference between the overall text input and speech input performance. This comprehensive comparison allows us to assess the alignment and robustness of various models and training approaches with respect to both input modalities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "speech",
                    "input",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of Large Speech Language Models (LSLMs) has revolutionized human-computer interaction by enabling direct processing of both speech representations and text inputs, subsequently generating textual or spoken outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib1\" title=\"\">2024</a>)</cite>.\nCompared to traditional pipeline architectures that sequentially chain Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-To-Speech (TTS) components, end-to-end LSLMs offer significant advantages, including reduced latency, inherent error resilience, and more expressive speech synthesis capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies in LSLMs have focused on aligning speech modalities with text space through speech tokenizers <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> or encoder-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>); Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib4\" title=\"\">2023</a>)</cite>. These cross-modal alignment strategies aim to harness the linguistic capabilities of pretrained LLMs while integrating speech processing functionalities <cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, significant performance disparities persist between LSLMs and conventional pipeline models in semantic understanding tasks. Benchmark results from VoiceBench <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite> reveal a striking contrast: the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + LLaMA-3.1-8B</span> pipeline achieves 79.06 overall score, while its LSLM counterpart <span class=\"ltx_text ltx_font_italic\">LLaMA-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>)</cite> scores merely 37.51. This pattern continues in Uro-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib29\" title=\"\">2025</a>)</cite> evaluations, where the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + Qwen2-7B-Instruct</span> pipeline attains 78.13 overall score compared to <span class=\"ltx_text ltx_font_italic\">Freeze-Omni</span>&#8217;s <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib28\" title=\"\">2024b</a>)</cite> 48.28, despite both systems employing the same underlying LLM. Notably, while Uro-bench&#8217;s dependence on transcribed speech outputs might inherently favor pipeline architectures, the magnitude of these performance drops remains substantial and warrants investigation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "results",
                    "speech",
                    "llama318b",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we take the persistent performance gap between end-to-end LSLMs and traditional ASR+LLM pipeline systems as our starting point. We systematically reproduce and quantify this discrepancy across various LLM backbones and training strategies, and, for the first time, empirically reveal the underlying mechanisms behind the performance difference. Specifically, after speech-text alignment training, a clear and consistent performance gap exists between text and speech inputs within the same model.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight into the modality gap, we systematically analyze the similarity between speech and text representations at both sequence and token levels, aiming to reveal the mechanisms of speech-text alignment within LSLMs. At the sequence (coarse-grained) level, we observe that as representations propagate through deeper layers of the model, their cosine similarity increases steadily, reflecting progressive directional alignment. In parallel, the Euclidean distance between modalities also increases, indicating a divergence in magnitude that likely reflects modality-specific characteristics learned by the model. At the token (fine-grained) level, we find that the model develops a spontaneous monotonic alignment pattern between speech and text tokens, indicating consistent local correspondence across modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, our study systematically examines the relationship between internal representation similarity and the modality gap exhibited on evaluation benchmarks. A clear linear correlation is observed at both the sequence and token levels, suggesting that the nature of internal cross-modal alignment is closely related to the performance disparity between speech and text inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "speech",
                    "performance",
                    "sdqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are threefold: (1) We systematically identify that the primary source of the performance gap between LSLMs and pipeline systems lies in the modality difference between speech and text inputs. (2) We analyze internal representations and find that the modality gap is closely linked to the similarity between speech and text representations at both sequence and token levels. (3) We provide the first empirical evidence that targeted interventions on speech representations can improve speech input performance on challenging cases.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By focusing on understanding and revealing the mechanisms behind modality alignment, our work offers a deeper exploration of the factors influencing LSLM performance. This approach not only addresses the current performance discrepancy but also paves the way for future advancements in integrating speech modalities into LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The alignment between speech and textual modalities is crucial for the performance of LSLMs. Recent studies have explored five distinct methodologies for this task <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>. The latent space mapping approach, exemplified by <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SALMONN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib24\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">VITA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib11\" title=\"\">2024</a>)</cite>, uses a joint audio encoder-adapter architecture to directly project speech inputs into the LLM&#8217;s latent textual space. This paradigm effectively reduces computational overhead by compressing the audio sequence length via the audio adapter module. Meanwhile, it also preserves the LLM&#8217;s inherent reasoning capabilities and has demonstrated competitive performance across multiple benchmarks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> adopts modality chaining by discretizing speech into symbolic units and expanding the LLM&#8217;s vocabulary, while <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite> and <span class=\"ltx_text ltx_font_italic\">Moshi</span> <cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib7\" title=\"\">2024</a>)</cite> utilize interleaved text-speech tokens and parallel generation architectures, respectively. <span class=\"ltx_text ltx_font_italic\">SyncLLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Veluri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib26\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">IntrinsicVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib34\" title=\"\">2024b</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Align-SLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib18\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">OmniFlatten</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib33\" title=\"\">2024a</a>)</cite> pioneers direct speech-to-speech interaction without textual intermediates. Although significant progress has been made with these methodologies in existing research, their performance on audio processing tasks remains suboptimal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has systematically analyzed the modality gap phenomenon and shown that it persists across a wide range of multimodal models <cite class=\"ltx_cite ltx_citemacro_cite\">Liang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib17\" title=\"\">2022</a>)</cite>. This gap largely arises from the cone effect, where embeddings from different modalities are restricted to distinct subspaces, leading to misalignment and degraded cross-modal performance.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech translation, the modality gap has also been a subject of investigation. Evidence suggests that this gap can emerge during the early phases of fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib12\" title=\"\">2023</a>)</cite>. Furthermore, it has been shown that the resulting misalignment between modalities leads to divergent predictions and degrades performance relative to text-only machine translation systems. This representational divergence was empirically quantified using the cosine similarity between speech and text embeddings, confirming a substantial gap <cite class=\"ltx_cite ltx_citemacro_cite\">Fang and Feng (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib9\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of comparing high-dimensional representations, Centered Kernel Alignment (CKA) was introduced as a robust similarity measure <cite class=\"ltx_cite ltx_citemacro_cite\">Kornblith et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib16\" title=\"\">2019</a>)</cite>. Subsequent work has shown that a simple, sample-wise cosine similarity can also effectively capture layer-wise similarity in transformer models, yielding results comparable to CKA with greater computational efficiency <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, the Wasserstein distance between paired speech and text embeddings has been used to measure cross-modal consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib19\" title=\"\">2025a</a>)</cite>. The Gramian Representation Alignment Measure (GRAM) is also designed to evaluate the alignment of multiple modalities simultaneously <cite class=\"ltx_cite ltx_citemacro_cite\">Cicchetti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib5\" title=\"\">2024</a>)</cite>.\nBoth methods have been integrated into training and effectively improve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates the performance degradation of LSLMs in processing speech inputs compared to their base models&#8217; performance on text inputs. Through comprehensive experiments conducted on multiple LLM backbones using both full-parameter and LoRA fine-tuning methods <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib13\" title=\"\">2021</a>)</cite>, we find that the primary contributor to this modality gap is the suboptimal alignment between textual and auditory modalities in LSLMs.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the architecture of the LSLM setup used in this study comprises three core components: a speech encoder, a speech adapter, and an LLM backbone.\nThe speech signal is first encoded by the speech encoder into a latent representation and then compressed by the speech adapter by a factor of <span class=\"ltx_text ltx_font_italic\">5</span> to reduce computational overhead.\nMeanwhile, text inputs are processed through a standard tokenization pipeline and embedded via the LLM&#8217;s embedding layer.\nThese speech and text embeddings are then concatenated to form a unified multimodal sequence that serves as input to the LLM backbone, enabling autoregressive generation of textual responses.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span>, a widely used ASR model, serves as the speech encoder. The speech adapter is implemented as a lightweight module with two fully connected layers. We conducted experiments with various LLM backbones, including <span class=\"ltx_text ltx_font_italic\">LLaMA3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">LLaMA3.1-8B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B-Instruct</span>, and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B-Instruct</span>. For brevity, henceforth we omit the suffix \"Instruct\" when referring to these model variants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset is constructed following the framework of <span class=\"ltx_text ltx_font_italic\">Ke-Speech-Chat</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>)</cite>, exclusively comprising single-turn dialogue samples. Each sample includes both speech and text instructions, as well as a text response. We refined the raw text using <span class=\"ltx_text ltx_font_italic\">Qwen2.5-72B-Instruct</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib30\" title=\"\">2024</a>); Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib25\" title=\"\">2024</a>)</cite>, aligning it with natural conversational patterns observed in real-world scenarios. Subsequently, a three-stage filtering mechanism was applied to purify the data, targeting safety, semantic clarity, and linguistic naturalness. The speech instruction-response pairs were synthesized using <span class=\"ltx_text ltx_font_italic\">CosyVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib8\" title=\"\">2024</a>)</cite>. Based on automated transcription via <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib22\" title=\"\">2022</a>)</cite>, speech samples exceeding a Word Error Rate (WER) threshold of 0.1 are discarded. Finally, Our training dataset contains 637,283 samples, with speech instructions totaling 1,604 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All LSLMs are trained for 2 epochs on our training dataset using the AdamW optimizer with a peak learning rate of 2e-5. For LoRA, we set <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, <math alttext=\"\\alpha=4.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>4.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=4.0</annotation></semantics></math>, and the dropout rate to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. All experiments were conducted on a distributed setup with 2 nodes, each equipped with 8 NVIDIA A100 GPUs. Training a single model requires approximately 384 GPU hours on this setup.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we adopt five subsets of the <span class=\"ltx_text ltx_font_italic\">VoiceBench</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8212;<span class=\"ltx_text ltx_font_typewriter\">AdvBench</span>, <span class=\"ltx_text ltx_font_typewriter\">IFEval</span>, <span class=\"ltx_text ltx_font_typewriter\">OBQA</span>, <span class=\"ltx_text ltx_font_typewriter\">MMSU</span>, and <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span>&#8212;yielding a total of 4,947 test samples after filtering.\nThese subsets collectively cover 93% of the full VoiceBench and are particularly suitable for robust evaluation as their metrics do not require additional LLMs, thereby minimizing variability.\nAll evaluations strictly adhere to the official VoiceBench evaluation protocol to ensure consistency and reproducibility.\n</p>\n\n",
                "matched_terms": [
                    "obqa",
                    "ifeval",
                    "full",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of LSLMs after full-parameter and LoRA fine-tuning on the 4 models introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.SS1\" title=\"3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, each tested under both speech and text modalities. Detailed results are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS1\" title=\"A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS2\" title=\"A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the performance of each model at epoch 2 on both text and speech inputs, alongside the corresponding base model&#8217;s text-only performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the data shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on average, LSLMs exhibit a 25% performance decline on speech inputs relative to their base models on text. This decline can be attributed to two factors: (1) fine-tuning&#8211;induced erosion of reasoning and generation capabilities, with an average drop of 8.79%; and (2) suboptimal speech&#8211;text alignment, with an average drop of 16.46%. Given the variety of model sizes and tuning methods evaluated, this trend appears general. This phenomenon indicates that the observed performance degradation stems primarily from the speech&#8211;text modality gap, and that bridging this gap is crucial to enhance LSLM speech processing.\nIndeed, this modality gap is not unique to our setup, as we observe a similar trend across diverse public LSLMs in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.SS1\" title=\"B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "speech",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> speech-text query pairs <math alttext=\"\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}</annotation></semantics></math>, where <math alttext=\"x_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{s}</annotation></semantics></math> denotes the speech input and <math alttext=\"x_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{t}</annotation></semantics></math> its corresponding text transcription, we process each sample through the model as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F3\" title=\"Figure 3 &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the speech modality, the input <math alttext=\"x^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">x^{s}</annotation></semantics></math> is encoded by the speech encoder and linear projector, resulting in an initial embedding sequence\n<math alttext=\"h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math> is the number of speech frames and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimension. This sequence, along with a system prompt, is fed into an <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>-layer model, yielding layer-wise representations\n<math alttext=\"h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>l</mi><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"l\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in\\{1,\\ldots,L\\}</annotation></semantics></math> indexes the model layer. Similarly, for the text modality, the input <math alttext=\"x^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">x^{t}</annotation></semantics></math> is tokenized and embedded, producing\n<math alttext=\"h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the length of the token sequence. The corresponding layer-wise representations are\n<math alttext=\"h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the relationship between speech and text representations, we employ two similarity metrics, denoted in a unified manner as <math alttext=\"f^{(\\cdot)}(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f^{(\\cdot)}(x,y)</annotation></semantics></math>, where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> indicates the choice of metric (<math alttext=\"\\mathrm{cos}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>cos</mi><annotation encoding=\"application/x-tex\">\\mathrm{cos}</annotation></semantics></math>: cosine similarity, <math alttext=\"\\mathrm{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">d</mi><annotation encoding=\"application/x-tex\">\\mathrm{d}</annotation></semantics></math>: Euclidean distance):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{l,i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,i}^{s}</annotation></semantics></math> and <math alttext=\"h_{l,j}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>j</mi></mrow><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,j}^{t}</annotation></semantics></math> denote the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame or token embedding at layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> for the speech and text modalities, respectively. The global relationship between modalities at each layer is then assessed by computing <math alttext=\"f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>cos</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math> and <math alttext=\"f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">d</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several notable patterns are observed in the similarity dynamics.\nFor <span class=\"ltx_text ltx_font_bold\">cosine similarity</span>, all models demonstrate a consistent increase as the network depth grows, indicating progressively stronger alignment between speech and text representations in deeper layers.\nMoreover, later training checkpoints consistently yield higher similarity scores across all layers, reflecting improved cross-modal alignment as training advances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the relationship between representation similarity and downstream performance, we compute a scalar similarity score for each model by averaging similarity across layers:</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify the modality gap as the drop in benchmark scores between text and speech inputs, as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">M^{t}</annotation></semantics></math> and <math alttext=\"M^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">M^{s}</annotation></semantics></math> are overall benchmark scores obtained from text and speech inputs, respectively.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F5\" title=\"Figure 5 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the linear relationship between similarity and\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>.\nEach point corresponds to a model checkpoint, with the <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> value indicating the strength of correlation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "overall",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings empirically validate the connection between internal cross-modal representations and performance-level modality gaps. The stronger correlations observed in LoRA-tuned models may stem from the constrained low-rank adaptation, which preserves the integrity of pretrained text representations while facilitating targeted speech-text alignment. In contrast, full fine-tuning grants more representational flexibility, potentially introducing overfitting that weakens this correlation.\nConsequently, representation similarity serves as a more reliable predictor of modality performance under LoRA than under full-parameter fine-tuning.</p>\n\n",
                "matched_terms": [
                    "full",
                    "alignment",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the coarse-grained sequence-level analysis in the previous section, this section focuses on token-level alignment patterns, examining <span class=\"ltx_text ltx_font_bold\">the role and contribution of each token in modality alignment</span>. We will begin with case studies, and subsequently introduce more detailed quantitative metrics to facilitate a finer-grained investigation. Through correlation analysis and intervention experiments, we explore the relationship between token-level alignment and downstream task performance, thereby further revealing the underlying speech-text alignment mechanisms in LSLMs.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f^{(\\cdot)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(\\cdot)}</annotation></semantics></math> denotes the selected similarity metric.\nAcross all models and training paradigms, we consistently observed that <span class=\"ltx_text ltx_font_bold\">the token-wise similarity matrix typically exhibits extreme values along a nearly monotonic path</span>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F6\" title=\"Figure 6 &#8227; Observations &#8227; 5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, with the increase in text token index, there is a monotonic alignment path in the speech frame sequence along which the similarity (or distance) values are locally maximized (or minimized). This monotonic path does not strictly align with the main diagonal, but reflects the actual temporal alignment structure between speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically quantify this alignment pattern, for each text token <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>, we identify the index of the speech frame with maximal similarity or minimal distance as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process produces an alignment path between the text and speech sequences. To verify the presence of monotonicity in these alignments, we use the Spearman rank correlation coefficient between text token indices and their aligned speech frame indices as the evaluation metric. Detailed statistics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. At the final training epoch across all models, the average Spearman coefficient is 0.85 for cosine similarity and 0.70 for Euclidean distance. The proportion of tokens with perfectly identical alignment paths under both similarity measures is 0.59, indicating substantial consistency in the alignment results.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "results",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The widespread emergence of this monotonic pattern suggests that the model not only aligns modalities globally, but also <span class=\"ltx_text ltx_font_bold\">spontaneously learns a soft, monotonic alignment between speech frames and text tokens at the token level</span>. Importantly, this alignment pattern emerges automatically in end-to-end speech-text alignment tasks, reflecting the model&#8217;s ability to capture and map the temporal structure of speech to the semantic structure of text in a robust manner.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the number of layers, <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the number of text tokens, and <math alttext=\"[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>A</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow><mrow><msubsup><mi>i</mi><mi>j</mi><mo>&#8727;</mo></msubsup><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}</annotation></semantics></math> represents the maximal similarity (or minimal distance) along the alignment path for each token.</p>\n\n",
                "matched_terms": [
                    "text",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically evaluate the relationship between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> defined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS3\" title=\"4.3 Correlation Between Representation Similarity and Modality Gap &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> on the LSLMs using a linear regression analysis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F7\" title=\"Figure 7 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LoRA-trained LSLMs yield higher <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values for both cosine (<math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math> vs. <math alttext=\"0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mn>0.75</mn><annotation encoding=\"application/x-tex\">0.75</annotation></semantics></math>) and Euclidean APS (<math alttext=\"0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mn>0.72</mn><annotation encoding=\"application/x-tex\">0.72</annotation></semantics></math> vs. <math alttext=\"0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mn>0.64</mn><annotation encoding=\"application/x-tex\">0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span>; <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math> vs. <math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>) compared to previous baselines, indicating stronger linear correlations with <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>. Under full-parameter finetuning, the correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> is similar to that of sequence-level metrics, with both showing low <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m11\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values. As previously suggested, the greater training noise and instability in this setting may limit the explanatory power of both sequence-level and token-level alignment metrics.</p>\n\n",
                "matched_terms": [
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results suggest that APS offers a more direct and sensitive measurement of the relationship between alignment quality and downstream performance. The stronger correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> highlights that fine-grained, token-level alignment is the key mechanism underlying LSLM speech understanding.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "results",
                    "speech",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in the previous section, our analyses revealed a strong correlation between the token-level alignment score (APS) and the modality gap in model performance (<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>). However, correlation does not necessarily imply causation. To further investigate whether the token-level alignment mechanism causally affects the speech understanding ability of LSLMs, we conducted a series of targeted intervention experiments.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "qwen257b",
                    "sdqa",
                    "speech",
                    "llama318b",
                    "text",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.T1\" title=\"Table 1 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Angle Projection yields improvements or maintains performance in 6 out of 8 intervention settings, demonstrating that increasing the angular similarity of token-level text and speech representations can enhance downstream outcomes. For LoRA fine-tuned models, applying angle projection to either the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> or <span class=\"ltx_text ltx_font_italic\">All</span> alignment-path tokens consistently improves results. Notably, intervening on only the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> tokens leads to more robust gains, with <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> improving by 7.52% and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> by 4.19%. In contrast, length normalization provides improvement in only one case, with performance declining in the remaining settings, indicating an overall detrimental effect on LSLM&#8217;s speech sequence modeling.</p>\n\n",
                "matched_terms": [
                    "qwen257b",
                    "overall",
                    "results",
                    "speech",
                    "llama318b",
                    "text",
                    "lora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further case analysis shows that angular or length-based interventions on speech tokens can correct cases where the model fails on speech input but succeeds on the corresponding text. These corrections fall into two categories: (1) resolving semantic misunderstandings from misinterpreting spoken content, and (2) rectifying factual errors despite correct semantic parsing. Representative examples for both are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS4\" title=\"A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, highlighting the potential of token-level interventions to improve linguistic comprehension and factual consistency for spoken queries.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalizability of Findings.</span>\nOur main experiments focus on a specific set of architectures and alignment frameworks. Although Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2\" title=\"Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> reports consistent phenomena across external LSLMs, broader validation on larger models and fundamentally novel alignment strategies remains necessary.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-hoc Nature of Interventions.</span>\nOur intervention strategies are applied post hoc at inference time and serve primarily as analytical probes of token-level alignment. An important direction is to integrate these insights into training to explicitly optimize cross-modal consistency and improve speech-input performance.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a comprehensive comparison with the end-to-end LSLMs analyzed in this work, we report the performance of traditional ASR+LLM pipeline systems. These systems utilize <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> as the ASR module, paired with the corresponding LLM backbones from our main experiments. All pipeline evaluations were conducted on the identical 4,947-sample VoiceBench test set and adhere strictly to the official protocol to ensure a fair comparison. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T3\" title=\"Table 3 &#8227; A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the measurement of alignment path statistics, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS1\" title=\"5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. We report these statistics across different training stages, model scales, and training strategies. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we consider three metrics: (1) alignment path monotonicity based on the cosine similarity matrix, which reflects the degree of order in the alignment between text tokens and speech frames; (2) alignment path monotonicity based on the Euclidean distance matrix, defined in a similar manner but using Euclidean distances for alignment construction; and (3) token-level alignment path consistency, defined as the proportion of tokens whose aligned speech frame indices are identical under both similarity measures.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "model",
                    "overall",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents representative cases from the intervention experiments detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, with results compiled in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> through&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The interventions involved two primary strategies: <span class=\"ltx_text ltx_font_bold\">angle</span> projection, which aligns the direction of the speech representation with its corresponding text representation, and <span class=\"ltx_text ltx_font_bold\">length</span> normalization, which matches their vector norms. These strategies were applied either to the three tokens with the lowest alignment confidence (<span class=\"ltx_text ltx_font_bold\">bot3</span>) or to all tokens along the alignment path (<span class=\"ltx_text ltx_font_bold\">all</span>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "alignment",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis of these cases reveals two primary categories of error correction. The first category involves the resolution of semantic misunderstandings arising from the spoken input. For example, in Case 1 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), the model initially misinterprets the spoken entity \"Brittany\" as \"Britain,\" leading to an irrelevant answer. Similarly, Case 2 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T5\" title=\"Table 5 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) shows an erroneous entity recognition from the speech input. After applying interventions to key tokens along the alignment path, the model successfully realigns its semantic representation with the ground-truth text, thereby recovering the correct understanding and generating an accurate response.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second category addresses factual errors that occur even when the initial semantic parsing of the spoken query is correct. In Case 3 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T6\" title=\"Table 6 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and Case 4 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>), the model demonstrates a correct understanding of the question&#8217;s topic but fails to produce factually accurate or complete answers. Our interventions, by modifying the representation direction or norm along the alignment path, also prove effective in these scenarios, guiding the model to generate factually correct responses consistent with the reference answers.</p>\n\n",
                "matched_terms": [
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Collectively, these case studies demonstrate that fine-grained interventions on the token alignment path, through either embedding direction or norm modification, consistently improve the model&#8217;s answer accuracy and robustness for spoken inputs. This effect is observed in correcting both semantic misinterpretations and factual knowledge errors, indicating that such interventions can enhance multimodal alignment and enable more reliable knowledge retrieval from speech input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results, presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T8\" title=\"Table 8 &#8227; B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, consistently reveal a significant performance drop for speech inputs across all models. This corroborates our central thesis that the modality gap is a prevalent challenge, independent of the specific LSLM architecture or alignment strategy.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "strategy",
                    "results",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4\" title=\"4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we analyzed the sequence-level similarity dynamics between speech and text representations in <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span>. The results are visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.F9\" title=\"Figure 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The observed layer-wise similarity dynamics, characterized by an increase in cosine similarity and a concurrent upward trend in Euclidean distance with network depth, are highly analogous to the phenomena identified in our primary experiments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further evaluated the token-level alignment path monotonicity for <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> using the three metrics defined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T9\" title=\"Table 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, these results are highly consistent with the values obtained from the models in the primary experiments at their final training stages, detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Such consistency across disparate training paradigms provides strong evidence for the spontaneous emergence of a monotonic alignment path as a generalizable phenomenon.</p>\n\n",
                "matched_terms": [
                    "across",
                    "alignment",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we replicated the token-level intervention experiments from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T10\" title=\"Table 10 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, applying angle projection to the least-aligned tokens (<span class=\"ltx_text ltx_font_italic\">Bottom3</span>) successfully improved performance on speech inputs. This result demonstrates that our proposed intervention strategy for mitigating the modality gap is also effective for models trained with a multi-stage approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "strategy",
                    "performance",
                    "sdqa"
                ]
            }
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Performance of pipeline baselines on the VoiceBench test set. Each system consists of a Whisper-large-v3 ASR frontend followed by the specified LLM backend.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">AdvBench</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">IfEval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">OBQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MMSU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">sd-qa</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">97.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">41.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">69.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">50.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">49.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 8.0pt;\">61.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_italic\">Llama3.2-3B</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">98.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">69.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">60.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">51.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">49.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">65.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">98.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">70.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">84.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">69.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">63.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 8.0pt;\">77.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\"><span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">98.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">71.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">72.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">62.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">58.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1pt 8.0pt;\">72.39</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "consists",
            "qwen2515b",
            "baselines",
            "overall",
            "each",
            "advbench",
            "voicebench",
            "obqa",
            "followed",
            "llm",
            "sdqa",
            "test",
            "system",
            "mmsu",
            "llama318b",
            "performance",
            "ifeval",
            "frontend",
            "pipeline",
            "asr",
            "llama323b",
            "backend",
            "set",
            "qwen257b",
            "specified",
            "model",
            "whisperlargev3"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For a comprehensive comparison with the end-to-end LSLMs analyzed in this work, we report the performance of traditional ASR+LLM pipeline systems. These systems utilize <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> as the ASR module, paired with the corresponding LLM backbones from our main experiments. All pipeline evaluations were conducted on the identical 4,947-sample VoiceBench test set and adhere strictly to the official protocol to ensure a fair comparison. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T3\" title=\"Table 3 &#8227; A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of Large Speech Language Models (LSLMs) has revolutionized human-computer interaction by enabling direct processing of both speech representations and text inputs, subsequently generating textual or spoken outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib1\" title=\"\">2024</a>)</cite>.\nCompared to traditional pipeline architectures that sequentially chain Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-To-Speech (TTS) components, end-to-end LSLMs offer significant advantages, including reduced latency, inherent error resilience, and more expressive speech synthesis capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, significant performance disparities persist between LSLMs and conventional pipeline models in semantic understanding tasks. Benchmark results from VoiceBench <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite> reveal a striking contrast: the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + LLaMA-3.1-8B</span> pipeline achieves 79.06 overall score, while its LSLM counterpart <span class=\"ltx_text ltx_font_italic\">LLaMA-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>)</cite> scores merely 37.51. This pattern continues in Uro-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib29\" title=\"\">2025</a>)</cite> evaluations, where the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + Qwen2-7B-Instruct</span> pipeline attains 78.13 overall score compared to <span class=\"ltx_text ltx_font_italic\">Freeze-Omni</span>&#8217;s <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib28\" title=\"\">2024b</a>)</cite> 48.28, despite both systems employing the same underlying LLM. Notably, while Uro-bench&#8217;s dependence on transcribed speech outputs might inherently favor pipeline architectures, the magnitude of these performance drops remains substantial and warrants investigation.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "llm",
                    "overall",
                    "whisperlargev3",
                    "llama318b",
                    "voicebench",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we take the persistent performance gap between end-to-end LSLMs and traditional ASR+LLM pipeline systems as our starting point. We systematically reproduce and quantify this discrepancy across various LLM backbones and training strategies, and, for the first time, empirically reveal the underlying mechanisms behind the performance difference. Specifically, after speech-text alignment training, a clear and consistent performance gap exists between text and speech inputs within the same model.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model",
                    "llm",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "voicebench",
                    "performance",
                    "sdqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are threefold: (1) We systematically identify that the primary source of the performance gap between LSLMs and pipeline systems lies in the modality difference between speech and text inputs. (2) We analyze internal representations and find that the modality gap is closely linked to the similarity between speech and text representations at both sequence and token levels. (3) We provide the first empirical evidence that targeted interventions on speech representations can improve speech input performance on challenging cases.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates the performance degradation of LSLMs in processing speech inputs compared to their base models&#8217; performance on text inputs. Through comprehensive experiments conducted on multiple LLM backbones using both full-parameter and LoRA fine-tuning methods <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib13\" title=\"\">2021</a>)</cite>, we find that the primary contributor to this modality gap is the suboptimal alignment between textual and auditory modalities in LSLMs.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the architecture of the LSLM setup used in this study comprises three core components: a speech encoder, a speech adapter, and an LLM backbone.\nThe speech signal is first encoded by the speech encoder into a latent representation and then compressed by the speech adapter by a factor of <span class=\"ltx_text ltx_font_italic\">5</span> to reduce computational overhead.\nMeanwhile, text inputs are processed through a standard tokenization pipeline and embedded via the LLM&#8217;s embedding layer.\nThese speech and text embeddings are then concatenated to form a unified multimodal sequence that serves as input to the LLM backbone, enabling autoregressive generation of textual responses.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span>, a widely used ASR model, serves as the speech encoder. The speech adapter is implemented as a lightweight module with two fully connected layers. We conducted experiments with various LLM backbones, including <span class=\"ltx_text ltx_font_italic\">LLaMA3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">LLaMA3.1-8B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B-Instruct</span>, and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B-Instruct</span>. For brevity, henceforth we omit the suffix \"Instruct\" when referring to these model variants.</p>\n\n",
                "matched_terms": [
                    "whisperlargev3",
                    "model",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset is constructed following the framework of <span class=\"ltx_text ltx_font_italic\">Ke-Speech-Chat</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>)</cite>, exclusively comprising single-turn dialogue samples. Each sample includes both speech and text instructions, as well as a text response. We refined the raw text using <span class=\"ltx_text ltx_font_italic\">Qwen2.5-72B-Instruct</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib30\" title=\"\">2024</a>); Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib25\" title=\"\">2024</a>)</cite>, aligning it with natural conversational patterns observed in real-world scenarios. Subsequently, a three-stage filtering mechanism was applied to purify the data, targeting safety, semantic clarity, and linguistic naturalness. The speech instruction-response pairs were synthesized using <span class=\"ltx_text ltx_font_italic\">CosyVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib8\" title=\"\">2024</a>)</cite>. Based on automated transcription via <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib22\" title=\"\">2022</a>)</cite>, speech samples exceeding a Word Error Rate (WER) threshold of 0.1 are discarded. Finally, Our training dataset contains 637,283 samples, with speech instructions totaling 1,604 hours.</p>\n\n",
                "matched_terms": [
                    "whisperlargev3",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All LSLMs are trained for 2 epochs on our training dataset using the AdamW optimizer with a peak learning rate of 2e-5. For LoRA, we set <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, <math alttext=\"\\alpha=4.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>4.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=4.0</annotation></semantics></math>, and the dropout rate to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. All experiments were conducted on a distributed setup with 2 nodes, each equipped with 8 NVIDIA A100 GPUs. Training a single model requires approximately 384 GPU hours on this setup.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we adopt five subsets of the <span class=\"ltx_text ltx_font_italic\">VoiceBench</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8212;<span class=\"ltx_text ltx_font_typewriter\">AdvBench</span>, <span class=\"ltx_text ltx_font_typewriter\">IFEval</span>, <span class=\"ltx_text ltx_font_typewriter\">OBQA</span>, <span class=\"ltx_text ltx_font_typewriter\">MMSU</span>, and <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span>&#8212;yielding a total of 4,947 test samples after filtering.\nThese subsets collectively cover 93% of the full VoiceBench and are particularly suitable for robust evaluation as their metrics do not require additional LLMs, thereby minimizing variability.\nAll evaluations strictly adhere to the official VoiceBench evaluation protocol to ensure consistency and reproducibility.\n</p>\n\n",
                "matched_terms": [
                    "voicebench",
                    "ifeval",
                    "test",
                    "mmsu",
                    "obqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of LSLMs after full-parameter and LoRA fine-tuning on the 4 models introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.SS1\" title=\"3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, each tested under both speech and text modalities. Detailed results are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS1\" title=\"A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS2\" title=\"A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the performance of each model at epoch 2 on both text and speech inputs, alongside the corresponding base model&#8217;s text-only performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the data shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on average, LSLMs exhibit a 25% performance decline on speech inputs relative to their base models on text. This decline can be attributed to two factors: (1) fine-tuning&#8211;induced erosion of reasoning and generation capabilities, with an average drop of 8.79%; and (2) suboptimal speech&#8211;text alignment, with an average drop of 16.46%. Given the variety of model sizes and tuning methods evaluated, this trend appears general. This phenomenon indicates that the observed performance degradation stems primarily from the speech&#8211;text modality gap, and that bridging this gap is crucial to enhance LSLM speech processing.\nIndeed, this modality gap is not unique to our setup, as we observe a similar trend across diverse public LSLMs in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.SS1\" title=\"B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> speech-text query pairs <math alttext=\"\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}</annotation></semantics></math>, where <math alttext=\"x_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{s}</annotation></semantics></math> denotes the speech input and <math alttext=\"x_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{t}</annotation></semantics></math> its corresponding text transcription, we process each sample through the model as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F3\" title=\"Figure 3 &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the speech modality, the input <math alttext=\"x^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">x^{s}</annotation></semantics></math> is encoded by the speech encoder and linear projector, resulting in an initial embedding sequence\n<math alttext=\"h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math> is the number of speech frames and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimension. This sequence, along with a system prompt, is fed into an <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>-layer model, yielding layer-wise representations\n<math alttext=\"h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>l</mi><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"l\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in\\{1,\\ldots,L\\}</annotation></semantics></math> indexes the model layer. Similarly, for the text modality, the input <math alttext=\"x^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">x^{t}</annotation></semantics></math> is tokenized and embedded, producing\n<math alttext=\"h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the length of the token sequence. The corresponding layer-wise representations are\n<math alttext=\"h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the relationship between representation similarity and downstream performance, we compute a scalar similarity score for each model by averaging similarity across layers:</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F5\" title=\"Figure 5 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the linear relationship between similarity and\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>.\nEach point corresponds to a model checkpoint, with the <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> value indicating the strength of correlation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the coarse-grained sequence-level analysis in the previous section, this section focuses on token-level alignment patterns, examining <span class=\"ltx_text ltx_font_bold\">the role and contribution of each token in modality alignment</span>. We will begin with case studies, and subsequently introduce more detailed quantitative metrics to facilitate a finer-grained investigation. Through correlation analysis and intervention experiments, we explore the relationship between token-level alignment and downstream task performance, thereby further revealing the underlying speech-text alignment mechanisms in LSLMs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in the previous section, our analyses revealed a strong correlation between the token-level alignment score (APS) and the modality gap in model performance (<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>). However, correlation does not necessarily imply causation. To further investigate whether the token-level alignment mechanism causally affects the speech understanding ability of LSLMs, we conducted a series of targeted intervention experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "qwen257b",
                    "sdqa",
                    "each",
                    "llama318b",
                    "voicebench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.T1\" title=\"Table 1 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Angle Projection yields improvements or maintains performance in 6 out of 8 intervention settings, demonstrating that increasing the angular similarity of token-level text and speech representations can enhance downstream outcomes. For LoRA fine-tuned models, applying angle projection to either the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> or <span class=\"ltx_text ltx_font_italic\">All</span> alignment-path tokens consistently improves results. Notably, intervening on only the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> tokens leads to more robust gains, with <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> improving by 7.52% and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> by 4.19%. In contrast, length normalization provides improvement in only one case, with performance declining in the remaining settings, indicating an overall detrimental effect on LSLM&#8217;s speech sequence modeling.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "llama318b",
                    "qwen257b",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T2\" title=\"Table 2 &#8227; A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the evaluation results of our models under different training paradigms and checkpoints. For each model and training strategy, we report the performance on both speech input and text input across multiple benchmark subsets, as well as their respective overall scores. Additionally, we provide the <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> metric, defined as the difference between the overall text input and speech input performance. This comprehensive comparison allows us to assess the alignment and robustness of various models and training approaches with respect to both input modalities.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "each",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "pipeline",
                    "model",
                    "each",
                    "test",
                    "voicebench",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we replicated the token-level intervention experiments from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T10\" title=\"Table 10 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, applying angle projection to the least-aligned tokens (<span class=\"ltx_text ltx_font_italic\">Bottom3</span>) successfully improved performance on speech inputs. This result demonstrates that our proposed intervention strategy for mitigating the modality gap is also effective for models trained with a multi-stage approach.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "sdqa"
                ]
            }
        ]
    },
    "A2.T8": {
        "caption": "Table 8: Performance comparison of various LSLMs on speech and text inputs.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Input Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">AdvBench</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">IfEval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">OBQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MMSU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">sd-qa</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T8.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T8.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SpeechGPT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Speech</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">85.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">18.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">25.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">25.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">26.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">36.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Text</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">87.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">23.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">28.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">25.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">55.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">44.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">BLSP</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Speech</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">18.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">21.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">24.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">49.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">24.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">15.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Text</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">37.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">52.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">37.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">65.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">40.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">GLM-4-Voice</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Speech</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">79.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">24.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">51.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">38.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">53.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">49.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">6.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Text</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">85.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">31.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">60.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">42.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">62.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">56.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Speech</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">97.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">19.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">42.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">35.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">43.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">47.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">13.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Text</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">98.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">28.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">70.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">44.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">63.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">61.19</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "inputs",
            "overall",
            "blsp",
            "glm4voice",
            "advbench",
            "↓downarrow",
            "obqa",
            "various",
            "comparison",
            "qwen2audio",
            "sdqa",
            "mmsu",
            "input",
            "text",
            "performance",
            "lslms",
            "ifeval",
            "speech",
            "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
            "model",
            "speechgpt"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results, presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T8\" title=\"Table 8 &#8227; B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, consistently reveal a significant performance drop for speech inputs across all models. This corroborates our central thesis that the modality gap is a prevalent challenge, independent of the specific LSLM architecture or alignment strategy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "speech",
                    "input",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of Large Speech Language Models (LSLMs) has revolutionized human-computer interaction by enabling direct processing of both speech representations and text inputs, subsequently generating textual or spoken outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib1\" title=\"\">2024</a>)</cite>.\nCompared to traditional pipeline architectures that sequentially chain Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-To-Speech (TTS) components, end-to-end LSLMs offer significant advantages, including reduced latency, inherent error resilience, and more expressive speech synthesis capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "inputs",
                    "lslms",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies in LSLMs have focused on aligning speech modalities with text space through speech tokenizers <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> or encoder-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>); Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib4\" title=\"\">2023</a>)</cite>. These cross-modal alignment strategies aim to harness the linguistic capabilities of pretrained LLMs while integrating speech processing functionalities <cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "lslms",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, significant performance disparities persist between LSLMs and conventional pipeline models in semantic understanding tasks. Benchmark results from VoiceBench <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite> reveal a striking contrast: the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + LLaMA-3.1-8B</span> pipeline achieves 79.06 overall score, while its LSLM counterpart <span class=\"ltx_text ltx_font_italic\">LLaMA-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>)</cite> scores merely 37.51. This pattern continues in Uro-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib29\" title=\"\">2025</a>)</cite> evaluations, where the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + Qwen2-7B-Instruct</span> pipeline attains 78.13 overall score compared to <span class=\"ltx_text ltx_font_italic\">Freeze-Omni</span>&#8217;s <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib28\" title=\"\">2024b</a>)</cite> 48.28, despite both systems employing the same underlying LLM. Notably, while Uro-bench&#8217;s dependence on transcribed speech outputs might inherently favor pipeline architectures, the magnitude of these performance drops remains substantial and warrants investigation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "lslms",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we take the persistent performance gap between end-to-end LSLMs and traditional ASR+LLM pipeline systems as our starting point. We systematically reproduce and quantify this discrepancy across various LLM backbones and training strategies, and, for the first time, empirically reveal the underlying mechanisms behind the performance difference. Specifically, after speech-text alignment training, a clear and consistent performance gap exists between text and speech inputs within the same model.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "model",
                    "speech",
                    "text",
                    "various",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight into the modality gap, we systematically analyze the similarity between speech and text representations at both sequence and token levels, aiming to reveal the mechanisms of speech-text alignment within LSLMs. At the sequence (coarse-grained) level, we observe that as representations propagate through deeper layers of the model, their cosine similarity increases steadily, reflecting progressive directional alignment. In parallel, the Euclidean distance between modalities also increases, indicating a divergence in magnitude that likely reflects modality-specific characteristics learned by the model. At the token (fine-grained) level, we find that the model develops a spontaneous monotonic alignment pattern between speech and text tokens, indicating consistent local correspondence across modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "lslms",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, our study systematically examines the relationship between internal representation similarity and the modality gap exhibited on evaluation benchmarks. A clear linear correlation is observed at both the sequence and token levels, suggesting that the nature of internal cross-modal alignment is closely related to the performance disparity between speech and text inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "inputs",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "sdqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are threefold: (1) We systematically identify that the primary source of the performance gap between LSLMs and pipeline systems lies in the modality difference between speech and text inputs. (2) We analyze internal representations and find that the modality gap is closely linked to the similarity between speech and text representations at both sequence and token levels. (3) We provide the first empirical evidence that targeted interventions on speech representations can improve speech input performance on challenging cases.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "speech",
                    "input",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By focusing on understanding and revealing the mechanisms behind modality alignment, our work offers a deeper exploration of the factors influencing LSLM performance. This approach not only addresses the current performance discrepancy but also paves the way for future advancements in integrating speech modalities into LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The alignment between speech and textual modalities is crucial for the performance of LSLMs. Recent studies have explored five distinct methodologies for this task <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>. The latent space mapping approach, exemplified by <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SALMONN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib24\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">VITA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib11\" title=\"\">2024</a>)</cite>, uses a joint audio encoder-adapter architecture to directly project speech inputs into the LLM&#8217;s latent textual space. This paradigm effectively reduces computational overhead by compressing the audio sequence length via the audio adapter module. Meanwhile, it also preserves the LLM&#8217;s inherent reasoning capabilities and has demonstrated competitive performance across multiple benchmarks.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "lslms",
                    "inputs",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> adopts modality chaining by discretizing speech into symbolic units and expanding the LLM&#8217;s vocabulary, while <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite> and <span class=\"ltx_text ltx_font_italic\">Moshi</span> <cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib7\" title=\"\">2024</a>)</cite> utilize interleaved text-speech tokens and parallel generation architectures, respectively. <span class=\"ltx_text ltx_font_italic\">SyncLLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Veluri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib26\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">IntrinsicVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib34\" title=\"\">2024b</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Align-SLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib18\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">OmniFlatten</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib33\" title=\"\">2024a</a>)</cite> pioneers direct speech-to-speech interaction without textual intermediates. Although significant progress has been made with these methodologies in existing research, their performance on audio processing tasks remains suboptimal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechgpt",
                    "glm4voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech translation, the modality gap has also been a subject of investigation. Evidence suggests that this gap can emerge during the early phases of fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib12\" title=\"\">2023</a>)</cite>. Furthermore, it has been shown that the resulting misalignment between modalities leads to divergent predictions and degrades performance relative to text-only machine translation systems. This representational divergence was empirically quantified using the cosine similarity between speech and text embeddings, confirming a substantial gap <cite class=\"ltx_cite ltx_citemacro_cite\">Fang and Feng (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib9\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, the Wasserstein distance between paired speech and text embeddings has been used to measure cross-modal consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib19\" title=\"\">2025a</a>)</cite>. The Gramian Representation Alignment Measure (GRAM) is also designed to evaluate the alignment of multiple modalities simultaneously <cite class=\"ltx_cite ltx_citemacro_cite\">Cicchetti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib5\" title=\"\">2024</a>)</cite>.\nBoth methods have been integrated into training and effectively improve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates the performance degradation of LSLMs in processing speech inputs compared to their base models&#8217; performance on text inputs. Through comprehensive experiments conducted on multiple LLM backbones using both full-parameter and LoRA fine-tuning methods <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib13\" title=\"\">2021</a>)</cite>, we find that the primary contributor to this modality gap is the suboptimal alignment between textual and auditory modalities in LSLMs.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the architecture of the LSLM setup used in this study comprises three core components: a speech encoder, a speech adapter, and an LLM backbone.\nThe speech signal is first encoded by the speech encoder into a latent representation and then compressed by the speech adapter by a factor of <span class=\"ltx_text ltx_font_italic\">5</span> to reduce computational overhead.\nMeanwhile, text inputs are processed through a standard tokenization pipeline and embedded via the LLM&#8217;s embedding layer.\nThese speech and text embeddings are then concatenated to form a unified multimodal sequence that serves as input to the LLM backbone, enabling autoregressive generation of textual responses.</p>\n\n",
                "matched_terms": [
                    "input",
                    "inputs",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span>, a widely used ASR model, serves as the speech encoder. The speech adapter is implemented as a lightweight module with two fully connected layers. We conducted experiments with various LLM backbones, including <span class=\"ltx_text ltx_font_italic\">LLaMA3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">LLaMA3.1-8B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B-Instruct</span>, and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B-Instruct</span>. For brevity, henceforth we omit the suffix \"Instruct\" when referring to these model variants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset is constructed following the framework of <span class=\"ltx_text ltx_font_italic\">Ke-Speech-Chat</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>)</cite>, exclusively comprising single-turn dialogue samples. Each sample includes both speech and text instructions, as well as a text response. We refined the raw text using <span class=\"ltx_text ltx_font_italic\">Qwen2.5-72B-Instruct</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib30\" title=\"\">2024</a>); Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib25\" title=\"\">2024</a>)</cite>, aligning it with natural conversational patterns observed in real-world scenarios. Subsequently, a three-stage filtering mechanism was applied to purify the data, targeting safety, semantic clarity, and linguistic naturalness. The speech instruction-response pairs were synthesized using <span class=\"ltx_text ltx_font_italic\">CosyVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib8\" title=\"\">2024</a>)</cite>. Based on automated transcription via <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib22\" title=\"\">2022</a>)</cite>, speech samples exceeding a Word Error Rate (WER) threshold of 0.1 are discarded. Finally, Our training dataset contains 637,283 samples, with speech instructions totaling 1,604 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All LSLMs are trained for 2 epochs on our training dataset using the AdamW optimizer with a peak learning rate of 2e-5. For LoRA, we set <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, <math alttext=\"\\alpha=4.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>4.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=4.0</annotation></semantics></math>, and the dropout rate to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. All experiments were conducted on a distributed setup with 2 nodes, each equipped with 8 NVIDIA A100 GPUs. Training a single model requires approximately 384 GPU hours on this setup.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we adopt five subsets of the <span class=\"ltx_text ltx_font_italic\">VoiceBench</span> dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8212;<span class=\"ltx_text ltx_font_typewriter\">AdvBench</span>, <span class=\"ltx_text ltx_font_typewriter\">IFEval</span>, <span class=\"ltx_text ltx_font_typewriter\">OBQA</span>, <span class=\"ltx_text ltx_font_typewriter\">MMSU</span>, and <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span>&#8212;yielding a total of 4,947 test samples after filtering.\nThese subsets collectively cover 93% of the full VoiceBench and are particularly suitable for robust evaluation as their metrics do not require additional LLMs, thereby minimizing variability.\nAll evaluations strictly adhere to the official VoiceBench evaluation protocol to ensure consistency and reproducibility.\n</p>\n\n",
                "matched_terms": [
                    "ifeval",
                    "obqa",
                    "mmsu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of LSLMs after full-parameter and LoRA fine-tuning on the 4 models introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.SS1\" title=\"3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, each tested under both speech and text modalities. Detailed results are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS1\" title=\"A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS2\" title=\"A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the performance of each model at epoch 2 on both text and speech inputs, alongside the corresponding base model&#8217;s text-only performance.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the data shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on average, LSLMs exhibit a 25% performance decline on speech inputs relative to their base models on text. This decline can be attributed to two factors: (1) fine-tuning&#8211;induced erosion of reasoning and generation capabilities, with an average drop of 8.79%; and (2) suboptimal speech&#8211;text alignment, with an average drop of 16.46%. Given the variety of model sizes and tuning methods evaluated, this trend appears general. This phenomenon indicates that the observed performance degradation stems primarily from the speech&#8211;text modality gap, and that bridging this gap is crucial to enhance LSLM speech processing.\nIndeed, this modality gap is not unique to our setup, as we observe a similar trend across diverse public LSLMs in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.SS1\" title=\"B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "various",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> speech-text query pairs <math alttext=\"\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}</annotation></semantics></math>, where <math alttext=\"x_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{s}</annotation></semantics></math> denotes the speech input and <math alttext=\"x_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{t}</annotation></semantics></math> its corresponding text transcription, we process each sample through the model as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F3\" title=\"Figure 3 &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the speech modality, the input <math alttext=\"x^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">x^{s}</annotation></semantics></math> is encoded by the speech encoder and linear projector, resulting in an initial embedding sequence\n<math alttext=\"h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math> is the number of speech frames and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimension. This sequence, along with a system prompt, is fed into an <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>-layer model, yielding layer-wise representations\n<math alttext=\"h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>l</mi><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"l\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in\\{1,\\ldots,L\\}</annotation></semantics></math> indexes the model layer. Similarly, for the text modality, the input <math alttext=\"x^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">x^{t}</annotation></semantics></math> is tokenized and embedded, producing\n<math alttext=\"h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the length of the token sequence. The corresponding layer-wise representations are\n<math alttext=\"h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the relationship between speech and text representations, we employ two similarity metrics, denoted in a unified manner as <math alttext=\"f^{(\\cdot)}(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f^{(\\cdot)}(x,y)</annotation></semantics></math>, where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> indicates the choice of metric (<math alttext=\"\\mathrm{cos}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>cos</mi><annotation encoding=\"application/x-tex\">\\mathrm{cos}</annotation></semantics></math>: cosine similarity, <math alttext=\"\\mathrm{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">d</mi><annotation encoding=\"application/x-tex\">\\mathrm{d}</annotation></semantics></math>: Euclidean distance):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{l,i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,i}^{s}</annotation></semantics></math> and <math alttext=\"h_{l,j}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>j</mi></mrow><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,j}^{t}</annotation></semantics></math> denote the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame or token embedding at layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> for the speech and text modalities, respectively. The global relationship between modalities at each layer is then assessed by computing <math alttext=\"f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>cos</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math> and <math alttext=\"f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">d</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lslms",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several notable patterns are observed in the similarity dynamics.\nFor <span class=\"ltx_text ltx_font_bold\">cosine similarity</span>, all models demonstrate a consistent increase as the network depth grows, indicating progressively stronger alignment between speech and text representations in deeper layers.\nMoreover, later training checkpoints consistently yield higher similarity scores across all layers, reflecting improved cross-modal alignment as training advances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the relationship between representation similarity and downstream performance, we compute a scalar similarity score for each model by averaging similarity across layers:</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify the modality gap as the drop in benchmark scores between text and speech inputs, as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "inputs",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">M^{t}</annotation></semantics></math> and <math alttext=\"M^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">M^{s}</annotation></semantics></math> are overall benchmark scores obtained from text and speech inputs, respectively.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "inputs",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F5\" title=\"Figure 5 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the linear relationship between similarity and\n<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>.\nEach point corresponds to a model checkpoint, with the <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> value indicating the strength of correlation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings empirically validate the connection between internal cross-modal representations and performance-level modality gaps. The stronger correlations observed in LoRA-tuned models may stem from the constrained low-rank adaptation, which preserves the integrity of pretrained text representations while facilitating targeted speech-text alignment. In contrast, full fine-tuning grants more representational flexibility, potentially introducing overfitting that weakens this correlation.\nConsequently, representation similarity serves as a more reliable predictor of modality performance under LoRA than under full-parameter fine-tuning.</p>\n\n",
                "matched_terms": [
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the coarse-grained sequence-level analysis in the previous section, this section focuses on token-level alignment patterns, examining <span class=\"ltx_text ltx_font_bold\">the role and contribution of each token in modality alignment</span>. We will begin with case studies, and subsequently introduce more detailed quantitative metrics to facilitate a finer-grained investigation. Through correlation analysis and intervention experiments, we explore the relationship between token-level alignment and downstream task performance, thereby further revealing the underlying speech-text alignment mechanisms in LSLMs.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f^{(\\cdot)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(\\cdot)}</annotation></semantics></math> denotes the selected similarity metric.\nAcross all models and training paradigms, we consistently observed that <span class=\"ltx_text ltx_font_bold\">the token-wise similarity matrix typically exhibits extreme values along a nearly monotonic path</span>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F6\" title=\"Figure 6 &#8227; Observations &#8227; 5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, with the increase in text token index, there is a monotonic alignment path in the speech frame sequence along which the similarity (or distance) values are locally maximized (or minimized). This monotonic path does not strictly align with the main diagonal, but reflects the actual temporal alignment structure between speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically quantify this alignment pattern, for each text token <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>, we identify the index of the speech frame with maximal similarity or minimal distance as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process produces an alignment path between the text and speech sequences. To verify the presence of monotonicity in these alignments, we use the Spearman rank correlation coefficient between text token indices and their aligned speech frame indices as the evaluation metric. Detailed statistics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. At the final training epoch across all models, the average Spearman coefficient is 0.85 for cosine similarity and 0.70 for Euclidean distance. The proportion of tokens with perfectly identical alignment paths under both similarity measures is 0.59, indicating substantial consistency in the alignment results.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The widespread emergence of this monotonic pattern suggests that the model not only aligns modalities globally, but also <span class=\"ltx_text ltx_font_bold\">spontaneously learns a soft, monotonic alignment between speech frames and text tokens at the token level</span>. Importantly, this alignment pattern emerges automatically in end-to-end speech-text alignment tasks, reflecting the model&#8217;s ability to capture and map the temporal structure of speech to the semantic structure of text in a robust manner.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically evaluate the relationship between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> defined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS3\" title=\"4.3 Correlation Between Representation Similarity and Modality Gap &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> on the LSLMs using a linear regression analysis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F7\" title=\"Figure 7 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LoRA-trained LSLMs yield higher <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values for both cosine (<math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math> vs. <math alttext=\"0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mn>0.75</mn><annotation encoding=\"application/x-tex\">0.75</annotation></semantics></math>) and Euclidean APS (<math alttext=\"0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mn>0.72</mn><annotation encoding=\"application/x-tex\">0.72</annotation></semantics></math> vs. <math alttext=\"0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mn>0.64</mn><annotation encoding=\"application/x-tex\">0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span>; <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math> vs. <math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>) compared to previous baselines, indicating stronger linear correlations with <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>. Under full-parameter finetuning, the correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> is similar to that of sequence-level metrics, with both showing low <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m11\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values. As previously suggested, the greater training noise and instability in this setting may limit the explanatory power of both sequence-level and token-level alignment metrics.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results suggest that APS offers a more direct and sensitive measurement of the relationship between alignment quality and downstream performance. The stronger correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> highlights that fine-grained, token-level alignment is the key mechanism underlying LSLM speech understanding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in the previous section, our analyses revealed a strong correlation between the token-level alignment score (APS) and the modality gap in model performance (<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>). However, correlation does not necessarily imply causation. To further investigate whether the token-level alignment mechanism causally affects the speech understanding ability of LSLMs, we conducted a series of targeted intervention experiments.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "model",
                    "speech",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "sdqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.T1\" title=\"Table 1 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Angle Projection yields improvements or maintains performance in 6 out of 8 intervention settings, demonstrating that increasing the angular similarity of token-level text and speech representations can enhance downstream outcomes. For LoRA fine-tuned models, applying angle projection to either the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> or <span class=\"ltx_text ltx_font_italic\">All</span> alignment-path tokens consistently improves results. Notably, intervening on only the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> tokens leads to more robust gains, with <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> improving by 7.52% and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> by 4.19%. In contrast, length normalization provides improvement in only one case, with performance declining in the remaining settings, indicating an overall detrimental effect on LSLM&#8217;s speech sequence modeling.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further case analysis shows that angular or length-based interventions on speech tokens can correct cases where the model fails on speech input but succeeds on the corresponding text. These corrections fall into two categories: (1) resolving semantic misunderstandings from misinterpreting spoken content, and (2) rectifying factual errors despite correct semantic parsing. Representative examples for both are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS4\" title=\"A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, highlighting the potential of token-level interventions to improve linguistic comprehension and factual consistency for spoken queries.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "inputs",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scope of Data and Tasks.</span>\nOur evaluation centers on English, single-turn dialogue with synthetic speech, which may limit the applicability of the findings to other languages, multi-turn conversations, and noisy real-world inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "inputs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T2\" title=\"Table 2 &#8227; A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the evaluation results of our models under different training paradigms and checkpoints. For each model and training strategy, we report the performance on both speech input and text input across multiple benchmark subsets, as well as their respective overall scores. Additionally, we provide the <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> metric, defined as the difference between the overall text input and speech input performance. This comprehensive comparison allows us to assess the alignment and robustness of various models and training approaches with respect to both input modalities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "overall",
                    "speech",
                    "input",
                    "g​a​pcolorrgb0089843750578125090234375definecolornamedpgfstrokecolorrgb0089843750578125090234375gap",
                    "text",
                    "various",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a comprehensive comparison with the end-to-end LSLMs analyzed in this work, we report the performance of traditional ASR+LLM pipeline systems. These systems utilize <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> as the ASR module, paired with the corresponding LLM backbones from our main experiments. All pipeline evaluations were conducted on the identical 4,947-sample VoiceBench test set and adhere strictly to the official protocol to ensure a fair comparison. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T3\" title=\"Table 3 &#8227; A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "lslms",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the measurement of alignment path statistics, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS1\" title=\"5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. We report these statistics across different training stages, model scales, and training strategies. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we consider three metrics: (1) alignment path monotonicity based on the cosine similarity matrix, which reflects the degree of order in the alignment between text tokens and speech frames; (2) alignment path monotonicity based on the Euclidean distance matrix, defined in a similar manner but using Euclidean distances for alignment construction; and (3) token-level alignment path consistency, defined as the proportion of tokens whose aligned speech frame indices are identical under both similarity measures.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents representative cases from the intervention experiments detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, with results compiled in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> through&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The interventions involved two primary strategies: <span class=\"ltx_text ltx_font_bold\">angle</span> projection, which aligns the direction of the speech representation with its corresponding text representation, and <span class=\"ltx_text ltx_font_bold\">length</span> normalization, which matches their vector norms. These strategies were applied either to the three tokens with the lowest alignment confidence (<span class=\"ltx_text ltx_font_bold\">bot3</span>) or to all tokens along the alignment path (<span class=\"ltx_text ltx_font_bold\">all</span>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis of these cases reveals two primary categories of error correction. The first category involves the resolution of semantic misunderstandings arising from the spoken input. For example, in Case 1 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), the model initially misinterprets the spoken entity \"Brittany\" as \"Britain,\" leading to an irrelevant answer. Similarly, Case 2 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T5\" title=\"Table 5 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) shows an erroneous entity recognition from the speech input. After applying interventions to key tokens along the alignment path, the model successfully realigns its semantic representation with the ground-truth text, thereby recovering the correct understanding and generating an accurate response.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Collectively, these case studies demonstrate that fine-grained interventions on the token alignment path, through either embedding direction or norm modification, consistently improve the model&#8217;s answer accuracy and robustness for spoken inputs. This effect is observed in correcting both semantic misinterpretations and factual knowledge errors, indicating that such interventions can enhance multimodal alignment and enable more reliable knowledge retrieval from speech input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "inputs",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "speechgpt",
                    "lslms",
                    "inputs",
                    "model",
                    "blsp",
                    "glm4voice",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4\" title=\"4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we analyzed the sequence-level similarity dynamics between speech and text representations in <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span>. The results are visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.F9\" title=\"Figure 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The observed layer-wise similarity dynamics, characterized by an increase in cosine similarity and a concurrent upward trend in Euclidean distance with network depth, are highly analogous to the phenomena identified in our primary experiments.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we replicated the token-level intervention experiments from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T10\" title=\"Table 10 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, applying angle projection to the least-aligned tokens (<span class=\"ltx_text ltx_font_italic\">Bottom3</span>) successfully improved performance on speech inputs. This result demonstrates that our proposed intervention strategy for mitigating the modality gap is also effective for models trained with a multi-stage approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "inputs",
                    "performance",
                    "sdqa"
                ]
            }
        ]
    },
    "A2.T9": {
        "caption": "Table 9: Alignment path statistics for Qwen2-Audio.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Statistic</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Cosine Path Monotonicity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Euclidean Path Monotonicity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Alignment Path Consistency</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Value</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.7891</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.7586</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.6875</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "qwen2audio",
            "path",
            "euclidean",
            "alignment",
            "statistic",
            "consistency",
            "statistics",
            "monotonicity",
            "value",
            "cosine"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We further evaluated the token-level alignment path monotonicity for <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> using the three metrics defined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T9\" title=\"Table 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, these results are highly consistent with the values obtained from the models in the primary experiments at their final training stages, detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Such consistency across disparate training paradigms provides strong evidence for the spontaneous emergence of a monotonic alignment path as a generalizable phenomenon.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "path",
                    "euclidean",
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight into the modality gap, we systematically analyze the similarity between speech and text representations at both sequence and token levels, aiming to reveal the mechanisms of speech-text alignment within LSLMs. At the sequence (coarse-grained) level, we observe that as representations propagate through deeper layers of the model, their cosine similarity increases steadily, reflecting progressive directional alignment. In parallel, the Euclidean distance between modalities also increases, indicating a divergence in magnitude that likely reflects modality-specific characteristics learned by the model. At the token (fine-grained) level, we find that the model develops a spontaneous monotonic alignment pattern between speech and text tokens, indicating consistent local correspondence across modalities.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The alignment between speech and textual modalities is crucial for the performance of LSLMs. Recent studies have explored five distinct methodologies for this task <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>. The latent space mapping approach, exemplified by <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SALMONN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib24\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">VITA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib11\" title=\"\">2024</a>)</cite>, uses a joint audio encoder-adapter architecture to directly project speech inputs into the LLM&#8217;s latent textual space. This paradigm effectively reduces computational overhead by compressing the audio sequence length via the audio adapter module. Meanwhile, it also preserves the LLM&#8217;s inherent reasoning capabilities and has demonstrated competitive performance across multiple benchmarks.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of comparing high-dimensional representations, Centered Kernel Alignment (CKA) was introduced as a robust similarity measure <cite class=\"ltx_cite ltx_citemacro_cite\">Kornblith et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib16\" title=\"\">2019</a>)</cite>. Subsequent work has shown that a simple, sample-wise cosine similarity can also effectively capture layer-wise similarity in transformer models, yielding results comparable to CKA with greater computational efficiency <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, the Wasserstein distance between paired speech and text embeddings has been used to measure cross-modal consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib19\" title=\"\">2025a</a>)</cite>. The Gramian Representation Alignment Measure (GRAM) is also designed to evaluate the alignment of multiple modalities simultaneously <cite class=\"ltx_cite ltx_citemacro_cite\">Cicchetti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib5\" title=\"\">2024</a>)</cite>.\nBoth methods have been integrated into training and effectively improve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "value",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the relationship between speech and text representations, we employ two similarity metrics, denoted in a unified manner as <math alttext=\"f^{(\\cdot)}(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f^{(\\cdot)}(x,y)</annotation></semantics></math>, where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> indicates the choice of metric (<math alttext=\"\\mathrm{cos}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>cos</mi><annotation encoding=\"application/x-tex\">\\mathrm{cos}</annotation></semantics></math>: cosine similarity, <math alttext=\"\\mathrm{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">d</mi><annotation encoding=\"application/x-tex\">\\mathrm{d}</annotation></semantics></math>: Euclidean distance):</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several notable patterns are observed in the similarity dynamics.\nFor <span class=\"ltx_text ltx_font_bold\">cosine similarity</span>, all models demonstrate a consistent increase as the network depth grows, indicating progressively stronger alignment between speech and text representations in deeper layers.\nMoreover, later training checkpoints consistently yield higher similarity scores across all layers, reflecting improved cross-modal alignment as training advances.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These trends suggest that deeper layers and extended training foster improved alignment in representational direction (cosine similarity), while preserving modality-specific distinctions in magnitude (Euclidean distance).\nThis alignment pattern could facilitate effective multimodal integration while preserving essential characteristics of each modality.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> denotes either cosine similarity or Euclidean distance.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-parameter fine-tuning, the same trend persists but with reduced strength: <math alttext=\"R^{2}=0.39\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.39</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.39</annotation></semantics></math> for cosine similarity, and <math alttext=\"R^{2}=0.53\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.53</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.53</annotation></semantics></math> and <math alttext=\"0.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mn>0.27</mn><annotation encoding=\"application/x-tex\">0.27</annotation></semantics></math> for Euclidean distance in <span class=\"ltx_text ltx_font_italic\">Llama</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span>, respectively.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f^{(\\cdot)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(\\cdot)}</annotation></semantics></math> denotes the selected similarity metric.\nAcross all models and training paradigms, we consistently observed that <span class=\"ltx_text ltx_font_bold\">the token-wise similarity matrix typically exhibits extreme values along a nearly monotonic path</span>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F6\" title=\"Figure 6 &#8227; Observations &#8227; 5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, with the increase in text token index, there is a monotonic alignment path in the speech frame sequence along which the similarity (or distance) values are locally maximized (or minimized). This monotonic path does not strictly align with the main diagonal, but reflects the actual temporal alignment structure between speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process produces an alignment path between the text and speech sequences. To verify the presence of monotonicity in these alignments, we use the Spearman rank correlation coefficient between text token indices and their aligned speech frame indices as the evaluation metric. Detailed statistics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. At the final training epoch across all models, the average Spearman coefficient is 0.85 for cosine similarity and 0.70 for Euclidean distance. The proportion of tokens with perfectly identical alignment paths under both similarity measures is 0.59, indicating substantial consistency in the alignment results.</p>\n\n",
                "matched_terms": [
                    "path",
                    "euclidean",
                    "alignment",
                    "consistency",
                    "statistics",
                    "monotonicity",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the observed token-level alignment patterns, we propose the <span class=\"ltx_text ltx_font_bold\">Alignment Path Score</span> (APS) to quantify the strength of speech-text alignment at the token level. Specifically, APS is defined as:</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the number of layers, <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the number of text tokens, and <math alttext=\"[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>A</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow><mrow><msubsup><mi>i</mi><mi>j</mi><mo>&#8727;</mo></msubsup><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">[A_{l}^{(\\cdot)}]_{i^{*}_{j},j}</annotation></semantics></math> represents the maximal similarity (or minimal distance) along the alignment path for each token.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically evaluate the relationship between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> defined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS3\" title=\"4.3 Correlation Between Representation Similarity and Modality Gap &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> on the LSLMs using a linear regression analysis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F7\" title=\"Figure 7 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LoRA-trained LSLMs yield higher <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values for both cosine (<math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math> vs. <math alttext=\"0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mn>0.75</mn><annotation encoding=\"application/x-tex\">0.75</annotation></semantics></math>) and Euclidean APS (<math alttext=\"0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mn>0.72</mn><annotation encoding=\"application/x-tex\">0.72</annotation></semantics></math> vs. <math alttext=\"0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mn>0.64</mn><annotation encoding=\"application/x-tex\">0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span>; <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math> vs. <math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>) compared to previous baselines, indicating stronger linear correlations with <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>. Under full-parameter finetuning, the correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> is similar to that of sequence-level metrics, with both showing low <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m11\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values. As previously suggested, the greater training noise and instability in this setting may limit the explanatory power of both sequence-level and token-level alignment metrics.</p>\n\n",
                "matched_terms": [
                    "euclidean",
                    "alignment",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-hoc Nature of Interventions.</span>\nOur intervention strategies are applied post hoc at inference time and serve primarily as analytical probes of token-level alignment. An important direction is to integrate these insights into training to explicitly optimize cross-modal consistency and improve speech-input performance.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the measurement of alignment path statistics, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS1\" title=\"5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. We report these statistics across different training stages, model scales, and training strategies. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we consider three metrics: (1) alignment path monotonicity based on the cosine similarity matrix, which reflects the degree of order in the alignment between text tokens and speech frames; (2) alignment path monotonicity based on the Euclidean distance matrix, defined in a similar manner but using Euclidean distances for alignment construction; and (3) token-level alignment path consistency, defined as the proportion of tokens whose aligned speech frame indices are identical under both similarity measures.</p>\n\n",
                "matched_terms": [
                    "path",
                    "euclidean",
                    "alignment",
                    "consistency",
                    "statistics",
                    "monotonicity",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "path",
                    "euclidean",
                    "alignment",
                    "consistency",
                    "monotonicity",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents representative cases from the intervention experiments detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, with results compiled in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> through&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The interventions involved two primary strategies: <span class=\"ltx_text ltx_font_bold\">angle</span> projection, which aligns the direction of the speech representation with its corresponding text representation, and <span class=\"ltx_text ltx_font_bold\">length</span> normalization, which matches their vector norms. These strategies were applied either to the three tokens with the lowest alignment confidence (<span class=\"ltx_text ltx_font_bold\">bot3</span>) or to all tokens along the alignment path (<span class=\"ltx_text ltx_font_bold\">all</span>).</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis of these cases reveals two primary categories of error correction. The first category involves the resolution of semantic misunderstandings arising from the spoken input. For example, in Case 1 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), the model initially misinterprets the spoken entity \"Brittany\" as \"Britain,\" leading to an irrelevant answer. Similarly, Case 2 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T5\" title=\"Table 5 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) shows an erroneous entity recognition from the speech input. After applying interventions to key tokens along the alignment path, the model successfully realigns its semantic representation with the ground-truth text, thereby recovering the correct understanding and generating an accurate response.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second category addresses factual errors that occur even when the initial semantic parsing of the spoken query is correct. In Case 3 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T6\" title=\"Table 6 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) and Case 4 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>), the model demonstrates a correct understanding of the question&#8217;s topic but fails to produce factually accurate or complete answers. Our interventions, by modifying the representation direction or norm along the alignment path, also prove effective in these scenarios, guiding the model to generate factually correct responses consistent with the reference answers.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Collectively, these case studies demonstrate that fine-grained interventions on the token alignment path, through either embedding direction or norm modification, consistently improve the model&#8217;s answer accuracy and robustness for spoken inputs. This effect is observed in correcting both semantic misinterpretations and factual knowledge errors, indicating that such interventions can enhance multimodal alignment and enable more reliable knowledge retrieval from speech input.</p>\n\n",
                "matched_terms": [
                    "path",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4\" title=\"4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we analyzed the sequence-level similarity dynamics between speech and text representations in <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span>. The results are visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.F9\" title=\"Figure 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The observed layer-wise similarity dynamics, characterized by an increase in cosine similarity and a concurrent upward trend in Euclidean distance with network depth, are highly analogous to the phenomena identified in our primary experiments.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "euclidean",
                    "cosine"
                ]
            }
        ]
    },
    "A2.T10": {
        "caption": "Table 10: \nPerformance (%, ↑\\uparrow) on the sd-qa subset for Qwen2-Audio under different token-level intervention strategies. Results that outperform the corresponding Speech Input are typeset in bold.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speech Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Angle Projection</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Length Normalization</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Bottom3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">All</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Bottom3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">All</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">58.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">35.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">35.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">34.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">33.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">34.72</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "bottom3",
            "length",
            "projection",
            "qwen2audio",
            "intervention",
            "corresponding",
            "strategies",
            "sdqa",
            "input",
            "text",
            "performance",
            "tokenlevel",
            "bold",
            "angle",
            "under",
            "results",
            "speech",
            "normalization",
            "↑uparrow",
            "model",
            "all",
            "outperform",
            "subset",
            "typeset",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Finally, we replicated the token-level intervention experiments from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T10\" title=\"Table 10 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, applying angle projection to the least-aligned tokens (<span class=\"ltx_text ltx_font_italic\">Bottom3</span>) successfully improved performance on speech inputs. This result demonstrates that our proposed intervention strategy for mitigating the modality gap is also effective for models trained with a multi-stage approach.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks. In this work, we reveal through systematic experimentation that although LSLMs lose some text input performance after speech-text alignment training, the performance gap between speech and text inputs is more pronounced, which we refer to as the <span class=\"ltx_text ltx_font_bold\">modality gap</span>.\nTo understand this gap, we analyze both coarse- and fine-grained text and speech representations.\nAt the coarse-grained level, representations of speech and text in deeper layers are found to be increasingly aligned in direction (cosine similarity), while concurrently diverging in magnitude (Euclidean distance). We further find that representation similarity is strongly correlated with the modality gap.\nAt the fine-grained level, a spontaneous token-level alignment pattern between text and speech representations is observed.\nBased on this, we introduce the Alignment Path Score to quantify token-level alignment quality, which exhibits stronger correlation with the modality gap.\nBuilding on these insights, we design targeted interventions on critical tokens through angle projection and length normalization. These strategies demonstrate the potential to improve correctness for speech inputs. Our study provides the first systematic empirical analysis of the modality gap and alignment mechanisms in LSLMs, offering both theoretical and methodological guidance for future optimization.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "tokenlevel",
                    "normalization",
                    "length",
                    "angle",
                    "speech",
                    "input",
                    "projection",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of Large Speech Language Models (LSLMs) has revolutionized human-computer interaction by enabling direct processing of both speech representations and text inputs, subsequently generating textual or spoken outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib1\" title=\"\">2024</a>)</cite>.\nCompared to traditional pipeline architectures that sequentially chain Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-To-Speech (TTS) components, end-to-end LSLMs offer significant advantages, including reduced latency, inherent error resilience, and more expressive speech synthesis capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies in LSLMs have focused on aligning speech modalities with text space through speech tokenizers <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> or encoder-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>); Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib4\" title=\"\">2023</a>)</cite>. These cross-modal alignment strategies aim to harness the linguistic capabilities of pretrained LLMs while integrating speech processing functionalities <cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "strategies",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, significant performance disparities persist between LSLMs and conventional pipeline models in semantic understanding tasks. Benchmark results from VoiceBench <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib2\" title=\"\">2024</a>)</cite> reveal a striking contrast: the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + LLaMA-3.1-8B</span> pipeline achieves 79.06 overall score, while its LSLM counterpart <span class=\"ltx_text ltx_font_italic\">LLaMA-Omni</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib10\" title=\"\">2024</a>)</cite> scores merely 37.51. This pattern continues in Uro-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib29\" title=\"\">2025</a>)</cite> evaluations, where the <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3 + Qwen2-7B-Instruct</span> pipeline attains 78.13 overall score compared to <span class=\"ltx_text ltx_font_italic\">Freeze-Omni</span>&#8217;s <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib28\" title=\"\">2024b</a>)</cite> 48.28, despite both systems employing the same underlying LLM. Notably, while Uro-bench&#8217;s dependence on transcribed speech outputs might inherently favor pipeline architectures, the magnitude of these performance drops remains substantial and warrants investigation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we take the persistent performance gap between end-to-end LSLMs and traditional ASR+LLM pipeline systems as our starting point. We systematically reproduce and quantify this discrepancy across various LLM backbones and training strategies, and, for the first time, empirically reveal the underlying mechanisms behind the performance difference. Specifically, after speech-text alignment training, a clear and consistent performance gap exists between text and speech inputs within the same model.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight into the modality gap, we systematically analyze the similarity between speech and text representations at both sequence and token levels, aiming to reveal the mechanisms of speech-text alignment within LSLMs. At the sequence (coarse-grained) level, we observe that as representations propagate through deeper layers of the model, their cosine similarity increases steadily, reflecting progressive directional alignment. In parallel, the Euclidean distance between modalities also increases, indicating a divergence in magnitude that likely reflects modality-specific characteristics learned by the model. At the token (fine-grained) level, we find that the model develops a spontaneous monotonic alignment pattern between speech and text tokens, indicating consistent local correspondence across modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, our study systematically examines the relationship between internal representation similarity and the modality gap exhibited on evaluation benchmarks. A clear linear correlation is observed at both the sequence and token levels, suggesting that the nature of internal cross-modal alignment is closely related to the performance disparity between speech and text inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These observations are further examined through targeted intervention experiments, where speech token embeddings along the alignment path are modified using either angle projection or length normalization. We find that such interventions can improve performance on challenging cases from the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "sdqa",
                    "length",
                    "angle",
                    "subset",
                    "speech",
                    "projection",
                    "normalization",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are threefold: (1) We systematically identify that the primary source of the performance gap between LSLMs and pipeline systems lies in the modality difference between speech and text inputs. (2) We analyze internal representations and find that the modality gap is closely linked to the similarity between speech and text representations at both sequence and token levels. (3) We provide the first empirical evidence that targeted interventions on speech representations can improve speech input performance on challenging cases.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By focusing on understanding and revealing the mechanisms behind modality alignment, our work offers a deeper exploration of the factors influencing LSLM performance. This approach not only addresses the current performance discrepancy but also paves the way for future advancements in integrating speech modalities into LLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The alignment between speech and textual modalities is crucial for the performance of LSLMs. Recent studies have explored five distinct methodologies for this task <cite class=\"ltx_cite ltx_citemacro_cite\">Ji et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib14\" title=\"\">2024</a>)</cite>. The latent space mapping approach, exemplified by <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SALMONN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib24\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">VITA</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib11\" title=\"\">2024</a>)</cite>, uses a joint audio encoder-adapter architecture to directly project speech inputs into the LLM&#8217;s latent textual space. This paradigm effectively reduces computational overhead by compressing the audio sequence length via the audio adapter module. Meanwhile, it also preserves the LLM&#8217;s inherent reasoning capabilities and has demonstrated competitive performance across multiple benchmarks.</p>\n\n",
                "matched_terms": [
                    "length",
                    "speech",
                    "qwen2audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite> adopts modality chaining by discretizing speech into symbolic units and expanding the LLM&#8217;s vocabulary, while <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite> and <span class=\"ltx_text ltx_font_italic\">Moshi</span> <cite class=\"ltx_cite ltx_citemacro_cite\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib7\" title=\"\">2024</a>)</cite> utilize interleaved text-speech tokens and parallel generation architectures, respectively. <span class=\"ltx_text ltx_font_italic\">SyncLLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Veluri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib26\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">IntrinsicVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib34\" title=\"\">2024b</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Align-SLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib18\" title=\"\">2024</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">OmniFlatten</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib33\" title=\"\">2024a</a>)</cite> pioneers direct speech-to-speech interaction without textual intermediates. Although significant progress has been made with these methodologies in existing research, their performance on audio processing tasks remains suboptimal.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous work has systematically analyzed the modality gap phenomenon and shown that it persists across a wide range of multimodal models <cite class=\"ltx_cite ltx_citemacro_cite\">Liang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib17\" title=\"\">2022</a>)</cite>. This gap largely arises from the cone effect, where embeddings from different modalities are restricted to distinct subspaces, leading to misalignment and degraded cross-modal performance.</p>\n\n",
                "matched_terms": [
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech translation, the modality gap has also been a subject of investigation. Evidence suggests that this gap can emerge during the early phases of fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib12\" title=\"\">2023</a>)</cite>. Furthermore, it has been shown that the resulting misalignment between modalities leads to divergent predictions and degrades performance relative to text-only machine translation systems. This representational divergence was empirically quantified using the cosine similarity between speech and text embeddings, confirming a substantial gap <cite class=\"ltx_cite ltx_citemacro_cite\">Fang and Feng (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib9\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, the Wasserstein distance between paired speech and text embeddings has been used to measure cross-modal consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib19\" title=\"\">2025a</a>)</cite>. The Gramian Representation Alignment Measure (GRAM) is also designed to evaluate the alignment of multiple modalities simultaneously <cite class=\"ltx_cite ltx_citemacro_cite\">Cicchetti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib5\" title=\"\">2024</a>)</cite>.\nBoth methods have been integrated into training and effectively improve cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates the performance degradation of LSLMs in processing speech inputs compared to their base models&#8217; performance on text inputs. Through comprehensive experiments conducted on multiple LLM backbones using both full-parameter and LoRA fine-tuning methods <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib13\" title=\"\">2021</a>)</cite>, we find that the primary contributor to this modality gap is the suboptimal alignment between textual and auditory modalities in LSLMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the architecture of the LSLM setup used in this study comprises three core components: a speech encoder, a speech adapter, and an LLM backbone.\nThe speech signal is first encoded by the speech encoder into a latent representation and then compressed by the speech adapter by a factor of <span class=\"ltx_text ltx_font_italic\">5</span> to reduce computational overhead.\nMeanwhile, text inputs are processed through a standard tokenization pipeline and embedded via the LLM&#8217;s embedding layer.\nThese speech and text embeddings are then concatenated to form a unified multimodal sequence that serves as input to the LLM backbone, enabling autoregressive generation of textual responses.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span>, a widely used ASR model, serves as the speech encoder. The speech adapter is implemented as a lightweight module with two fully connected layers. We conducted experiments with various LLM backbones, including <span class=\"ltx_text ltx_font_italic\">LLaMA3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">LLaMA3.1-8B-Instruct</span>, <span class=\"ltx_text ltx_font_italic\">Qwen2.5-1.5B-Instruct</span>, and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B-Instruct</span>. For brevity, henceforth we omit the suffix \"Instruct\" when referring to these model variants.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training dataset is constructed following the framework of <span class=\"ltx_text ltx_font_italic\">Ke-Speech-Chat</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib35\" title=\"\">2024</a>)</cite>, exclusively comprising single-turn dialogue samples. Each sample includes both speech and text instructions, as well as a text response. We refined the raw text using <span class=\"ltx_text ltx_font_italic\">Qwen2.5-72B-Instruct</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib30\" title=\"\">2024</a>); Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib25\" title=\"\">2024</a>)</cite>, aligning it with natural conversational patterns observed in real-world scenarios. Subsequently, a three-stage filtering mechanism was applied to purify the data, targeting safety, semantic clarity, and linguistic naturalness. The speech instruction-response pairs were synthesized using <span class=\"ltx_text ltx_font_italic\">CosyVoice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib8\" title=\"\">2024</a>)</cite>. Based on automated transcription via <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib22\" title=\"\">2022</a>)</cite>, speech samples exceeding a Word Error Rate (WER) threshold of 0.1 are discarded. Finally, Our training dataset contains 637,283 samples, with speech instructions totaling 1,604 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All LSLMs are trained for 2 epochs on our training dataset using the AdamW optimizer with a peak learning rate of 2e-5. For LoRA, we set <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, <math alttext=\"\\alpha=4.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>4.0</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=4.0</annotation></semantics></math>, and the dropout rate to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. All experiments were conducted on a distributed setup with 2 nodes, each equipped with 8 NVIDIA A100 GPUs. Training a single model requires approximately 384 GPU hours on this setup.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of LSLMs after full-parameter and LoRA fine-tuning on the 4 models introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.SS1\" title=\"3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, each tested under both speech and text modalities. Detailed results are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS1\" title=\"A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS2\" title=\"A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the performance of each model at epoch 2 on both text and speech inputs, alongside the corresponding base model&#8217;s text-only performance.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "under",
                    "results",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the data shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Model Architecture &#8227; 3 Preliminary &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on average, LSLMs exhibit a 25% performance decline on speech inputs relative to their base models on text. This decline can be attributed to two factors: (1) fine-tuning&#8211;induced erosion of reasoning and generation capabilities, with an average drop of 8.79%; and (2) suboptimal speech&#8211;text alignment, with an average drop of 16.46%. Given the variety of model sizes and tuning methods evaluated, this trend appears general. This phenomenon indicates that the observed performance degradation stems primarily from the speech&#8211;text modality gap, and that bridging this gap is crucial to enhance LSLM speech processing.\nIndeed, this modality gap is not unique to our setup, as we observe a similar trend across diverse public LSLMs in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.SS1\" title=\"B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we examine the dynamic relationship between text and speech modality representations at a coarse-grained sequence level using similarity measurement techniques.\nOur analysis uncovers consistent patterns across various LLM architectures and training paradigms.\nThrough extensive experimentation, we observe a strong linear correlation between representation alignment and performance disparities across modalities, particularly under LoRA fine-tuning, highlighting the predictive value of embedding similarity for modality gap estimation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "under",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> speech-text query pairs <math alttext=\"\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i}^{s},x_{i}^{t})\\}_{i=1}^{N}</annotation></semantics></math>, where <math alttext=\"x_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{s}</annotation></semantics></math> denotes the speech input and <math alttext=\"x_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">x_{i}^{t}</annotation></semantics></math> its corresponding text transcription, we process each sample through the model as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F3\" title=\"Figure 3 &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "speech",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the speech modality, the input <math alttext=\"x^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">x^{s}</annotation></semantics></math> is encoded by the speech encoder and linear projector, resulting in an initial embedding sequence\n<math alttext=\"h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math> is the number of speech frames and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimension. This sequence, along with a system prompt, is fed into an <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>-layer model, yielding layer-wise representations\n<math alttext=\"h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>l</mi><mi>s</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{l}^{s}\\in\\mathbb{R}^{T_{s}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"l\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in\\{1,\\ldots,L\\}</annotation></semantics></math> indexes the model layer. Similarly, for the text modality, the input <math alttext=\"x^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">x^{t}</annotation></semantics></math> is tokenized and embedded, producing\n<math alttext=\"h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>h</mi><mn>0</mn><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">h_{0}^{t}\\in\\mathbb{R}^{T_{t}\\times d},</annotation></semantics></math>\nwhere <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math> is the length of the token sequence. The corresponding layer-wise representations are\n<math alttext=\"h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>t</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">h_{l}^{t}\\in\\mathbb{R}^{T_{t}\\times d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "length",
                    "speech",
                    "text",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the relationship between speech and text representations, we employ two similarity metrics, denoted in a unified manner as <math alttext=\"f^{(\\cdot)}(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f^{(\\cdot)}(x,y)</annotation></semantics></math>, where <math alttext=\"(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\cdot)</annotation></semantics></math> indicates the choice of metric (<math alttext=\"\\mathrm{cos}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mi>cos</mi><annotation encoding=\"application/x-tex\">\\mathrm{cos}</annotation></semantics></math>: cosine similarity, <math alttext=\"\\mathrm{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">d</mi><annotation encoding=\"application/x-tex\">\\mathrm{d}</annotation></semantics></math>: Euclidean distance):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{l,i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,i}^{s}</annotation></semantics></math> and <math alttext=\"h_{l,j}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mrow><mi>l</mi><mo>,</mo><mi>j</mi></mrow><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">h_{l,j}^{t}</annotation></semantics></math> denote the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame or token embedding at layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> for the speech and text modalities, respectively. The global relationship between modalities at each layer is then assessed by computing <math alttext=\"f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi>cos</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{cos})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math> and <math alttext=\"f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>f</mi><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">d</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>h</mi><mo>&#175;</mo></mover><mi>l</mi><mi>t</mi></msubsup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{l}^{(\\mathrm{d})}\\left(\\bar{h}_{l}^{s},\\bar{h}_{l}^{t}\\right)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the sequence-level similarity at each layer for all test samples using the methodology outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS1\" title=\"4.1 Methodology &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Sequence-level Speech-Text Representation Dynamics &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes these results for various LSLMs under different training regimes, with cosine similarity shown in blue and Euclidean distance in orange. Each subplot corresponds to a specific model configuration, and each curve within a subplot represents a distinct training checkpoint, depicting the layer-wise similarity metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "under",
                    "results",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several notable patterns are observed in the similarity dynamics.\nFor <span class=\"ltx_text ltx_font_bold\">cosine similarity</span>, all models demonstrate a consistent increase as the network depth grows, indicating progressively stronger alignment between speech and text representations in deeper layers.\nMoreover, later training checkpoints consistently yield higher similarity scores across all layers, reflecting improved cross-modal alignment as training advances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the relationship between representation similarity and downstream performance, we compute a scalar similarity score for each model by averaging similarity across layers:</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We quantify the modality gap as the drop in benchmark scores between text and speech inputs, as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"M^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>t</mi></msup><annotation encoding=\"application/x-tex\">M^{t}</annotation></semantics></math> and <math alttext=\"M^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>M</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">M^{s}</annotation></semantics></math> are overall benchmark scores obtained from text and speech inputs, respectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Under LoRA fine-tuning, a strong linear relationship is observed between cosine similarity and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> (<math alttext=\"R^{2}=0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.75</annotation></semantics></math>), suggesting that better cross-modal alignment leads to smaller performance disparities.\nAlthough Euclidean distance shows a weaker correlation overall, it becomes more pronounced within specific model families (<math alttext=\"R^{2}=0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.64</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span> and <math alttext=\"R^{2}=0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">R^{2}=0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>).</p>\n\n",
                "matched_terms": [
                    "under",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings empirically validate the connection between internal cross-modal representations and performance-level modality gaps. The stronger correlations observed in LoRA-tuned models may stem from the constrained low-rank adaptation, which preserves the integrity of pretrained text representations while facilitating targeted speech-text alignment. In contrast, full fine-tuning grants more representational flexibility, potentially introducing overfitting that weakens this correlation.\nConsequently, representation similarity serves as a more reliable predictor of modality performance under LoRA than under full-parameter fine-tuning.</p>\n\n",
                "matched_terms": [
                    "under",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the coarse-grained sequence-level analysis in the previous section, this section focuses on token-level alignment patterns, examining <span class=\"ltx_text ltx_font_bold\">the role and contribution of each token in modality alignment</span>. We will begin with case studies, and subsequently introduce more detailed quantitative metrics to facilitate a finer-grained investigation. Through correlation analysis and intervention experiments, we explore the relationship between token-level alignment and downstream task performance, thereby further revealing the underlying speech-text alignment mechanisms in LSLMs.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "tokenlevel",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f^{(\\cdot)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(\\cdot)}</annotation></semantics></math> denotes the selected similarity metric.\nAcross all models and training paradigms, we consistently observed that <span class=\"ltx_text ltx_font_bold\">the token-wise similarity matrix typically exhibits extreme values along a nearly monotonic path</span>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F6\" title=\"Figure 6 &#8227; Observations &#8227; 5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, with the increase in text token index, there is a monotonic alignment path in the speech frame sequence along which the similarity (or distance) values are locally maximized (or minimized). This monotonic path does not strictly align with the main diagonal, but reflects the actual temporal alignment structure between speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically quantify this alignment pattern, for each text token <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>, we identify the index of the speech frame with maximal similarity or minimal distance as:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process produces an alignment path between the text and speech sequences. To verify the presence of monotonicity in these alignments, we use the Spearman rank correlation coefficient between text token indices and their aligned speech frame indices as the evaluation metric. Detailed statistics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. At the final training epoch across all models, the average Spearman coefficient is 0.85 for cosine similarity and 0.70 for Euclidean distance. The proportion of tokens with perfectly identical alignment paths under both similarity measures is 0.59, indicating substantial consistency in the alignment results.</p>\n\n",
                "matched_terms": [
                    "all",
                    "under",
                    "results",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The widespread emergence of this monotonic pattern suggests that the model not only aligns modalities globally, but also <span class=\"ltx_text ltx_font_bold\">spontaneously learns a soft, monotonic alignment between speech frames and text tokens at the token level</span>. Importantly, this alignment pattern emerges automatically in end-to-end speech-text alignment tasks, reflecting the model&#8217;s ability to capture and map the temporal structure of speech to the semantic structure of text in a robust manner.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically evaluate the relationship between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> defined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4.SS3\" title=\"4.3 Correlation Between Representation Similarity and Modality Gap &#8227; 4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> on the LSLMs using a linear regression analysis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.F7\" title=\"Figure 7 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LoRA-trained LSLMs yield higher <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values for both cosine (<math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math> vs. <math alttext=\"0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mn>0.75</mn><annotation encoding=\"application/x-tex\">0.75</annotation></semantics></math>) and Euclidean APS (<math alttext=\"0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mn>0.72</mn><annotation encoding=\"application/x-tex\">0.72</annotation></semantics></math> vs. <math alttext=\"0.64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mn>0.64</mn><annotation encoding=\"application/x-tex\">0.64</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Qwen</span>; <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math> vs. <math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> for <span class=\"ltx_text ltx_font_italic\">Llama</span>) compared to previous baselines, indicating stronger linear correlations with <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>. Under full-parameter finetuning, the correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> is similar to that of sequence-level metrics, with both showing low <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m11\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> values. As previously suggested, the greater training noise and instability in this setting may limit the explanatory power of both sequence-level and token-level alignment metrics.</p>\n\n",
                "matched_terms": [
                    "under",
                    "tokenlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results suggest that APS offers a more direct and sensitive measurement of the relationship between alignment quality and downstream performance. The stronger correlation between APS and <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> highlights that fine-grained, token-level alignment is the key mechanism underlying LSLM speech understanding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokenlevel",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in the previous section, our analyses revealed a strong correlation between the token-level alignment score (APS) and the modality gap in model performance (<math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math>). However, correlation does not necessarily imply causation. To further investigate whether the token-level alignment mechanism causally affects the speech understanding ability of LSLMs, we conducted a series of targeted intervention experiments.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "model",
                    "tokenlevel",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we focused on the <span class=\"ltx_text ltx_font_typewriter\">sd-qa</span> subset of VoiceBench and selected both <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> and <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> models, each under LoRA and full-parameter fine-tuning settings. For each sample, we first used the APS path to identify the three speech tokens with the lowest alignment scores (<span class=\"ltx_text ltx_font_italic\">bottom3</span>) as well as all tokens along the alignment path (<span class=\"ltx_text ltx_font_italic\">All</span>). We then applied two types of interventions: (1) <span class=\"ltx_text ltx_font_bold\">Angle projection</span>, where the selected speech token embeddings were projected to have the same direction as their corresponding text token embeddings; and (2) <span class=\"ltx_text ltx_font_bold\">Length normalization</span>, where the norm of the speech token embeddings was scaled to match that of the corresponding text tokens. We evaluated the downstream QA accuracy before and after intervention.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "corresponding",
                    "bottom3",
                    "normalization",
                    "sdqa",
                    "length",
                    "angle",
                    "all",
                    "under",
                    "subset",
                    "speech",
                    "projection",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.T1\" title=\"Table 1 &#8227; 5.2 Alignment Path Score &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Angle Projection yields improvements or maintains performance in 6 out of 8 intervention settings, demonstrating that increasing the angular similarity of token-level text and speech representations can enhance downstream outcomes. For LoRA fine-tuned models, applying angle projection to either the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> or <span class=\"ltx_text ltx_font_italic\">All</span> alignment-path tokens consistently improves results. Notably, intervening on only the <span class=\"ltx_text ltx_font_italic\">Bottom3</span> tokens leads to more robust gains, with <span class=\"ltx_text ltx_font_italic\">Llama3.1-8B</span> improving by 7.52% and <span class=\"ltx_text ltx_font_italic\">Qwen2.5-7B</span> by 4.19%. In contrast, length normalization provides improvement in only one case, with performance declining in the remaining settings, indicating an overall detrimental effect on LSLM&#8217;s speech sequence modeling.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "bottom3",
                    "tokenlevel",
                    "normalization",
                    "length",
                    "angle",
                    "all",
                    "results",
                    "speech",
                    "projection",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further case analysis shows that angular or length-based interventions on speech tokens can correct cases where the model fails on speech input but succeeds on the corresponding text. These corrections fall into two categories: (1) resolving semantic misunderstandings from misinterpreting spoken content, and (2) rectifying factual errors despite correct semantic parsing. Representative examples for both are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS4\" title=\"A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, highlighting the potential of token-level interventions to improve linguistic comprehension and factual consistency for spoken queries.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "model",
                    "tokenlevel",
                    "speech",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work systematically investigates the modality gap in LSLMs, defined as the performance disparity between speech and text inputs within the same trained model. To uncover the mechanisms behind this gap, we analyze speech-text alignment at both sequence and token levels.\nSequence-level analysis tracks representation similarity across layers and training, establishing its linear relationship with the modality gap.\nAt the token level, we reveal word-frame alignment structures and propose the Alignment Path Score, which shows a stronger correlation with the proposed modality gap. Targeted intervention experiments further demonstrate that improving token-level alignment can enhance speech inference accuracy.\nThis study deepens understanding of how large language models process and comprehend spoken language.</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "model",
                    "tokenlevel",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-hoc Nature of Interventions.</span>\nOur intervention strategies are applied post hoc at inference time and serve primarily as analytical probes of token-level alignment. An important direction is to integrate these insights into training to explicitly optimize cross-modal consistency and improve speech-input performance.</p>\n\n",
                "matched_terms": [
                    "tokenlevel",
                    "intervention",
                    "performance",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T2\" title=\"Table 2 &#8227; A.1 Speech vs. Text Performance Across Training Strategies &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the evaluation results of our models under different training paradigms and checkpoints. For each model and training strategy, we report the performance on both speech input and text input across multiple benchmark subsets, as well as their respective overall scores. Additionally, we provide the <math alttext=\"{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathcolor=\"#1793E6\" style=\"--ltx-fg-color:#1793E6;\">P</mi></mrow><annotation encoding=\"application/x-tex\">{\\color[rgb]{0.08984375,0.578125,0.90234375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.08984375,0.578125,0.90234375}GAP}</annotation></semantics></math> metric, defined as the difference between the overall text input and speech input performance. This comprehensive comparison allows us to assess the alignment and robustness of various models and training approaches with respect to both input modalities.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "under",
                    "speech",
                    "results",
                    "input",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a comprehensive comparison with the end-to-end LSLMs analyzed in this work, we report the performance of traditional ASR+LLM pipeline systems. These systems utilize <span class=\"ltx_text ltx_font_italic\">Whisper-large-v3</span> as the ASR module, paired with the corresponding LLM backbones from our main experiments. All pipeline evaluations were conducted on the identical 4,947-sample VoiceBench test set and adhere strictly to the official protocol to ensure a fair comparison. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T3\" title=\"Table 3 &#8227; A.2 Performance of Pipeline System Baselines &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "performance",
                    "results",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the measurement of alignment path statistics, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS1\" title=\"5.1 Monotonic Patterns in Token-wise Similarity Matrices &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. We report these statistics across different training stages, model scales, and training strategies. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we consider three metrics: (1) alignment path monotonicity based on the cosine similarity matrix, which reflects the degree of order in the alignment between text tokens and speech frames; (2) alignment path monotonicity based on the Euclidean distance matrix, defined in a similar manner but using Euclidean distances for alignment construction; and (3) token-level alignment path consistency, defined as the proportion of tokens whose aligned speech frame indices are identical under both similarity measures.</p>\n\n",
                "matched_terms": [
                    "different",
                    "strategies",
                    "model",
                    "tokenlevel",
                    "under",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results reveal the following trends: (1) Both alignment path monotonicity metrics exhibit an overall increasing tendency as training progresses, suggesting that the model incrementally acquires more structured and monotonic alignments. (2) The monotonicity measured via cosine similarity remains consistently higher than that based on Euclidean distance, indicating that cosine similarity may be more effective in capturing ordered relationships in high-dimensional spaces. (3) Token-level alignment path consistency also demonstrates a general upward trend during training, implying that the alignment paths derived from the two similarity measures become increasingly similar. These observations are consistent across different model scales and training strategies, underscoring the robustness and effectiveness of the learned alignment mechanism.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "tokenlevel",
                    "results",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents representative cases from the intervention experiments detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S5.SS3\" title=\"5.3 Intervention Experiments: Probing the Causal Role of Token-level Alignment &#8227; 5 Empirical Analysis of Finer-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, with results compiled in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> through&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T7\" title=\"Table 7 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The interventions involved two primary strategies: <span class=\"ltx_text ltx_font_bold\">angle</span> projection, which aligns the direction of the speech representation with its corresponding text representation, and <span class=\"ltx_text ltx_font_bold\">length</span> normalization, which matches their vector norms. These strategies were applied either to the three tokens with the lowest alignment confidence (<span class=\"ltx_text ltx_font_bold\">bot3</span>) or to all tokens along the alignment path (<span class=\"ltx_text ltx_font_bold\">all</span>).</p>\n\n",
                "matched_terms": [
                    "intervention",
                    "corresponding",
                    "strategies",
                    "normalization",
                    "length",
                    "angle",
                    "all",
                    "results",
                    "speech",
                    "projection",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis of these cases reveals two primary categories of error correction. The first category involves the resolution of semantic misunderstandings arising from the spoken input. For example, in Case 1 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T4\" title=\"Table 4 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), the model initially misinterprets the spoken entity \"Brittany\" as \"Britain,\" leading to an irrelevant answer. Similarly, Case 2 (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.T5\" title=\"Table 5 &#8227; A.4 Case Analysis of Intervention Experiments &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) shows an erroneous entity recognition from the speech input. After applying interventions to key tokens along the alignment path, the model successfully realigns its semantic representation with the ground-truth text, thereby recovering the correct understanding and generating an accurate response.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Collectively, these case studies demonstrate that fine-grained interventions on the token alignment path, through either embedding direction or norm modification, consistently improve the model&#8217;s answer accuracy and robustness for spoken inputs. This effect is observed in correcting both semantic misinterpretations and factual knowledge errors, indicating that such interventions can enhance multimodal alignment and enable more reliable knowledge retrieval from speech input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To substantiate the generality of the modality gap, we extended our analysis to several publicly available LSLMs that represent diverse alignment paradigms. These models include <span class=\"ltx_text ltx_font_italic\">SpeechGPT</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib32\" title=\"\">2023</a>)</cite>, which relies on speech discretization; <span class=\"ltx_text ltx_font_italic\">BLSP</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib27\" title=\"\">2024a</a>)</cite>, which aligns the speech and text modalities via bootstrapped behavior alignment; <span class=\"ltx_text ltx_font_italic\">GLM-4-Voice</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib31\" title=\"\">2024</a>)</cite>, which utilizes interleaved text-speech tokens; and <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>, which employs a three-stage training pipeline (pre-training, SFT, and DPO) and uses natural language prompts to unify large-scale audio tasks during pre-training. Each model was evaluated on our standardized VoiceBench test set, comparing performance on speech inputs against their corresponding text transcriptions.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "corresponding",
                    "model",
                    "speech",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results, presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T8\" title=\"Table 8 &#8227; B.1 Modality Gap Across Diverse Paradigms &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, consistently reveal a significant performance drop for speech inputs across all models. This corroborates our central thesis that the modality gap is a prevalent challenge, independent of the specific LSLM architecture or alignment strategy.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the robustness of our analytical framework and conclusions across different training paradigms, we conducted an in-depth analysis of <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#bib.bib3\" title=\"\">2024</a>)</cite>. This model is particularly representative as it utilizes a multi-stage training pipeline involving pre-training, supervised fine-tuning, and direct preference optimization. As demonstrated below, despite its distinct training methodology, <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> exhibits patterns in its modality alignment mechanism that are highly consistent with our core findings.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#S4\" title=\"4 Empirical Analysis of Coarse-grained Speech-Text Representations &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we analyzed the sequence-level similarity dynamics between speech and text representations in <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span>. The results are visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.F9\" title=\"Figure 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The observed layer-wise similarity dynamics, characterized by an increase in cosine similarity and a concurrent upward trend in Euclidean distance with network depth, are highly analogous to the phenomena identified in our primary experiments.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "speech",
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further evaluated the token-level alignment path monotonicity for <span class=\"ltx_text ltx_font_italic\">Qwen2-Audio</span> using the three metrics defined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.SS3\" title=\"A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A2.T9\" title=\"Table 9 &#8227; Coarse-Grained Representation Dynamics. &#8227; B.2 In-depth Analysis of Modality Alignment in Qwen2-Audio &#8227; Appendix B Validation on External LSLM &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, these results are highly consistent with the values obtained from the models in the primary experiments at their final training stages, detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12116v1#A1.F8\" title=\"Figure 8 &#8227; A.3 Analysis of Alignment Path Monotonicity and Consistency &#8227; Appendix A Primary Experimental Details &#8227; Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Such consistency across disparate training paradigms provides strong evidence for the spontaneous emergence of a monotonic alignment path as a generalizable phenomenon.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "tokenlevel",
                    "results"
                ]
            }
        ]
    }
}