{
    "S4.T1": {
        "source_file": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
        "caption": "Table 1: Performance comparison of sound event detection methods.",
        "body": "Method\nOV-SED\nPSDS1A ↑\\uparrow\nPSDS1T ↑\\uparrow\n\n\nATST-Frame-SED\n×\\times\n0.3846\n0.5387\n\n\nDasheng-SED\n×\\times\n0.3986\n0.5349\n\n\nAudioSep-SED\n✓\n0.3260\n0.5332\n\n\n\n\\rowcolorlightblue\nFlexSED\n\n✓\n0.4484\n0.5863\n\n\n  w/o Neg. Filtering\n\n0.4366\n0.5627",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">OV-SED</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">A</span></sub>&#160;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">T</span></sub>&#160;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">ATST-Frame-SED</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.3846</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.5387</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">Dasheng-SED</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.3986</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.5349</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">AudioSep-SED</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">&#10003;</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.3260</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.5332</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightblue\n<span class=\"ltx_text ltx_font_bold\">FlexSED</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">&#10003;</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.4484</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.5863</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">&#8195;&#8195;w/o Neg. Filtering</th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.4366</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">0.5627</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detection",
            "filtering",
            "neg",
            "methods",
            "dashengsed",
            "event",
            "psds1t",
            "×times",
            "rowcolorlightblue",
            "↑uparrow",
            "performance",
            "flexsed",
            "psds1a",
            "audiosepsed",
            "sound",
            "ovsed",
            "method",
            "comparison",
            "atstframesed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T1\" title=\"In 4.1 Comparison with Vanilla SED &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, the proposed FlexSED significantly outperforms both the vanilla SED models and the target sound separation-based model. This improvement can be largely attributed to the integration of knowledge from the pre-trained CLAP and DaSheng models. CLAP enhances FlexSED&#8217;s ability to capture semantic relationships among sound classes compared to vanilla SED models, while DaSheng provides stronger temporal and acoustic modeling through its pre-trained audio transformer than AudioSep&#8217;s convolutional U-Net architecture. In addition to the performance boost, FlexSED also offers open-vocabulary capabilities for simpler usability. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.SS3\" title=\"4.3 Zero-shot and Few-shot Performance &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.3</span></a>, we demonstrate its effectiveness in zero-shot settings and its expandability in few-shot scenarios.</p>\n\n",
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T1\" title=\"In 4.1 Comparison with Vanilla SED &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, FlexSED without the proposed negative query filtering strategy exhibits degraded performance, underscoring the effectiveness of our method in removing potentially conflicting negative queries that could otherwise confuse training and impair model performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability.\nAlthough text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.\n</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "methods",
                    "performance",
                    "flexsed",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sound event detection (SED) focuses on identifying and temporally locating meaningful sounds in audio recordings. Given an input audio stream, SED systems aim to determine what happened and when, recognizing events such as speech, alarms, animal sounds, or environmental noises and marking their precise start and end times <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib2\" title=\"\">2</a>]</cite>. This technology underpins a wide range of real-world applications, including smart home monitoring, health and safety surveillance, wildlife tracking, multimedia content analysis, and assistive technologies for the hearing impaired.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Advances in deep neural networks have enabled modern SED models to achieve strong performance, though mainly on small-scale corpora focused on domestic environments&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib4\" title=\"\">4</a>]</cite>. More recently, breakthroughs in audio spectrogram transformers, pretrained on large-scale unlabeled data via self-supervised learning (SSL), have significantly expanded SED capabilities, allowing models to address complex and diverse acoustic scenes involving over 400 sound classes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib7\" title=\"\">7</a>]</cite>. However, these approaches still operate under a closed-vocabulary assumption, treating sound event detection as a multi-class classification task constrained to a predefined set of classes established during training. This limitation not only requires users to manually specify and map target classes for each use case, complicating real-world deployment, but also restricts the model&#8217;s ability to capture rich semantic and acoustic relationships among sound categories. In addition, traditional models do not support direct inference beyond the predefined label space without retraining and incorporating additional labeled data. Moreover, adapting to new events is challenging, as it requires modifying the classification head.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "performance",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these challenges continue to constrain the flexibility and scalability of current SED systems, related advances in computer vision offer a compelling contrast. Large-scale open-vocabulary detection (OVD) has advanced rapidly in computer vision <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib9\" title=\"\">9</a>]</cite>. By enabling automatic detection of arbitrary objects without predefined categories or extensive human annotation, OVD has driven image understanding toward more flexible, scalable, and adaptable recognition. These capabilities have unlocked a range of versatile applications, including evaluating text-to-image generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib10\" title=\"\">10</a>]</cite> and labeling data for tasks such as visual question answering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib11\" title=\"\">11</a>]</cite>. However, such large-scale open-vocabulary functionality remains largely absent in current SED systems, limiting their generalization and hindering progress in broader audio tasks, including evaluating text-to-audio generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib13\" title=\"\">13</a>]</cite>, enhancing audio language models &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib15\" title=\"\">15</a>]</cite>, and serving as a reward signal for methods such as Direct Preference Optimization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although recent efforts have explored text-prompt-based separation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib18\" title=\"\">18</a>]</cite>, these approaches primarily aim to extract sound events rather than directly predict their temporal locations. Moreover, they often rely on computationally intensive methods, such as diffusion models, which are designed to improve separation quality when the target sound source is specified. This focus makes them less suitable for scenarios requiring detection across a large number of potential classes, as in SED. While text-query-based separation models have been applied to text-query-based SED&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib19\" title=\"\">19</a>]</cite>, this approach was developed on a small-scale dataset, lacks demonstrated zero-shot and few-shot capabilities, does not offer true open-vocabulary support, and incurs substantial computational overhead due to its reliance on separation processes. As a result, it remains ill-suited for more challenging scenarios involving diverse and numerous sound classes.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by recent advances in large-scale OVD and the need to overcome the limitations of both conventional multi-class classification frameworks and separation-based text-query SED, we investigate the concept of <span class=\"ltx_text ltx_font_bold\">open-vocabulary sound event detection (OV-SED)</span> in this work. However, pursuing OV-SED introduces several key challenges. Unlike OVD in vision, where large-scale datasets are available, the largest temporally labeled dataset for SED, AudioSet-Strong&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib7\" title=\"\">7</a>]</cite>, contains only about 100k audio clips, limiting the amount of training data. Moreover, negative queries play a crucial role in OV-SED training to improve classification accuracy. Yet, the presence of missing labels in AudioSet-Strong complicates the reliable construction of negatives. Poorly selected negatives risk introducing noise or incorrect supervision, which can mislead the model during training and ultimately degrade performance.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "ovsed",
                    "performance",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and advance OV-SED, we propose FlexSED, a framework specifically designed to tackle the core issues outlined above. Its key design components are as follows:</p>\n\n",
                "matched_terms": [
                    "flexsed",
                    "ovsed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1) Architecture design for prompt-aware audio modeling.</span> To address data scarcity and enable efficient prompt-based audio modeling, we leverage two pretrained models: a frame-based audio SSL model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib20\" title=\"\">20</a>]</cite> for fine-grained acoustic representation and the CLAP text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib21\" title=\"\">21</a>]</cite> for sound class prompt comprehension. We decompose the audio SSL model into an audio-only encoder and an audio-text fusion decoder specialized for prompt-audio integration, balancing performance and inference efficiency. Furthermore, we introduce a seamless adaptive fusion strategy to enable continued training from pretrained weights.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(2) Mitigating missing label issues.</span> To mitigate the risk of incorrect negative queries arising from incomplete labels, we propose a negative candidate filtering strategy that leverages large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib22\" title=\"\">22</a>]</cite> to infer semantic relationships among sound classes. This approach helps avoid conflicts between positive and negative queries, ensuring more reliable and consistent supervision.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(3) Effectiveness and flexibility.</span> FlexSED surpasses conventional multi-class classification methods on the AudioSet-Strong evaluation set, while also demonstrating strong zero-shot capabilities and few-shot expandability.</p>\n\n",
                "matched_terms": [
                    "flexsed",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></span>&#160;(a)</a>, OV-SED takes an audio clip along with a list of sound event candidate prompts as input. The audio signal is first processed by the encoder independently, without interacting with the candidate prompts. Meanwhile, the text prompts are processed by a text encoder. The decoder then fuses the audio-text embedding pairs in parallel, enabling interactions between candidate prompts, and produces the corresponding posterior probabilities.</p>\n\n",
                "matched_terms": [
                    "ovsed",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build a simple yet effective open-vocabulary sound event detector capable of identifying and localizing sound events from arbitrary text inputs, we propose a unified model that integrates audio and text representations. At its core, the model adopts a vanilla transformer-based SED architecture, modified to predict only the posterior corresponding to the input text prompt, rather than conventional multi-class outputs. This enables flexible, prompt-based sound event detection.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text representation, we leverage CLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib21\" title=\"\">21</a>]</cite>, which produces rich and semantically aligned embeddings that bridge the gap between textual descriptions and audio content. For audio processing, we adopt a frame-based audio spectrogram transformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib20\" title=\"\">20</a>]</cite> built on self-supervised learning, which provides strong audio representations, accelerates convergence on downstream audio and speech tasks, and operates at a latent sample rate of 25 Hz, making it well-suited for sound event detection.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, we denote this method as FlexSED for its flexible design that leverages pre-trained models to enable efficient OV-SED.</p>\n\n",
                "matched_terms": [
                    "flexsed",
                    "ovsed",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When training FlexSED, we randomly compose positive and negative queries, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></span>&#160;(c)</a>. Positive queries correspond to labeled sound events, while negative queries refer to unlabeled events. However, we observe that <span class=\"ltx_text ltx_font_bold\">AudioSet-Strong</span> suffers from potential missing labels. Treating such unlabeled yet present events as negative queries can confuse the model during training, undermine prompt-event alignment, and ultimately degrade performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "flexsed",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on audio sampled at 16&#160;kHz. The transformer backbone is based on Dasheng-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mispeech/dasheng-base\" title=\"\">https://huggingface.co/mispeech/dasheng-base</a></span></span></span>, which operates on 64-dimensional log-Mel spectrograms extracted every 10&#160;ms using a window size of 32&#160;ms. For text encoding, we adopt the text encoder from Laion-CLAP<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/clap-htsat-fused\" title=\"\">https://huggingface.co/laion/clap-htsat-fused</a></span></span></span>, employing simple prompts of the form &#8220;A sound of {<span class=\"ltx_text ltx_font_italic\">class</span>}&#8221; for sound event queries, where {<span class=\"ltx_text ltx_font_italic\">class</span>} refers to the sound class names defined in the AudioSet ontology.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FlexSED transformer is initialized with the pre-trained Dasheng model, while the CLAP model remains frozen during training. FlexSED is optimized using AdamW with learning rates of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> for the decoder and <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> for the encoder. A batch size of 16 audio samples and 20 prompts per audio sample is used. Among the 20 prompts, up to 10 are positive queries sampled from the labels&#8212;if more than 10 positives are available, 10 are randomly selected; if fewer, all are used. The remaining prompts are filled with negative queries randomly sampled from negative candidates. The model is trained for 10 epochs, and the checkpoint with the best validation performance is selected for final evaluation. We apply simple Mel-spectrogram augmentations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib25\" title=\"\">25</a>]</cite>, such as frame shift and filter augmentation, during training for the proposed FlexSED model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We leave the integration of mixup and mean-teacher into FlexSED for future work, as they are not directly compatible with the current framework.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "flexsed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior studies, we adopt the PSDS1 metric <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib27\" title=\"\">27</a>]</cite>, which captures the intersection between ground truth and detected events, emphasizing low reaction time and accurate sound event localization. Unlike earlier works that rely on coarser metrics with resolutions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib7\" title=\"\">7</a>]</cite>, we employ this finer-grained approach for greater temporal precision. Consistent with prior research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>]</cite>, we omit the variance penalty, as it was originally intended for datasets with fewer and more balanced classes. Furthermore, in line with previous work, we exclude PSDS2 from this study, as it is tuned to audio tagging rather than sound event detection and is impractical<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fgnt/sed_scores_eval/issues/5\" title=\"\">https://github.com/fgnt/sed_scores_eval/issues/5</a></span></span></span> to compute on a server with 128 GB of memory when processing over 400 classes. We apply a median filter with a window length of 7 to smooth the model predictions.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We refer to PSDS1 computed across all classes as <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">A</sub></span>. While this metric emphasizes accurate timing, it still penalizes false positives. In addition, we introduce a variant, <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">T</sub></span>, which focuses solely on target sounds. In this setting, inference is restricted to the sound classes present in each audio clip, rather than spanning all 407 classes. This reflects a practical use case in which the target sounds are already known to the user, but precise temporal localization is still required. Such a scenario highlights the model&#8217;s potential for annotating weakly labeled data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib28\" title=\"\">28</a>]</cite> or integrating with audio-tagging models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib29\" title=\"\">29</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "psds1a",
                    "sound",
                    "psds1t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train two vanilla SED models for comparison. First, we reproduce ATST-Frame-SED on AudioSet-Strong following the procedure outlined in the original paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>]</cite>. In parallel, we train Dasheng-SED using the same vanilla multi-class classification setup and training procedure. Notably, the Dasheng-based backbone demonstrates superior overall performance and exhibits reduced sensitivity to hyperparameters compared to ATST-Frame. Based on these observations, we adopt Dasheng as the backbone for FlexSED in our experiments. Additionally, we extend a target sound separation model for OV-SED by adding a classification layer on the pre-trained AudioSep<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "ovsed",
                    "dashengsed",
                    "performance",
                    "flexsed",
                    "comparison",
                    "atstframesed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T2\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we compare several fusion strategies with AdaLN-One. Specifically, AdaLN-Zero initializes the gating parameter to zero; Token Fusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib30\" title=\"\">30</a>]</cite> directly inserts the prompt token into the decoder sequence; and Cross-Attention (CrossAttn) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib31\" title=\"\">31</a>]</cite> introduces an additional cross-attention layer in the decoder blocks, using the audio embedding as the query and the prompt token as the key and value. Among these methods, AdaLN-One achieves the best performance. Cross-Attention performs slightly worse, likely due to the substantial architectural changes it introduces. AdaLN-Zero also lags behind AdaLN-One, highlighting the importance of seamless AdaLN parameter initialization for stable and effective continuous training. Token Fusion performs the worst, presumably because the pre-trained model is optimized to attend to audio embeddings and struggles to shift focus to the newly added prompt token.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We explore different configurations for splitting the original Transformer into encoder and decoder components.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Due to computational constraints and inefficiency with large number of sound classes, we do not use all blocks for decoding.</span></span></span>. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T2\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, dividing the encoder and decoder equally yields the best performance. Reducing the decoder size results in a performance drop, likely because fewer decoder layers limit the model&#8217;s ability to capture prompt-conditioned representations.\nOn the other hand, increasing the decoder beyond half of the total blocks impairs performance, as too few encoder layers remain to learn prompt-independent acoustic features. Additionally, we experiment with adding two extra decoder blocks copied from the last two layers of the original model. This strategy performs worse than directly using the last two blocks as the decoder, suggesting that maintaining the original structure is important for effective continuous training.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s zero-shot performance on unseen sound classes and its adaptability through fine-tuning with limited labeled data, we conduct zero-shot and few-shot experiments. Specifically, we randomly exclude <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> sound classes from training, ensuring these classes remain unseen for both positive and negative samples. We then assess the model&#8217;s performance on these excluded classes and compare it to its performance when trained on all classes. To ensure robustness, this procedure is repeated across three random folds of unseen classes, and we report the mean retained PSDS ratios (R-PSDS1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">E</span></sup>) relative to the full-model performance on the excluded classes.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings highlight the model&#8217;s ability to generalize reasonably well to unseen classes in zero-shot settings, especially for temporal localization, and its strong adaptability through few-shot learning to recover classification performance. This demonstrates the practicality and scalability of the proposed approach for sound event detection in dynamic and evolving environments.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "performance",
                    "sound",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present FlexSED, a framework for open-vocabulary sound event detection. FlexSED effectively addresses key challenges in OV-SED by leveraging pretrained audio SSL and text models, introducing an adaptive fusion strategy for efficient prompt-audio integration, and mitigating label noise through LLM-guided pair filtering. These design choices enable FlexSED to outperform traditional classification approaches, while supporting zero-shot and few-shot capabilities for diverse real-world applications. For future work, we plan to explore strategies such as knowledge distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>]</cite> to further enhance model performance, scale training to larger and more diverse audio datasets, and investigate multimodal extensions that support richer query interactions beyond text prompts.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "filtering",
                    "sound",
                    "ovsed",
                    "performance",
                    "flexsed",
                    "event"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
        "caption": "Table 2: Ablation study of model design choices.",
        "body": "Fusion\nEnc. Blocks\nDec. Blocks\nPSDS1A ↑\\uparrow\nPSDS1T ↑\\uparrow\n\n\nToken Fusion\nFirst 6\nLast 6\n0.4378\n0.5696\n\n\nCrossAttn\nFirst 6\nLast 6\n0.4430\n0.5800\n\n\nAdaLN-Zero\nFirst 6\nLast 6\n0.4383\n0.5717\n\n\nAdaLN-One\nFirst 4\nLast 8\n0.4374\n0.5861\n\n\n\n\\rowcolorlightblue\nAdaLN-One\n\nFirst 6\nLast 6\n0.4484\n0.5863\n\n\nAdaLN-One\nFirst 8\nLast 4\n0.4393\n0.5795\n\n\nAdaLN-One\nFirst 10\nLast 2\n0.4249\n0.5826\n\n\nAdaLN-One\nFirst 12\nCopied Last 2\n0.4205\n0.5728",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Fusion</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Enc. Blocks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dec. Blocks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">A</span></sub>&#160;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">T</span></sub>&#160;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Token Fusion</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">First 6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Last 6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.4378</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5696</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CrossAttn</th>\n<td class=\"ltx_td ltx_align_center\">First 6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Last 6</td>\n<td class=\"ltx_td ltx_align_center\">0.4430</td>\n<td class=\"ltx_td ltx_align_center\">0.5800</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AdaLN-Zero</th>\n<td class=\"ltx_td ltx_align_center\">First 6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Last 6</td>\n<td class=\"ltx_td ltx_align_center\">0.4383</td>\n<td class=\"ltx_td ltx_align_center\">0.5717</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AdaLN-One</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">First 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Last 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.4374</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5861</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightblue\n<span class=\"ltx_text ltx_font_bold\">AdaLN-One</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">First 6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Last 6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.4484</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.5863</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AdaLN-One</th>\n<td class=\"ltx_td ltx_align_center\">First 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Last 4</td>\n<td class=\"ltx_td ltx_align_center\">0.4393</td>\n<td class=\"ltx_td ltx_align_center\">0.5795</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AdaLN-One</th>\n<td class=\"ltx_td ltx_align_center\">First 10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Last 2</td>\n<td class=\"ltx_td ltx_align_center\">0.4249</td>\n<td class=\"ltx_td ltx_align_center\">0.5826</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">AdaLN-One</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">First 12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Copied Last 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.4205</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.5728</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ablation",
            "blocks",
            "first",
            "dec",
            "adalnone",
            "copied",
            "last",
            "model",
            "psds1t",
            "adalnzero",
            "rowcolorlightblue",
            "token",
            "fusion",
            "↑uparrow",
            "psds1a",
            "study",
            "enc",
            "design",
            "choices",
            "crossattn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T2\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we compare several fusion strategies with AdaLN-One. Specifically, AdaLN-Zero initializes the gating parameter to zero; Token Fusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib30\" title=\"\">30</a>]</cite> directly inserts the prompt token into the decoder sequence; and Cross-Attention (CrossAttn) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib31\" title=\"\">31</a>]</cite> introduces an additional cross-attention layer in the decoder blocks, using the audio embedding as the query and the prompt token as the key and value. Among these methods, AdaLN-One achieves the best performance. Cross-Attention performs slightly worse, likely due to the substantial architectural changes it introduces. AdaLN-Zero also lags behind AdaLN-One, highlighting the importance of seamless AdaLN parameter initialization for stable and effective continuous training. Token Fusion performs the worst, presumably because the pre-trained model is optimized to attend to audio embeddings and struggles to shift focus to the newly added prompt token.</p>\n\n",
            "<p class=\"ltx_p\">We explore different configurations for splitting the original Transformer into encoder and decoder components.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Due to computational constraints and inefficiency with large number of sound classes, we do not use all blocks for decoding.</span></span></span>. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T2\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, dividing the encoder and decoder equally yields the best performance. Reducing the decoder size results in a performance drop, likely because fewer decoder layers limit the model&#8217;s ability to capture prompt-conditioned representations.\nOn the other hand, increasing the decoder beyond half of the total blocks impairs performance, as too few encoder layers remain to learn prompt-independent acoustic features. Additionally, we experiment with adding two extra decoder blocks copied from the last two layers of the original model. This strategy performs worse than directly using the last two blocks as the decoder, suggesting that maintaining the original structure is important for effective continuous training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability.\nAlthough text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1) Architecture design for prompt-aware audio modeling.</span> To address data scarcity and enable efficient prompt-based audio modeling, we leverage two pretrained models: a frame-based audio SSL model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib20\" title=\"\">20</a>]</cite> for fine-grained acoustic representation and the CLAP text encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib21\" title=\"\">21</a>]</cite> for sound class prompt comprehension. We decompose the audio SSL model into an audio-only encoder and an audio-text fusion decoder specialized for prompt-audio integration, balancing performance and inference efficiency. Furthermore, we introduce a seamless adaptive fusion strategy to enable continued training from pretrained weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fusion",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the audio transformer does not natively support text fusion. To address this, inspired by AdaLN-Zero <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib23\" title=\"\">23</a>]</cite>, commonly used in diffusion transformers for effective feature fusion, we introduce AdaLN into the pre-trained audio SSL blocks, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Fig.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></span>&#160;(b)</a>. Unlike AdaLN-Zero, which initializes the residual scaling gate to zero, we initialize this gate to one, creating a variant we call AdaLN-One. This design preserves the original SSL feature flow at initialization, while allowing the model to gradually learn to integrate text prompts during training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "blocks",
                    "adalnone",
                    "design",
                    "adalnzero",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, while directly fusing prompts into each transformer block is feasible, this approach becomes inefficient when handling a large number of sound classes. To overcome this limitation, we propose an encoder-decoder architecture: the encoder focuses solely on audio representation, while the decoder integrates text prompts for fusion. This design enables encoder features to be cached during inference, improving efficiency when evaluating numerous candidate sound classes.</p>\n\n",
                "matched_terms": [
                    "fusion",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior studies, we adopt the PSDS1 metric <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib27\" title=\"\">27</a>]</cite>, which captures the intersection between ground truth and detected events, emphasizing low reaction time and accurate sound event localization. Unlike earlier works that rely on coarser metrics with resolutions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib7\" title=\"\">7</a>]</cite>, we employ this finer-grained approach for greater temporal precision. Consistent with prior research <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>]</cite>, we omit the variance penalty, as it was originally intended for datasets with fewer and more balanced classes. Furthermore, in line with previous work, we exclude PSDS2 from this study, as it is tuned to audio tagging rather than sound event detection and is impractical<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fgnt/sed_scores_eval/issues/5\" title=\"\">https://github.com/fgnt/sed_scores_eval/issues/5</a></span></span></span> to compute on a server with 128 GB of memory when processing over 400 classes. We apply a median filter with a window length of 7 to smooth the model predictions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We refer to PSDS1 computed across all classes as <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">A</sub></span>. While this metric emphasizes accurate timing, it still penalizes false positives. In addition, we introduce a variant, <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">T</sub></span>, which focuses solely on target sounds. In this setting, inference is restricted to the sound classes present in each audio clip, rather than spanning all 407 classes. This reflects a practical use case in which the target sounds are already known to the user, but precise temporal localization is still required. Such a scenario highlights the model&#8217;s potential for annotating weakly labeled data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib28\" title=\"\">28</a>]</cite> or integrating with audio-tagging models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib29\" title=\"\">29</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "psds1a",
                    "psds1t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train two vanilla SED models for comparison. First, we reproduce ATST-Frame-SED on AudioSet-Strong following the procedure outlined in the original paper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>]</cite>. In parallel, we train Dasheng-SED using the same vanilla multi-class classification setup and training procedure. Notably, the Dasheng-based backbone demonstrates superior overall performance and exhibits reduced sensitivity to hyperparameters compared to ATST-Frame. Based on these observations, we adopt Dasheng as the backbone for FlexSED in our experiments. Additionally, we extend a target sound separation model for OV-SED by adding a classification layer on the pre-trained AudioSep<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present FlexSED, a framework for open-vocabulary sound event detection. FlexSED effectively addresses key challenges in OV-SED by leveraging pretrained audio SSL and text models, introducing an adaptive fusion strategy for efficient prompt-audio integration, and mitigating label noise through LLM-guided pair filtering. These design choices enable FlexSED to outperform traditional classification approaches, while supporting zero-shot and few-shot capabilities for diverse real-world applications. For future work, we plan to explore strategies such as knowledge distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>]</cite> to further enhance model performance, scale training to larger and more diverse audio datasets, and investigate multimodal extensions that support richer query interactions beyond text prompts.</p>\n\n",
                "matched_terms": [
                    "choices",
                    "model",
                    "fusion",
                    "design"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
        "caption": "Table 3: Zero-shot and few-shot performance with different numbers of excluded classes (KoutK_{\\mathrm{out}}). Mean retained performance ratios (R-PSDS1) are computed relative to the full-model PSDS1AE{}^{E}_{A} and PSDS1TE{}^{E}_{T} on excluded classes across 3 folds.",
        "body": "KoutK_{\\mathrm{out}}\nSetting\nR-PSDS1AE{}^{E}_{A} ↑\\uparrow\nR-PSDS1↑TE{}^{E}_{T}\\uparrow\n\n\n\n\n20\nzero-shot\n64.99%\n88.27%\n\n\n5-shot\n80.23%\n93.94%\n\n\n10-shot\n84.58%\n95.43%\n\n\n20-shot\n87.02%\n96.81%\n\n\n40\nzero-shot\n55.04%\n85.74%\n\n\n5-shot\n75.74%\n94.26%\n\n\n10-shot\n77.41%\n96.84%\n\n\n20-shot\n82.83%\n98.40%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:14.0pt;padding-right:14.0pt;\"><math alttext=\"K_{\\mathrm{out}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><msub><mi>K</mi><mi>out</mi></msub><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:14.0pt;padding-right:14.0pt;\"><span class=\"ltx_text ltx_font_bold\">Setting</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:14.0pt;padding-right:14.0pt;\"><span class=\"ltx_text ltx_font_bold\">R-PSDS1<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:14.0pt;padding-right:14.0pt;\"><span class=\"ltx_text ltx_font_bold\">R-PSDS1<math alttext=\"{}^{E}_{T}\\uparrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8593;</mo><mprescripts/><mi>T</mi><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{T}\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">20</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">zero-shot</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">64.99%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">88.27%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">5-shot</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">80.23%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">93.94%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">10-shot</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">84.58%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">95.43%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">20-shot</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">87.02%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">96.81%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">40</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">zero-shot</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">55.04%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">85.74%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">5-shot</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">75.74%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">94.26%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">10-shot</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">77.41%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">96.84%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">20-shot</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">82.83%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:14.0pt;padding-right:14.0pt;\">98.40%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "computed",
            "relative",
            "rpsds1aeea",
            "folds",
            "numbers",
            "classes",
            "5shot",
            "across",
            "zeroshot",
            "20shot",
            "mean",
            "koutkmathrmout",
            "ratios",
            "fewshot",
            "fullmodel",
            "rpsds1↑teetuparrow",
            "rpsds1",
            "performance",
            "↑uparrow",
            "excluded",
            "retained",
            "psds1teet",
            "setting",
            "10shot",
            "different",
            "psds1aeea"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T3\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, the results reveal several important trends regarding the model&#8217;s generalization and adaptability. In the zero-shot setting, the model retains a significant portion of its performance on unseen classes, particularly for PSDS1<math alttext=\"{}^{E}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>T</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{T}</annotation></semantics></math>, where retention rates reach around 88% with <math alttext=\"K_{\\mathrm{out}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>K</mi><mi>out</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}=20</annotation></semantics></math> and 86% with <math alttext=\"K_{\\mathrm{out}}=40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>K</mi><mi>out</mi></msub><mo>=</mo><mn>40</mn></mrow><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}=40</annotation></semantics></math>. In contrast, PSDS1<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> exhibits more pronounced degradation, dropping from around 65% to 55% as the number of excluded classes increases. This is expected, as PSDS<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m5\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> reflects both localization and classification performance, making it more sensitive to false positives and false negatives. Unseen classes can easily cause misclassification and missed detections, which jointly impact PSDS<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m6\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> more severely. On the other hand, PSDS<math alttext=\"{}^{E}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m7\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>T</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{T}</annotation></semantics></math> focuses primarily on timing accuracy and does not penalize false positives, making it more robust in zero-shot scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability.\nAlthough text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.\n</p>\n\n",
                "matched_terms": [
                    "classes",
                    "across",
                    "zeroshot",
                    "fewshot",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Advances in deep neural networks have enabled modern SED models to achieve strong performance, though mainly on small-scale corpora focused on domestic environments&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib4\" title=\"\">4</a>]</cite>. More recently, breakthroughs in audio spectrogram transformers, pretrained on large-scale unlabeled data via self-supervised learning (SSL), have significantly expanded SED capabilities, allowing models to address complex and diverse acoustic scenes involving over 400 sound classes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib7\" title=\"\">7</a>]</cite>. However, these approaches still operate under a closed-vocabulary assumption, treating sound event detection as a multi-class classification task constrained to a predefined set of classes established during training. This limitation not only requires users to manually specify and map target classes for each use case, complicating real-world deployment, but also restricts the model&#8217;s ability to capture rich semantic and acoustic relationships among sound categories. In addition, traditional models do not support direct inference beyond the predefined label space without retraining and incorporating additional labeled data. Moreover, adapting to new events is challenging, as it requires modifying the classification head.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "classes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although recent efforts have explored text-prompt-based separation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib18\" title=\"\">18</a>]</cite>, these approaches primarily aim to extract sound events rather than directly predict their temporal locations. Moreover, they often rely on computationally intensive methods, such as diffusion models, which are designed to improve separation quality when the target sound source is specified. This focus makes them less suitable for scenarios requiring detection across a large number of potential classes, as in SED. While text-query-based separation models have been applied to text-query-based SED&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib19\" title=\"\">19</a>]</cite>, this approach was developed on a small-scale dataset, lacks demonstrated zero-shot and few-shot capabilities, does not offer true open-vocabulary support, and incurs substantial computational overhead due to its reliance on separation processes. As a result, it remains ill-suited for more challenging scenarios involving diverse and numerous sound classes.</p>\n\n",
                "matched_terms": [
                    "classes",
                    "fewshot",
                    "across",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(3) Effectiveness and flexibility.</span> FlexSED surpasses conventional multi-class classification methods on the AudioSet-Strong evaluation set, while also demonstrating strong zero-shot capabilities and few-shot expandability.</p>\n\n",
                "matched_terms": [
                    "fewshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We refer to PSDS1 computed across all classes as <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">A</sub></span>. While this metric emphasizes accurate timing, it still penalizes false positives. In addition, we introduce a variant, <span class=\"ltx_text ltx_font_bold\">PSDS1<sub class=\"ltx_sub\">T</sub></span>, which focuses solely on target sounds. In this setting, inference is restricted to the sound classes present in each audio clip, rather than spanning all 407 classes. This reflects a practical use case in which the target sounds are already known to the user, but precise temporal localization is still required. Such a scenario highlights the model&#8217;s potential for annotating weakly labeled data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib28\" title=\"\">28</a>]</cite> or integrating with audio-tagging models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib29\" title=\"\">29</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "classes",
                    "computed",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T1\" title=\"In 4.1 Comparison with Vanilla SED &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, the proposed FlexSED significantly outperforms both the vanilla SED models and the target sound separation-based model. This improvement can be largely attributed to the integration of knowledge from the pre-trained CLAP and DaSheng models. CLAP enhances FlexSED&#8217;s ability to capture semantic relationships among sound classes compared to vanilla SED models, while DaSheng provides stronger temporal and acoustic modeling through its pre-trained audio transformer than AudioSep&#8217;s convolutional U-Net architecture. In addition to the performance boost, FlexSED also offers open-vocabulary capabilities for simpler usability. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.SS3\" title=\"4.3 Zero-shot and Few-shot Performance &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.3</span></a>, we demonstrate its effectiveness in zero-shot settings and its expandability in few-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "classes",
                    "fewshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We explore different configurations for splitting the original Transformer into encoder and decoder components.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Due to computational constraints and inefficiency with large number of sound classes, we do not use all blocks for decoding.</span></span></span>. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#S4.T2\" title=\"In 4.2.3 Negative Query Filtering Strategy &#8227; 4.2 Ablation Studies &#8227; 4 Results and Discussion &#8227; FlexSED: Towards Open-Vocabulary Sound Event Detection\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, dividing the encoder and decoder equally yields the best performance. Reducing the decoder size results in a performance drop, likely because fewer decoder layers limit the model&#8217;s ability to capture prompt-conditioned representations.\nOn the other hand, increasing the decoder beyond half of the total blocks impairs performance, as too few encoder layers remain to learn prompt-independent acoustic features. Additionally, we experiment with adding two extra decoder blocks copied from the last two layers of the original model. This strategy performs worse than directly using the last two blocks as the decoder, suggesting that maintaining the original structure is important for effective continuous training.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "classes",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the model&#8217;s zero-shot performance on unseen sound classes and its adaptability through fine-tuning with limited labeled data, we conduct zero-shot and few-shot experiments. Specifically, we randomly exclude <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> sound classes from training, ensuring these classes remain unseen for both positive and negative samples. We then assess the model&#8217;s performance on these excluded classes and compare it to its performance when trained on all classes. To ensure robustness, this procedure is repeated across three random folds of unseen classes, and we report the mean retained PSDS ratios (R-PSDS1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">E</span></sup>) relative to the full-model performance on the excluded classes.</p>\n\n",
                "matched_terms": [
                    "classes",
                    "retained",
                    "across",
                    "relative",
                    "zeroshot",
                    "ratios",
                    "mean",
                    "folds",
                    "fewshot",
                    "fullmodel",
                    "performance",
                    "excluded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot learning, we fine-tune the model on the excluded classes using a small number of labeled audio clips. The decoder is updated with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>, and the encoder with <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, over 25 epochs with a batch size of 8 audio clips.</p>\n\n",
                "matched_terms": [
                    "classes",
                    "fewshot",
                    "excluded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once few-shot fine-tuning is applied, performance improves considerably. With as few as 5 shots, PSDS1<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> increases to about 80% for <math alttext=\"K_{\\mathrm{out}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>K</mi><mi>out</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}=20</annotation></semantics></math> and 76% for <math alttext=\"K_{\\mathrm{out}}=40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>K</mi><mi>out</mi></msub><mo>=</mo><mn>40</mn></mrow><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}=40</annotation></semantics></math>. Additional labeled samples continue to boost performance, and with 20-shot fine-tuning, PSDS1<math alttext=\"{}^{E}_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m4\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>A</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{A}</annotation></semantics></math> reaches roughly 87% and 83%, while PSDS1<math alttext=\"{}^{E}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m5\" intent=\":literal\"><semantics><mmultiscripts><mi/><mprescripts/><mi>T</mi><mrow/><mrow/><mi>E</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{}^{E}_{T}</annotation></semantics></math> surpasses 96% and 98%, nearly closing the gap to full training. Although larger <math alttext=\"K_{\\mathrm{out}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m6\" intent=\":literal\"><semantics><msub><mi>K</mi><mi>out</mi></msub><annotation encoding=\"application/x-tex\">K_{\\mathrm{out}}</annotation></semantics></math> values naturally introduce greater challenges due to a broader unseen class space, fine-tuning effectively mitigates these difficulties.</p>\n\n",
                "matched_terms": [
                    "psds1teet",
                    "20shot",
                    "fewshot",
                    "koutkmathrmout",
                    "psds1aeea",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these findings highlight the model&#8217;s ability to generalize reasonably well to unseen classes in zero-shot settings, especially for temporal localization, and its strong adaptability through few-shot learning to recover classification performance. This demonstrates the practicality and scalability of the proposed approach for sound event detection in dynamic and evolving environments.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "classes",
                    "fewshot",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present FlexSED, a framework for open-vocabulary sound event detection. FlexSED effectively addresses key challenges in OV-SED by leveraging pretrained audio SSL and text models, introducing an adaptive fusion strategy for efficient prompt-audio integration, and mitigating label noise through LLM-guided pair filtering. These design choices enable FlexSED to outperform traditional classification approaches, while supporting zero-shot and few-shot capabilities for diverse real-world applications. For future work, we plan to explore strategies such as knowledge distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18606v1#bib.bib6\" title=\"\">6</a>]</cite> to further enhance model performance, scale training to larger and more diverse audio datasets, and investigate multimodal extensions that support richer query interactions beyond text prompts.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fewshot",
                    "zeroshot"
                ]
            }
        ]
    }
}