{
    "S4.T1": {
        "source_file": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation",
        "caption": "TABLE I: Common Training Practices in Modern ASR.",
        "body": "Practice\n\n\nPurpose\n\n\n\n\nNotes\n\n\n\n\n\n\nSpecAugment\n\n\nData augmentation for improved robustness\n\n\n\n\nApplies time warping, frequency masking, and time masking to spectrograms [1].\n\n\n\n\nLabel Smoothing\n\n\nRegularization to prevent overconfidence\n\n\n\n\nReplaces hard one-hot labels with a smoothed distribution. Typical α∈[0.05,0.2]\\alpha\\in[0.05,0.2] [21].\n\n\n\n\nLM Fusion\n\n\nAccuracy improvement via external knowledge\n\n\n\n\nLinearly combines AM and LM scores during beam search decoding (shallow fusion) [25].\n\n\n\n\nSelf-Supervised Pre-training\n\n\nLearn representations from unlabeled data\n\n\n\n\ne.g., wav2vec 2.0 uses a contrastive task on masked latent audio [7].\n\n\n\n\nCurriculum Learning\n\n\nImproved convergence and robustness\n\n\n\n\nTraining progresses from easy examples (high SNR) to hard examples (low SNR) [24].",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Practice</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\"><span class=\"ltx_text ltx_font_bold\">Purpose</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\"><span class=\"ltx_text ltx_font_bold\">Notes</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SpecAugment</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Data augmentation for improved robustness</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Applies time warping, frequency masking, and time masking to spectrograms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib1\" title=\"\">1</a>]</cite>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Label Smoothing</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Regularization to prevent overconfidence</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Replaces hard one-hot labels with a smoothed distribution. Typical <math alttext=\"\\alpha\\in[0.05,0.2]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0.05</mn><mo>,</mo><mn>0.2</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in[0.05,0.2]</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib21\" title=\"\">21</a>]</cite>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LM Fusion</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Accuracy improvement via external knowledge</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Linearly combines AM and LM scores during beam search decoding (shallow fusion) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib25\" title=\"\">25</a>]</cite>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Supervised Pre-training</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Learn representations from unlabeled data</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">e.g., wav2vec 2.0 uses a contrastive task on masked latent audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Curriculum Learning</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Improved convergence and robustness</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:76.8pt;\">Training progresses from easy examples (high SNR) to hard examples (low SNR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib24\" title=\"\">24</a>]</cite>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "curriculum",
            "improved",
            "progresses",
            "smoothing",
            "α∈00502alphain00502",
            "frequency",
            "search",
            "snr",
            "data",
            "high",
            "applies",
            "notes",
            "hard",
            "typical",
            "pretraining",
            "masking",
            "via",
            "common",
            "time",
            "beam",
            "robustness",
            "labels",
            "smoothed",
            "asr",
            "label",
            "accuracy",
            "regularization",
            "latent",
            "practice",
            "specaugment",
            "distribution",
            "audio",
            "scores",
            "onehot",
            "knowledge",
            "selfsupervised",
            "learning",
            "convergence",
            "replaces",
            "linearly",
            "decoding",
            "learn",
            "during",
            "examples",
            "warping",
            "training",
            "representations",
            "from",
            "augmentation",
            "modern",
            "purpose",
            "shallow",
            "external",
            "unlabeled",
            "task",
            "spectrograms",
            "easy",
            "improvement",
            "uses",
            "contrastive",
            "wav2vec",
            "low",
            "masked",
            "combines",
            "practices",
            "fusion",
            "prevent",
            "overconfidence"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of large-scale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "learning",
                    "modern",
                    "wav2vec",
                    "asr",
                    "data",
                    "high",
                    "specaugment",
                    "training",
                    "from",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR), the task of converting spoken language into text, is a cornerstone of modern human-computer interaction, powering applications from voice assistants and dictation software to in-car control systems and automated transcription services <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib1\" title=\"\">1</a>]</cite>. The field has witnessed remarkable progress, evolving from early systems rooted in statistical methods to the highly performant deep learning models of today <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "modern",
                    "asr",
                    "task",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, ASR systems were constructed from a complex pipeline of independently trained components: an acoustic model (AM), typically a Gaussian Mixture Model-Hidden Markov Model (GMM-HMM), a pronunciation model (lexicon), and a language model (LM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib4\" title=\"\">4</a>]</cite>. The advent of deep neural networks (DNNs) replaced GMMs in hybrid DNN-HMM systems, yielding significant accuracy improvements <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib2\" title=\"\">2</a>]</cite>. However, this modular design was complex, required domain expertise for each component, and suffered from error propagation between stages.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asr",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The modern era of ASR is defined by a paradigm shift towards end-to-end (E2E) models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib6\" title=\"\">6</a>]</cite>. These systems learn a direct mapping from acoustic features to text using a single, jointly optimized neural network, simplifying the training process and often improving performance. This survey focuses on the architectures, training paradigms, and evaluation methodologies that characterize this modern, E2E-centric landscape.</p>\n\n",
                "matched_terms": [
                    "modern",
                    "asr",
                    "learn",
                    "training",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second, equally transformative trend has been the shift in data paradigms. While supervised learning on large, transcribed corpora drove early deep learning successes, the field has increasingly moved towards methods that leverage vast quantities of unlabeled audio data. Self-supervised learning (SSL) frameworks, most notably wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>, have demonstrated the ability to learn powerful speech representations from raw audio, which can then be fine-tuned with minimal labeled data to achieve state-of-the-art results. Concurrently, large-scale weak supervision, exemplified by OpenAI&#8217;s Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>, has shown that training on massive, diverse, and multilingual web-scale data can produce a single model with remarkable zero-shot robustness across numerous domains and conditions.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "learning",
                    "unlabeled",
                    "from",
                    "data",
                    "learn",
                    "training",
                    "wav2vec",
                    "representations",
                    "audio",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A detailed analysis of modern training paradigms, tracing the evolution from supervised methods and data augmentation to the transformative impact of self-supervised and weakly-supervised learning.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "modern",
                    "learning",
                    "data",
                    "training",
                    "from",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An examination of practical deployment challenges, such as streaming inference for real-time applications and model efficiency for on-device processing, and a discussion of critical ethical considerations, including system robustness, fairness, and data privacy.</p>\n\n",
                "matched_terms": [
                    "data",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Numerous surveys have chronicled the progress of ASR over the years. Early reviews focused on the statistical foundations of GMM-HMM systems and the initial integration of neural networks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib9\" title=\"\">9</a>]</cite>. More recent surveys have documented the transition to deep learning, often comparing hybrid DNN-HMM systems with the first generation of E2E models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib10\" title=\"\">10</a>]</cite>. For instance, Saon et al. provided an overview of English conversational speech recognition, highlighting the challenges of the Switchboard task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib11\" title=\"\">11</a>]</cite>. A comprehensive survey by Battenberg et al. explored various E2E approaches, comparing the merits of CTC and attention-based models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "asr",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the field has advanced at a rapid pace since these publications. The consolidation of the Transformer architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib13\" title=\"\">13</a>]</cite> and its speech-specific variant, the Conformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite>, has established a new baseline for acoustic modeling. Simultaneously, the rise of self-supervised learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite> and large-scale pre-training <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite> has fundamentally altered the data requirements and training methodologies for state-of-the-art ASR. These developments have rendered many earlier surveys incomplete.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "learning",
                    "asr",
                    "data",
                    "training",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper distinguishes itself by focusing specifically on this contemporary landscape, which we define as the period marked by the dominance of Transformer-based architectures and the widespread adoption of self-supervised and weakly-supervised training. We position our work to complement, rather than replace, prior surveys by providing a holistic narrative that integrates four key themes that are often treated in isolation:</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Paradigms:</span> We place significant emphasis on SSL and large-scale weak supervision, which represent the most significant shift in ASR development in recent years.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ethics and Robustness:</span> We address the growing importance of evaluating ASR systems not just for accuracy, but also for their robustness in noisy conditions and their fairness across diverse speaker populations.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "accuracy",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By weaving these threads together, this survey provides a cohesive and up-to-date reference for the modern era of ASR.</p>\n\n",
                "matched_terms": [
                    "modern",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transition from hybrid to E2E systems was driven by the development of neural network architectures capable of learning the alignment between a variable-length audio input sequence and a variable-length text output sequence. This section details the three foundational E2E paradigms&#8212;CTC, AED, and RNN-T&#8212;and their evolution into the current state-of-the-art Transformer and Conformer models. A canonical overview of the modern ASR pipeline is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#S3.F1\" title=\"Figure 1 &#8227; III Architectures and Decoding &#8227; Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "modern",
                    "asr",
                    "from",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduced by Graves et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib5\" title=\"\">5</a>]</cite>, Connectionist Temporal Classification (CTC) was a pioneering approach to E2E ASR. It addresses the alignment problem by augmenting the output vocabulary (e.g., characters) with a special &#8216;blank&#8216; token, denoted as <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>. The acoustic model, typically a Recurrent Neural Network (RNN), processes the input audio sequence <math alttext=\"X=(x_{1},\\dots,x_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=(x_{1},\\dots,x_{T})</annotation></semantics></math> and produces a probability distribution over this augmented vocabulary for each time step, resulting in an output matrix of size <math alttext=\"T\\times(|\\mathcal{V}|+1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">T\\times(|\\mathcal{V}|+1)</annotation></semantics></math>, where <math alttext=\"|\\mathcal{V}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{V}|</annotation></semantics></math> is the size of the original vocabulary.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "time",
                    "asr",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary strength of CTC is its non-autoregressive nature. The prediction at each time step is conditionally independent of other predictions given the acoustic input. This allows the probabilities for all time steps to be computed in parallel, making it fast for both training and inference. However, this same independence assumption is its main weakness; CTC models do not inherently learn linguistic constraints or a language model, as the probability of a character does not depend on the previously emitted characters. Consequently, CTC-based systems often require a strong external language model during decoding to achieve competitive performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "external",
                    "decoding",
                    "learn",
                    "during",
                    "training",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Decoder (Speller):</span> This is an autoregressive model that generates the output sequence <math alttext=\"Y=(y_{1},\\dots,y_{U})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>U</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{1},\\dots,y_{U})</annotation></semantics></math> one token at a time. At each step <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the decoder computes a context vector <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">c_{i}</annotation></semantics></math> via an attention mechanism over the encoder outputs <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math>. This context vector represents the most relevant acoustic information for predicting the current token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. The probability of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is then conditioned on the previous tokens and the context vector: <math alttext=\"P(y_{i}|y_{1..i-1},X)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.I1.i2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><mrow><msub><mi>y</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}|y_{1..i-1},X)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "via",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The attention mechanism is the key component, as it explicitly learns a soft alignment between the input audio frames and the output text tokens. Unlike CTC, AED models are not conditionally independent; they directly model the probability of the entire output sequence using the chain rule, thereby learning an implicit language model from the training data. This often leads to better performance than CTC models without an external LM. The main drawback of AEDs is their autoregressive nature, which makes decoding inherently sequential and slower than CTC.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "external",
                    "decoding",
                    "from",
                    "data",
                    "training",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Recurrent Neural Network Transducer (RNN-T), first proposed by Graves <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib17\" title=\"\">17</a>]</cite>, combines the strengths of both CTC and AED models. It has become a dominant architecture for production-grade streaming ASR systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib18\" title=\"\">18</a>]</cite>. The RNN-T architecture consists of three components:</p>\n\n",
                "matched_terms": [
                    "combines",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Joint Network:</span> A feed-forward network that combines the outputs of the acoustic encoder (<math alttext=\"f_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">f_{t}</annotation></semantics></math>) and the prediction network (<math alttext=\"g_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">g_{u}</annotation></semantics></math>) to produce a probability distribution over the augmented vocabulary (labels + &#8216;blank&#8216;).</p>\n\n",
                "matched_terms": [
                    "labels",
                    "combines",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each time step <math alttext=\"(t,u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,u)</annotation></semantics></math>, the joint network computes <math alttext=\"P(y|t,u)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mrow><mi>t</mi><mo>,</mo><mi>u</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y|t,u)</annotation></semantics></math>. If a non-blank label is emitted, the label index <math alttext=\"u\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>u</mi><annotation encoding=\"application/x-tex\">u</annotation></semantics></math> is incremented. If a &#8216;blank&#8216; is emitted, the acoustic frame index <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is incremented. This process continues until the end of the audio sequence is reached. Like CTC, the RNN-T loss function is calculated by summing over all possible alignments using a forward-backward algorithm.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "time",
                    "label"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The RNN-T architecture is naturally suited for streaming because the computation at each step depends only on the current acoustic frame and the previously emitted labels. It does not need to see the entire audio sequence, making it ideal for low-latency applications. It also incorporates an internal language model via the prediction network, overcoming a key limitation of CTC.</p>\n\n",
                "matched_terms": [
                    "via",
                    "labels",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Speech-Transformer</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib19\" title=\"\">19</a>]</cite> was one of the first works to successfully adapt the Transformer for ASR. The encoder and decoder are stacks of multi-head self-attention and feed-forward layers. Self-attention allows the model to weigh the importance of all other frames in the sequence when encoding a specific frame, enabling it to capture long-range dependencies more effectively than RNNs. The removal of recurrence also allows for significantly more parallelization during training.</p>\n\n",
                "matched_terms": [
                    "during",
                    "asr",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Conformer</span> architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite> further improved upon the Transformer by explicitly combining self-attention with convolutions to model both global and local context, respectively. A Conformer block typically consists of a feed-forward module, a multi-head self-attention module, a convolution module, and a final feed-forward module, arranged in a &#8221;macaron-like&#8221; structure. This hybrid approach has proven extremely effective, and Conformers, often paired with a CTC or RNN-T loss function, represent the state of the art for many ASR benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "improved",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inference in E2E models, especially autoregressive ones, involves a search algorithm to find the most probable output sequence. Simple greedy decoding (picking the most likely token at each step) is fast but suboptimal. <span class=\"ltx_text ltx_font_bold\">Beam search</span> is more commonly used, where a fixed number of candidate hypotheses (the &#8221;beam&#8221;) are maintained at each step.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "search",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve accuracy, an external language model (LM), trained on large text corpora, can be integrated during decoding. This is particularly crucial for CTC-based models. A common technique is <span class=\"ltx_text ltx_font_bold\">shallow fusion</span>, where the LM score is linearly interpolated with the acoustic model score at each step of the beam search:</p>\n\n",
                "matched_terms": [
                    "shallow",
                    "external",
                    "linearly",
                    "decoding",
                    "search",
                    "accuracy",
                    "during",
                    "beam",
                    "common",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is a tunable weight. More advanced techniques like cold fusion integrate the LM directly into the model&#8217;s training process.</p>\n\n",
                "matched_terms": [
                    "fusion",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid progress in ASR has been facilitated by powerful open-source toolkits. <span class=\"ltx_text ltx_font_bold\">Kaldi</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib4\" title=\"\">4</a>]</cite> has been the long-standing standard for building traditional hybrid ASR systems. For modern E2E research, <span class=\"ltx_text ltx_font_bold\">ESPnet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib10\" title=\"\">10</a>]</cite> provides a comprehensive and unified framework for a wide range of architectures and tasks. <span class=\"ltx_text ltx_font_bold\">Fairseq</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib20\" title=\"\">20</a>]</cite>, developed by Facebook AI, is a general sequence modeling toolkit that provides reference implementations for influential models like wav2vec 2.0. These toolkits provide reproducible recipes for major benchmarks, lowering the barrier to entry and fostering innovation.</p>\n\n",
                "matched_terms": [
                    "modern",
                    "asr",
                    "wav2vec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of modern ASR architectures is inextricably linked to the paradigms used for their training. While fully supervised learning remains a foundation, recent years have seen a decisive shift towards techniques that either augment existing labeled data or leverage vast quantities of unlabeled data to build more robust and accurate models.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "modern",
                    "unlabeled",
                    "asr",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The conventional approach to training ASR models involves supervised learning on a dataset of paired audio utterances and their corresponding ground-truth transcriptions. The model is trained to minimize a loss function (e.g., CTC, cross-entropy) that measures the discrepancy between its predictions and the reference text.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "asr",
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve model robustness and generalization, data augmentation is a critical step. While traditional methods involve adding noise or reverberation to raw audio, a highly effective and computationally inexpensive technique is <span class=\"ltx_text ltx_font_bold\">SpecAugment</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib1\" title=\"\">1</a>]</cite>. Applied directly to the log-mel spectrogram features, SpecAugment consists of three transformations:</p>\n\n",
                "matched_terms": [
                    "data",
                    "robustness",
                    "specaugment",
                    "audio",
                    "augmentation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Time Warping:</span> A random deformation of the spectrogram along the time axis.</p>\n\n",
                "matched_terms": [
                    "time",
                    "warping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Frequency Masking:</span> Masking out a random contiguous block of frequency channels. This makes the model more robust to partial loss of frequency information.</p>\n\n",
                "matched_terms": [
                    "masking",
                    "frequency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Time Masking:</span> Masking out a random contiguous block of time steps. This encourages the model to learn from the surrounding context and improves robustness to occlusions or short bursts of noise.</p>\n\n",
                "matched_terms": [
                    "learn",
                    "masking",
                    "from",
                    "time",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpecAugment has become a standard component in ASR training pipelines, as it significantly reduces WER without requiring any additional data or complex feature engineering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "specaugment",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Label smoothing</span> is a regularization technique that prevents a model from becoming overconfident in its predictions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib21\" title=\"\">21</a>]</cite>. Instead of using one-hot encoded target labels (where the correct class has a probability of 1.0 and all others have 0.0), the target distribution is &#8221;smoothed&#8221; by distributing a small probability mass <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> across all classes. This encourages the model to produce softer output distributions, which has been shown to improve both accuracy and model calibration <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib22\" title=\"\">22</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "onehot",
                    "labels",
                    "smoothing",
                    "accuracy",
                    "label",
                    "regularization",
                    "distribution",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curriculum learning</span> is a training strategy inspired by human learning, where the model is first presented with easier examples and the difficulty is gradually increased <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib23\" title=\"\">23</a>]</cite>. In ASR, this can be implemented by first training the model on high-quality, clean speech and progressively introducing samples with lower signal-to-noise ratios (SNRs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib24\" title=\"\">24</a>]</cite>. This staged approach can help the model converge faster and achieve better noise robustness.</p>\n\n",
                "matched_terms": [
                    "curriculum",
                    "learning",
                    "asr",
                    "examples",
                    "training",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most significant shift in ASR training has been the rise of self-supervised learning (SSL), which enables models to learn powerful representations from unlabeled audio data, dramatically reducing the need for expensive transcribed corpora. The seminal work in this area is <span class=\"ltx_text ltx_font_bold\">wav2vec 2.0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "learning",
                    "unlabeled",
                    "asr",
                    "from",
                    "data",
                    "learn",
                    "training",
                    "wav2vec",
                    "representations",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pre-training:</span> A large model, typically a Transformer, is trained on thousands of hours of unlabeled speech. The raw audio waveform is first passed through a convolutional feature encoder to obtain a sequence of latent representations. A random subset of these representations is then masked. The model is trained on a contrastive task: for each masked time step, it must identify the correct <span class=\"ltx_text ltx_font_italic\">quantized</span> version of its latent representation from a set of distractors. This forces the model to learn high-level contextualized representations of the speech signal.</p>\n\n",
                "matched_terms": [
                    "masked",
                    "unlabeled",
                    "task",
                    "from",
                    "learn",
                    "latent",
                    "contrastive",
                    "pretraining",
                    "representations",
                    "audio",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning:</span> After pre-training, a small, randomly initialized linear layer is added on top of the Transformer encoder, and the entire model is fine-tuned on a small amount of labeled data for a specific ASR task (e.g., transcribing English).</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr",
                    "data",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The power of this approach is its data efficiency. Baevski et al. demonstrated that a wav2vec 2.0 model pre-trained on 53k hours of unlabeled audio could achieve state-of-the-art results on the LibriSpeech benchmark using as little as ten minutes of labeled data for fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>. This has democratized the development of high-performance ASR for low-resource languages and domains where transcribed data is scarce.</p>\n\n",
                "matched_terms": [
                    "unlabeled",
                    "asr",
                    "data",
                    "wav2vec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is a large Transformer-based encoder-decoder model trained on an enormous dataset of 680,000 hours of multilingual and multitask audio collected from the internet. The &#8221;weak supervision&#8221; comes from the fact that these transcripts are not guaranteed to be perfectly accurate. However, by training on such a vast and diverse dataset, the model learns to be inherently robust to a wide range of acoustic conditions, including background noise, different accents, and various speaking styles. A key finding of this work is that a single, large model trained this way can achieve remarkable <span class=\"ltx_text ltx_font_italic\">zero-shot</span> performance on many standard ASR benchmarks, often competitive with systems that were specifically fine-tuned on those datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>. This approach suggests that data scale and diversity can be a powerful substitute for meticulously curated datasets and complex training objectives.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "from",
                    "data",
                    "training",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and evaluation of ASR systems are grounded in standardized datasets that serve as common benchmarks for the research community. These datasets vary widely in size, domain, recording conditions, and linguistic content, allowing for the assessment of different aspects of system performance.</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib26\" title=\"\">26</a>]</cite> is arguably the most widely used benchmark for ASR in the academic community. It consists of approximately 1000 hours of 16 kHz read English speech derived from audiobooks from the LibriVox project. The data is segmented, aligned, and organized into standardized training, development, and test sets. A key feature is the division of its evaluation sets into &#8216;test-clean&#8216; and &#8216;test-other&#8216; subsets, which represent recordings from speakers who are easier and harder to recognize, respectively. This allows for a nuanced evaluation of model performance under both ideal and more challenging (but still clean) acoustic conditions. Its permissive CC BY 4.0 license has contributed to its widespread adoption.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asr",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Switchboard corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib27\" title=\"\">27</a>]</cite> is a foundational dataset for conversational telephone speech. It contains approximately 300 hours of spontaneous, two-sided telephone conversations between 543 speakers of American English. Participants were given a topic to discuss, resulting in natural, unscripted speech complete with disfluencies, hesitations, and interruptions. The audio is sampled at 8 kHz, reflecting its telephone channel origin. Despite its age, Switchboard remains a challenging and relevant benchmark for evaluating ASR performance on spontaneous conversational speech. It is distributed by the Linguistic Data Consortium (LDC) and requires a license for use.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "asr",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TED-LIUM corpus is derived from TED Talks, offering a large collection of speech from a diverse set of speakers on a wide range of topics. The third release, TED-LIUM 3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib28\" title=\"\">28</a>]</cite>, contains 452 hours of 16 kHz English speech. The domain is prepared, semi-spontaneous speech from public talks, which presents different challenges from read audiobooks or telephone conversations, including diverse accents, variable speaking rates, and occasional audience noise. It is a valuable resource for training and evaluating systems intended for lecture or presentation transcription.</p>\n\n",
                "matched_terms": [
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CHiME (Computational Hearing in Multisource Environments) challenge series is designed to spur research in robust ASR in noisy, real-world environments <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib29\" title=\"\">29</a>]</cite>. The datasets feature distant-microphone recordings in challenging acoustic settings. For example, the CHiME-6 challenge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib30\" title=\"\">30</a>]</cite> uses data from dinner parties recorded in real homes using multiple microphone arrays. This captures complex acoustic scenes with high levels of background noise, reverberation, and overlapping speech from multiple speakers. The CHiME datasets are essential for evaluating the true robustness of ASR systems beyond clean, laboratory-like conditions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "data",
                    "high",
                    "uses",
                    "from",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Common Voice project by Mozilla is a massive, crowdsourced initiative to create a free and publicly available speech dataset for a wide range of languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib31\" title=\"\">31</a>]</cite>. Contributors record their voices by reading sentences from a text corpus, and other users validate the recordings. The dataset&#8217;s scale is its primary advantage; recent releases contain thousands of validated hours across over 100 languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib32\" title=\"\">32</a>]</cite>. Furthermore, it includes optional demographic metadata from contributors, such as age, gender, and accent. This makes Common Voice an invaluable resource not only for building multilingual ASR systems but also for studying and mitigating demographic bias and fairness issues in speech technology. The data is released under the permissive CC0 license.</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most common metric for ASR accuracy is the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, which measures the dissimilarity between the ASR system&#8217;s hypothesis and a ground-truth reference transcript. It is calculated based on the Levenshtein distance at the word level:</p>\n\n",
                "matched_terms": [
                    "common",
                    "accuracy",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Latency:</span> The time delay (in milliseconds) from when a word is spoken to when its transcript is finalized.</p>\n\n",
                "matched_terms": [
                    "from",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-Time Factor (RTF):</span> The ratio of the processing time to the duration of the audio. An RTF less than 1.0 is necessary for a system to keep up with a live audio stream.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive evaluation of an ASR system requires assessing the trade-off between accuracy (WER) and these performance metrics. Statistical significance tests, such as matched pairs tests, are often used to confirm that differences in WER between systems are not due to chance.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech benchmark serves as a standard for comparing the performance of different ASR architectures and training methods. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#S6.T3\" title=\"TABLE III &#8227; VI-B Reported Results on LibriSpeech &#8227; VI Evaluation and Reported Results &#8227; Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> provides an illustrative snapshot of published WERs for several prominent models on the &#8216;test-clean&#8216; and &#8216;test-other&#8216; evaluation sets. These results highlight the significant progress made by modern E2E systems. Conformer-based models and SSL-based models like wav2vec 2.0, when fully supervised or fine-tuned on the 960-hour training set, have pushed performance to very low error rates. The Whisper model is notable for achieving competitive performance in a zero-shot setting, without any fine-tuning on LibriSpeech, demonstrating the power of large-scale, diverse pre-training.</p>\n\n",
                "matched_terms": [
                    "modern",
                    "wav2vec",
                    "asr",
                    "training",
                    "pretraining",
                    "low"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While achieving high accuracy on offline benchmarks is a primary research goal, deploying ASR in real-world applications imposes strict constraints on latency, computational resources, and privacy. This section explores the techniques used to build streaming, on-device, and efficient ASR systems.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "asr",
                    "high"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many ASR applications, such as live captioning and voice assistants, require the system to transcribe speech in real time as it is being spoken. This necessitates a streaming inference capability, where the model processes audio in small chunks and produces output with minimal delay.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "time",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-streaming (or full-context) models, which require the entire utterance before starting transcription, are unsuitable for this task. Architectures like the RNN-Transducer are naturally streamable due to their monotonic alignment property <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib17\" title=\"\">17</a>]</cite>. For Transformer and Conformer models, which rely on self-attention over the entire sequence, streaming is enabled through techniques like <span class=\"ltx_text ltx_font_bold\">chunked attention</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib34\" title=\"\">34</a>]</cite>. Here, the input audio is divided into fixed-size chunks. When processing a given chunk, the self-attention mechanism is restricted to attend only to the current chunk and a limited number of preceding chunks (the &#8221;left context&#8221;). This prevents the model from accessing future audio frames, enabling causal, chunk-by-chunk processing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, on-device deployment is challenging due to the limited computational power and memory of edge devices. State-of-the-art ASR models can have hundreds of millions of parameters and require significant computational resources. To make these models feasible for on-device use, <span class=\"ltx_text ltx_font_bold\">model compression</span> techniques are essential. The two most common methods are:</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pruning:</span> This technique involves removing redundant or less important connections (weights) from the neural network. In <span class=\"ltx_text ltx_font_italic\">unstructured pruning</span>, individual weights with small magnitudes are set to zero, creating sparse weight matrices. In <span class=\"ltx_text ltx_font_italic\">structured pruning</span>, entire groups of weights, such as filters or network layers, are removed. While unstructured pruning can achieve higher compression rates, structured pruning is often more effective at achieving practical speedups on modern hardware designed for dense matrix operations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "modern"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These techniques, often used in combination, allow for the creation of compact and efficient models that can deliver high-quality ASR directly on edge devices, balancing the trade-off between accuracy, latency, and resource consumption as illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#S7.F4\" title=\"Figure 4 &#8227; VII-A Streaming ASR &#8227; VII Streaming, On-Device, and Efficiency &#8227; Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As ASR technology becomes more pervasive, its societal impact grows, making it crucial to address issues of robustness, fairness, and ethics. A system that is highly accurate on benchmark data but fails in real-world noise or performs inequitably across different populations is not truly effective.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "data",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Real-world audio is often corrupted by background noise (e.g., street sounds, music) and reverberation (reflections of sound in an enclosed space). These acoustic distortions can cause a significant degradation in ASR performance. The <span class=\"ltx_text ltx_font_bold\">CHiME challenges</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib29\" title=\"\">29</a>]</cite> have been instrumental in driving research on this problem. The datasets, recorded in challenging, noisy environments like cafes and homes, provide a benchmark for evaluating noise-robust ASR. Successful approaches often combine multi-microphone signal processing techniques (e.g., beamforming to isolate a target speaker) with multi-condition training, where the ASR model is trained on data that has been artificially corrupted with a wide variety of noise types and SNRs. The success of models like Whisper also demonstrates that training on massive and diverse real-world data is a powerful strategy for achieving inherent robustness <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "data",
                    "training",
                    "audio",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gender:</span> Some studies have reported higher WERs for female speakers than for male speakers, a gap often attributed to the underrepresentation of female speech in training data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Age:</span> ASR systems tend to perform worse for children and older adults compared to middle-aged adults, whose speech patterns are typically better represented in training corpora <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib38\" title=\"\">38</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary cause of these biases is the composition of the training data. If a particular demographic group is underrepresented, the model will not learn to recognize their speech patterns as effectively. This can lead to a cycle of exclusion, where poor performance discourages usage by marginalized groups, further limiting data collection and perpetuating the bias. Addressing this requires a concerted effort to collect more diverse and representative datasets, such as the Mozilla Common Voice corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib31\" title=\"\">31</a>]</cite>, and to develop fairness-aware training and evaluation methodologies.</p>\n\n",
                "matched_terms": [
                    "learn",
                    "common",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of ASR raises important privacy considerations. Cloud-based ASR services require users to send their voice data, which can be highly personal and sensitive, to remote servers for processing. This creates potential risks of data breaches or misuse. On-device ASR (as discussed in Section VII) is a powerful technical solution that mitigates these risks by keeping all data local. When data is collected for training, clear privacy policies and user consent are essential <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib40\" title=\"\">40</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, data licensing is a critical component of the ASR ecosystem. The progress of academic research depends heavily on open datasets with permissive licenses (e.g., CC0, CC BY) that allow for sharing and reproduction of results. Corpora like LibriSpeech and Common Voice have been pivotal in this regard, while commercially licensed datasets like Switchboard, though valuable, can present barriers to access.</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ASR for high-resource languages like English has reached maturity, performance for the vast majority of the world&#8217;s languages remains poor due to data scarcity. A major challenge is building robust <span class=\"ltx_text ltx_font_bold\">multilingual ASR</span> systems that can recognize speech from many languages using a single model. This often involves pooling data from multiple languages to leverage phonetic and linguistic similarities. Recent work on low-resource languages, such as Bangla, continues to explore effective architectures and training strategies in data-constrained settings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib41\" title=\"\">41</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "asr",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more complex related problem is <span class=\"ltx_text ltx_font_bold\">code-switching</span>, where speakers alternate between two or more languages within a single conversation or even a single sentence. This is common in multilingual communities but poses a significant challenge for ASR systems, which must simultaneously manage multiple vocabularies, grammars, and phonetic inventories <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib42\" title=\"\">42</a>]</cite>. Developing models that can seamlessly handle code-switching is a critical frontier for making ASR truly global.</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generic ASR models often struggle with user-specific vocabulary, such as contact names, local places, or technical jargon. <span class=\"ltx_text ltx_font_bold\">Personalization</span>, or adapting a model to an individual user&#8217;s voice, accent, and vocabulary, can dramatically improve usability. However, this must be balanced with user privacy. Future research will likely focus on privacy-preserving adaptation techniques, such as on-device fine-tuning or federated learning, where models are updated on user data without the raw data ever leaving the device.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "asr",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word Error Rate, while a useful metric, is a blunt instrument. It treats all errors equally, yet the semantic impact of errors can vary dramatically; misrecognizing &#8221;two&#8221; as &#8221;to&#8221; is less severe than misrecognizing &#8221;accept&#8221; as &#8221;except&#8221;. There is a growing need for evaluation metrics that go <span class=\"ltx_text ltx_font_bold\">beyond lexical accuracy</span> to measure semantic correctness, robustness against critical errors, and the overall usability of ASR output <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib43\" title=\"\">43</a>]</cite>. Furthermore, developing label-free evaluation methods that can estimate ASR performance without ground-truth transcripts would enable assessment across a much wider range of real-world domains <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib44\" title=\"\">44</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "asr",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The underlying architectures and training methods developed for ASR are also being applied to a broader set of speech processing tasks. For example, <span class=\"ltx_text ltx_font_bold\">Speech Emotion Recognition (SER)</span> aims to identify the emotional state of a speaker from their vocal cues. While it uses similar front-end processing, its goal is classification of affect rather than transcription of content. Research in SER explores different feature extraction methods, such as those based on wavelet transforms, and classification models like Support Vector Machines to identify emotions like happiness, sadness, or anger from speech signals <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib45\" title=\"\">45</a>]</cite>. The synergy between ASR and other speech intelligence tasks represents a rich area for future cross-pollination of ideas.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "from",
                    "asr",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of Automatic Speech Recognition has transitioned into a new era, characterized by the dominance of end-to-end neural architectures and a fundamental shift in how data is leveraged. The evolution from CTC and attention-based models to the powerful and efficient Conformer architecture has marked a period of rapid improvement in acoustic modeling. This architectural progress has been matched, and perhaps even surpassed, by a revolution in training paradigms. The move from purely supervised learning to self-supervised methods like wav2vec 2.0 and large-scale weak supervision with models like Whisper has unlocked the potential of massive unlabeled and web-scale datasets, drastically reducing the dependency on curated, transcribed corpora and imbuing models with unprecedented robustness.</p>\n\n",
                "matched_terms": [
                    "selfsupervised",
                    "learning",
                    "unlabeled",
                    "data",
                    "improvement",
                    "training",
                    "wav2vec",
                    "from",
                    "robustness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This survey has provided a structured overview of this modern landscape. We have detailed the key E2E architectures, analyzed the training methodologies that power them, and summarized the datasets and metrics used to benchmark them. Furthermore, we have examined the critical, practical dimensions of the field, including the engineering challenges of building low-latency streaming and efficient on-device systems, and the pressing ethical need to ensure that ASR technology is robust, fair, and privacy-preserving.</p>\n\n",
                "matched_terms": [
                    "modern",
                    "asr",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ASR has achieved near-human performance on several benchmarks, significant challenges remain. The frontiers of research are now pushing towards true multilingual understanding, seamless handling of code-switching, private and effective personalization, and more meaningful, beyond-WER evaluation. The continued convergence of advanced architectures, large-scale data, and a growing focus on real-world applicability promises to make speech technology even more capable and ubiquitous in the years to come.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "convergence",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progress in modern ASR is heavily reliant on open and reproducible research. The following open-source toolkits are central to the development and evaluation of the models discussed in this survey:</p>\n\n",
                "matched_terms": [
                    "modern",
                    "asr"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation",
        "caption": "TABLE II: Characteristics of Representative ASR Datasets.",
        "body": "Dataset\nHours\nSpeakers\nDomain / Notes\n\n\n\n\nLibriSpeech [26]\n\n960960\n24842484\nRead English audiobooks\n\n\nSwitchboard [27]\n\n300300\n543543\nEnglish telephone conversations\n\n\nTED-LIUM 3 [28]\n\n452452\n23512351\nEnglish talks, diverse accents\n\n\nCHiME-6 [30]\n\n5050\n2020\nNoisy, distant-mic conversations\n\n\nCommon Voice 17.0 [32]\n\n¿20000\n¿100k\nCrowdsourced, 124 languages",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speakers</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Domain / Notes</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"960\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mn>960</mn><annotation encoding=\"application/x-tex\">960</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"2484\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mn>2484</mn><annotation encoding=\"application/x-tex\">2484</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Read English audiobooks</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Switchboard <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"543\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mn>543</mn><annotation encoding=\"application/x-tex\">543</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">English telephone conversations</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TED-LIUM 3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib28\" title=\"\">28</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"452\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mn>452</mn><annotation encoding=\"application/x-tex\">452</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"2351\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mn>2351</mn><annotation encoding=\"application/x-tex\">2351</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">English talks, diverse accents</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CHiME-6 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib30\" title=\"\">30</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m8\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">Noisy, distant-mic conversations</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Common Voice 17.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib32\" title=\"\">32</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#191;20000</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#191;100k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Crowdsourced, 124 languages</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "representative",
            "audiobooks",
            "noisy",
            "characteristics",
            "read",
            "datasets",
            "accents",
            "crowdsourced",
            "diverse",
            "notes",
            "common",
            "telephone",
            "talks",
            "asr",
            "domain",
            "languages",
            "conversations",
            "¿20000",
            "voice",
            "¿100k",
            "speakers",
            "switchboard",
            "tedlium",
            "distantmic",
            "hours",
            "dataset",
            "english",
            "librispeech",
            "chime6"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of large-scale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "switchboard",
                    "datasets",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR), the task of converting spoken language into text, is a cornerstone of modern human-computer interaction, powering applications from voice assistants and dictation software to in-car control systems and automated transcription services <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib1\" title=\"\">1</a>]</cite>. The field has witnessed remarkable progress, evolving from early systems rooted in statistical methods to the highly performant deep learning models of today <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, ASR systems were constructed from a complex pipeline of independently trained components: an acoustic model (AM), typically a Gaussian Mixture Model-Hidden Markov Model (GMM-HMM), a pronunciation model (lexicon), and a language model (LM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib4\" title=\"\">4</a>]</cite>. The advent of deep neural networks (DNNs) replaced GMMs in hybrid DNN-HMM systems, yielding significant accuracy improvements <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib2\" title=\"\">2</a>]</cite>. However, this modular design was complex, required domain expertise for each component, and suffered from error propagation between stages.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comprehensive summary of the key datasets, benchmarks, and evaluation metrics that are used to measure and compare ASR system performance.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Numerous surveys have chronicled the progress of ASR over the years. Early reviews focused on the statistical foundations of GMM-HMM systems and the initial integration of neural networks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib9\" title=\"\">9</a>]</cite>. More recent surveys have documented the transition to deep learning, often comparing hybrid DNN-HMM systems with the first generation of E2E models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib10\" title=\"\">10</a>]</cite>. For instance, Saon et al. provided an overview of English conversational speech recognition, highlighting the challenges of the Switchboard task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib11\" title=\"\">11</a>]</cite>. A comprehensive survey by Battenberg et al. explored various E2E approaches, comparing the merits of CTC and attention-based models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "switchboard",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ethics and Robustness:</span> We address the growing importance of evaluating ASR systems not just for accuracy, but also for their robustness in noisy conditions and their fairness across diverse speaker populations.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "asr",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The conventional approach to training ASR models involves supervised learning on a dataset of paired audio utterances and their corresponding ground-truth transcriptions. The model is trained to minimize a loss function (e.g., CTC, cross-entropy) that measures the discrepancy between its predictions and the reference text.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning:</span> After pre-training, a small, randomly initialized linear layer is added on top of the Transformer encoder, and the entire model is fine-tuned on a small amount of labeled data for a specific ASR task (e.g., transcribing English).</p>\n\n",
                "matched_terms": [
                    "asr",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The power of this approach is its data efficiency. Baevski et al. demonstrated that a wav2vec 2.0 model pre-trained on 53k hours of unlabeled audio could achieve state-of-the-art results on the LibriSpeech benchmark using as little as ten minutes of labeled data for fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>. This has democratized the development of high-performance ASR for low-resource languages and domains where transcribed data is scarce.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "asr",
                    "hours",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative approach to leveraging large datasets is weak supervision. Instead of a carefully designed self-supervised objective, this paradigm relies on the sheer scale and diversity of existing, albeit noisy, audio-transcript pairs available on the web. The most prominent example is OpenAI&#8217;s <span class=\"ltx_text ltx_font_bold\">Whisper</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is a large Transformer-based encoder-decoder model trained on an enormous dataset of 680,000 hours of multilingual and multitask audio collected from the internet. The &#8221;weak supervision&#8221; comes from the fact that these transcripts are not guaranteed to be perfectly accurate. However, by training on such a vast and diverse dataset, the model learns to be inherently robust to a wide range of acoustic conditions, including background noise, different accents, and various speaking styles. A key finding of this work is that a single, large model trained this way can achieve remarkable <span class=\"ltx_text ltx_font_italic\">zero-shot</span> performance on many standard ASR benchmarks, often competitive with systems that were specifically fine-tuned on those datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>. This approach suggests that data scale and diversity can be a powerful substitute for meticulously curated datasets and complex training objectives.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "datasets",
                    "accents",
                    "diverse",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development and evaluation of ASR systems are grounded in standardized datasets that serve as common benchmarks for the research community. These datasets vary widely in size, domain, recording conditions, and linguistic content, allowing for the assessment of different aspects of system performance.</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr",
                    "domain",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib26\" title=\"\">26</a>]</cite> is arguably the most widely used benchmark for ASR in the academic community. It consists of approximately 1000 hours of 16 kHz read English speech derived from audiobooks from the LibriVox project. The data is segmented, aligned, and organized into standardized training, development, and test sets. A key feature is the division of its evaluation sets into &#8216;test-clean&#8216; and &#8216;test-other&#8216; subsets, which represent recordings from speakers who are easier and harder to recognize, respectively. This allows for a nuanced evaluation of model performance under both ideal and more challenging (but still clean) acoustic conditions. Its permissive CC BY 4.0 license has contributed to its widespread adoption.</p>\n\n",
                "matched_terms": [
                    "audiobooks",
                    "english",
                    "librispeech",
                    "speakers",
                    "asr",
                    "read",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Switchboard corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib27\" title=\"\">27</a>]</cite> is a foundational dataset for conversational telephone speech. It contains approximately 300 hours of spontaneous, two-sided telephone conversations between 543 speakers of American English. Participants were given a topic to discuss, resulting in natural, unscripted speech complete with disfluencies, hesitations, and interruptions. The audio is sampled at 8 kHz, reflecting its telephone channel origin. Despite its age, Switchboard remains a challenging and relevant benchmark for evaluating ASR performance on spontaneous conversational speech. It is distributed by the Linguistic Data Consortium (LDC) and requires a license for use.</p>\n\n",
                "matched_terms": [
                    "english",
                    "telephone",
                    "speakers",
                    "asr",
                    "switchboard",
                    "conversations",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TED-LIUM corpus is derived from TED Talks, offering a large collection of speech from a diverse set of speakers on a wide range of topics. The third release, TED-LIUM 3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib28\" title=\"\">28</a>]</cite>, contains 452 hours of 16 kHz English speech. The domain is prepared, semi-spontaneous speech from public talks, which presents different challenges from read audiobooks or telephone conversations, including diverse accents, variable speaking rates, and occasional audience noise. It is a valuable resource for training and evaluating systems intended for lecture or presentation transcription.</p>\n\n",
                "matched_terms": [
                    "audiobooks",
                    "english",
                    "telephone",
                    "speakers",
                    "talks",
                    "domain",
                    "read",
                    "accents",
                    "tedlium",
                    "diverse",
                    "conversations",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CHiME (Computational Hearing in Multisource Environments) challenge series is designed to spur research in robust ASR in noisy, real-world environments <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib29\" title=\"\">29</a>]</cite>. The datasets feature distant-microphone recordings in challenging acoustic settings. For example, the CHiME-6 challenge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib30\" title=\"\">30</a>]</cite> uses data from dinner parties recorded in real homes using multiple microphone arrays. This captures complex acoustic scenes with high levels of background noise, reverberation, and overlapping speech from multiple speakers. The CHiME datasets are essential for evaluating the true robustness of ASR systems beyond clean, laboratory-like conditions.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "noisy",
                    "asr",
                    "datasets",
                    "chime6"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Common Voice project by Mozilla is a massive, crowdsourced initiative to create a free and publicly available speech dataset for a wide range of languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib31\" title=\"\">31</a>]</cite>. Contributors record their voices by reading sentences from a text corpus, and other users validate the recordings. The dataset&#8217;s scale is its primary advantage; recent releases contain thousands of validated hours across over 100 languages <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib32\" title=\"\">32</a>]</cite>. Furthermore, it includes optional demographic metadata from contributors, such as age, gender, and accent. This makes Common Voice an invaluable resource not only for building multilingual ASR systems but also for studying and mitigating demographic bias and fairness issues in speech technology. The data is released under the permissive CC0 license.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "asr",
                    "crowdsourced",
                    "languages",
                    "common",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating the performance of ASR systems requires well-defined metrics and standardized benchmarks. This section outlines the primary metrics used in the field and presents a snapshot of reported results for leading models on the LibriSpeech benchmark.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most common metric for ASR accuracy is the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, which measures the dissimilarity between the ASR system&#8217;s hypothesis and a ground-truth reference transcript. It is calculated based on the Levenshtein distance at the word level:</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech benchmark serves as a standard for comparing the performance of different ASR architectures and training methods. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#S6.T3\" title=\"TABLE III &#8227; VI-B Reported Results on LibriSpeech &#8227; VI Evaluation and Reported Results &#8227; Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> provides an illustrative snapshot of published WERs for several prominent models on the &#8216;test-clean&#8216; and &#8216;test-other&#8216; evaluation sets. These results highlight the significant progress made by modern E2E systems. Conformer-based models and SSL-based models like wav2vec 2.0, when fully supervised or fine-tuned on the 960-hour training set, have pushed performance to very low error rates. The Whisper model is notable for achieving competitive performance in a zero-shot setting, without any fine-tuning on LibriSpeech, demonstrating the power of large-scale, diverse pre-training.</p>\n\n",
                "matched_terms": [
                    "diverse",
                    "asr",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many ASR applications, such as live captioning and voice assistants, require the system to transcribe speech in real time as it is being spoken. This necessitates a streaming inference capability, where the model processes audio in small chunks and produces output with minimal delay.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, on-device deployment is challenging due to the limited computational power and memory of edge devices. State-of-the-art ASR models can have hundreds of millions of parameters and require significant computational resources. To make these models feasible for on-device use, <span class=\"ltx_text ltx_font_bold\">model compression</span> techniques are essential. The two most common methods are:</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Real-world audio is often corrupted by background noise (e.g., street sounds, music) and reverberation (reflections of sound in an enclosed space). These acoustic distortions can cause a significant degradation in ASR performance. The <span class=\"ltx_text ltx_font_bold\">CHiME challenges</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib29\" title=\"\">29</a>]</cite> have been instrumental in driving research on this problem. The datasets, recorded in challenging, noisy environments like cafes and homes, provide a benchmark for evaluating noise-robust ASR. Successful approaches often combine multi-microphone signal processing techniques (e.g., beamforming to isolate a target speaker) with multi-condition training, where the ASR model is trained on data that has been artificially corrupted with a wide variety of noise types and SNRs. The success of models like Whisper also demonstrates that training on massive and diverse real-world data is a powerful strategy for achieving inherent robustness <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "asr",
                    "datasets",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Accent and Dialect:</span> Systems typically perform best on dominant, &#8221;standard&#8221; accents (e.g., Standard American English) and show significantly higher WERs for speakers with regional or non-native accents <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib38\" title=\"\">38</a>]</cite>. One study found that commercial ASR systems had nearly double the error rate for speakers of African American Vernacular English (AAVE) compared to white speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib37\" title=\"\">37</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "asr",
                    "english",
                    "accents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary cause of these biases is the composition of the training data. If a particular demographic group is underrepresented, the model will not learn to recognize their speech patterns as effectively. This can lead to a cycle of exclusion, where poor performance discourages usage by marginalized groups, further limiting data collection and perpetuating the bias. Addressing this requires a concerted effort to collect more diverse and representative datasets, such as the Mozilla Common Voice corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib31\" title=\"\">31</a>]</cite>, and to develop fairness-aware training and evaluation methodologies.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "voice",
                    "datasets",
                    "diverse",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of ASR raises important privacy considerations. Cloud-based ASR services require users to send their voice data, which can be highly personal and sensitive, to remote servers for processing. This creates potential risks of data breaches or misuse. On-device ASR (as discussed in Section VII) is a powerful technical solution that mitigates these risks by keeping all data local. When data is collected for training, clear privacy policies and user consent are essential <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib40\" title=\"\">40</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, data licensing is a critical component of the ASR ecosystem. The progress of academic research depends heavily on open datasets with permissive licenses (e.g., CC0, CC BY) that allow for sharing and reproduction of results. Corpora like LibriSpeech and Common Voice have been pivotal in this regard, while commercially licensed datasets like Switchboard, though valuable, can present barriers to access.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "librispeech",
                    "asr",
                    "switchboard",
                    "datasets",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ASR for high-resource languages like English has reached maturity, performance for the vast majority of the world&#8217;s languages remains poor due to data scarcity. A major challenge is building robust <span class=\"ltx_text ltx_font_bold\">multilingual ASR</span> systems that can recognize speech from many languages using a single model. This often involves pooling data from multiple languages to leverage phonetic and linguistic similarities. Recent work on low-resource languages, such as Bangla, continues to explore effective architectures and training strategies in data-constrained settings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib41\" title=\"\">41</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "asr",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A more complex related problem is <span class=\"ltx_text ltx_font_bold\">code-switching</span>, where speakers alternate between two or more languages within a single conversation or even a single sentence. This is common in multilingual communities but poses a significant challenge for ASR systems, which must simultaneously manage multiple vocabularies, grammars, and phonetic inventories <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib42\" title=\"\">42</a>]</cite>. Developing models that can seamlessly handle code-switching is a critical frontier for making ASR truly global.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "common",
                    "asr",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generic ASR models often struggle with user-specific vocabulary, such as contact names, local places, or technical jargon. <span class=\"ltx_text ltx_font_bold\">Personalization</span>, or adapting a model to an individual user&#8217;s voice, accent, and vocabulary, can dramatically improve usability. However, this must be balanced with user privacy. Future research will likely focus on privacy-preserving adaptation techniques, such as on-device fine-tuning or federated learning, where models are updated on user data without the raw data ever leaving the device.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This survey has provided a structured overview of this modern landscape. We have detailed the key E2E architectures, analyzed the training methodologies that power them, and summarized the datasets and metrics used to benchmark them. Furthermore, we have examined the critical, practical dimensions of the field, including the engineering challenges of building low-latency streaming and efficient on-device systems, and the pressing ethical need to ensure that ASR technology is robust, fair, and privacy-preserving.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "datasets"
                ]
            }
        ]
    },
    "S6.T3": {
        "source_file": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation",
        "caption": "TABLE III: Published Word Error Rates (%) on the LibriSpeech Benchmark (Illustrative Snapshot).",
        "body": "Model\ntest-clean\ntest-other\nLatency/RTF\nSource\n\n\n\n\nConformer-T (non-streaming, with LM)\n1.9\n3.9\n—\nGulati et al. (2020) [14]\n\n\n\nwav2vec 2.0 (LARGE, fine-tuned 960h, with LM)\n1.8\n3.3\n—\nBaevski et al. (2020) [7]\n\n\n\nWhisper (large-v2, zero-shot)\n2.7\n5.0\n—\nRadford et al. (2023) [8]\n\n\n\nStreaming Conformer (960ms chunk)\n2.72\n6.47\nStreaming\nSpeechBrain [33]",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">test-clean</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">test-other</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Latency/RTF</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Source</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Conformer-T (non-streaming, with LM)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Gulati et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">wav2vec 2.0 (LARGE, fine-tuned 960h, with LM)</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Baevski et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper (large-v2, zero-shot)</td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">5.0</td>\n<td class=\"ltx_td ltx_align_center\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Radford et al. (2023) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Streaming Conformer (960ms chunk)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Streaming</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">SpeechBrain <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib33\" title=\"\">33</a>]</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "snapshot",
            "testother",
            "radford",
            "benchmark",
            "source",
            "streaming",
            "error",
            "nonstreaming",
            "baevski",
            "conformer",
            "960h",
            "rates",
            "large",
            "latencyrtf",
            "testclean",
            "iii",
            "zeroshot",
            "chunk",
            "gulati",
            "finetuned",
            "word",
            "published",
            "whisper",
            "librispeech",
            "speechbrain",
            "largev2",
            "conformert",
            "wav2vec",
            "illustrative",
            "960ms",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The LibriSpeech benchmark serves as a standard for comparing the performance of different ASR architectures and training methods. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#S6.T3\" title=\"TABLE III &#8227; VI-B Reported Results on LibriSpeech &#8227; VI Evaluation and Reported Results &#8227; Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> provides an illustrative snapshot of published WERs for several prominent models on the &#8216;test-clean&#8216; and &#8216;test-other&#8216; evaluation sets. These results highlight the significant progress made by modern E2E systems. Conformer-based models and SSL-based models like wav2vec 2.0, when fully supervised or fine-tuned on the 960-hour training set, have pushed performance to very low error rates. The Whisper model is notable for achieving competitive performance in a zero-shot setting, without any fine-tuning on LibriSpeech, demonstrating the power of large-scale, diverse pre-training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of large-scale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "word",
                    "whisper",
                    "error",
                    "librispeech",
                    "conformer",
                    "wav2vec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, ASR systems were constructed from a complex pipeline of independently trained components: an acoustic model (AM), typically a Gaussian Mixture Model-Hidden Markov Model (GMM-HMM), a pronunciation model (lexicon), and a language model (LM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib4\" title=\"\">4</a>]</cite>. The advent of deep neural networks (DNNs) replaced GMMs in hybrid DNN-HMM systems, yielding significant accuracy improvements <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib2\" title=\"\">2</a>]</cite>. However, this modular design was complex, required domain expertise for each component, and suffered from error propagation between stages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second, equally transformative trend has been the shift in data paradigms. While supervised learning on large, transcribed corpora drove early deep learning successes, the field has increasingly moved towards methods that leverage vast quantities of unlabeled audio data. Self-supervised learning (SSL) frameworks, most notably wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>, have demonstrated the ability to learn powerful speech representations from raw audio, which can then be fine-tuned with minimal labeled data to achieve state-of-the-art results. Concurrently, large-scale weak supervision, exemplified by OpenAI&#8217;s Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>, has shown that training on massive, diverse, and multilingual web-scale data can produce a single model with remarkable zero-shot robustness across numerous domains and conditions.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "model",
                    "large",
                    "wav2vec",
                    "zeroshot",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An examination of practical deployment challenges, such as streaming inference for real-time applications and model efficiency for on-device processing, and a discussion of critical ethical considerations, including system robustness, fairness, and data privacy.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The RNN-T architecture is naturally suited for streaming because the computation at each step depends only on the current acoustic frame and the previously emitted labels. It does not need to see the entire audio sequence, making it ideal for low-latency applications. It also incorporates an internal language model via the prediction network, overcoming a key limitation of CTC.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Conformer</span> architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite> further improved upon the Transformer by explicitly combining self-attention with convolutions to model both global and local context, respectively. A Conformer block typically consists of a feed-forward module, a multi-head self-attention module, a convolution module, and a final feed-forward module, arranged in a &#8221;macaron-like&#8221; structure. This hybrid approach has proven extremely effective, and Conformers, often paired with a CTC or RNN-T loss function, represent the state of the art for many ASR benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib14\" title=\"\">14</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "conformer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve accuracy, an external language model (LM), trained on large text corpora, can be integrated during decoding. This is particularly crucial for CTC-based models. A common technique is <span class=\"ltx_text ltx_font_bold\">shallow fusion</span>, where the LM score is linearly interpolated with the acoustic model score at each step of the beam search:</p>\n\n",
                "matched_terms": [
                    "large",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pre-training:</span> A large model, typically a Transformer, is trained on thousands of hours of unlabeled speech. The raw audio waveform is first passed through a convolutional feature encoder to obtain a sequence of latent representations. A random subset of these representations is then masked. The model is trained on a contrastive task: for each masked time step, it must identify the correct <span class=\"ltx_text ltx_font_italic\">quantized</span> version of its latent representation from a set of distractors. This forces the model to learn high-level contextualized representations of the speech signal.</p>\n\n",
                "matched_terms": [
                    "large",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning:</span> After pre-training, a small, randomly initialized linear layer is added on top of the Transformer encoder, and the entire model is fine-tuned on a small amount of labeled data for a specific ASR task (e.g., transcribing English).</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The power of this approach is its data efficiency. Baevski et al. demonstrated that a wav2vec 2.0 model pre-trained on 53k hours of unlabeled audio could achieve state-of-the-art results on the LibriSpeech benchmark using as little as ten minutes of labeled data for fine-tuning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib7\" title=\"\">7</a>]</cite>. This has democratized the development of high-performance ASR for low-resource languages and domains where transcribed data is scarce.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "benchmark",
                    "baevski",
                    "wav2vec",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative approach to leveraging large datasets is weak supervision. Instead of a carefully designed self-supervised objective, this paradigm relies on the sheer scale and diversity of existing, albeit noisy, audio-transcript pairs available on the web. The most prominent example is OpenAI&#8217;s <span class=\"ltx_text ltx_font_bold\">Whisper</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "large",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is a large Transformer-based encoder-decoder model trained on an enormous dataset of 680,000 hours of multilingual and multitask audio collected from the internet. The &#8221;weak supervision&#8221; comes from the fact that these transcripts are not guaranteed to be perfectly accurate. However, by training on such a vast and diverse dataset, the model learns to be inherently robust to a wide range of acoustic conditions, including background noise, different accents, and various speaking styles. A key finding of this work is that a single, large model trained this way can achieve remarkable <span class=\"ltx_text ltx_font_italic\">zero-shot</span> performance on many standard ASR benchmarks, often competitive with systems that were specifically fine-tuned on those datasets <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>. This approach suggests that data scale and diversity can be a powerful substitute for meticulously curated datasets and complex training objectives.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "zeroshot",
                    "large",
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LibriSpeech corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib26\" title=\"\">26</a>]</cite> is arguably the most widely used benchmark for ASR in the academic community. It consists of approximately 1000 hours of 16 kHz read English speech derived from audiobooks from the LibriVox project. The data is segmented, aligned, and organized into standardized training, development, and test sets. A key feature is the division of its evaluation sets into &#8216;test-clean&#8216; and &#8216;test-other&#8216; subsets, which represent recordings from speakers who are easier and harder to recognize, respectively. This allows for a nuanced evaluation of model performance under both ideal and more challenging (but still clean) acoustic conditions. Its permissive CC BY 4.0 license has contributed to its widespread adoption.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "model",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The TED-LIUM corpus is derived from TED Talks, offering a large collection of speech from a diverse set of speakers on a wide range of topics. The third release, TED-LIUM 3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib28\" title=\"\">28</a>]</cite>, contains 452 hours of 16 kHz English speech. The domain is prepared, semi-spontaneous speech from public talks, which presents different challenges from read audiobooks or telephone conversations, including diverse accents, variable speaking rates, and occasional audience noise. It is a valuable resource for training and evaluating systems intended for lecture or presentation transcription.</p>\n\n",
                "matched_terms": [
                    "large",
                    "rates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating the performance of ASR systems requires well-defined metrics and standardized benchmarks. This section outlines the primary metrics used in the field and presents a snapshot of reported results for leading models on the LibriSpeech benchmark.</p>\n\n",
                "matched_terms": [
                    "snapshot",
                    "librispeech",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most common metric for ASR accuracy is the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, which measures the dissimilarity between the ASR system&#8217;s hypothesis and a ground-truth reference transcript. It is calculated based on the Levenshtein distance at the word level:</p>\n\n",
                "matched_terms": [
                    "word",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many ASR applications, such as live captioning and voice assistants, require the system to transcribe speech in real time as it is being spoken. This necessitates a streaming inference capability, where the model processes audio in small chunks and produces output with minimal delay.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-streaming (or full-context) models, which require the entire utterance before starting transcription, are unsuitable for this task. Architectures like the RNN-Transducer are naturally streamable due to their monotonic alignment property <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib17\" title=\"\">17</a>]</cite>. For Transformer and Conformer models, which rely on self-attention over the entire sequence, streaming is enabled through techniques like <span class=\"ltx_text ltx_font_bold\">chunked attention</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib34\" title=\"\">34</a>]</cite>. Here, the input audio is divided into fixed-size chunks. When processing a given chunk, the self-attention mechanism is restricted to attend only to the current chunk and a limited number of preceding chunks (the &#8221;left context&#8221;). This prevents the model from accessing future audio frames, enabling causal, chunk-by-chunk processing.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "nonstreaming",
                    "model",
                    "conformer",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Real-world audio is often corrupted by background noise (e.g., street sounds, music) and reverberation (reflections of sound in an enclosed space). These acoustic distortions can cause a significant degradation in ASR performance. The <span class=\"ltx_text ltx_font_bold\">CHiME challenges</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib29\" title=\"\">29</a>]</cite> have been instrumental in driving research on this problem. The datasets, recorded in challenging, noisy environments like cafes and homes, provide a benchmark for evaluating noise-robust ASR. Successful approaches often combine multi-microphone signal processing techniques (e.g., beamforming to isolate a target speaker) with multi-condition training, where the ASR model is trained on data that has been artificially corrupted with a wide variety of noise types and SNRs. The success of models like Whisper also demonstrates that training on massive and diverse real-world data is a powerful strategy for achieving inherent robustness <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A significant ethical challenge in ASR is performance bias. A growing body of research has demonstrated that ASR systems often exhibit substantially higher error rates for certain demographic groups compared to others <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib37\" title=\"\">37</a>]</cite>. These disparities can occur across various axes:</p>\n\n",
                "matched_terms": [
                    "rates",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word Error Rate, while a useful metric, is a blunt instrument. It treats all errors equally, yet the semantic impact of errors can vary dramatically; misrecognizing &#8221;two&#8221; as &#8221;to&#8221; is less severe than misrecognizing &#8221;accept&#8221; as &#8221;except&#8221;. There is a growing need for evaluation metrics that go <span class=\"ltx_text ltx_font_bold\">beyond lexical accuracy</span> to measure semantic correctness, robustness against critical errors, and the overall usability of ASR output <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib43\" title=\"\">43</a>]</cite>. Furthermore, developing label-free evaluation methods that can estimate ASR performance without ground-truth transcripts would enable assessment across a much wider range of real-world domains <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12827v1#bib.bib44\" title=\"\">44</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of Automatic Speech Recognition has transitioned into a new era, characterized by the dominance of end-to-end neural architectures and a fundamental shift in how data is leveraged. The evolution from CTC and attention-based models to the powerful and efficient Conformer architecture has marked a period of rapid improvement in acoustic modeling. This architectural progress has been matched, and perhaps even surpassed, by a revolution in training paradigms. The move from purely supervised learning to self-supervised methods like wav2vec 2.0 and large-scale weak supervision with models like Whisper has unlocked the potential of massive unlabeled and web-scale datasets, drastically reducing the dependency on curated, transcribed corpora and imbuing models with unprecedented robustness.</p>\n\n",
                "matched_terms": [
                    "conformer",
                    "whisper",
                    "wav2vec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This survey has provided a structured overview of this modern landscape. We have detailed the key E2E architectures, analyzed the training methodologies that power them, and summarized the datasets and metrics used to benchmark them. Furthermore, we have examined the critical, practical dimensions of the field, including the engineering challenges of building low-latency streaming and efficient on-device systems, and the pressing ethical need to ensure that ASR technology is robust, fair, and privacy-preserving.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "benchmark"
                ]
            }
        ]
    }
}