{
    "S4.T1": {
        "source_file": "Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction",
        "caption": "Table 1: Evaluation results on BEAT2 (all speakers).\nFGD-B: Fréchet Gesture Distance (Body), BC: Beat Consistency, Div: L1-Diversity,\nWER: Word Error Rate (%), NMOS: Neural MOS, SS: Speaker Similarity (×100).\nBold = best, underlined = 2nd best. WER and NMOS are reported with 95% confidence intervals",
        "body": "Model\n\nFGD-B ↓\\downarrow\n\n\nBC ∼\\sim\n\n\nDiv. ∼\\sim\n\n\nWER ↓\\downarrow\n\n\nNMOS ↑\\uparrow\n\nSS\n\n\nHuman\n0.0\n0.684\n4.14\n6.5 ±.54\n3.72 ±.04\n69.1\n\n\nTokenizers\n0.0118\n0.667\n3.91\n11.03 ±.7\n3.19 ±.04\n66.8\n\n\nCAMN\n0.1097\n0.551\n2.96\n-\n-\n-\n\n\nEMAGE\n0.1679\n0.766\n3.92\n-\n-\n-\n\n\nRAG\n0.1781\n0.700\n5.13\n-\n-\n-\n\n\nGelina\n0.2310\n0.744\n3.20\n11.3 ±1.0\n2.96 ±.04\n-\n\n\nGelina Clon.\n0.0839\n0.738\n3.15\n9.2 ±.84\n3.21 ±.04\n61.3\n\n\nGelina S2G\n0.1950\n0.768\n4.03\n-\n-\n-\n\n\nGelina - Flow\n0.6107\n0.824\n4.28\n9.2 ±.84\n3.21 ±.04\n61.3\n\n\nLina-Speech\n-\n-\n-\n10.9 ±.9\n2.98 ±.05\n60.1\n\n\nCosyVoice-2\n-\n-\n-\n3.5 ±.5\n3.70 ±.04\n63.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FGD-B</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">BC</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Div.</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">NMOS</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Human</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.684</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.5 &#177;.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.72 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">69.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tokenizers</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.0118</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.667</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.03 &#177;.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.19 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">66.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CAMN</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.1097</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.551</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">EMAGE</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.1679</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.766</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">RAG</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.1781</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.700</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gelina</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.2310</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.744</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.3 &#177;1.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.96 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gelina Clon.</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.0839</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.738</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">9.2 &#177;.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.21 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">61.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gelina S2G</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.1950</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.768</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gelina - Flow</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.6107</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.824</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.2 &#177;.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.21 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">61.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Lina-Speech</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.9 &#177;.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.98 &#177;.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">60.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice-2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.5 &#177;.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.70 &#177;.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">63.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "l1diversity",
            "confidence",
            "linaspeech",
            "intervals",
            "rate",
            "↓downarrow",
            "±04",
            "2nd",
            "flow",
            "div",
            "nmos",
            "distance",
            "±84",
            "all",
            "neural",
            "human",
            "±54",
            "wer",
            "fréchet",
            "camn",
            "speaker",
            "±10",
            "results",
            "tokenizers",
            "model",
            "clon",
            "rag",
            "mos",
            "±05",
            "evaluation",
            "word",
            "similarity",
            "emage",
            "consistency",
            "bold",
            "speakers",
            "↑uparrow",
            "cosyvoice2",
            "∼sim",
            "gesture",
            "underlined",
            "reported",
            "body",
            "fgdb",
            "best",
            "s2g",
            "beat2",
            "×100",
            "gelina",
            "beat",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech synthesis quality is assessed with Word Error Rate (WER), reflecting intelligibility, Natural MOS (NMOS) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a predictor of naturalness, and Speaker Similarity (SS), which measures the similarity between generated speech and reference speakers. WER is computed with Whisper-large-V3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SS follows </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using cosine similarity of WavLM-large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> embeddings.\nWe evaluated Gelina under three configurations: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gelina</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the base model; Gelina Cloning (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gelina clon.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), where sequence continuation is conditioned on an additional speech-gesture input to clone both the speaker&#8217;s voice and gestural style; and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gelina S2G</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where ground-truth speech is provided as input and only the generated gesture sequence is assessed. We included </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gelina - Flow</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an ablation of </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gelina Clon.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without the flow-matching decoding, as well as </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tokenizers</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which reconstruct speech and gesture directly from the codecs without autoregressive modeling.\nTable </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Evaluation and results &#8227; 4 Experiments &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports objective evaluation results on the BEAT2 dataset.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Human communication is multimodal, with speech and gestures tightly coupled, yet most computational methods for generating speech and gestures synthesize them sequentially, weakening synchrony and prosody alignment. We introduce Gelina, a unified framework that jointly synthesizes speech and co-speech gestures from text using interleaved token sequences in a discrete autoregressive backbone, with modality-specific decoders. Gelina supports multi-speaker and multi-style cloning and enables gesture-only synthesis from speech inputs. Subjective and objective evaluations demonstrate competitive speech quality and improved gesture generation over unimodal baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "gelina",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nText-to-speech (TTS); co-speech gesture generation; unified multimodal synthesis; autoregressive transformers; flow-matching; Human behavior synthesis;</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Human communication is inherently multimodal. Speech and gestures are jointly realized, making speech and gestures coordinated expressions of the same communicative process </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Many approaches have been proposed to computationally capture and generate such multimodal dynamics. Important research directions include text-to-speech (TTS) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, co-speech gesture generation </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, human pose synthesis </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, and facial animation </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Recent advances in deep learning and the growing availability of multimodal corpora </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> have made it possible to envision integrated models that generate several modalities together </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. However, most multimodal systems process speech and gestures separately, typically following a cascaded design in which speech is generated first and gestures are added afterward. This strategy is largely driven by the scarcity of large-scale paired corpora and the constraints of working with datasets built in a single modality, either speech or gestures. In such setups, the speech generation process remains unaware of gestural type and timing, leading to weakened synchrony, limited prosodic alignment, and reduced expressiveness </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, this cascaded approach contradicts psycholinguistic evidence showing that speech and gestures are jointly planned in human communication </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. A unified computational framework not only improves efficiency by giving gesture models direct access to linguistic-prosodic features, but also resonates with psycholinguistic theories of human communication, such as the growth-point hypothesis </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Such a framework paves the way for more natural and seamless integration of multimodal behaviors in applications, including Embodied Conversational Agents and social robots.\n</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce Gelina, a unified framework that simultaneously synthesizes speech and co-speech gestures across multiple voices and gestural styles, using only text as input. Gelina is a discrete, autoregressive model capable of predicting interleaved speech-gesture token sequences. Gestures are subsequently decoded with a gesture flow-matching model </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">.\n</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">Our main contributions are the following:</span></p>\n\n",
                "matched_terms": [
                    "gesture",
                    "gelina",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We demonstrate </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">bimodal style cloning</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (voice and gesture jointly) through sequence continuation, without explicit speaker embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Co-speech gesture synthesis:</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n\nGesture generation has recently shifted to data-driven methods </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Early approaches used autoregressive sequence modeling to map speech or text to motion sequences </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, while diffusion-based generators now dominate for their ability to produce detailed, temporally consistent, and natural gestures </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Other works explore discrete motion representations, enabling more controllable synthesis </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. These models accept either speech or text as input and typically rely on speaker embeddings for multi-speaker modeling, which limits their generalization ability to speakers unseen during training.\nIn contrast, Gelina generates both speech and gestures directly from text, and can also clone voice and gestural style through sequence continuation using a speech-gesture prompt, without relying on speaker embeddings.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-to-speech approaches:</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n\nLately, TTS has shifted toward data-driven methods, with notable advances in discrete code modeling </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. These systems discretize audio into tokens using large pre-trained codecs </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> and employ attention-based alignment to map text sequences to speech. Speaker identity is often cloned from a short reference utterance, enabling multi-speaker synthesis. For example, Lina-Speech </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> adopts an autoregressive encoder-decoder with linear attention, where tokenized text is mapped to speech tokens extracted from WavTokenizer </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> using a position-aware cross-attention mechanism </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. Such models achieve competitive, human-like naturalness, but remain unimodal.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Unified speech and gesture synthesis:</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n\nJoint synthesis of speech and co-speech gestures is a relatively recent research direction. Early work such as </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib22\" title=\"\">22</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> explored rule-based integration of gesture and speech. Tacotron-ISG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> later represented an early neural attempt by extending Tacotron 2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> with a gesture prediction module, though speech and gesture remained only loosely coupled. More recent works have adapted diffusion frameworks to multimodal tasks: in Diff-TTSG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, text is mapped via a duration predictor and decoded with two parallel diffusion heads for speech and gesture, while </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> simplified this design with Match-TTSG, by concatenating speech and gesture features and applying a single unified flow-matching head. Both Diff-TTSG and Match-TTSG were restricted to single-speaker settings due to the scarcity of paired multimodal data. Magi </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> attempted to address this limitation by augmenting existing corpora with synthetic speech-gesture pairs to enable multi-speaker modeling; however, synthetic data overall quality remains below that of human recordings. In contrast, our model is natively multi-speaker: we pre-train on a large multi-speaker TTS corpus and fine-tune on BEAT2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, the largest multi-speaker speech-gesture dataset, whereas prior work depended on mono-speaker or synthetic datasets.</span></p>\n\n",
                "matched_terms": [
                    "gesture",
                    "linaspeech",
                    "model",
                    "neural",
                    "gelina",
                    "human",
                    "beat2",
                    "speaker",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Gelina is a bimodal generative model that has three core components, which are depicted for gestures in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 GELINA architecture &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The tokenizers independently convert continuous speech and gestures to discrete indices corresponding to latent codes in a vocabulary. The discrete autoregressive transformer temporally aligns text to the sequence of speech and gesture tokens. The decoders convert the outputs of the autoregressive model to the continuous domain using the tokenizer&#8217;s decoder for speech and flow-matching decoder for gestures.</span>\n</p>\n\n",
                "matched_terms": [
                    "tokenizers",
                    "gesture",
                    "gelina",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For speech tokenization, we chose to use WavTokenizer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a discrete audio codec that converts speech waveforms sampled at 24 kHz into discrete tokens at a rate of 75 Hz. Although we observed limitations in WavTokenizer&#8217;s ability to encode and decode speech from out-of-distribution voices (such as strong accents, noisy speech, or high-pitched voices), we selected it primarily because a single step of quantization suffices to encode the speech signal, unlike popular codecs like Encodec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which require additional residual quantization levels.\nFor gesture tokenization, we followed the method proposed by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We trained a Residual Vector Quantized Variational Autoencoder (RVQ-VAE) to discretize continuous motion sequences into a hierarchical set of discrete tokens. Specifically, the RVQ-VAE first encodes motion into an initial discrete representation and then iteratively encodes the residual error (the difference between the original and reconstructed motion) into additional discrete tokens. This multi-level approach refines the representation at each level, progressively reducing the reconstruction error. We adopted the loss function introduced by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as it has proven more effective in capturing fine-grained whole-body movements. Our gesture tokenizer is presented in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 GELINA architecture &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the text tokenization, we used a standard byte-pair encoding algorithm (BPE) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "rate",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed synchronized speech-gesture generation architecture is based on Lina-Speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an autoregressive model originally designed for text-to-speech synthesis. Lina-Speech is parameter-efficient compared to other state-of-the-art architectures and integrates linear attention with cross-attention-based text-speech alignment </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Gelina introduces an extension of this architecture to multimodal generation of speech and gestures. To do so, a modality-interleaving scheme is proposed: speech and gesture tokens are interleaved by inserting a gesture token every 15 speech tokens. This ratio reflects the encoding rates of WavTokenizer (75 Hz) and Gesture RVQ-VAE (5 Hz). Additionally, separate input embeddings and output projections are maintained for each modality. The complete AR backbone is presented in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 GELINA architecture &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn practice, the AR backbone is trained with next-token prediction. First, it is pretrained on text-speech pairs from large-scale text-speech datasets, conditioning solely on text transcripts. During pretraining, gesture tokens are replaced by random tokens uniformly sampled from the gesture vocabulary and excluded from the Cross-Entropy loss computation, preserving sequence alignment. Second, the model is fine-tuned on paired text-speech-gesture data to enable synchronized speech-gesture synthesis, still conditioning only on text.\nThis two-stage training enables the model to establish robust text-speech alignment and, by jointly generating speech and gesture tokens within a single autoregressive stream, ensures synchronized multimodal outputs without relying on external alignment or post hoc gesture prediction. It also allows us to efficiently exploit existing mono- and bi-modal data resources, mitigating the scarcity of large fully paired datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "linaspeech",
                    "model",
                    "gelina"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In preliminary experiments, we observed that decoding gestures directly with the RVQ-VAE decoder yields limited quality, as it is sensitive to noisy gesture token sequences. Moreover, we hypothesize that the AR backbone&#8217;s embedding space is semantically richer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, since it integrates multimodal information. To improve decoding, we employ a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">conditional flow-matching decoder</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which learns a mapping from noisy gestures to continuous gestures conditioned on the backbone embeddings. The architecture follows Matcha-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where the flow predictor is a 1D-convolution-transformer UNet. The full gesture flow-decoder is depicted in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 GELINA architecture &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">We pre-trained Gelina on GigaSpeech </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, LibriTTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, and MLS-10k </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, totaling 18.19k h.\nWe then fine-tuned our model on the BEAT2 dataset </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, which contains aligned speech, gesture, and text sequences. Because of inconsistencies in the provided transcriptions, we re-transcribed the audio using Whisper-large-v3 (temp=0) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">.\nGesture sequences are represented as SMPL-X motion sequences </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib30\" title=\"\">30</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, with 55 joints represented as 3D axis-angles rotations. Following </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, these pose sequences are converted to Rot6D </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib31\" title=\"\">31</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, with additional translation and foot contacts information. In our experiments, we removed the joints corresponding to fingers to facilitate training, resulting in motion sequences in </span><math alttext=\"\\mathbb{R}^{T\\times(25_{\\text{joints}}\\times 6+4_{\\text{feet}}+3_{\\text{trans}})}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><mi mathsize=\"0.900em\">T</mi><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mrow><msub><mn mathsize=\"0.900em\">25</mn><mtext mathsize=\"0.900em\">joints</mtext></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">6</mn></mrow><mo mathsize=\"0.900em\">+</mo><msub><mn mathsize=\"0.900em\">4</mn><mtext mathsize=\"0.900em\">feet</mtext></msub><mo mathsize=\"0.900em\">+</mo><msub><mn mathsize=\"0.900em\">3</mn><mtext mathsize=\"0.900em\">trans</mtext></msub></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{T\\times(25_{\\text{joints}}\\times 6+4_{\\text{feet}}+3_{\\text{trans}})}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">. </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data.</span><span class=\"ltx_text\" style=\"font-size:90%;\"> It should be noted that the text-speech datasets used for pre-training are primarily composed of audiobook or read speech </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, whereas BEAT2 contains more spontaneous speech and includes several non-native English speakers with noticeable accents.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gesture tokenizer.</span><span class=\"ltx_text\" style=\"font-size:90%;\"> Our RVQ-VAE has 6 residual layers, 512-entry codebooks, and a latent dimension of 512. Input gesture sequences are at 20 fps and are downsampled by temporal convolutions to 5 Hz embeddings (see </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> for architecture). We trained the RVQ-VAE for 90k steps on 1&#215;A6000.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AR backbone.</span><span class=\"ltx_text\" style=\"font-size:90%;\"> The backbone is a 168M-parameter encoder-decoder Transformer with linear attention: a 6-layer text encoder, a 12-layer causal decoder, and a 1024-dimensional latent space. The speech and gesture vocabularies contain 4096 and 512 tokens, respectively; other hyperparameters follow </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. We observed that discarding motion residuals significantly stabilizes training, so we keep a single residual-quantization level for gestures and rely on the flow-matching decoder to recover the dropped detail. Pre-training runs for 100k steps on 4&#215;H100 with 60k tokens per batch and a learning rate of </span><math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; fine-tuning runs for 5k steps on 1&#215;H100 with 15k tokens per batch and a learning rate of </span><math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gesture decoder.</span><span class=\"ltx_text\" style=\"font-size:90%;\"> We use an 11.5M-parameter U-Net with Matcha-TTS hyperparameters </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, plus a projection from gesture embeddings (</span><math alttext=\"d{=}1024\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">d</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">1024</mn></mrow><annotation encoding=\"application/x-tex\">d{=}1024</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">) to continuous motion (</span><math alttext=\"d{=}157\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">d</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">157</mn></mrow><annotation encoding=\"application/x-tex\">d{=}157</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">). The flow-matching model is trained for 300k steps on 3&#215;H100 with </span><math alttext=\"\\lambda_{\\text{vel}}{=}0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">&#955;</mi><mtext mathsize=\"0.900em\">vel</mtext></msub><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{vel}}{=}0.05</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> and </span><math alttext=\"\\lambda_{\\text{geo}}{=}0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">&#955;</mi><mtext mathsize=\"0.900em\">geo</mtext></msub><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0.8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{geo}}{=}0.8</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">. At inference, we set the number of sampling steps to 100, which we empirically found to balance speed and output quality.</span></p>\n\n",
                "matched_terms": [
                    "gesture",
                    "model",
                    "rate",
                    "gelina",
                    "beat2",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate Gelina on four aspects: (i) speech quality, (ii) gesture quality, (iii) speech-gesture synchrony, and (iv) voice similarity, with unimodal baselines as references. We also assess cloning mode, which transfers voice and gestural style.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">We selected three baselines for gesture evaluation: </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CAMN <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_upright\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text ltx_font_upright\">]</span></cite></span><span class=\"ltx_text\" style=\"font-size:90%;\">, </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EMAGE</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, and </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">RAG-Gesture (RAG)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib32\" title=\"\">32</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. CAMN and EMAGE were retrained with their original implementations to support multi-speaker voice input using a speaker embedding. For the evaluation of the speech modality alone, we considered two baselines: </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Lina-Speech</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\"> and </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CosyVoice-2</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. CosyVoice-2 uses a 0.5B-parameter text-speech LM trained on over 168k h of multilingual data. We evaluated CosyVoice-2 with the released checkpoints in a zero-shot voice cloning setting. Our Lina-Speech baseline matches Gelina in size (</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">168M) and data (18.19k&#160;h) but omits gesture tokens. </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">This comparison setup is somewhat unfavorable to Gelina, since unimodal baselines are specialized for a single modality. Achieving competitive results against specialized unimodal baselines underscores the strength of a unified approach.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">While several speech-gesture generation models exist </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">, they rely on upper-body pose representations and are trained exclusively on the Trinity Speech-Gesture dataset </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib33\" title=\"\">33</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" style=\"font-size:90%;\">. These constraints make them poorly suited for our setting, which targets full-body synthesis in SMPL-X. Adapting such models would require substantial redesign and re-training, and their performance would remain non-comparable due to their specialization for Trinity. We therefore focus our evaluation on BEAT2, the largest SMPL-X dataset with multiple speakers, which provides a more appropriate and scalable benchmark.</span></p>\n\n",
                "matched_terms": [
                    "gesture",
                    "linaspeech",
                    "rag",
                    "evaluation",
                    "gelina",
                    "similarity",
                    "emage",
                    "beat2",
                    "camn",
                    "speaker",
                    "speakers",
                    "results",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We evaluate all systems&#8217; gesture generation using three standard metrics: Fr&#233;chet Gesture Distance </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> restricted to body gestures (FGD-B), which measures the distance between the distribution of generated gestures and that of human-recorded gestures; Beat Consistency (BC) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which assesses temporal alignment between gesture beats and audio beats; and L1-Diversity (Div.) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#bib.bib35\" title=\"\">35</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which quantifies the variability of generated gesture sequences. As our evaluation does not account for finger motion, all finger joints were zeroed before computing Div. and FGD. </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>To match prior work that reports full-body metrics, we also compute BEAT2 full-body FGD and Div (hands included) for the gesture baselines. Relative to human (FGD = 0.0; Div. = 14.31): CAMN 0.501 / 9.90, RAG 0.538 / 16.65, EMAGE 0.841 / 11.15.</span>\n    </span>\n  </span>\n</p>\n\n",
                "matched_terms": [
                    "l1diversity",
                    "gesture",
                    "div",
                    "distance",
                    "rag",
                    "body",
                    "all",
                    "evaluation",
                    "fgdb",
                    "emage",
                    "consistency",
                    "human",
                    "beat2",
                    "fréchet",
                    "camn",
                    "beat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Among gesture metrics, Gelina Cloning achieves the lowest FGD-B, indicating closest match to human distributions, and competitive BC and diversity scores compared to baselines. Gesture-only models such as RAG obtain closer beat consistency and higher diversity, but at the cost of higher distributional distance. For speech, Gelina Cloning reduces WER compared to Lina-Speech and increases NMOS. Gelina Cloning further approaches CosyVoice-2 in terms of similarity, while maintaining multimodal capability. These results demonstrate that Gelina achieves a favorable balance between gesture fidelity and speech quality, outperforming unimodal gesture baselines and remaining competitive with strong speech-only systems.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Runtime.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> On a GPU A5000, RTFs (mean over 30 clips) are 1.47 for Gelina, 1.26 for Lina-Speech, and 0.50 for CosyVoice-2. While CosyVoice-2 and Lina-Speech are faster, they are speech-only; Gelina remains near real-time despite synthesizing both speech and gestures.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "linaspeech",
                    "nmos",
                    "distance",
                    "rag",
                    "beat",
                    "fgdb",
                    "gelina",
                    "similarity",
                    "human",
                    "wer",
                    "consistency",
                    "cosyvoice2",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">User study</span><span class=\"ltx_text\" style=\"font-size:90%;\">. While objective metrics provide useful indicators of model performance, user studies remain the gold standard for assessing speech and gesture generation. We conducted a large-scale user study with 96 participants, divided into three independent tasks: (i) rating the human-likeness of generated speech (audio only), (ii) rating the human-likeness of generated gestures (animation only), and (iii) rating the synchrony between speech and gestures (audio-visual). Each participant rated 30 stimuli with durations between 8 and 15 seconds and completed 2 attention checks. Participants were recruited through the Prolific platform and received an average compensation of &#163;2.10 per test. Ratings were collected on a 5-point Likert scale. We report Mean Opinion Scores (MOS) with 95% confidence intervals in Figure&#160;</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12834v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Evaluation and results &#8227; 4 Experiments &#8227; Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction\"><span class=\"ltx_text ltx_ref_tag\">2</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">. For this study, Gelina is evaluated in the cloning setting.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" style=\"font-size:90%;\">Pairwise Student&#8217;s t-tests (with one-way repeated measure ANOVA (</span><math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">p</mi><mo mathsize=\"0.900em\">&lt;</mo><mn mathsize=\"0.900em\">0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">) and Holm correction) show that Gelina significantly outperforms Lina-Speech in voice human-likeness, with scores not significantly different from the speech tokenizer, suggesting the tokenizer as the main bottleneck. The gain over Lina-Speech may come from gesture conditioning, which adds context for emphasis and cloning, or simply from more extensive hyperparameter tuning in Gelina. For synchrony and gesture human-likeness, Gelina and RAG both significantly outperform EMAGE and CAMN; synchrony scores between Gelina and RAG are not significantly different, though RAG rates marginally higher for gesture human-likeness. Gelina achieves this level of performance while also synthesizing speech, demonstrating that competitive gesture quality can be obtained within a unified speech-gesture generation model.</span></p>\n\n",
                "matched_terms": [
                    "confidence",
                    "gesture",
                    "linaspeech",
                    "intervals",
                    "model",
                    "rag",
                    "mos",
                    "emage",
                    "camn",
                    "gelina"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We have presented Gelina, a model for joint speech-gesture generation. We evaluated it through both objective metrics and a user study. Gelina significantly outperforms two gesture baselines, EMAGE and CAMN, and reaches performance comparable to the strongest system, RAG-Gesture, while also delivering competitive speech quality relative to speech-only systems such as CosyVoice-2 and Lina-Speech. Our findings demonstrate that joint speech-gesture generation can remain competitive, and even outperform unimodal baselines, despite the added complexity of synthesizing two modalities. Current limitations are that Gelina models only body gestures, and its speech quality is constrained by the tokenizer. Future work will address these limitations by improving the tokenizer, extending gesture coverage to fingers and facial expressions, and supporting longer sequence generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "linaspeech",
                    "model",
                    "body",
                    "gelina",
                    "emage",
                    "camn",
                    "cosyvoice2"
                ]
            }
        ]
    }
}