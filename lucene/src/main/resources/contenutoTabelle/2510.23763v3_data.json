{
    "S5.T1": {
        "source_file": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
        "caption": "Table 1: Performance of different robot manipulation models on the OmniAction-LIBERO-TTS benchmark, evaluated across four task suites (Spatial, Goal, Object, Long-Horizon) under six contextual instruction types. Values in bold denote the best performance.",
        "body": "Task\nGround-truth Textual Prompt\nAudio →\\rightarrow ASR →\\rightarrow Text Prompt\nRoboOmni\n\n\nOpenVLA\nOFT\nNORA\nπ0\\pi_{0}\nOpenVLA\nOFT\nNORA\nπ0\\pi_{0}\n\n\nSpatial\nSentiment\n4.0\n9.0\n40.0\n8.0\n1.0\n8.0\n43.0\n11.0\n\n\\cellcolordeltaBg93.0\n\n\n\nNon-Verbal\n2.0\n8.0\n61.0\n7.0\n3.0\n8.0\n68.0\n14.0\n\n\\cellcolordeltaBg91.0\n\n\n\nIdentity\n1.0\n8.0\n53.0\n4.0\n2.0\n18.0\n56.0\n7.0\n\n\\cellcolordeltaBg92.0\n\n\n\nOverlapping\n6.0\n7.0\n43.0\n7.0\n11.0\n6.0\n58.0\n18.0\n\n\\cellcolordeltaBg93.0\n\n\n\nDyadic\n7.0\n6.0\n51.0\n5.0\n4.0\n17.0\n57.0\n3.0\n\n\\cellcolordeltaBg95.0\n\n\n\nTriadic\n1.0\n7.0\n51.0\n6.0\n2.0\n6.0\n57.0\n6.0\n\n\\cellcolordeltaBg94.0\n\n\n\nAvg\n3.5\n7.5\n49.8\n6.2\n3.8\n10.5\n56.5\n9.8\n\n\\cellcolordeltaBg93.0\n\n\n\nGoal\nSentiment\n0.0\n0.0\n11.0\n0.0\n0.0\n0.0\n9.0\n3.0\n\n\\cellcolordeltaBg89.0\n\n\n\nNon-Verbal\n0.0\n0.0\n18.0\n0.0\n1.0\n0.0\n22.0\n4.0\n\n\\cellcolordeltaBg79.0\n\n\n\nIdentity\n0.0\n0.0\n11.0\n3.0\n0.0\n0.0\n11.0\n1.0\n\n\\cellcolordeltaBg82.0\n\n\n\nOverlapping\n0.0\n0.0\n21.0\n0.0\n0.0\n0.0\n23.0\n1.0\n\n\\cellcolordeltaBg97.0\n\n\n\nDyadic\n0.0\n0.0\n7.0\n1.0\n1.0\n10.0\n18.0\n0.0\n\n\\cellcolordeltaBg85.0\n\n\n\nTriadic\n0.0\n0.0\n7.0\n2.0\n0.0\n0.0\n15.0\n0.0\n\n\\cellcolordeltaBg83.0\n\n\n\nAvg\n0.0\n0.0\n12.5\n1.0\n0.3\n1.7\n16.3\n1.5\n\n\\cellcolordeltaBg85.8\n\n\n\nObject\nSentiment\n1.0\n0.0\n9.0\n4.0\n2.0\n0.0\n5.0\n6.0\n\n\\cellcolordeltaBg83.0\n\n\n\nNon-Verbal\n2.0\n0.0\n7.0\n1.0\n3.0\n0.0\n17.0\n8.0\n\n\\cellcolordeltaBg82.0\n\n\n\nIdentity\n4.0\n0.0\n4.0\n5.0\n5.0\n0.0\n15.0\n8.0\n\n\\cellcolordeltaBg85.0\n\n\n\nOverlapping\n14.0\n7.0\n1.0\n6.0\n26.0\n0.0\n16.0\n9.0\n\n\\cellcolordeltaBg84.0\n\n\n\nDyadic\n20.0\n0.0\n14.0\n7.0\n20.0\n10.0\n19.0\n7.0\n\n\\cellcolordeltaBg88.0\n\n\n\nTriadic\n2.0\n0.0\n3.0\n5.0\n2.0\n10.0\n11.0\n2.0\n\n\\cellcolordeltaBg82.0\n\n\n\nAvg\n7.2\n1.2\n6.3\n4.7\n9.7\n3.3\n13.8\n6.7\n\n\\cellcolordeltaBg84.0\n\n\n\nLong\nSentiment\n0.0\n0.0\n26.0\n4.0\n0.0\n0.0\n50.0\n5.0\n\n\\cellcolordeltaBg76.0\n\n\n\nNon-Verbal\n0.0\n0.0\n35.0\n1.0\n0.0\n0.0\n57.0\n2.0\n\n\\cellcolordeltaBg76.0\n\n\n\nIdentity\n0.0\n0.0\n29.0\n4.0\n1.0\n0.0\n43.0\n4.0\n\n\\cellcolordeltaBg79.0\n\n\n\nOverlapping\n0.0\n0.0\n35.0\n5.0\n3.0\n0.0\n56.0\n6.0\n\n\\cellcolordeltaBg79.0\n\n\n\nDyadic\n1.0\n0.0\n42.0\n1.0\n1.0\n0.0\n59.0\n5.0\n\n\\cellcolordeltaBg85.0\n\n\n\nTriadic\n0.0\n0.0\n27.0\n5.0\n2.0\n10.0\n41.0\n8.0\n\n\\cellcolordeltaBg82.0\n\n\n\nAvg\n0.2\n0.0\n32.3\n3.3\n1.2\n1.7\n51.0\n5.0\n\n\\cellcolordeltaBg79.5\n\n\n\nAvg\n\n2.6\n0.4\n16.3\n3.0\n3.9\n2.3\n25.9\n4.4\n\n\\cellcolordeltaBg85.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"2\" rowspan=\"2\" style=\"padding:-1pt 0.0pt;\">Task</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding:-1pt 0.0pt;\">Ground-truth Textual Prompt</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding:-1pt 0.0pt;\">Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text Prompt</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding:-1pt 0.0pt;\">RoboOmni</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">OpenVLA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">OFT</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">NORA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">OpenVLA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">OFT</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">NORA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" rowspan=\"7\" style=\"padding:-1pt 0.0pt;\">Spatial</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Sentiment</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">9.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">40.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">43.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">93.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Non-Verbal</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">61.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">68.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">14.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">91.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Identity</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">53.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">18.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">56.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">92.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Overlapping</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">43.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">58.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">18.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">93.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Dyadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">17.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">95.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Triadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">94.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">10.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">56.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">9.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">93.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" rowspan=\"7\" style=\"padding:-1pt 0.0pt;\">Goal</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Sentiment</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">9.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">89.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Non-Verbal</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">18.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">22.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">79.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Identity</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">82.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Overlapping</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">21.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">23.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">97.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Dyadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">18.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">85.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Triadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">15.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">83.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">12.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">16.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">85.8</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" rowspan=\"7\" style=\"padding:-1pt 0.0pt;\">Object</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Sentiment</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">9.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">83.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Non-Verbal</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">17.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">82.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Identity</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">15.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">85.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Overlapping</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">14.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">26.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">16.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">9.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">84.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Dyadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">20.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">14.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">20.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">19.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">88.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Triadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">11.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">82.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">7.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">9.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">13.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">84.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" rowspan=\"7\" style=\"padding:-1pt 0.0pt;\">Long</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Sentiment</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">26.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">50.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">76.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Non-Verbal</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">76.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Identity</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">29.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">43.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">4.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">79.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Overlapping</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">35.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">56.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">79.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Dyadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">42.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">59.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">85.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\">Triadic</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">27.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">41.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">8.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">82.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">32.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">3.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">1.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">51.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">79.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">2.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">0.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">16.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">3.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">3.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">2.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">4.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>deltaBg<span class=\"ltx_text ltx_font_bold\">85.6</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "identity",
            "nora",
            "sentiment",
            "prompt",
            "oft",
            "cellcolordeltabg910",
            "instruction",
            "longhorizon",
            "goal",
            "long",
            "benchmark",
            "groundtruth",
            "avg",
            "omniactionliberotts",
            "text",
            "contextual",
            "dyadic",
            "cellcolordeltabg940",
            "types",
            "best",
            "openvla",
            "asr",
            "π0pi0",
            "cellcolordeltabg890",
            "different",
            "cellcolordeltabg830",
            "triadic",
            "audio",
            "bold",
            "cellcolordeltabg950",
            "cellcolordeltabg858",
            "cellcolordeltabg760",
            "cellcolordeltabg820",
            "cellcolordeltabg840",
            "object",
            "manipulation",
            "performance",
            "robot",
            "four",
            "evaluated",
            "under",
            "cellcolordeltabg795",
            "cellcolordeltabg850",
            "spatial",
            "overlapping",
            "cellcolordeltabg930",
            "cellcolordeltabg880",
            "roboomni",
            "across",
            "cellcolordeltabg920",
            "suites",
            "textual",
            "models",
            "task",
            "→rightarrow",
            "six",
            "cellcolordeltabg970",
            "cellcolordeltabg856",
            "cellcolordeltabg790",
            "denote",
            "values",
            "nonverbal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To comprehensively evaluate RoboOmni on diverse cross-modal contextual instructions, we conduct extensive experiments on the OmniAction-LIBERO across four task suites with six audio variants. &#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.T1\" title=\"Table 1 &#8227; Baseline Models &#8227; 5.1 Experiment Setup &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Tab.&#160;1</span></a> demonstrates that RoboOmni achieves an overall 85.6% success rate, substantially outperforming the strongest baseline (NORA, 25.9%) and other cascaded methods (all below 10%). Our analysis yields three key insights:\n(1)\n<span class=\"ltx_text ltx_font_bold\">End-to-end auditory integration is crucial for paralinguistic cues.</span>\nText-only models, whether using ASR transcripts or ground-truth text, fail to capture paralinguistic cues (e.g., prosody, overlapping speech), with best scores of\n25.9% (textual baseline).\nIn contrast, RoboOmni&#8217;s direct audio processing enables it to consistently exceed 76% across all types, demonstrating the importance of preserving acoustic information.\n(2)\n<span class=\"ltx_text ltx_font_bold\">Auditory integration enhances robust intent recognition under ambiguity.</span>\nGoal and Object suites are challenging due to multiple manipulable objects and valid actions, where baselines collapse (averaging 16.3% and 13.8% for the best baselines), exposing limits in contextual instruction understanding. RoboOmni sustains high performance (Goal: 85.8% v.s. Object: 84.0%), demonstrating robust generalization under semantic ambiguity.\n(3)\n<span class=\"ltx_text ltx_font_bold\">Instruction type complexity reveals varying cognitive demands.</span>\nFor end-to-end models, <span class=\"ltx_text ltx_font_italic\">dyadic</span> and <span class=\"ltx_text ltx_font_italic\">overlapping</span> tasks are easier, averaging <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>88%. <span class=\"ltx_text ltx_font_italic\">Non-verbal</span> instructions are hardest (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>82%), as they require recognizing non-verbal sounds and integrating them with visual and speech cues. The remaining tasks average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>85%, reflecting moderate complexity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision&#8211;Language&#8211;Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively.\nIn this work, we introduce <span class=\"ltx_text ltx_font_italic\">cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands.</span> To address this new setting, we present <span class=\"ltx_text ltx_font_bold\">RoboOmni</span>, a <span class=\"ltx_text ltx_font_italic\">Perceiver-Thinker-Talker-Executor</span> framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction.\nTo address the absence of training data for proactive intention recognition in robotic manipulation, we build <span class=\"ltx_text ltx_font_bold\">OmniAction</span>, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "contextual",
                    "roboomni",
                    "manipulation",
                    "types",
                    "models",
                    "six",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vision&#8211;Language&#8211;Action (VLA) models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib52\" title=\"\">2023</a>; Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib10\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib6\" title=\"\">2024</a>)</cite> have achieved remarkable advances in robotic manipulation, leveraging large-scale cross-embodiment datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Padalkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib33\" title=\"\">2023</a>; AgiBot-World-Contributors et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib2\" title=\"\">2025</a>; Khazatsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib18\" title=\"\">2024</a>)</cite> and Multimodal Large Language Models (MLLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib43\" title=\"\">2024</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib3\" title=\"\">2025a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib21\" title=\"\">2025</a>)</cite>.\nVLA models are generally categorized as (1) end-to-end models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brohan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib7\" title=\"\">2023</a>; Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib52\" title=\"\">2023</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib6\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib20\" title=\"\">2025</a>)</cite>, which map vision&#8211;language inputs directly to motor actions, and (2) modular Brain&#8211;Cerebellum models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib12\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib13\" title=\"\">2024</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib37\" title=\"\">2025</a>)</cite>, which use LLMs or VLMs as planners to decompose tasks into sub-goals for low-level controllers.\nWhile modular systems emphasize explicit planning, they suffer from fragmentation and interface constraints. In contrast, end-to-end models unify vision, language, and action in a shared latent space, enabling more natural and flexible responses.</p>\n\n",
                "matched_terms": [
                    "models",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite notable advances in VLA research, two fundamental limitations remain. (1) From the perspective of instruction type:\nmost works <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>)</cite> focus on direct commands (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(a)), later extended to more complex (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(b)) yet explicit forms&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib37\" title=\"\">2025</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_citet\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib46\" title=\"\">2025a</a>)</cite> recently introduced a dataset for inferential text-based instructions (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(c)), but system studies remain scarce.\n(2) From the perspective of the instruction source: current systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>; Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib52\" title=\"\">2023</a>)</cite> predominantly rely on textual instructions (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(d)) or ASR-transcribed speech (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(e)), the latter discarding essential paralinguistic cues such as tone, intonation, and affective signals. Recently, <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib50\" title=\"\">2025</a>)</cite> investigated models that accept speech instructions (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>-(f)) by converting existing textual commands into speech, but neglected real-world environmental sounds.\nOverall, existing works assume that instructions are explicitly issued, and there is a lack of study on jointly reasoning over speech, environmental sounds, and visual observations for proactive intent recognition and reasoning.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "models",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There arises a key research question: <span class=\"ltx_text ltx_font_bold\">Can a robot integrate cross-modal context, including speech, environmental audio, and visual observations, to proactively infer and verify user intent?</span>\nAs illustrated in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;1</span></a>, in a living-room scene, the robot integrates dialogue (audio), refrigerator observation (vision), and the juicer sound (environmental audio) to infer that John prefers cola over hand-made sour juice, and proactively seeks confirmation rather than waiting for an explicit command. Since humans rarely issue direct instructions in daily life, we define such scenarios as <span class=\"ltx_text ltx_font_italic\">cross-modal contextual instructions, where auditory (speech and environmental sound) and visual cues are fused to infer latent user intent and verified proactively by interaction</span>, in contrast to conventional setups that assume explicit commands.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "contextual",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and answer the research question, we propose <span class=\"ltx_text ltx_font_bold\">RoboOmni</span>, an end-to-end omni-modal framework for manipulation that closes the loop of intent recognition, interaction confirmation, and action execution. Unlike prior approaches, RoboOmni supports direct speech interaction without ASR, infers latent commands by fusing human speech, environmental audio, and vision through spatiotemporal modeling, and verifies intent via interaction.\nTo overcome data scarcity, we construct OmniAction, a dataset with 140k episodes, over 5k speakers, 2.4k event sounds, 640 background sounds, and six contextual instruction types.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "roboomni",
                    "types",
                    "manipulation",
                    "asr",
                    "six",
                    "instruction",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments in both simulation and real-world settings show that RoboOmni substantially outperforms text- and ASR-based baselines, achieving higher accuracy (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS2\" title=\"5.2 Evaluation on Cross-modal Contextual Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS3\" title=\"5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.3</span></a>), faster inference (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS6\" title=\"5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.6</span></a>), more effective proactive assistance (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS5\" title=\"5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.5</span></a>), and improved intention recognition (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS5\" title=\"5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.5</span></a>). Our contributions are fourfold:</p>\n\n",
                "matched_terms": [
                    "text",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1. We introduce <span class=\"ltx_text ltx_font_italic\">cross-modal contextual instructions</span>, a new setting for robotic manipulation that requires robots to proactively infer user commands from multimodal context (vision, environmental sounds, and speech) rather than passively await explicit instructions.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3. To address the lack of datasets for proactive intention reasoning, we introduce OmniAction, comprising 140k episodes with 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types, along with OmniAction-LIBERO for simulation-based evaluation.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "instruction",
                    "types",
                    "six"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid development of Large Language Models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib1\" title=\"\">2023</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib41\" title=\"\">2023a</a>)</cite> has spurred progress in multimodal extensions. Multimodal LLMs (MLLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib30\" title=\"\">2023</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib3\" title=\"\">2025a</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib27\" title=\"\">2023b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib8\" title=\"\">2023</a>)</cite> augment text-based reasoning with visual perception, enabling instruction following grounded in images.\nEarly attempts toward omni-modality relied on modular pipelines that separately process speech and vision&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib44\" title=\"\">2023</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib49\" title=\"\">2024</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib28\" title=\"\">2023</a>)</cite>, which makes temporal alignment across modalities difficult and limits accurate understanding of situated semantics. More recent work has shifted toward end-to-end omni-modal models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib15\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib47\" title=\"\">2025b</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib45\" title=\"\">2024</a>)</cite>, which can jointly model speech, vision, and text in a unified representation. However, these models remain oriented toward linguistic outputs (text or audio) and do not generate embodied actions, restricting their applicability in robotics.\nIn contrast, our work brings omni-modality into the embodied domain by introducing RoboOmni, an end-to-end framework that integrates speech, environmental sounds, visual context, and text for both action execution and proactive human&#8211;robot interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "roboomni",
                    "across",
                    "models",
                    "instruction",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies have explored the application of large Vision&#8211;Language Models (VLMs) in robotics <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib29\" title=\"\">2024</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib51\" title=\"\">2025</a>)</cite>, leveraging their ability to align linguistic instructions with visual scenes.\nBuilding on large-scale demonstrations, recent works develop end-to-end Vision&#8211;Language&#8211;Action (VLA) models that map vision and language to actions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brohan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib7\" title=\"\">2023</a>; Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib52\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib24\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib40\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib20\" title=\"\">2025</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib6\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib22\" title=\"\">2024a</a>; Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib35\" title=\"\">2025</a>)</cite>, but these typically assume short, explicit commands and fail on compositional or context-dependent tasks.\nCascaded or hierarchical extensions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Intelligence et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib16\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib37\" title=\"\">2025</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib39\" title=\"\">2025b</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib25\" title=\"\">2025</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib38\" title=\"\">2025a</a>)</cite> decompose instructions into sub-goals, yet remain fragmented and rigid, and neither paradigm captures <em class=\"ltx_emph ltx_font_italic\">contextual instructions</em>&#8212;implicit intent conveyed by dialogue, tone, or visual context, which is common in human&#8211;robot interaction.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, most prior studies further treat text as the main channel, using ASR/TTS cascades to bridge speech and action&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib37\" title=\"\">2025</a>; Khan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib17\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib23\" title=\"\">2024b</a>)</cite>.\nSuch pipelines discard paralinguistic cues (e.g., emotion, speaker identity), add latency, and disrupt temporal alignment with vision.\nA few recent efforts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib50\" title=\"\">2025</a>)</cite> extend VLAs to handle direct speech-based commands, yet these remain restricted to atomic or complex speech instructions and can only output actions, without the ability to respond through speech.\nIn contrast, our work introduces an end-to-end omni-modal framework that directly integrates speech, environmental sounds, vision, and text, enabling both action execution and cross-modal contextual instruction following for natural human&#8211;robot interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "contextual",
                    "instruction",
                    "identity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proactive robots must infer implicit intent from audio and visual observations, yet existing datasets lack such a combination of modalities (most of them lack audio modality) and inferential instructions needed for intent reasoning.\nTo address this gap, we introduce OmniAction, a large-scale corpus that encodes contextual instructions&#8212;latent intents grounded in speech, environmental audio, sound events, and vision. OmniAction covers six instruction categories and three non-speech sounds.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "contextual",
                    "instruction",
                    "six"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diverse Contextual Instructions.</span>\n(1) <span class=\"ltx_text ltx_font_italic\">Sentiment Cues</span>:\nEmotionally tinted expressions, or subtle vocalizations, that indirectly reveal user preferences or intentions (e.g., &#8220;Ugh, this juice is too sour&#8221; implying a request for an alternative).\n(2) <span class=\"ltx_text ltx_font_italic\">Overlapping Voices</span>:\nMulti-speaker audio segments with temporal overlaps, testing intent extraction under crosstalk and partial masking.\n(3) <span class=\"ltx_text ltx_font_italic\">Non-Verbal Cues</span>:\nSalient non-linguistic audio events (e.g., alarms, phone rings) that carry situational information relevant to the task.\n(4) <span class=\"ltx_text ltx_font_italic\">Identity Cues</span>:\nSpeaker attributes such as age and gender, inferred from voice and not available from text, are needed to decide whose intent to satisfy.\n(5) <span class=\"ltx_text ltx_font_italic\">Dyadic Dialogue</span>:\nTwo-participant dialogues where intent emerges from conversational flow rather than explicit commands.\n(6) <span class=\"ltx_text ltx_font_italic\">Triadic Dialogue</span>:\nThree participants interact with turn-taking and indirect references,\nincreasing the complexity of intent attribution.\nTo preserve general command-following ability beyond dialogue, we also include a portion of single-person text instructions during training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "contextual",
                    "dyadic",
                    "identity",
                    "sentiment",
                    "under",
                    "task",
                    "overlapping",
                    "triadic",
                    "audio",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diverse Non-Speech Sounds.</span>\nWe also investigate three types of acoustic variation:\n(1) <span class=\"ltx_text ltx_font_italic\">Speaker Timbre</span>.\n5,096 distinct voices spanning six categories by age (elderly, adult, child) and gender (male, female). Reference audio clips are used for timbre cloning to ensure within-dialogue consistency and cross-speaker diversity.\n(2) <span class=\"ltx_text ltx_font_italic\">Sound Events</span>.\n2,482 non-verbal events (e.g., thunder, doorbell) were inserted at scripted anchors to provide cues beyond speech.\n(3) <span class=\"ltx_text ltx_font_italic\">Environmental Backgrounds</span>.\n640 ambient soundscapes (e.g., running water, stir-fry sizzling) mixed at controlled signal-to-noise ratios (SNRs) to mimic daily environments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "types",
                    "nonverbal",
                    "six"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Statistics and Formats.</span>\nOmniAction comprises 141,162 multimodal episodes, spanning 112 skills (e.g., <span class=\"ltx_text ltx_font_italic\">pick-place</span>, <span class=\"ltx_text ltx_font_italic\">open/close</span>) and 748 objects (e.g., <span class=\"ltx_text ltx_font_italic\">can</span>), with 5,096 distinct speaker timbres, 2,482 non-verbal sound events, and 640 environmental backgrounds.\nEach sample is represented as a triplet <math alttext=\"(C,I,A)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><mi>I</mi><mo>,</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(C,I,A)</annotation></semantics></math>: a multi-turn conversation <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> (user turns as audio, assistant turns as text, with <span class=\"ltx_text ltx_font_typewriter\">[ACT]</span> marking action onset), a visual observation sequence <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, and an action trajectory <math alttext=\"A=\\{a_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">A=\\{a_{t}\\}_{t=1}^{T}</annotation></semantics></math>, where <math alttext=\"a_{t}\\in\\mathbb{R}^{7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>7</mn></msup></mrow><annotation encoding=\"application/x-tex\">a_{t}\\in\\mathbb{R}^{7}</annotation></semantics></math> denotes the delta control vector of the end-effector.\nThe distribution of instruction type is detailed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S3.F3\" title=\"Figure 3 &#8227; 3.1 Overview &#8227; 3 OmniAction Dataset Construction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;3</span></a>.\nMore detailed statistics and examples are shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A1.SS1\" title=\"A.1 Data Statics &#8227; Appendix A Details of OmniAction &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;A.1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A1.SS2\" title=\"A.2 Data Example &#8227; Appendix A Details of OmniAction &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "instruction",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Textual Scripting.</span>\nWe sample tasks and trajectories from Open-X Embodiment datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Padalkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib33\" title=\"\">2023</a>)</cite> and transform each atomic instruction into a contextual one with GPT-4o through:\n(1) <span class=\"ltx_text ltx_font_italic\">Filtering</span>: removing trivial samples with low-information visual states.\n(2) <span class=\"ltx_text ltx_font_italic\">Dialogue synthesis</span>: rewriting instructions into multi-turn household dialogues that span six contextual instruction types.\n(3) <span class=\"ltx_text ltx_font_italic\">Interaction extension</span>: constructing follow-up human&#8211;robot exchanges that simulate natural interactions.\n(4) <span class=\"ltx_text ltx_font_italic\">Validation</span>: ensuring intent consistency with the original instruction.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "types",
                    "textual",
                    "six",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditory Realization.</span>\nTo capture paralinguistic cues and environmental acoustics beyond text, we convert dialogues into audio that reflects real household conditions, augmented with diverse sound events and background environments.\nThe conversion process includes four steps:\n(1) <span class=\"ltx_text ltx_font_italic\">Speech synthesis</span>: rendering user turns into audio via multiple high-fidelity, neural TTS engines&#8212;MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenMOSS, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib31\" title=\"\">2025</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib9\" title=\"\">2025</a>)</cite>, and Gemini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cloud.google.com/text-to-speech/docs/gemini-tts\" title=\"\">https://cloud.google.com/text-to-speech/docs/gemini-tts</a></span></span></span> &#160;with voice cloning for timbre consistency and cross-dialogue diversity.\n(2) <span class=\"ltx_text ltx_font_italic\">Multi-speaker simulation</span>: generating each speaker&#8217;s turns separately, concatenating them on the timeline, and inserting overlaps at controlled offsets with CTC-based methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib11\" title=\"\">2006</a>)</cite> to enable realistic crosstalk and interruption.\n(3) <span class=\"ltx_text ltx_font_italic\">Non-verbal event insertion</span>: mixing contextual sounds (e.g., alarms, utensil clatter) at scripted anchors.\n(4) <span class=\"ltx_text ltx_font_italic\">Environmental backgrounds</span>: randomly adding ambient textures (e.g., water flow, frying, fan hum) at varying SNRs, spanning a wide range to simulate varying acoustic difficulty.</p>\n\n",
                "matched_terms": [
                    "text",
                    "contextual",
                    "four",
                    "audio",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of simulation benchmarks, we construct <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO</span> based on LIBERO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib26\" title=\"\">2023a</a>)</cite>, with two variants.\n(1) <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO-TTS</span> augments the LIBERO using the pipeline described above. Starting from 40 manipulation tasks across four suites (Spatial, Object, Goal, Long-Horizon), we generate six variants for each task based on the six contextual instruction types, yielding 240 evaluation tasks. Example dialogues and task scenes are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A4\" title=\"Appendix D OmniAction-LIBERO &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">App.&#160;D</span></a>.\n(2) <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO-Real</span> evaluates RoboOmni under real speech conditions, where 10 volunteers provide spoken instructions collected in real environments.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "object",
                    "roboomni",
                    "types",
                    "manipulation",
                    "across",
                    "suites",
                    "four",
                    "under",
                    "task",
                    "six",
                    "spatial",
                    "instruction",
                    "longhorizon",
                    "goal",
                    "omniactionliberotts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose RoboOmni, an end-to-end omni-modal LLM framework organized as Perceiver&#8211;Thinker&#8211;Talker&#8211;Executor, unifying speech, environmental audio, vision, and robotic actions within a single autoregressive model <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S4.F4\" title=\"Figure 4 &#8227; 4 Methods &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;4</span></a>.\nRoboOmni employs a Perceiver for multimodal input, a Thinker backbone, a Talker for speech, and an Executor for actions. To align inputs with linguistic and motor outputs, RoboOmni uses unified tokenization to encode all modalities into a shared semantic space, which the Thinker processes into high-level representations and specialized decoders render into speech and executable actions, enabling seamless perception-to-action generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perceiver: Multimodal Input Encoding.</span>\nThe Perceiver handles the encoding of heterogeneous input modalities into a unified embedding space. We follow the multimodal processing pipeline of Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib47\" title=\"\">2025b</a>)</cite> for encoding text, audio, and visual inputs into a series of hidden representations. At timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, given visual observation <math alttext=\"I_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>I</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">I_{t}</annotation></semantics></math> and audio segment <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, we obtain visual embedding <math alttext=\"\\mathbf{v}_{t}=f_{v}(I_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119855;</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{v}_{t}=f_{v}(I_{t})</annotation></semantics></math> and audio embedding <math alttext=\"\\mathbf{s}_{t}=f_{s}(x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mi>s</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}=f_{s}(x_{t})</annotation></semantics></math>. Together with textual context <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math>, these form the unified input representation <math alttext=\"\\mathbf{X}_{t}=[\\mathbf{v}_{t};\\mathbf{s}_{t};\\mathbf{c}_{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119855;</mi><mi>t</mi></msub><mo>;</mo><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>;</mo><msub><mi>&#119836;</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}_{t}=[\\mathbf{v}_{t};\\mathbf{s}_{t};\\mathbf{c}_{t}]</annotation></semantics></math> that serves as input to the Thinker backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "textual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Thinker: Omni-Modal Reasoning.</span>\nThe Thinker serves as the central reasoning engine, built upon the LLM backbone. It processes the unified multimodal representations from the Perceiver and generates contextually appropriate outputs in the joint vocabulary space <math alttext=\"\\mathcal{V}\\cup\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>&#8746;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}\\cup\\mathcal{A}</annotation></semantics></math>. The Thinker auto-regressively produces sequences that seamlessly interleave text tokens, speech representations, and action tokens, enabling unified reasoning across perception, language, and robotic control.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Executor: Action Generation.</span>\nTo enable seamless integration of robotic control within the language model framework, we extend the vocabulary of the Thinker with a set <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> of 2048 discrete action tokens introduced by the FAST+ tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib34\" title=\"\">2025</a>)</cite>. Rather than mapping each action dimension to a separate token, FAST+ represents a continuous action vector <math alttext=\"a_{t}\\in\\mathbb{R}^{7}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>7</mn></msup></mrow><annotation encoding=\"application/x-tex\">a_{t}\\in\\mathbb{R}^{7}</annotation></semantics></math> (e.g., 7-DoF control) by a short sequence of discrete symbols <math alttext=\"r_{t}\\subset\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>&#8834;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><annotation encoding=\"application/x-tex\">r_{t}\\subset\\mathcal{A}</annotation></semantics></math>. This enables the model to auto-regressively generate from the joint space <math alttext=\"\\mathcal{V}\\cup\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>&#8746;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}\\cup\\mathcal{A}</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> represents the text vocabulary, seamlessly bridging language understanding and robotic control within a single sequence. The Executor then decodes these action tokens back into executable robot commands.</p>\n\n",
                "matched_terms": [
                    "text",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train RoboOmni using a unified autoregressive objective that handles both conversational and manipulation capabilities within the same framework. Given a training episode, the model receives multimodal input <math alttext=\"\\mathbf{X}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{t}</annotation></semantics></math> and learns to predict appropriate responses&#8212;either conversational replies for dialogue or action sequences for manipulation.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As current open-source Vision-Language-Action (VLA) models are primarily designed for textual instructions and cannot directly process audio inputs, we construct two baseline paradigms to validate the necessity of end-to-end audio processing: (i) <span class=\"ltx_text ltx_font_bold\">Ground-truth Textual Prompt</span>, which directly feeds pre-annotated transcriptions of speech instructions into VLA models; (ii) <span class=\"ltx_text ltx_font_bold\">Speech-ASR-Textual Prompt</span>, where speech instructions are first transcribed to text using the ASR model Whisper large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib36\" title=\"\">2023</a>)</cite>, then fed into VLA models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "textual",
                    "prompt",
                    "models",
                    "asr",
                    "groundtruth",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct evaluations comparing RoboOmni with four representative VLA baselines representing both paradigms:\n(1) <span class=\"ltx_text ltx_font_bold\">OpenVLA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>)</cite>,\nbuilt on Llama-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib42\" title=\"\">2023b</a>)</cite> with DINOv2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib32\" title=\"\">2023</a>)</cite> and SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib48\" title=\"\">2023</a>)</cite> encoders, pretrained on <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>970k demonstrations from Open-X-Embodiment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Padalkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib33\" title=\"\">2023</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">OpenVLA-OFT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib20\" title=\"\">2025</a>)</cite>, a variant of OpenVLA augmented with action chunking and optimized with an <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss on continuous action.\n(3) <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib6\" title=\"\">2024</a>)</cite>,\nbased on PaliGemma&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib5\" title=\"\">2024</a>)</cite> with diffusion action experts, trained on both large-scale internet multimodal data and robot datasets.\n(4) <span class=\"ltx_text ltx_font_bold\">NORA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib14\" title=\"\">2025</a>)</cite>,\nbuilt on Qwen2.5-VL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib4\" title=\"\">2025b</a>)</cite> with FAST+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib34\" title=\"\">2025</a>)</cite> discrete action decoding.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "robot",
                    "openvla",
                    "π0pi0",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train the model with an input image resolution of <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math>, an audio sampling rate of 16,000 Hz, and an action chunk size of 6.\nFor large-scale pretraining, RoboOmni is optimized on a cluster of 64 A100 GPUs over 10 days, corresponding to a total of 15,360 A100-hours, with a batch size of 512. The training runs for 10 epochs using a learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math>, with the first 1k steps reserved for warm-up. For downstream task supervised fine-tuning (SFT), we adopt a learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> and train with 8 A100 GPUs for 10-30k steps.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel\">We further evaluate RoboOmni&#8217;s robustness under real human-recorded speech with direct audio instructions.\nAs shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS3\" title=\"5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.3</span></a>, on the OmniAction-LIBERO-Real benchmark, RoboOmni achieves the highest average performance (76.6%),\nsurpassing strong text-based VLAs including <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> (73.8%), OpenVLA (40.1%), and NORA (17.4%).\nASR-based VLAs suffer from acoustic variability: accents, coarticulation, and background noise frequently cause recognition\nerrors, and even minor word deviations can degrade VLAs&#8217; performance.\n<math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m5\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> shows some robustness, likely due to large-scale co-training on diverse web data.\nIn contrast, RoboOmni processes speech directly, avoiding ASR pipeline errors. Pretraining on diverse speakers and sounds improves robustness to acoustic variability and paralinguistic cues, yielding more consistent performance.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "performance",
                    "openvla",
                    "π0pi0",
                    "asr",
                    "under",
                    "benchmark",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify that RoboOmni&#8217;s capabilities transfer beyond simulation, we fine-tune our pretrained model by utilizing our demonstration dataset on WidowX 250S, where speech was recorded by 10 volunteers in real environments.\nThis enables RoboOmni to run on real robots and handle diverse speech instructions (e.g., sentiment, overlapping cues).\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;5</span></a> highlights RoboOmni&#8217;s real-world competence across three dimensions: (1) <span class=\"ltx_text ltx_font_bold\">Strong intent recognition</span>, accurately inferring user intention from both visual and auditory cues (e.g., identifying the object based on audio and determining the receptacle is the pot from the visual scene); (2) <span class=\"ltx_text ltx_font_bold\">Effective Interaction</span>, proactively asking clarifying questions after inferring the user&#8217;s latent intent (e.g., &#8220;should I &#8230;?&#8221;) and executing the action after receiving confirmation, ensuring that actions are deliberate and aligned with the user&#8217;s intent; (3) <span class=\"ltx_text ltx_font_bold\">Reliable Execution</span>, successfully carries out confirmed actions, such as locating the correct object amidst multiple distractors and placing it in the designated pot.\nMore detailed real-world cases are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.SS1\" title=\"B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;B.1</span></a></p>\n\n",
                "matched_terms": [
                    "object",
                    "roboomni",
                    "across",
                    "sentiment",
                    "overlapping",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intent Recognition Capability.</span>\n\nWe further evaluate the models&#8217; ability to recognize user intent under contextual instructions, shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F5.sf1\" title=\"5(a) &#8227; Figure 6 &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">5(a)</span></a>. Specifically, we compare Qwen2.5-Omni-3B (our backbone), Qwen2.5-Omni-7B, and ASR+GPT-4o, against our proposed RoboOmni.\nWe observe that RoboOmni achieves the highest accuracy (88.9%), confirming the advantage of end-to-end speech&#8211;action modeling that preserves paralinguistic cues and dialogue context.\nNotably, although ASR introduces recognition noise compared with end-to-end models, GPT-4o still surpasses the smaller Omni models (55.6% vs. 27.8%/50.0%) because its stronger multimodal reasoning compensates for transcription loss. This highlights that contextual instructions cannot be resolved by acoustic modeling alone, but also demand robust reasoning capabilities</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "roboomni",
                    "models",
                    "asr",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interaction Capability.</span>\nWe conduct qualitative analysis to evaluate how different models handle multi-modal contextual instructions.\nAs shown in &#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F5.sf2\" title=\"5(b) &#8227; Figure 6 &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>, RoboOmni excels by proactively clarifying, integrating cross-modal signals, and sustaining natural dialogue, whereas baselines often fail in one or more aspects.\nSpecifically, RoboOmni demonstrates superior interaction capabilities across three key aspects: (1) <span class=\"ltx_text ltx_font_bold\">Proactive clarification</span>: When encountering ambiguous instructions like &#8220;egg dumplings\" without explicit commands, RoboOmni asks &#8220;Would you like me to put the egg dumpling into the hot pot?\" rather than making assumptions and blind execution like baseline models. (2) <span class=\"ltx_text ltx_font_bold\">Multi-modal integration</span>: In the doorbell scenario, RoboOmni successfully combines speech context with environmental sounds, asking &#8220;I just heard the doorbell&#8212;should I put the fish ball into the hot pot?\" while baselines ignore auditory cues or provide irrelevant responses. (3) <span class=\"ltx_text ltx_font_bold\">Natural dialogue flow</span>: RoboOmni maintains collaborative language patterns (&#8220;Would you like me to&#8230;?\") that respect human agency, contrasting with baseline models that often issue direct commands or statements.\nAdditional case studies for all instruction types appear in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.SS2\" title=\"B.2 Interaction Capability &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "roboomni",
                    "across",
                    "types",
                    "models",
                    "different",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Does OmniAction Pretraining Improve Training Efficiency?</span>\nTo evaluate the benefit of pretraining on OmniAction, we compare finetuning efficiency on the six Spatial variants in OmniAction-LIBERO, contrasting OmniAction-pretrained + SFT with from-scratch SFT (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F7\" title=\"Figure 7 &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;7</span></a>). The pretrained model converges rapidly, reaching nearly 90% accuracy within 2k steps, while the from-scratch counterpart only attains <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>30% after 20k steps and even degrades at 30k steps. This highlights that pretraining on OmniAction providing strong generalizable priors for fast and stable adaptation with minimal fine-tuning.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "six"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Can Cascaded Pipelines Handle Contextual Instructions Effectively with High-level Planner?</span>\nWe compare RoboOmni with planner&#8211;controller pipelines, where Qwen2.5-Omni-3B serves as the planner and text-based VLAs as controllers, shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F8\" title=\"Figure 8 &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;8</span></a>.\nRoboOmni outperforms all cascaded pipelines, demonstrating the advantage of end-to-end speech&#8211;action learning: jointly modeling audio, vision, and action avoids the lossy planner&#8211;controller interface and preserves intent fidelity. Cascaded pipelines perform worse due to (1) semantic drift, as planners are not co-trained with VLAs and generate commands controllers cannot execute, and (2) poor handling of speaker identity, since Qwen2.5-Omni fails to capture paralinguistic cues, leading to the weakest results on <span class=\"ltx_text ltx_font_italic\">Identity Cues</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "contextual",
                    "identity",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Does End-to-End Modeling Improve Inference Efficiency?</span>\nTo assess whether end-to-end modeling improves runtime efficiency, we measure per-inference latency on a single RTX 4090 GPU.\nUsing ASR + OpenVLA as the baseline (1.0&#215;), we find that other cascaded pipelines (ASR + Nora: 1.02&#215;, ASR + <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math>: 0.96&#215;) incur similar costs since the ASR stage dominates computation. In contrast, RoboOmni runs at 0.49&#215; latency, showing that end-to-end audio&#8211;action modeling eliminates the ASR bottleneck and substantially improves efficiency (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F9\" title=\"Figure 9 &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;9</span></a>).</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "openvla",
                    "π0pi0",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we introduced cross-modal contextual instructions, a new paradigm for robotic manipulation where robots proactively infer user intent from multimodal context&#8212;vision, environmental sounds, and speech&#8212;rather than passively awaiting explicit commands. Building on this setting, we proposed RoboOmni, a Perceiver&#8211;Thinker&#8211;Talker&#8211;Executor framework built on end-to-end omni-modal LLMs that integrates auditory and visual inputs, unifying intention recognition, confirmation, and action execution. To address data scarcity, we constructed OmniAction, a large-scale corpus of 140k episodes with diverse speakers, event sounds, and backgrounds, together with OmniAction-LIBERO for simulation-based evaluation.\nComprehensive experiments in both simulation and the real world demonstrate that RoboOmni exhibits emerging cognitive intelligence, significantly outperforming text- and ASR-based baselines in success rate, inference speed, proactive assistance, and intention recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "contextual",
                    "roboomni",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of contextual instruction, <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A1.F9.sf2\" title=\"9(b) &#8227; Figure 10 &#8227; A.1 Data Statics &#8227; Appendix A Details of OmniAction &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">9(b)</span></a> presents the distribution of audio segment lengths across different types of instructions. The majority of clips range from <span class=\"ltx_text ltx_font_bold\">10 to 80 seconds</span>. Overlapping dialogues tend to be shorter in duration, while non-verbal sequences are longer due to the insertion of sound events.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "across",
                    "types",
                    "different",
                    "overlapping",
                    "instruction",
                    "audio",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"449\" id=\"A1.SS2.p1.g1\" src=\"imgs/case_study/events.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p1.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Non-Verbal Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p1.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>Move the pan to the right of the bottle and banana.</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p1.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p1.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Daughter: Dad, can you help me with this? I&#8217;m not sure where to put the pan.</span>\n<span class=\"ltx_p\">Dad: Sure, where do you think it should go?</span>\n<span class=\"ltx_p\">Daughter: I think it should be moved, but I&#8217;m not sure where.</span>\n<span class=\"ltx_p\">Dad: If you hear the ceramic collision, move the pan to the right of the bottle and banana. If you hear the door knob noise, leave it where it is.</span>\n<span class=\"ltx_p\">Daughter: Got it. Let&#8217;s see what happens.</span>\n<span class=\"ltx_p\">Robot: \"Should I move the orange pan to the right of the bottle and banana on the countertop?\"</span>\n<span class=\"ltx_p\">Dad; Yes, that&#8217;s exactly what I want you to do right now, thank you!</span>\n<span class=\"ltx_p\">Robot: Alright, I&#8217;ll move the orange pan to the right of the bottle and banana. [ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "robot",
                    "audio",
                    "instruction",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"479\" id=\"A1.SS2.p2.g1\" src=\"imgs/case_study/emotion.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p2.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Sentiment Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p2.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>put knife on cutting board</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p2.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p2.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mom: Hmm, there are a few interesting items here. We could pick up that green chip bag, the sponge, or even the Red Bull can. What do you think we should go for first?</span>\n<span class=\"ltx_p\">Son: Hmm&#8230; I don&#8217;t think the sponge is what we&#8217;re looking for&#8212;it&#8217;s more for cleaning, right?</span>\n<span class=\"ltx_p\">Mom Ah, not quite sure about the Red Bull either&#8230; It&#8217;s small and doesn&#8217;t seem to fit with what we need now.</span>\n<span class=\"ltx_p\">Son: Let me think&#8230; The chip bag looks like the most likely option here.</span>\n<span class=\"ltx_p\">Mom: Hmm, I see what you mean. Alright, let&#8217;s go with that one.</span>\n<span class=\"ltx_p\">Robot: Should I pick up the green jalapeno chip bag for you?</span>\n<span class=\"ltx_p\">Mom: Yes, please pick up the green jalapeno chip bag.</span>\n<span class=\"ltx_p\">Robot: OK, I will do that. [ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "robot",
                    "audio",
                    "instruction",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"479\" id=\"A1.SS2.p3.g1\" src=\"imgs/case_study/identity.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p3.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Identity Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p3.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>place the silver pot on the yellow napkin</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p3.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p3.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Grandpa: Ah, it&#8217;s good to see everything organized here. I know my little grandson loves fiddling with things, especially the top drawer. And your dad, well, he keeps saying he needs his tools from the bottom one.</span>\n<span class=\"ltx_p\">Father: Haha, you know me too well, Dad. The bottom drawer has all my essentials, but I guess I could wait.</span>\n<span class=\"ltx_p\">Grandpa:Well, patience runs in the family, doesn&#8217;t it?</span>\n<span class=\"ltx_p\">Son: Grandpa, can I check the top drawer first? Please? It&#8217;s my favorite spot!</span>\n<span class=\"ltx_p\">Robot: Do you need assistance to open the top drawer?</span>\n<span class=\"ltx_p\">Father: Uh, yeah, sure, that&#8217;d be great!,</span>\n<span class=\"ltx_p\">Robot: Alright. I will open the top drawer for you. [ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "robot",
                    "audio",
                    "instruction",
                    "identity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"449\" id=\"A1.SS2.p4.g1\" src=\"imgs/case_study/overlap.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p4.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Overlapping Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p4.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>put lid on pot or pan</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p4.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p4.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mother: Hey, did you see the lid sitting over there?</span>\n<span class=\"ltx_p\">Son: Yeah, should we use it for the pot?</span>\n<span class=\"ltx_p\">Mother: I was thinking about covering the pan instead [Overlap] wait, the pot?</span>\n<span class=\"ltx_p\">[Overlap_Son] Yes, the pot! It&#8217;s right there.</span>\n<span class=\"ltx_p\">Mother: Oh, okay, you&#8217;re right. Let&#8217;s cover the pot then.</span>\n<span class=\"ltx_p\">Son: Haha, I knew you&#8217;d agree!</span>\n<span class=\"ltx_p\">Robot: Do you need me to put the lid on the pot?</span>\n<span class=\"ltx_p\">Mother: Yes, please put the lid on the pot.</span>\n<span class=\"ltx_p\">Robot: OK, I will do that. [ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "robot",
                    "audio",
                    "instruction",
                    "overlapping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"449\" id=\"A1.SS2.p5.g1\" src=\"imgs/case_study/two.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p5.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Dyadic Dialogue</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p5.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>Move pot onto the towel</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p5.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p5.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Dad: Oh, look at that pot sitting there.</span>\n<span class=\"ltx_p\">Mom: Yeah, it&#8217;s right next to the towel.</span>\n<span class=\"ltx_p\">Dad: Hmm, if we move it onto the towel, it&#8217;ll be easier to clean later.</span>\n<span class=\"ltx_p\">Mom: Good idea, let me handle that.</span>\n<span class=\"ltx_p\">Robot: Do you need me to move the pot onto the towel?</span>\n<span class=\"ltx_p\">Mom: Uh, yeah, that&#8217;d be great. It&#8217;s better to keep it there to avoid scratches.</span>\n<span class=\"ltx_p\">Robot: Alright, I will move the pot onto the towel now. [ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "dyadic",
                    "instruction",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A1.SS2.p6.g1\" src=\"imgs/case_study/three.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A1.SS2.p6.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Triadic Dialogue</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p6.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>: \n<br class=\"ltx_break\"/>Place the brush on top of orange cloth</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A1.SS2.p6.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A1.SS2.p6.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Son: Hey, Lily, look at that steak meat over there!</span>\n<span class=\"ltx_p\">Daughter: Oh, yeah! It&#8217;s just sitting there like it&#8217;s waiting for a prince to rescue it.</span>\n<span class=\"ltx_p\">Grandpa: You two are hilarious. I guess the &#8217;prince&#8217; could be me, huh?</span>\n<span class=\"ltx_p\">Son: Go for it, Grandpa. You&#8217;ve got that royal touch.</span>\n<span class=\"ltx_p\">Grandpa: Alright, kids, I&#8217;ll pick it up and get it ready for dinner. Can&#8217;t let a good steak go to waste!,</span>\n<span class=\"ltx_p\">Robot: Hey, do you need me to pick up the red ball for you?</span>\n<span class=\"ltx_p\">Daughter: Uh, no, we don&#8217;t need that right now!</span>\n<span class=\"ltx_p\">Robot: Alright, how about I pick up the steak meat instead?</span>\n<span class=\"ltx_p\">Daughter: Yeah, that&#8217;s what we&#8217;re looking for!</span>\n<span class=\"ltx_p\">Robot: OK. I will perform operation pick up the steak meat now.[ACT]</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "robot",
                    "audio",
                    "instruction",
                    "triadic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intent recoverability:</span> Whether the latent task intent (i.e., the canonical atomic instruction) can be unambiguously inferred from the dialogue and multimodal context. Annotators were asked to answer <span class=\"ltx_text ltx_font_italic\">Yes/No</span>, with <span class=\"ltx_text ltx_font_italic\">Yes</span> requiring that the original intent could be reasonably reconstructed without external information.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Overlapping:</span> the audio contains genuine temporal overlaps such that ASR alone would be challenged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "overlapping",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Dyadic / Triadic:</span> the task intent is embedded within multi-turn, two- or three-party exchanges.</p>\n\n",
                "matched_terms": [
                    "dyadic",
                    "task",
                    "triadic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.F12\" title=\"Figure 12 &#8227; B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;12</span></a>, we present the model&#8217;s real-world performance on <span class=\"ltx_text ltx_font_italic\">Identity Cues</span> and <span class=\"ltx_text ltx_font_italic\">non-verbal cues</span>, while <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.F13\" title=\"Figure 13 &#8227; B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;13</span></a> illustrates its performance on <span class=\"ltx_text ltx_font_italic\">Dyadic Dialogues</span> and <span class=\"ltx_text ltx_font_italic\">Triadic Dialogues</span>. The results demonstrate that the model not only accurately infers user intent from visual and audio cues, but also engages in natural interactive questioning and reliably executes the corresponding actions.</p>\n\n",
                "matched_terms": [
                    "dyadic",
                    "identity",
                    "performance",
                    "triadic",
                    "audio",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#A2.F14\" title=\"Figure 14 &#8227; B.2 Interaction Capability &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, RoboOmni demonstrates superior interaction capabilities across three key aspects: (1) <span class=\"ltx_text ltx_font_bold\">Proactive clarification</span>: When encountering ambiguous instructions like &#8220;egg dumplings\" without explicit commands, RoboOmni asks &#8220;Would you like me to put the egg dumpling into the hot pot?\" rather than making assumptions and blind execution like baseline models. (2) <span class=\"ltx_text ltx_font_bold\">Multi-modal integration</span>: In the doorbell scenario, RoboOmni successfully combines speech context with environmental sounds, asking &#8220;I just heard the doorbell&#8212;should I put the fish ball into the hot pot?\" while baselines ignore auditory cues or provide irrelevant responses. (3) <span class=\"ltx_text ltx_font_bold\">Natural dialogue flow</span>: RoboOmni maintains collaborative language patterns (&#8220;Would you like me to&#8230;?\") that respect human agency, contrasting with baseline models that often issue direct commands or statements.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "across",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p1.g1\" src=\"imgs/case_study/D-01.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p1.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Sentiment Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p1.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.3pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p1.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>:</span>\n<span class=\"ltx_p\">Libero 10</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">turn on the stove and put the moka pot on it</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p1.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p1.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Dad: Alright, we need to get things ready for coffee. Should we place the frying pan on the stove, or maybe the moka pot?</span>\n<span class=\"ltx_p\">Daughter: Hmm&#8230; Doesn&#8217;t seem quite right&#8230;</span>\n<span class=\"ltx_p\">Dad: Okay, how about turning on the burner first and preparing the stovetop?</span>\n<span class=\"ltx_p\">Daughter: Hmm&#8230; let&#8217;s think&#8230;</span>\n<span class=\"ltx_p\">Dad: Hmm&#8230; I see now which one we need to turn on.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "instruction",
                    "task",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p2.g1\" src=\"imgs/case_study/D-02.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Overlapping Voices</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p2.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>:</span>\n<span class=\"ltx_p\">Libero Object</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the cream cheese and place it in the basket</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p2.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mother: Hey, can you help me sort these things out?</span>\n<span class=\"ltx_p\">Daughter: Sure, what do you want to start with?</span>\n<span class=\"ltx_p\">Mother: Let&#8217;s put something creamy in the basket. Maybe the cream cheese?</span>\n<span class=\"ltx_p\">Daughter: Oh, you mean the small rectangular one?</span>\n<span class=\"ltx_p\">Mother: No, the taller one, next to the [Overlap]orange box.</span>\n<span class=\"ltx_p\">Daughter:[Overlap_S2] Oh, got it, the cream cheese!</span>\n<span class=\"ltx_p\">Mother: Exactly! Let&#8217;s put that in the basket.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "object",
                    "task",
                    "overlapping",
                    "instruction",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p3.g1\" src=\"imgs/case_study/D-03.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Non-Verbal Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p3.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>:</span>\n<span class=\"ltx_p\">Libero Object</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the alphabet soup and place it in the basket</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p3.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Daughter: Dad, can you help me with these groceries?</span>\n<span class=\"ltx_p\">Dad: Sure, what do you need me to do?</span>\n<span class=\"ltx_p\">Daughter: Well, if you hear the sound of the drawer closing, pick the alphabet soup and place it in the basket. If you hear the sound of the coffee machine brewing, pick the dressing bottle and place it in the basket.</span>\n<span class=\"ltx_p\">Dad: Got it. Let me know if you need help with anything else.</span>\n<span class=\"ltx_p\">Daughter: Thanks, Dad. I&#8217;ll finish sorting the rest of these.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "object",
                    "task",
                    "instruction",
                    "audio",
                    "nonverbal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p4.g1\" src=\"imgs/case_study/D-04.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>:</span>\n<span class=\"ltx_p\">Identity Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p4.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>: Libero Spatial</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the black bowl on the ramekin and place it on the plate</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p4.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Son: Mum, Dad said he need that black bowl on the ramekin. He said he need it for dinner!</span>\n<span class=\"ltx_p\">Mum: Oh really? Well, I was planning to use that ramekin for baking tonight, and I need it free.</span>\n<span class=\"ltx_p\">Son: Haha! Looks like we&#8217;ve got a little competition going on here!</span>\n<span class=\"ltx_p\">Dad: Oh, come on! Can I have mine ready first? Just put it on the plate, OK?</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "identity",
                    "task",
                    "spatial",
                    "instruction",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p5.g1\" src=\"imgs/case_study/D-05.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>:</span>\n<span class=\"ltx_p\">Dyadic Dialogue</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p5.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>: Libero Goal</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">open the middle drawer of the cabinet</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p5.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mother: Mom, where&#8217;s that recipe card we used last week?</span>\n<span class=\"ltx_p\">Grandma: Oh, I think I left it near the drawer. Why?</span>\n<span class=\"ltx_p\">Mother: I just remembered we kept it in the middle layer for safekeeping.</span>\n<span class=\"ltx_p\">Grandma: Ah, clever idea! Go check there, it should still be inside.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "dyadic",
                    "task",
                    "instruction",
                    "goal",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p6.g1\" src=\"imgs/case_study/D-06.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p6.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>:</span>\n<span class=\"ltx_p\">Triadic Dialogue</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p6.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p6.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>: Libero 10</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">put both the alphabet soup and the cream cheese box in the basket</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p6.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p6.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Daughter: Grandpa, do you think I could juggle these two cans?</span>\n<span class=\"ltx_p\">Grandpa: Haha, Lucy, I wouldn&#8217;t try that. You might end up with soup all over the floor.</span>\n<span class=\"ltx_p\">Daughter: Aw, you&#8217;re no fun! What about this cream cheese box then?</span>\n<span class=\"ltx_p\">Mother: Lucy, stop teasing your grandpa. Just help me put the soup and the cream cheese in the basket, please.</span>\n<span class=\"ltx_p\">Daughter: Fine, fine, but only because I&#8217;m such a helpful superstar!</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "instruction",
                    "triadic",
                    "task"
                ]
            }
        ]
    },
    "S5.SS3.tab1": {
        "source_file": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
        "caption": "Table 2: Performance comparison on OmniAction-LIBERO-Real.",
        "body": "Spatial\nGoal\nObject\nLong\nAvg\n\n\n\n\\rowcolorgray!30  Audio →\\rightarrow ASR →\\rightarrow Text Prompt\n\n\nOpenVLA\n51.6\n38.2\n38.0\n32.4\n40.1\n\n\nOpenVLA-OFT\n6.6\n9.8\n9.8\n0.0\n6.5\n\n\nNORA\n2.0\n5.6\n26.8\n35.4\n17.4\n\n\nπ0\\pi_{0}\n86.0\n60.0\n70.0\n79.0\n73.8\n\n\n\n\\rowcolordeltaBg  Ours (Audio Input)\n\n\nRoboOmni\n89.0\n71.6\n75.1\n75.0\n76.6",
        "html_code": "<table class=\"ltx_tabular ltx_figure_panel ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt\" style=\"padding:-1pt 0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-1pt 0.0pt;\">Spatial</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-1pt 0.0pt;\">Goal</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-1pt 0.0pt;\">Object</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-1pt 0.0pt;\">Long</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-1pt 0.0pt;\">Avg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!30  Audio <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Text Prompt</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-1pt 0.0pt;\">OpenVLA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">51.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">38.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">38.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">32.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-1pt 0.0pt;\">40.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">OpenVLA-OFT</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">9.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">9.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">6.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\">NORA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">5.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">26.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">35.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">17.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:-1pt 0.0pt;\"><math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">86.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">60.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">70.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\">79.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-1pt 0.0pt;\">73.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding:-1pt 0.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>deltaBg  Ours (Audio Input)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">RoboOmni</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\">89.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\">75.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\">75.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\">76.6</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "nora",
            "prompt",
            "rowcolorgray30",
            "rowcolordeltabg",
            "goal",
            "long",
            "avg",
            "text",
            "openvla",
            "π0pi0",
            "asr",
            "audio",
            "object",
            "performance",
            "comparison",
            "spatial",
            "openvlaoft",
            "omniactionliberoreal",
            "input",
            "ours",
            "roboomni",
            "→rightarrow"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision&#8211;Language&#8211;Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively.\nIn this work, we introduce <span class=\"ltx_text ltx_font_italic\">cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands.</span> To address this new setting, we present <span class=\"ltx_text ltx_font_bold\">RoboOmni</span>, a <span class=\"ltx_text ltx_font_italic\">Perceiver-Thinker-Talker-Executor</span> framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction.\nTo address the absence of training data for proactive intention recognition in robotic manipulation, we build <span class=\"ltx_text ltx_font_bold\">OmniAction</span>, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and answer the research question, we propose <span class=\"ltx_text ltx_font_bold\">RoboOmni</span>, an end-to-end omni-modal framework for manipulation that closes the loop of intent recognition, interaction confirmation, and action execution. Unlike prior approaches, RoboOmni supports direct speech interaction without ASR, infers latent commands by fusing human speech, environmental audio, and vision through spatiotemporal modeling, and verifies intent via interaction.\nTo overcome data scarcity, we construct OmniAction, a dataset with 140k episodes, over 5k speakers, 2.4k event sounds, 640 background sounds, and six contextual instruction types.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "asr",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments in both simulation and real-world settings show that RoboOmni substantially outperforms text- and ASR-based baselines, achieving higher accuracy (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS2\" title=\"5.2 Evaluation on Cross-modal Contextual Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.2</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS3\" title=\"5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.3</span></a>), faster inference (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS6\" title=\"5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.6</span></a>), more effective proactive assistance (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS5\" title=\"5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.5</span></a>), and improved intention recognition (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS5\" title=\"5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.5</span></a>). Our contributions are fourfold:</p>\n\n",
                "matched_terms": [
                    "text",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid development of Large Language Models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib1\" title=\"\">2023</a>; Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib41\" title=\"\">2023a</a>)</cite> has spurred progress in multimodal extensions. Multimodal LLMs (MLLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib30\" title=\"\">2023</a>; Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib3\" title=\"\">2025a</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib27\" title=\"\">2023b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib8\" title=\"\">2023</a>)</cite> augment text-based reasoning with visual perception, enabling instruction following grounded in images.\nEarly attempts toward omni-modality relied on modular pipelines that separately process speech and vision&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib44\" title=\"\">2023</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib49\" title=\"\">2024</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib28\" title=\"\">2023</a>)</cite>, which makes temporal alignment across modalities difficult and limits accurate understanding of situated semantics. More recent work has shifted toward end-to-end omni-modal models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib15\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib47\" title=\"\">2025b</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib45\" title=\"\">2024</a>)</cite>, which can jointly model speech, vision, and text in a unified representation. However, these models remain oriented toward linguistic outputs (text or audio) and do not generate embodied actions, restricting their applicability in robotics.\nIn contrast, our work brings omni-modality into the embodied domain by introducing RoboOmni, an end-to-end framework that integrates speech, environmental sounds, visual context, and text for both action execution and proactive human&#8211;robot interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diverse Contextual Instructions.</span>\n(1) <span class=\"ltx_text ltx_font_italic\">Sentiment Cues</span>:\nEmotionally tinted expressions, or subtle vocalizations, that indirectly reveal user preferences or intentions (e.g., &#8220;Ugh, this juice is too sour&#8221; implying a request for an alternative).\n(2) <span class=\"ltx_text ltx_font_italic\">Overlapping Voices</span>:\nMulti-speaker audio segments with temporal overlaps, testing intent extraction under crosstalk and partial masking.\n(3) <span class=\"ltx_text ltx_font_italic\">Non-Verbal Cues</span>:\nSalient non-linguistic audio events (e.g., alarms, phone rings) that carry situational information relevant to the task.\n(4) <span class=\"ltx_text ltx_font_italic\">Identity Cues</span>:\nSpeaker attributes such as age and gender, inferred from voice and not available from text, are needed to decide whose intent to satisfy.\n(5) <span class=\"ltx_text ltx_font_italic\">Dyadic Dialogue</span>:\nTwo-participant dialogues where intent emerges from conversational flow rather than explicit commands.\n(6) <span class=\"ltx_text ltx_font_italic\">Triadic Dialogue</span>:\nThree participants interact with turn-taking and indirect references,\nincreasing the complexity of intent attribution.\nTo preserve general command-following ability beyond dialogue, we also include a portion of single-person text instructions during training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Statistics and Formats.</span>\nOmniAction comprises 141,162 multimodal episodes, spanning 112 skills (e.g., <span class=\"ltx_text ltx_font_italic\">pick-place</span>, <span class=\"ltx_text ltx_font_italic\">open/close</span>) and 748 objects (e.g., <span class=\"ltx_text ltx_font_italic\">can</span>), with 5,096 distinct speaker timbres, 2,482 non-verbal sound events, and 640 environmental backgrounds.\nEach sample is represented as a triplet <math alttext=\"(C,I,A)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><mi>I</mi><mo>,</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(C,I,A)</annotation></semantics></math>: a multi-turn conversation <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> (user turns as audio, assistant turns as text, with <span class=\"ltx_text ltx_font_typewriter\">[ACT]</span> marking action onset), a visual observation sequence <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, and an action trajectory <math alttext=\"A=\\{a_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">A=\\{a_{t}\\}_{t=1}^{T}</annotation></semantics></math>, where <math alttext=\"a_{t}\\in\\mathbb{R}^{7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>7</mn></msup></mrow><annotation encoding=\"application/x-tex\">a_{t}\\in\\mathbb{R}^{7}</annotation></semantics></math> denotes the delta control vector of the end-effector.\nThe distribution of instruction type is detailed in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S3.F3\" title=\"Figure 3 &#8227; 3.1 Overview &#8227; 3 OmniAction Dataset Construction &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;3</span></a>.\nMore detailed statistics and examples are shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A1.SS1\" title=\"A.1 Data Statics &#8227; Appendix A Details of OmniAction &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;A.1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A1.SS2\" title=\"A.2 Data Example &#8227; Appendix A Details of OmniAction &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditory Realization.</span>\nTo capture paralinguistic cues and environmental acoustics beyond text, we convert dialogues into audio that reflects real household conditions, augmented with diverse sound events and background environments.\nThe conversion process includes four steps:\n(1) <span class=\"ltx_text ltx_font_italic\">Speech synthesis</span>: rendering user turns into audio via multiple high-fidelity, neural TTS engines&#8212;MOSS-TTSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenMOSS, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib31\" title=\"\">2025</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib9\" title=\"\">2025</a>)</cite>, and Gemini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cloud.google.com/text-to-speech/docs/gemini-tts\" title=\"\">https://cloud.google.com/text-to-speech/docs/gemini-tts</a></span></span></span> &#160;with voice cloning for timbre consistency and cross-dialogue diversity.\n(2) <span class=\"ltx_text ltx_font_italic\">Multi-speaker simulation</span>: generating each speaker&#8217;s turns separately, concatenating them on the timeline, and inserting overlaps at controlled offsets with CTC-based methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib11\" title=\"\">2006</a>)</cite> to enable realistic crosstalk and interruption.\n(3) <span class=\"ltx_text ltx_font_italic\">Non-verbal event insertion</span>: mixing contextual sounds (e.g., alarms, utensil clatter) at scripted anchors.\n(4) <span class=\"ltx_text ltx_font_italic\">Environmental backgrounds</span>: randomly adding ambient textures (e.g., water flow, frying, fan hum) at varying SNRs, spanning a wide range to simulate varying acoustic difficulty.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of simulation benchmarks, we construct <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO</span> based on LIBERO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib26\" title=\"\">2023a</a>)</cite>, with two variants.\n(1) <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO-TTS</span> augments the LIBERO using the pipeline described above. Starting from 40 manipulation tasks across four suites (Spatial, Object, Goal, Long-Horizon), we generate six variants for each task based on the six contextual instruction types, yielding 240 evaluation tasks. Example dialogues and task scenes are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A4\" title=\"Appendix D OmniAction-LIBERO &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">App.&#160;D</span></a>.\n(2) <span class=\"ltx_text ltx_font_bold\">OmniAction-LIBERO-Real</span> evaluates RoboOmni under real speech conditions, where 10 volunteers provide spoken instructions collected in real environments.</p>\n\n",
                "matched_terms": [
                    "object",
                    "roboomni",
                    "spatial",
                    "goal",
                    "omniactionliberoreal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose RoboOmni, an end-to-end omni-modal LLM framework organized as Perceiver&#8211;Thinker&#8211;Talker&#8211;Executor, unifying speech, environmental audio, vision, and robotic actions within a single autoregressive model <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S4.F4\" title=\"Figure 4 &#8227; 4 Methods &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;4</span></a>.\nRoboOmni employs a Perceiver for multimodal input, a Thinker backbone, a Talker for speech, and an Executor for actions. To align inputs with linguistic and motor outputs, RoboOmni uses unified tokenization to encode all modalities into a shared semantic space, which the Thinker processes into high-level representations and specialized decoders render into speech and executable actions, enabling seamless perception-to-action generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "roboomni",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perceiver: Multimodal Input Encoding.</span>\nThe Perceiver handles the encoding of heterogeneous input modalities into a unified embedding space. We follow the multimodal processing pipeline of Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib47\" title=\"\">2025b</a>)</cite> for encoding text, audio, and visual inputs into a series of hidden representations. At timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, given visual observation <math alttext=\"I_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>I</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">I_{t}</annotation></semantics></math> and audio segment <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, we obtain visual embedding <math alttext=\"\\mathbf{v}_{t}=f_{v}(I_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119855;</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{v}_{t}=f_{v}(I_{t})</annotation></semantics></math> and audio embedding <math alttext=\"\\mathbf{s}_{t}=f_{s}(x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mi>s</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}=f_{s}(x_{t})</annotation></semantics></math>. Together with textual context <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math>, these form the unified input representation <math alttext=\"\\mathbf{X}_{t}=[\\mathbf{v}_{t};\\mathbf{s}_{t};\\mathbf{c}_{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119855;</mi><mi>t</mi></msub><mo>;</mo><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>;</mo><msub><mi>&#119836;</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}_{t}=[\\mathbf{v}_{t};\\mathbf{s}_{t};\\mathbf{c}_{t}]</annotation></semantics></math> that serves as input to the Thinker backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train RoboOmni using a unified autoregressive objective that handles both conversational and manipulation capabilities within the same framework. Given a training episode, the model receives multimodal input <math alttext=\"\\mathbf{X}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{t}</annotation></semantics></math> and learns to predict appropriate responses&#8212;either conversational replies for dialogue or action sequences for manipulation.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As current open-source Vision-Language-Action (VLA) models are primarily designed for textual instructions and cannot directly process audio inputs, we construct two baseline paradigms to validate the necessity of end-to-end audio processing: (i) <span class=\"ltx_text ltx_font_bold\">Ground-truth Textual Prompt</span>, which directly feeds pre-annotated transcriptions of speech instructions into VLA models; (ii) <span class=\"ltx_text ltx_font_bold\">Speech-ASR-Textual Prompt</span>, where speech instructions are first transcribed to text using the ASR model Whisper large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib36\" title=\"\">2023</a>)</cite>, then fed into VLA models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "prompt",
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct evaluations comparing RoboOmni with four representative VLA baselines representing both paradigms:\n(1) <span class=\"ltx_text ltx_font_bold\">OpenVLA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib19\" title=\"\">2024</a>)</cite>,\nbuilt on Llama-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib42\" title=\"\">2023b</a>)</cite> with DINOv2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib32\" title=\"\">2023</a>)</cite> and SigLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib48\" title=\"\">2023</a>)</cite> encoders, pretrained on <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>970k demonstrations from Open-X-Embodiment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Padalkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib33\" title=\"\">2023</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">OpenVLA-OFT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib20\" title=\"\">2025</a>)</cite>, a variant of OpenVLA augmented with action chunking and optimized with an <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss on continuous action.\n(3) <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib6\" title=\"\">2024</a>)</cite>,\nbased on PaliGemma&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib5\" title=\"\">2024</a>)</cite> with diffusion action experts, trained on both large-scale internet multimodal data and robot datasets.\n(4) <span class=\"ltx_text ltx_font_bold\">NORA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib14\" title=\"\">2025</a>)</cite>,\nbuilt on Qwen2.5-VL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib4\" title=\"\">2025b</a>)</cite> with FAST+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23763v3#bib.bib34\" title=\"\">2025</a>)</cite> discrete action decoding.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "openvla",
                    "π0pi0",
                    "openvlaoft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train the model with an input image resolution of <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math>, an audio sampling rate of 16,000 Hz, and an action chunk size of 6.\nFor large-scale pretraining, RoboOmni is optimized on a cluster of 64 A100 GPUs over 10 days, corresponding to a total of 15,360 A100-hours, with a batch size of 512. The training runs for 10 epochs using a learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math>, with the first 1k steps reserved for warm-up. For downstream task supervised fine-tuning (SFT), we adopt a learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math> and train with 8 A100 GPUs for 10-30k steps.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "roboomni",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate RoboOmni on diverse cross-modal contextual instructions, we conduct extensive experiments on the OmniAction-LIBERO across four task suites with six audio variants. &#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.T1\" title=\"Table 1 &#8227; Baseline Models &#8227; 5.1 Experiment Setup &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Tab.&#160;1</span></a> demonstrates that RoboOmni achieves an overall 85.6% success rate, substantially outperforming the strongest baseline (NORA, 25.9%) and other cascaded methods (all below 10%). Our analysis yields three key insights:\n(1)\n<span class=\"ltx_text ltx_font_bold\">End-to-end auditory integration is crucial for paralinguistic cues.</span>\nText-only models, whether using ASR transcripts or ground-truth text, fail to capture paralinguistic cues (e.g., prosody, overlapping speech), with best scores of\n25.9% (textual baseline).\nIn contrast, RoboOmni&#8217;s direct audio processing enables it to consistently exceed 76% across all types, demonstrating the importance of preserving acoustic information.\n(2)\n<span class=\"ltx_text ltx_font_bold\">Auditory integration enhances robust intent recognition under ambiguity.</span>\nGoal and Object suites are challenging due to multiple manipulable objects and valid actions, where baselines collapse (averaging 16.3% and 13.8% for the best baselines), exposing limits in contextual instruction understanding. RoboOmni sustains high performance (Goal: 85.8% v.s. Object: 84.0%), demonstrating robust generalization under semantic ambiguity.\n(3)\n<span class=\"ltx_text ltx_font_bold\">Instruction type complexity reveals varying cognitive demands.</span>\nFor end-to-end models, <span class=\"ltx_text ltx_font_italic\">dyadic</span> and <span class=\"ltx_text ltx_font_italic\">overlapping</span> tasks are easier, averaging <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>88%. <span class=\"ltx_text ltx_font_italic\">Non-verbal</span> instructions are hardest (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>82%), as they require recognizing non-verbal sounds and integrating them with visual and speech cues. The remaining tasks average <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>85%, reflecting moderate complexity.</p>\n\n",
                "matched_terms": [
                    "text",
                    "object",
                    "roboomni",
                    "nora",
                    "performance",
                    "asr",
                    "goal",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel\">We further evaluate RoboOmni&#8217;s robustness under real human-recorded speech with direct audio instructions.\nAs shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.SS3\" title=\"5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;5.3</span></a>, on the OmniAction-LIBERO-Real benchmark, RoboOmni achieves the highest average performance (76.6%),\nsurpassing strong text-based VLAs including <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> (73.8%), OpenVLA (40.1%), and NORA (17.4%).\nASR-based VLAs suffer from acoustic variability: accents, coarticulation, and background noise frequently cause recognition\nerrors, and even minor word deviations can degrade VLAs&#8217; performance.\n<math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.m5\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> shows some robustness, likely due to large-scale co-training on diverse web data.\nIn contrast, RoboOmni processes speech directly, avoiding ASR pipeline errors. Pretraining on diverse speakers and sounds improves robustness to acoustic variability and paralinguistic cues, yielding more consistent performance.</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "performance",
                    "openvla",
                    "π0pi0",
                    "asr",
                    "audio",
                    "omniactionliberoreal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify that RoboOmni&#8217;s capabilities transfer beyond simulation, we fine-tune our pretrained model by utilizing our demonstration dataset on WidowX 250S, where speech was recorded by 10 volunteers in real environments.\nThis enables RoboOmni to run on real robots and handle diverse speech instructions (e.g., sentiment, overlapping cues).\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;5</span></a> highlights RoboOmni&#8217;s real-world competence across three dimensions: (1) <span class=\"ltx_text ltx_font_bold\">Strong intent recognition</span>, accurately inferring user intention from both visual and auditory cues (e.g., identifying the object based on audio and determining the receptacle is the pot from the visual scene); (2) <span class=\"ltx_text ltx_font_bold\">Effective Interaction</span>, proactively asking clarifying questions after inferring the user&#8217;s latent intent (e.g., &#8220;should I &#8230;?&#8221;) and executing the action after receiving confirmation, ensuring that actions are deliberate and aligned with the user&#8217;s intent; (3) <span class=\"ltx_text ltx_font_bold\">Reliable Execution</span>, successfully carries out confirmed actions, such as locating the correct object amidst multiple distractors and placing it in the designated pot.\nMore detailed real-world cases are provided in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.SS1\" title=\"B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Sec.&#160;B.1</span></a></p>\n\n",
                "matched_terms": [
                    "audio",
                    "roboomni",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intent Recognition Capability.</span>\n\nWe further evaluate the models&#8217; ability to recognize user intent under contextual instructions, shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F5.sf1\" title=\"5(a) &#8227; Figure 6 &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">5(a)</span></a>. Specifically, we compare Qwen2.5-Omni-3B (our backbone), Qwen2.5-Omni-7B, and ASR+GPT-4o, against our proposed RoboOmni.\nWe observe that RoboOmni achieves the highest accuracy (88.9%), confirming the advantage of end-to-end speech&#8211;action modeling that preserves paralinguistic cues and dialogue context.\nNotably, although ASR introduces recognition noise compared with end-to-end models, GPT-4o still surpasses the smaller Omni models (55.6% vs. 27.8%/50.0%) because its stronger multimodal reasoning compensates for transcription loss. This highlights that contextual instructions cannot be resolved by acoustic modeling alone, but also demand robust reasoning capabilities</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Can Cascaded Pipelines Handle Contextual Instructions Effectively with High-level Planner?</span>\nWe compare RoboOmni with planner&#8211;controller pipelines, where Qwen2.5-Omni-3B serves as the planner and text-based VLAs as controllers, shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F8\" title=\"Figure 8 &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;8</span></a>.\nRoboOmni outperforms all cascaded pipelines, demonstrating the advantage of end-to-end speech&#8211;action learning: jointly modeling audio, vision, and action avoids the lossy planner&#8211;controller interface and preserves intent fidelity. Cascaded pipelines perform worse due to (1) semantic drift, as planners are not co-trained with VLAs and generate commands controllers cannot execute, and (2) poor handling of speaker identity, since Qwen2.5-Omni fails to capture paralinguistic cues, leading to the weakest results on <span class=\"ltx_text ltx_font_italic\">Identity Cues</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Does End-to-End Modeling Improve Inference Efficiency?</span>\nTo assess whether end-to-end modeling improves runtime efficiency, we measure per-inference latency on a single RTX 4090 GPU.\nUsing ASR + OpenVLA as the baseline (1.0&#215;), we find that other cascaded pipelines (ASR + Nora: 1.02&#215;, ASR + <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS6.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math>: 0.96&#215;) incur similar costs since the ASR stage dominates computation. In contrast, RoboOmni runs at 0.49&#215; latency, showing that end-to-end audio&#8211;action modeling eliminates the ASR bottleneck and substantially improves efficiency (<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#S5.F9\" title=\"Figure 9 &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;9</span></a>).</p>\n\n",
                "matched_terms": [
                    "roboomni",
                    "nora",
                    "openvla",
                    "π0pi0",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we introduced cross-modal contextual instructions, a new paradigm for robotic manipulation where robots proactively infer user intent from multimodal context&#8212;vision, environmental sounds, and speech&#8212;rather than passively awaiting explicit commands. Building on this setting, we proposed RoboOmni, a Perceiver&#8211;Thinker&#8211;Talker&#8211;Executor framework built on end-to-end omni-modal LLMs that integrates auditory and visual inputs, unifying intention recognition, confirmation, and action execution. To address data scarcity, we constructed OmniAction, a large-scale corpus of 140k episodes with diverse speakers, event sounds, and backgrounds, together with OmniAction-LIBERO for simulation-based evaluation.\nComprehensive experiments in both simulation and the real world demonstrate that RoboOmni exhibits emerging cognitive intelligence, significantly outperforming text- and ASR-based baselines in success rate, inference speed, proactive assistance, and intention recognition.</p>\n\n",
                "matched_terms": [
                    "text",
                    "roboomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Overlapping:</span> the audio contains genuine temporal overlaps such that ASR alone would be challenged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.F12\" title=\"Figure 12 &#8227; B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;12</span></a>, we present the model&#8217;s real-world performance on <span class=\"ltx_text ltx_font_italic\">Identity Cues</span> and <span class=\"ltx_text ltx_font_italic\">non-verbal cues</span>, while <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2510.23763v3#A2.F13\" title=\"Figure 13 &#8227; B.1 Success Cases &#8227; Appendix B Real-World Experiments &#8227; 6 Conclusion &#8227; 5.6 Further Analysis &#8227; 5.5 Evaluation of Proactive Assistance Capabilities &#8227; 5.4 Real-World Experiments &#8227; 5.3 Evaluation on Real Human Audio Direct Instructions &#8227; 5 Experiment &#8227; RoboOmni: Proactive Robot Manipulation in Omni-modal Context\"><span class=\"ltx_text ltx_ref_tag\">Fig.&#160;13</span></a> illustrates its performance on <span class=\"ltx_text ltx_font_italic\">Dyadic Dialogues</span> and <span class=\"ltx_text ltx_font_italic\">Triadic Dialogues</span>. The results demonstrate that the model not only accurately infers user intent from visual and audio cues, but also engages in natural interactive questioning and reliably executes the corresponding actions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p2.g1\" src=\"imgs/case_study/D-02.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Overlapping Voices</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p2.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>:</span>\n<span class=\"ltx_p\">Libero Object</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the cream cheese and place it in the basket</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p2.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p2.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mother: Hey, can you help me sort these things out?</span>\n<span class=\"ltx_p\">Daughter: Sure, what do you want to start with?</span>\n<span class=\"ltx_p\">Mother: Let&#8217;s put something creamy in the basket. Maybe the cream cheese?</span>\n<span class=\"ltx_p\">Daughter: Oh, you mean the small rectangular one?</span>\n<span class=\"ltx_p\">Mother: No, the taller one, next to the [Overlap]orange box.</span>\n<span class=\"ltx_p\">Daughter:[Overlap_S2] Oh, got it, the cream cheese!</span>\n<span class=\"ltx_p\">Mother: Exactly! Let&#8217;s put that in the basket.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p3.g1\" src=\"imgs/case_study/D-03.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>: Non-Verbal Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p3.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>:</span>\n<span class=\"ltx_p\">Libero Object</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the alphabet soup and place it in the basket</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p3.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p3.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Daughter: Dad, can you help me with these groceries?</span>\n<span class=\"ltx_p\">Dad: Sure, what do you need me to do?</span>\n<span class=\"ltx_p\">Daughter: Well, if you hear the sound of the drawer closing, pick the alphabet soup and place it in the basket. If you hear the sound of the coffee machine brewing, pick the dressing bottle and place it in the basket.</span>\n<span class=\"ltx_p\">Dad: Got it. Let me know if you need help with anything else.</span>\n<span class=\"ltx_p\">Daughter: Thanks, Dad. I&#8217;ll finish sorting the rest of these.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p4.g1\" src=\"imgs/case_study/D-04.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>:</span>\n<span class=\"ltx_p\">Identity Cues</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p4.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>: Libero Spatial</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">pick up the black bowl on the ramekin and place it on the plate</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p4.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p4.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Son: Mum, Dad said he need that black bowl on the ramekin. He said he need it for dinner!</span>\n<span class=\"ltx_p\">Mum: Oh really? Well, I was planning to use that ramekin for baking tonight, and I need it free.</span>\n<span class=\"ltx_p\">Son: Haha! Looks like we&#8217;ve got a little competition going on here!</span>\n<span class=\"ltx_p\">Dad: Oh, come on! Can I have mine ready first? Just put it on the plate, OK?</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_align_right ltx_inline-block\" style=\"width:0.0pt;position:relative; bottom:-0.3pt;\">&#8195;</span>\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:130.1pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"598\" id=\"A4.SS1.p5.g1\" src=\"imgs/case_study/D-05.png\" width=\"598\"/>\n</span>&#8195;<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_middle\" style=\"width:293.5pt;\">\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Type</span>:</span>\n<span class=\"ltx_p\">Dyadic Dialogue</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p5.p1\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Suite</span>: Libero Goal</span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Original Instruction</span>:</span>\n<span class=\"ltx_p\">open the middle drawer of the cabinet</span>\n</span>\n<span class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p5.p2\"><span class=\"ltx_rule\" style=\"width:433.6pt;height:0.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span>\n</span>\n<span class=\"ltx_para\" id=\"A4.SS1.p5.p5\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conversation (Transcripts)</span>:</span>\n<span class=\"ltx_p\">Mother: Mom, where&#8217;s that recipe card we used last week?</span>\n<span class=\"ltx_p\">Grandma: Oh, I think I left it near the drawer. Why?</span>\n<span class=\"ltx_p\">Mother: I just remembered we kept it in the middle layer for safekeeping.</span>\n<span class=\"ltx_p\">Grandma: Ah, clever idea! Go check there, it should still be inside.</span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "goal"
                ]
            }
        ]
    }
}