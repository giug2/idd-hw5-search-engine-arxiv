{
    "S3.T1": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 1. Model candidates of each component in EMSFoundation.",
        "body": "Backbone Text encoder(3)\nTinyBERT, MobileBERT, BERTBase\n\n\nBackbone Vitals encoder(3)\nRNN, LSTM, GRU\n\n\nBackbone Image encoder(1)\nfully connect (FC)\n\n\nHeaders(1)\nFeature concatenator + header1 + header2 + header3\n\n\nSpeech-to-text(4)\nWhisper-tiny, -base, -small, -medium\n\n\nObject detection(2)\nYOLO11n, YOLO11x\n\n\nOCR(4)\nEasyOCR, TesseractOCR, PaddleOCR, CRNN\n\n\nBarcode scanner(1)\nML Kit\n\n\nMed Math(1)\nA division operator",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Backbone Text encoder(3)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">TinyBERT, MobileBERT, BERTBase</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Backbone Vitals encoder(3)</td>\n<td class=\"ltx_td ltx_align_left\">RNN, LSTM, GRU</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Backbone Image encoder(1)</td>\n<td class=\"ltx_td ltx_align_left\">fully connect (FC)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Headers(1)</td>\n<td class=\"ltx_td ltx_align_left\">Feature concatenator + header1 + header2 + header3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Speech-to-text(4)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Whisper-tiny, -base, -small, -medium</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Object detection(2)</td>\n<td class=\"ltx_td ltx_align_left\">YOLO11n, YOLO11x</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">OCR(4)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">EasyOCR, TesseractOCR, PaddleOCR, CRNN</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Barcode scanner(1)</td>\n<td class=\"ltx_td ltx_align_left\">ML Kit</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">Med Math(1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">A division operator</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "candidates",
            "math1",
            "fully",
            "ocr4",
            "emsfoundation",
            "whispertiny",
            "encoder3",
            "lstm",
            "base",
            "barcode",
            "kit",
            "header2",
            "component",
            "med",
            "text",
            "connect",
            "mobilebert",
            "feature",
            "yolo11n",
            "crnn",
            "operator",
            "image",
            "each",
            "division",
            "object",
            "small",
            "vitals",
            "medium",
            "header1",
            "easyocr",
            "backbone",
            "gru",
            "headers1",
            "detection2",
            "bertbase",
            "tinybert",
            "tesseractocr",
            "encoder1",
            "scanner1",
            "concatenator",
            "speechtotext4",
            "rnn",
            "paddleocr",
            "header3",
            "model",
            "yolo11x"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
            "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
            "<p class=\"ltx_p\">For the OCR module in EMSGlass, we evaluate an image dataset of size 204 captured by Google Glass&#8217;s 8MP camera, featuring a user holding a labeled medicine bottle at three distances&#8212;full arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.6m), half arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> 0.3m), and quarter arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.15m)&#8212;to account for varying EMT arm lengths. Each image has two versions: the original image without cropping and the cropped bottle segment. We use the word error rate (WER) and character error rate (CER) to measure OCR accuracy. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, EMSGlass OCR performance was evaluated using four state-of-the-art (SOTA) models listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: EasyOCR, TesseractOCR, PaddleOCR, CRNN. In general, EasyOCR achieves the lowest WER and CER. Our edit distance (ED)-based matching discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS4\" title=\"3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> is applied after each OCR prediction. As shown, the OCR models with ED-match can achieve significantly decreased WER and CER, indicating the effectiveness of our proposed ED-match method. ED-match helps EasyOCR to decrease WER and CER by 89% and 83%, respectively. EMSGlass employs Easy-Match as its OCR framework, which consistently outperforms other models with WER (CER) below 0.12 (0.05) across all arm distance and cropping conditions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mobile assistant systems for EMS have been evolving to harness both cloud and edge (e.g., smart glasses) resources for real-time cognitive assistance and protocol recommendations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite>. For instance, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> allow EMTs to wear smart glasses for capturing the voice of patient symptoms. They utilize domain-specific speech-to-text models (e.g., Whisper-tiny) to transcribe the speech and data-driven models (e.g., MobileBERT) to help select protocols. These models are trained in the cloud and deployed on hands-free smart glasses (e.g., Google Glass, Vuzix M4000) and edge servers to address challenges such as unreliable cloud connectivity in disaster scenarios while also enhancing user privacy. Despite these advancements, several clinical and technical challenges remain unresolved:</p>\n\n",
                "matched_terms": [
                    "mobilebert",
                    "whispertiny"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #2: High-latency serving frameworks for multimodal models in EMS scenarios.</span> Existing multimodal model serving frameworks, such as PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite> and TensorFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tensorflow2016</span>)</cite> used in SOTA assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, assume simultaneous availability of all data modalities. However, in the EMS scenario shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, smart glasses receive different data modes asynchronously (i.e., asynchronous arrival times) as EMTs move through the scene. Without caching intermediate processing results for early arrived modes of data, directly using multimodal model serving frameworks has to repeatedly process early arrived voice data when later-arriving data like vitals and images become available, leading to redundant computations and higher latency. To achieve timely EMS delivery given the asynchronous arrival of multimodal data, a low-latency multimodal serving framework in EMS scenarios is essential.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The national EMS dataset (NEMSIS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite> mandates reporting pill and alcohol presence at EMS scenes due to their strong association with overdose and respiratory emergencies. Accurate detection of these objects can narrow protocol options (e.g., from over 100 to fewer than 5), yet EMS-specific object detectors remain underdeveloped. In parallel, accurate medication administration requires precise Med-Math (e.g., administering <math alttext=\"21mg\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>21</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">21mg</annotation></semantics></math> of Adrenaline from a <math alttext=\"4.2mg/ml\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><mn>4.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">4.2mg/ml</annotation></semantics></math> solution: <math alttext=\"\\frac{21mg}{4.2mg/ml}=5ml\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mfrac><mrow><mn>21</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mrow><mrow><mn>4.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></mfrac><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{21mg}{4.2mg/ml}=5ml</annotation></semantics></math>), but complex unit conversions and vendor-specific concentration increase cognitive load and delay critical care. Smart-glass cameras can automate medication recognition through OCR and barcode scanning, enabling instant extraction of drug names and concentrations, linking to patient history, and reducing errors. Despite its clinical importance, no prior work has focused on developing such integrated object detection and OCR pipelines for EMS.</p>\n\n",
                "matched_terms": [
                    "barcode",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D2(3-modal: text, vitals,scene)</span>: Similarly, we extract D2&#8217;s text and vitals with D2&#8217;s PCR Keys, a disjoint set of D1 corresponding to events with 3-modal input: text, vitals, and scene data. We also extract the EMS scene information indicating the existence of alcohol and pills. The scene information is one-hot encoded, giving a 3-modal dataset D2(text, vitals, scene) with 3,005 samples.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "object",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "backbone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "feature",
                    "backbone",
                    "each",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "whispertiny",
                    "small"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These challenges stem from data distribution shift&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HwangDistributionECCV2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChipDataDistributionOnline2024</span>)</cite>, where real-world inputs&#8212;affected by diverse accents and microphones&#8212;differ from training data, yet the labels remain unchanged. The root cause is the small size of existing models, all under 100 million parameters (EMSConformer: 10m, Whisper-tiny: 74m, Whisper-base: 74m), limiting both learning and generalization. Empirical evidence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaplan2020scalinglawsneurallanguage</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">alabdulmohsin2022revisitingscalinglaws</span>)</cite> supports this approach, showing that larger models achieve lower training loss. Specifically, we propose training Whisper-small (242m) and Whisper-medium (764m) on dataset D3(audio) to enhance generalization (as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Here we select one order of magnitude higher than the SOTA to demonstrate the effectiveness of EMSWhisper&#8217;s idea in enhancing generalization capability. Further explorations of alternative scaling options could be considered future work.</p>\n\n",
                "matched_terms": [
                    "whispertiny",
                    "small"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce the manual effort required for data annotation and model training in developing an EMS-specific object detector, we assessed the SOTA open-set detection model Grounding DINO<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024groundingdinomarryingdino</span>)</cite> (GD) using prompts of varying difficulty for humans to come up with (See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> (left): easy, medium, and hard).</p>\n\n",
                "matched_terms": [
                    "object",
                    "model",
                    "medium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "text",
                    "object",
                    "barcode",
                    "easyocr",
                    "image",
                    "kit",
                    "division",
                    "med"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "lstm",
                    "rnn",
                    "gru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key idea of EMSServe&#8211;feature cache</span>: To eliminate redundant text submodule inference, EMSServe employs feature cache (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, right). When voice data arrives, instead of only running the text model, we simultaneously compute and cache the text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> in the text-vital model. When vitals1 arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, unlike conventional frameworks that rerun the costly text submodule, EMSServe reuses the cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and processes only the vitals module (vitals encoder) to obtain <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math>, which significantly reduces inference costs. The cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and newly computed <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math> are then concatenated as inputs to model headers. By addressing asynchronous EMS data arrival times, EMSServe mitigates redundant and costly submodule computation to improve inference efficiency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "object",
                    "yolo11n"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on our insights, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> abstracts EMSServe, the first serving framework for multimodal model inference in EMS. EMSServe comprises three key components:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> a modality-aware splitter that decomposes multimodal models into single-mode modules,  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> model inference time profiling to measure inference latency on the smart glasses and the edge server, and  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic3\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> adaptive offloading with feature caching to minimize latency in dynamic EMS scenarios. While we evaluate EMSGlass on PH1 and Edge-64X in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for generality, our primary setup uses Google Glass as the mobile device and a manpack-mounted Edge-4C as the edge server.</p>\n\n",
                "matched_terms": [
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the modality-aware splitter decomposes the multimodal model <math alttext=\"M2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">M2</annotation></semantics></math> (<math alttext=\"M3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">M3</annotation></semantics></math>) into single-mode modules <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math> (<math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math>, and <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>), enabling precomputation and cache of the text module output features <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> (<math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math>) before vitals data arrive.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates how EMSServe dynamically serves multimodal inference in an EMS scenario where symptom speech, vitals, and image data arrive asynchronously. The EMT wearing Google Glass and the EMT carrying the manpack-mounted Edge-4C move independently, causing bandwidth (BW) fluctuations in Glass-edge communication. To monitor this variation in real-time, we implement a lightweight heartbeat monitor on the smart glasses, periodically (e.g., every second) measuring the file transmission bandwidth <math alttext=\"\\Delta t=BW*filesize\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>W</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta t=BW*filesize</annotation></semantics></math>. Unlike RTT, <math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> represents the actual file transfer time. Based on this measurement, EMSServe optimizes offloading decisions:</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Low Latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> is low): offload compute-heavy tasks (e.g., text submodule inference, object detection) to the edge server for its higher processing power.</p>\n\n",
                "matched_terms": [
                    "text",
                    "object"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "image",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel cache computation</span>: We utilize multiple parallel threads to reduce cache computation overheads. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (right) shows the compute time for the text feature cache (step 1) and vitals cache (step 2) across all hardware platforms, including Google Glass. For the text module cache (top), parallel computation takes nearly the same time as running the text model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> alone, effectively hiding the text feature cache computation from users since <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> must be computed regardless. However, for the vitals module (bottom), which inherently takes much less time to compute by up to four orders of magnitude, launching multiple parallel threads often incurs higher costs than serial execution. Hence, we adopt serial feature cache computations for vitals.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fault tolerance for edge server crashes</span>: To address potential failures of manpack edge server, such as battery depletion, we implement a dual-perspective fault tolerance mechanism. First, at each step, when the edge server transmits both inference results and recommendation results to the smart glasses, it also returns the computed feature cache. This ensures that the cache on the smart glasses is never outdated by more than one step. Second, if the edge server crashes during feature cache computation, the smart glasses seamlessly switch to on-device inference using the current step&#8217;s data. This design maintains real-time recommendation updates with minimal disruption.</p>\n\n",
                "matched_terms": [
                    "each",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "backbone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tinybert",
                    "mobilebert",
                    "small"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F12\" title=\"Figure 12 &#8227; 5.1.4. Accuracy of Objection Detection &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> compares the object detection performance of fine-tuning YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite> on the image dataset D4(image) labeled by Grounding Dino and labeled by our human-in-the-loop (HITL) annotation adjustment. Our HITL annotation strategy outperforms Grounding Dino&#8217;s annotations, bringing the test mAP value close to 0.8. In contrast, the mAP of all YOLO11 models on images annotated with Grounding Dino is below 0.6, which is unacceptable. The high mAP on the validate set but low mAP and recall on the test set means fine-tuning YOLO11 with Grounding Dino&#8217;s annotations causes the overfitting during the fine-tuning process. For example, using Grounding Dino with the hard text prompts from Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> can achieve an mAP above 0.8, but the test mAP of all YOLO11 models is nearly 0. The recall metric is similar: high validate recall while much lower test recall. This results from Grounding Dino&#8217;s high false positive predictions as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2\" title=\"2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our HITL avoids this problem because we add the human adjustments to correct Grounding Dino&#8217;s false positive annotations, enabling much higher test mAP and recall. It&#8217;s important to note that when compared to direct manual annotations,\nthe advantage of our HITL is that we exploit Grounding Dino&#8217;s accurate (i.e., true positives) annotations to save human labor time on annotations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "object",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the barcode scanner, we only use the original image without cropping. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> (right), we use the success rate as the metric to measure the performance of barcode scanner (developed based on ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> in Android). At the 1/4 Arm, our scanner can achieve a 100% accuracy, indicating its effectiveness. On average, it takes less than 0.5 seconds to process one image, ensuring minimal latency.</p>\n\n",
                "matched_terms": [
                    "barcode",
                    "image",
                    "kit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T5\" title=\"Table 5 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows EMSNet&#8217;s end-to-end (E2E) accuracy on the 3-modal dataset D2, i.e., from the end of original 3-modal inputs to the end of tasks 1-3. Here we use our Whisper-s and Whisper-m for the speech recognition (SR) module, YOLO11n for the object detection (OD) module, and 3 models in the backbone: unimodal MobileBERT, 2-modal MobileBERT-GRU, and 3-modal MobileBERT-GRU-FC. We make two observations from the table: 1) the multimodal models, including the 2-modal and 3-modal models, achieve higher accuracy, 2) the addition of speech recognition and object detection models don&#8217;t degrade the E2E accuracy, indicating seamless integrations with the backbone models.</p>\n\n",
                "matched_terms": [
                    "object",
                    "mobilebert",
                    "backbone",
                    "yolo11n"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EMSServe is the first multimodal model serving framework addressing the issue of asynchronous arrival times, which is inherently introduced by using smart glasses in multimodal EMS scenarios with high mobility. Here, we make the following evaluation plan to show EMSServe&#8217;s advantage in serving such asynchronously arrived multimodal data. Note that, when evaluating EMSServe, the BERTBase-GRU-FC backbone is used with Whisper-tiny and YOLO11n as the multimodal EMSNet model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whispertiny",
                    "backbone",
                    "yolo11n"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three episodes</span>: As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we evaluate EMSServe by running three episodes of multimodal data arriving at different timestamps. Episode 1 includes one speech data, followed by ten continuous vitals data, and then ends up with ten continuous image data. Episode 1 echoes the typical data arrival sequence illustrated in the aforementioned Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Episodes 2 and 3 randomly shuffle the data sequence in episode 1 with two different random seeds. These three episodes collectively cover different scene data arrival times of multimodal data in real-world EMS events.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "emsfoundation",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.SS1.SSS2\" title=\"5.1.2. 3-modal EMSNet backbone accuracy on D2 &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the full details of evaluating the EMSNet on 3-modal dataset D2 (text, vitals, image).</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before each user study, each EMT participant signed a consent form and received a brief orientation to the EMSGlass system and Google Glass hardware. The study began with a short hands-on tutorial introducing key hardware components (display prism, camera, microphone, and touchpad frame), and training participants to operate EMSGlass through simple tap and voice interactions. Participants were then guided through a demonstration scenario showing how EMSGlass transcribes symptoms, displays real-time vitals, detects scene objects (e.g., pills, alcohol bottles), and recommends corresponding EMS protocols, medicines, and dosages. Following the demonstration, participants performed a full simulation independently, interacting with the app to assess a manikin patient by describing symptoms, observing dynamic updates in vitals and recommendations, and completing the scenario by scanning medication labels. Each participant could repeat dry runs until comfortable with the workflow. This structured process ensured that all participants understood EMSGlass&#8217;s multimodal functionalities before proceeding to the formal evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vitals"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 2. Hardware and software platform specifications",
        "body": "Edge-64X\n\n\nCPU\n64 x Intel Xeon Silver 4314\n\n\nRAM\n64 GB\n\n\nOS\nUbuntu 22.04\n\n\nTorch version\nPyTorch 2.3\n\n\nEdge-4C\n\n\nCPU\n4 x Intel Core i7 7567U\n\n\nRAM\n16 GB\n\n\nOS\nUbuntu 22.04\n\n\nTorch version\nPyTorch 2.3\n\n\nPH1 Phone\n\n\nSoC\nQualcomm Snapdragon 835\n\n\nOS\nAndroid 9\n\n\nTorch version\nPyTorch Android Lite 2.1\n\n\nGoogle Glass Enterprise Edition 2\n\n\nSoC\nQualcomm XR1\n\n\nOS\nAndroid 8\n\n\nTorch version\nPyTorch Android Lite 2.1",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Edge-64X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CPU</td>\n<td class=\"ltx_td ltx_align_left\">64 x Intel Xeon Silver 4314</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">RAM</td>\n<td class=\"ltx_td ltx_align_left\">64 GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OS</td>\n<td class=\"ltx_td ltx_align_left\">Ubuntu 22.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Torch version</td>\n<td class=\"ltx_td ltx_align_left\">PyTorch 2.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Edge-4C</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CPU</td>\n<td class=\"ltx_td ltx_align_left\">4 x Intel Core i7 7567U</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">RAM</td>\n<td class=\"ltx_td ltx_align_left\">16 GB</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OS</td>\n<td class=\"ltx_td ltx_align_left\">Ubuntu 22.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Torch version</td>\n<td class=\"ltx_td ltx_align_left\">PyTorch 2.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">PH1 Phone</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SoC</td>\n<td class=\"ltx_td ltx_align_left\">Qualcomm Snapdragon 835</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OS</td>\n<td class=\"ltx_td ltx_align_left\">Android 9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Torch version</td>\n<td class=\"ltx_td ltx_align_left\">PyTorch Android Lite 2.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Google Glass Enterprise Edition 2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SoC</td>\n<td class=\"ltx_td ltx_align_left\">Qualcomm XR1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OS</td>\n<td class=\"ltx_td ltx_align_left\">Android 8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Torch version</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">PyTorch Android Lite 2.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "intel",
            "edge4c",
            "xeon",
            "torch",
            "hardware",
            "software",
            "qualcomm",
            "cpu",
            "specifications",
            "platform",
            "google",
            "edge64x",
            "version",
            "glass",
            "lite",
            "soc",
            "edition",
            "core",
            "ph1",
            "phone",
            "ubuntu",
            "android",
            "silver",
            "ram",
            "snapdragon",
            "7567u",
            "enterprise",
            "pytorch",
            "xr1"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "pytorch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mobile assistant systems for EMS have been evolving to harness both cloud and edge (e.g., smart glasses) resources for real-time cognitive assistance and protocol recommendations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite>. For instance, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> allow EMTs to wear smart glasses for capturing the voice of patient symptoms. They utilize domain-specific speech-to-text models (e.g., Whisper-tiny) to transcribe the speech and data-driven models (e.g., MobileBERT) to help select protocols. These models are trained in the cloud and deployed on hands-free smart glasses (e.g., Google Glass, Vuzix M4000) and edge servers to address challenges such as unreliable cloud connectivity in disaster scenarios while also enhancing user privacy. Despite these advancements, several clinical and technical challenges remain unresolved:</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(2)</span> We develop <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, the first serving framework for multimodal models to address the challenges introduced by different data arrival times in EMS scenarios. With the adaptive edge-assisted offloading, EMSServe harnesses the computing resources on both the glass and edge servers. Comprehensive evaluations show EMSServe generally outperforms direct usage of PyTorch by 1.9&#215; &#8211; 11.7&#215; speedup across diverse edge devices (Google Glass) and edge servers.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "pytorch",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(3)</span> We implement a <span class=\"ltx_text ltx_font_bold\">seamless on-display user interface and comprehensive user study</span>. The user interface enables continuous and intuitive recommendation updates on the Google Glass display with lower latencies. The quantitative user study results validate the effectiveness of interaction design. The qualitative results inform actionable system improvements to promote real-world user adoptions.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the release of Google Glass Explorer Edition as a ubiquitous computing platform&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleGlassExplorer2012</span>)</cite>, smart glasses have been integrated into various medical applications, including surgical assistance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pedisurgery2014</span>)</cite>, eating behavior monitoring&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eating2015PervasiveHealth</span>)</cite>, physiological vitals sensing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SpiderNie2020</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SPIDERSNie2021</span>)</cite>, and cognitive state and ocular health tracking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BlinkMental2025</span>)</cite>. Despite their promising hands-free interaction and heads-up display capabilities, the potential of on-glass assistants to enhance EMS delivery remains largely underexplored. Recent systems such as EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> have made initial attempts to assist EMTs with protocol selection on smart glasses. However, they either lack multimodal multitasking capabilities or comprehensive real-world user studies, leaving a critical gap between technological advancement and practical adoption in EMS.</p>\n\n",
                "matched_terms": [
                    "google",
                    "edition",
                    "glass",
                    "platform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D3(audio)</span>: To train an accurate speech-to-text model, we use the open-sourced audio dataset (1123 for training and 600 for validation) from EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>. For testing, with 50 randomly sampled symptom texts from D2, five users recorded 50 audio samples using HyperX Solocast microphone&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HyperXSolocast2024</span>)</cite> and Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite>, giving a set of 500 audio samples.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "edge64x",
                    "hardware",
                    "glass",
                    "google",
                    "ph1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on our insights, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> abstracts EMSServe, the first serving framework for multimodal model inference in EMS. EMSServe comprises three key components:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> a modality-aware splitter that decomposes multimodal models into single-mode modules,  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> model inference time profiling to measure inference latency on the smart glasses and the edge server, and  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic3\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> adaptive offloading with feature caching to minimize latency in dynamic EMS scenarios. While we evaluate EMSGlass on PH1 and Edge-64X in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for generality, our primary setup uses Google Glass as the mobile device and a manpack-mounted Edge-4C as the edge server.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "edge64x",
                    "glass",
                    "google",
                    "ph1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After splitting, we profile all models and modules to obtain their inference times <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> on Google Glass and Edge-4C. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the profiling results. Both modality-aware splitting and profiling are one-time offline efforts, providing <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> as inputs for EMSServe&#8217;s real-time multimodal request serving.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates how EMSServe dynamically serves multimodal inference in an EMS scenario where symptom speech, vitals, and image data arrive asynchronously. The EMT wearing Google Glass and the EMT carrying the manpack-mounted Edge-4C move independently, causing bandwidth (BW) fluctuations in Glass-edge communication. To monitor this variation in real-time, we implement a lightweight heartbeat monitor on the smart glasses, periodically (e.g., every second) measuring the file transmission bandwidth <math alttext=\"\\Delta t=BW*filesize\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>W</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta t=BW*filesize</annotation></semantics></math>. Unlike RTT, <math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> represents the actual file transfer time. Based on this measurement, EMSServe optimizes offloading decisions:</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel cache computation</span>: We utilize multiple parallel threads to reduce cache computation overheads. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (right) shows the compute time for the text feature cache (step 1) and vitals cache (step 2) across all hardware platforms, including Google Glass. For the text module cache (top), parallel computation takes nearly the same time as running the text model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> alone, effectively hiding the text feature cache computation from users since <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> must be computed regardless. However, for the vitals module (bottom), which inherently takes much less time to compute by up to four orders of magnitude, launching multiple parallel threads often incurs higher costs than serial execution. Hence, we adopt serial feature cache computations for vitals.</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seamless UI design is crucial for EMTs to adopt networked smart glasses, given their mobility and resource constraints (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> illustrates the on-display UI: the left screen continuously updates recommendations for EMS tasks while tapping the frame (right bottom) triggers EMSGlass to capture EMTs&#8217; symptoms speech, patients&#8217; vitals, and scene images. Synchronously arrived data is offloaded adaptively (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.SS2.SSS3\" title=\"4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2.3</span></a>) via WiFi to the Edge-4C server carried in a manpack, ensuring low-latency inference and real-time recommendation on-display updates in high mobility EMS scenarios.\nEMSGlass is implemented as an Android app on Google Glass Enterprise Edition 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">glassSpecs2023</span>)</cite>, comprising over 3,000 lines of Java. EMSServe, built on Torch Serve&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">torchserve</span>)</cite>, runs on Edge-4C with &#160;2,000 lines of Python. HTTPS communication enables multimodal inference offloading, ensuring efficient processing despite smart glasses&#8217; resource limitations.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "torch",
                    "edition",
                    "android",
                    "glass",
                    "google",
                    "enterprise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> demonstrates EMSWhisper&#8217;s superior transcription accuracy and robustness. Our Whisper-s and Whisper-m achieve significantly lower WER and CER than three smaller SOTA speech-to-text models across validation and test sets. For instance, on user5&#8217;s Google Glass (GG) test set, Whisper-m attains a WER(CER) of 0.056(0.027), whereas Whisper-t&#8217;s WER(CER) rises to 0.315(0.242), about 5.6(9) times higher. As a WER of 0.1 is generally regarded as the error standard for usable voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MSTestAccMSLearn2024</span>)</cite>, SOTA models struggle with data distribution shifts, with the test set WER exceeding 0.1 due to varying accents and microphone hardware. This challenge is even more critical in EMS settings, where stricter WER requirements render SOTA models impractical for EMS scenarios, further accrediting EMSWhisper&#8217;s ultra-low error rates. Moreover, EMSWhisper, including the Whisper-s and Whisper-m motivated by the scaling law, exhibits exceptional generalization with lower WER variance across users (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)-(b)) and microphones(Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(c)-(d)), underscoring its robustness to real-world distribution shifts.</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #2: offloading without user mobility</span>. When compared to Google Glass, Edge-4C&#8217;s lower inference latency illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> expose opportunities to offload compute-expensive inference workloads for the benefit of faster inference. We use episode 1 to evaluate the impact of non-line-of-sight (NLOS) distances in inference workloads offloading from Google Glass to Edge-4C, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. We put the Google Glass at static distances (0-30 meters) to the Edge-4C in a building. This scenario #2 reflects a typical EMS realism, where the EMTs wearing smart glasses and the EMTs carrying the manpack collaboratively search for patients and perform EMS interventions in different indoor rooms. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(a) provides the evaluation results, where 5m indicates an NLOS room between Google Glass and Edge-4C while 30m indicates 6 rooms. As we increase the distances, offloading the symptom speech or vitals produces similar cumulative inference latencies while offloading the 10 images tells the essence of offloading in EMS scenarios: it&#8217;s more advantageous to offload images to Edge-4C when the distances are long(e.g., &#191;5m). This is mainly because the size of images used in the episode is relatively large, making offloading images sensitive to the glass-edge distances.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "edge4c",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3) Hardware ergonomics and interaction design.</span> Hardware limitations were a recurring theme. Participants described the current Google Glass hardware as &#8220;outdated&#8221;, citing fit issues, instability during movement, and awkward display positioning. Suggestions included rear support bands, hardware buttons usable with gloves, and integration with newer AR glasses platforms:</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "glass",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">5) Adoption drivers: accuracy, speed, reliability.</span> Across interviews, participants consistently identified accuracy, real-time speed, and reliability as the three core adoption drivers. Medication recognition accuracy was viewed positively, but slow scene processing and occasional recommendation mismatches were cited as the direction for hardware and software improvements in the future:</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "software",
                    "core"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before each user study, each EMT participant signed a consent form and received a brief orientation to the EMSGlass system and Google Glass hardware. The study began with a short hands-on tutorial introducing key hardware components (display prism, camera, microphone, and touchpad frame), and training participants to operate EMSGlass through simple tap and voice interactions. Participants were then guided through a demonstration scenario showing how EMSGlass transcribes symptoms, displays real-time vitals, detects scene objects (e.g., pills, alcohol bottles), and recommends corresponding EMS protocols, medicines, and dosages. Following the demonstration, participants performed a full simulation independently, interacting with the app to assess a manikin patient by describing symptoms, observing dynamic updates in vitals and recommendations, and completing the scenario by scanning medication labels. Each participant could repeat dry runs until comfortable with the workflow. This structured process ensured that all participants understood EMSGlass&#8217;s multimodal functionalities before proceeding to the formal evaluation.</p>\n\n",
                "matched_terms": [
                    "hardware",
                    "glass",
                    "google"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 3. Compare the 2-modal EMSNet with state-of-the-art (SOTA) unimodal models in tasks 1-3 on the 2-modal dataset D1 (text, vitals).",
        "body": "Backbone: T/V\nTask1-Protocol Selection (top-1/3/5)\nTask2-Medicine Type Prescription (top-1/3/5)\nTask3-Medicine Quant. Prescription (mse/pearsonr/spearmanr)\n\n\n\nT\nV\nP\nP-M\nP-Q\nP-M-Q\nM\nP-M\nM-Q\nP-M-Q\nQ\nP-Q\nM-Q\nP-M-Q\n\n\n\n\nLSTM\n0.43/0.66/0.76\n0.38/0.60/0.71\n0.38/0.61/0.71\n0.38/0.61/0.71\n0.50/0.77/0.87\n0.49/0.77/0.87\n0.50/0.77/0.87\n0.49/0.76/0.87\n2.63/0.21/0.22\n2.73/0.20/0.20\n2.63/0.21/0.22\n2.72/0.20/0.20\n\n\n\n\nRNN\n 0.44/0.68/0.77\n0.40/0.63/0.74\n0.39/0.63/0.74\n0.39/0.63/0.74\n0.51/0.80/0.90\n0.50/0.80/0.89\n0.50/0.79/0.89\n0.50/0.79/0.89\n2.64/0.20/0.20\n2.71/0.21/0.23\n2.62/0.22/0.23\n2.71/0.21/0.23\n\n\n\n\nGRU\n0.45/0.68/0.78\n0.41/0.64/0.75\n0.40/0.63/0.74\n0.40/0.63/0.74\n0.51/0.81/0.90\n0.50/0.81/0.90\n0.50/0.80/0.89\n0.50/0.79/0.89\n2.62/0.21/0.22\n2.70/0.22/0.24\n 2.61/0.22/0.24\n2.71/0.22/0.23\n\n\n\nBERTBase\n\n0.76/0.93/0.97\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.64/0.94/0.98\n0.64/0.94/0.98\n0.64/0.94/0.97\n0.64/0.94/0.98\n2.75/0.13/0.16\n2.64/0.28/0.34\n2.68/0.26/0.32\n2.64/0.28/0.35\n\n\n\nTinyBERT\n\n0.77/0.93/0.96\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.64/0.94/0.97\n 0.65/0.94/0.98\n0.64/0.94/0.97\n0.65/0.94/0.97\n2.54/0.28/0.35\n2.60/0.29/0.36\n2.51/0.30/0.36\n2.60/0.29/0.35\n\n\n\n\nUnimodal(SOTA)\n\n\nMobileBERT\n\n0.77/0.94/0.97\n0.75/0.93/0.96\n0.73/0.90/0.94\n0.74/0.91/0.95\n0.64/0.94/0.98\n0.64/0.94/0.98\n0.64/0.93/0.97\n0.64/0.94/0.97\n2.53/0.29/0.34\n2.62/0.28/0.35\n2.51/0.30/0.35\n2.60/0.29/0.36\n\n\n\n\nLSTM\n 0.76/0.94/0.97\n0.75/0.93/0.96\n0.39/0.62/0.73\n0.75/0.92/0.96\n0.66/0.94/0.98\n0.66/0.95/0.98\n0.66/0.94/0.98\n0.66/0.94/0.98\n2.63/0.21/0.22\n2.72/0.21/0.22\n2.53/0.31/0.37\n2.64/0.30/0.36\n\n\n\n\nRNN\n0.77/0.94/0.97\n0.75/0.93/0.96\n0.74/0.93/0.96\n0.75/0.92/0.96\n0.66/0.94/0.98\n0.66/0.95/0.98\n0.50/0.79/0.89\n0.66/0.95/0.98\n2.63/0.21/0.22\n2.65/0.29/0.36\n 2.63/0.21/0.23\n2.55/0.32/0.39\n\n\n\nBERTBase\nGRU\n0.77/0.94/0.97\n0.75/0.93/0.96\n0.75/0.93/0.96\n0.74/0.92/0.96\n0.61/0.93/0.97\n0.66/0.95/0.98\n0.66/0.94/0.98\n0.66/0.95/0.98\n2.62/0.22/0.23\n2.58/0.31/0.37\n2.47/0.32/0.37\n2.52/0.33/0.39\n\n\n\n\nLSTM\n0.77/0.93/0.97\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.66/0.94/0.98\n 0.64/0.94/0.98\n0.66/0.94/0.98\n0.65/0.94/0.98\n2.46/0.33/0.38\n2.56/0.32/0.37\n2.45/0.33/0.38\n2.55/0.32/0.37\n\n\n\n\nRNN\n0.77/0.94/0.97\n0.75/0.93/0.96\n0.75/0.92/0.96\n0.75/0.93/0.96\n0.67/0.95/0.98\n0.66/0.95/0.98\n0.66/0.94/0.98\n0.66/0.95/0.98\n2.47/0.32/0.37\n2.54/0.33/0.38\n2.45/0.33/0.38\n2.53/0.33/0.38\n\n\n\nTinyBERT\nGRU\n 0.77/0.94/0.97\n0.75/0.93/0.96\n0.75/0.93/0.96\n0.75/0.92/0.96\n0.67/0.95/0.98\n0.67/0.95/0.98\n0.66/0.94/0.98\n0.67/0.95/0.98\n2.46/0.33/0.38\n2.55/0.32/0.38\n\n2.45/0.33/0.39\n2.53/0.33/0.38\n\n\n\n\nLSTM\n0.77/0.94/0.97\n0.75/0.93/0.96\n0.74/0.92/0.95\n0.73/0.91/0.95\n0.65/0.94/0.98\n0.65/0.94/0.98\n0.66/0.94/0.98\n0.65/0.94/0.97\n2.47/0.32/0.37\n2.59/0.30/0.36\n 2.44/0.33/0.38\n2.56/0.32/0.37\n\n\n\n\nRNN\n0.77/0.94/0.97\n0.75/0.93/0.97\n0.74/0.92/0.96\n0.73/0.91/0.95\n0.66/0.94/0.98\n0.66/0.95/0.98\n0.66/0.94/0.98\n0.64/0.94/0.97\n2.46/0.32/0.37\n2.53/0.33/0.38\n2.45/0.33/0.38\n2.55/0.32/0.37\n\n\n\n\nMultimodal(our)\n\n\nMobileBERT\nGRU\n0.77/0.94/0.97\n0.75/0.93/0.97\n0.74/0.92/0.96\n0.74/0.92/0.96\n0.66/0.94/0.98\n 0.66/0.95/0.98\n0.66/0.94/0.98\n0.66/0.94/0.98\n2.46/0.32/0.37\n2.55/0.32/0.37\n2.44/0.33/0.38\n\n2.54/0.33/0.38",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">Backbone: T/V</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task1-Protocol Selection (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task2-Medicine Type Prescription (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task3-Medicine Quant. Prescription (mse/pearsonr/spearmanr)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">T</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">V</th>\n<td class=\"ltx_td ltx_align_center\">P</td>\n<td class=\"ltx_td ltx_align_center\">P-M</td>\n<td class=\"ltx_td ltx_align_center\">P-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">M</td>\n<td class=\"ltx_td ltx_align_center\">P-M</td>\n<td class=\"ltx_td ltx_align_center\">M-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">Q</td>\n<td class=\"ltx_td ltx_align_center\">P-Q</td>\n<td class=\"ltx_td ltx_align_center\">M-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LSTM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.38/0.60/0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.38/0.61/0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.38/0.61/0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.77/0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.77/0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.77/0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.76/0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.63/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.73/0.20/0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.63/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.72/0.20/0.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RNN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.44/0.68/0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.63/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.63/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.63/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.51/0.80/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.80/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.79/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.79/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.64/0.20/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.71/0.21/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.62/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.71/0.21/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GRU</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.68/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.41/0.64/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.63/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.63/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.51/0.81/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.81/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.80/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.79/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.62/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.70/0.22/0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.61/0.22/0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.71/0.22/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTBase</th>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.76/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.75/0.13/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.64/0.28/0.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.68/0.26/0.32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.64/0.28/0.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TinyBERT</th>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.65/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.54/0.28/0.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.60/0.29/0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.51/0.30/0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.60/0.29/0.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:77.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:77.8pt;transform:translate(-33.9pt,-33.9pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Unimodal(SOTA)</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MobileBERT</th>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.53/0.29/0.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.62/0.28/0.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.51/0.30/0.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.60/0.29/0.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LSTM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.76/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.62/0.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.63/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.72/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.53/0.31/0.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.64/0.30/0.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RNN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.79/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.63/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.65/0.29/0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.63/0.21/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.55/0.32/0.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTBase</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GRU</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">0.75<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.93<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.96</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.62/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.58/0.31/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.47/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\">2.52<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.33<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LSTM</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.64/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.46/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.56/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.45/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.55/0.32/0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RNN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">0.75<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.93<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.67<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.95<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.47/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.54/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.45/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.53/0.33/0.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TinyBERT</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GRU</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">0.67<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.95<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\">0.66<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.94<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\">0.67<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.95<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\">2.46<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.33<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.38</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.55/0.32/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.45/0.33/</span>0.39</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.53/0.33/0.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LSTM</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.47/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.59/0.30/0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.44/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.56/0.32/0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RNN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.77/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.75/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.46/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center\">2.53<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.33<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.38</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.45/0.33/0.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.55/0.32/0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:84.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:84.8pt;transform:translate(-37.4pt,-37.4pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal(our)</span></p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">MobileBERT</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">GRU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.77<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.94<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.75<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.93<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.46/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.55/0.32/0.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2.44<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.33<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.38</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.54/0.33/0.38</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "lstm",
            "unimodalsota",
            "text",
            "mobilebert",
            "2modal",
            "selection",
            "pmq",
            "quant",
            "task1protocol",
            "top135",
            "prescription",
            "emsnet",
            "vitals",
            "msepearsonrspearmanr",
            "stateoftheart",
            "sota",
            "backbone",
            "gru",
            "dataset",
            "unimodal",
            "bertbase",
            "task2medicine",
            "tinybert",
            "models",
            "multimodalour",
            "compare",
            "tasks",
            "rnn",
            "task3medicine",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T3\" title=\"Table 3 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we compare the accuracy of the multimodal backbone with SOTA unimodal options in accomplishing tasks 1-3 on the 2-modal dataset D1. The P, M, and Q mean the backbone is trained to separately accomplish a single Task 1, 2, and 3. P-M means the backbone is trained to accomplish two tasks 1 and 2 simultaneously while P-M-Q means accomplishing three tasks at the same time. The lower mse and higher pearsonr/spearsonr indicate better performance in task3. As demonstrated, our multimodal backbone consistently outperforms unimodal options adopted in SOTA assistant systems. Specifically, it improves the top-1/3/5 accuracy for task2 by 2-3%, reduces the MSE by <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.1, and enhances Pearsonr and Spearmanr for Task 3, which are newly proposed EMS tasks in this work.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "stateoftheart",
                    "emsnet",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mobile assistant systems for EMS have been evolving to harness both cloud and edge (e.g., smart glasses) resources for real-time cognitive assistance and protocol recommendations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite>. For instance, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> allow EMTs to wear smart glasses for capturing the voice of patient symptoms. They utilize domain-specific speech-to-text models (e.g., Whisper-tiny) to transcribe the speech and data-driven models (e.g., MobileBERT) to help select protocols. These models are trained in the cloud and deployed on hands-free smart glasses (e.g., Google Glass, Vuzix M4000) and edge servers to address challenges such as unreliable cloud connectivity in disaster scenarios while also enhancing user privacy. Despite these advancements, several clinical and technical challenges remain unresolved:</p>\n\n",
                "matched_terms": [
                    "models",
                    "mobilebert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "selection",
                    "tasks",
                    "stateoftheart",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #2: High-latency serving frameworks for multimodal models in EMS scenarios.</span> Existing multimodal model serving frameworks, such as PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite> and TensorFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tensorflow2016</span>)</cite> used in SOTA assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, assume simultaneous availability of all data modalities. However, in the EMS scenario shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, smart glasses receive different data modes asynchronously (i.e., asynchronous arrival times) as EMTs move through the scene. Without caching intermediate processing results for early arrived modes of data, directly using multimodal model serving frameworks has to repeatedly process early arrived voice data when later-arriving data like vitals and images become available, leading to redundant computations and higher latency. To achieve timely EMS delivery given the asynchronous arrival of multimodal data, a low-latency multimodal serving framework in EMS scenarios is essential.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "models",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> We build <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model trained on massive, real-world multimodal EMS datasets to simultaneously accomplish five critical EMS tasks: protocol selection, recommendation for medicine type, quantity, dosage, and disease history inference.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "selection",
                    "type",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">visionFoundation2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DISTMM2024NSDI</span>)</cite> are designed to understand and fuse complementary information from multiple sources. These models rely on one or more deep learning submodules to process and integrate data effectively. They aim to generate intermediate unified numeric representations from multimodal inputs for multiple simultaneous tasks across diverse domains, e.g., general healthcare&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2020fusion</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lian2024npj</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2019early</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharazmi2018feature</span>)</cite>. However, few has explored the feasibility and advantage of such multimodal multitask capability in the EMS domain.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the release of Google Glass Explorer Edition as a ubiquitous computing platform&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleGlassExplorer2012</span>)</cite>, smart glasses have been integrated into various medical applications, including surgical assistance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pedisurgery2014</span>)</cite>, eating behavior monitoring&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eating2015PervasiveHealth</span>)</cite>, physiological vitals sensing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SpiderNie2020</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SPIDERSNie2021</span>)</cite>, and cognitive state and ocular health tracking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BlinkMental2025</span>)</cite>. Despite their promising hands-free interaction and heads-up display capabilities, the potential of on-glass assistants to enhance EMS delivery remains largely underexplored. Recent systems such as EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> have made initial attempts to assist EMTs with protocol selection on smart glasses. However, they either lack multimodal multitasking capabilities or comprehensive real-world user studies, leaving a critical gap between technological advancement and practical adoption in EMS.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "selection",
                    "tasks",
                    "backbone",
                    "dataset",
                    "type",
                    "emsnet",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "2modal",
                    "vitals",
                    "selection",
                    "dataset",
                    "type",
                    "emsnet",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D2(3-modal: text, vitals,scene)</span>: Similarly, we extract D2&#8217;s text and vitals with D2&#8217;s PCR Keys, a disjoint set of D1 corresponding to events with 3-modal input: text, vitals, and scene data. We also extract the EMS scene information indicating the existence of alcohol and pills. The scene information is one-hot encoded, giving a 3-modal dataset D2(text, vitals, scene) with 3,005 samples.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "models",
                    "lstm",
                    "backbone",
                    "rnn",
                    "gru",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "text",
                    "2modal",
                    "vitals",
                    "backbone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "2modal",
                    "vitals",
                    "models",
                    "backbone",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "models",
                    "stateoftheart",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These challenges stem from data distribution shift&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HwangDistributionECCV2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChipDataDistributionOnline2024</span>)</cite>, where real-world inputs&#8212;affected by diverse accents and microphones&#8212;differ from training data, yet the labels remain unchanged. The root cause is the small size of existing models, all under 100 million parameters (EMSConformer: 10m, Whisper-tiny: 74m, Whisper-base: 74m), limiting both learning and generalization. Empirical evidence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaplan2020scalinglawsneurallanguage</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">alabdulmohsin2022revisitingscalinglaws</span>)</cite> supports this approach, showing that larger models achieve lower training loss. Specifically, we propose training Whisper-small (242m) and Whisper-medium (764m) on dataset D3(audio) to enhance generalization (as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Here we select one order of magnitude higher than the SOTA to demonstrate the effectiveness of EMSWhisper&#8217;s idea in enhancing generalization capability. Further explorations of alternative scaling options could be considered future work.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "models",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Close-set detection models (e.g., YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite>), in contrast to the aforementioned open-set Grounding Dino (GD) model, require manual dataset annotation and model training when generalized onto a new domain, e.g., EMS. To relieve the demanding annotation workloads, we propose <span class=\"ltx_text ltx_font_italic\">human-in-the-loop (HITL) annotation adjustment</span>, leveraging the open-set strength of the GD into the human annotation process. There are two rounds of annotations: in the first round, we feed unlabeled EMS scene images D4(image) to GD to get bounding box labels. In the second round, instead of annotating from scratch, we adjust GD&#8217;s annotations from the first round (e.g., relabel GD&#8217;s incorrect annotations). After two rounds, we fine-tune the close-set YOLO11. When compared to conventional manual annotation without GD&#8217;s auto-labeling process, our HITL annotation adjustment saves the annotation time by exploiting the correct annotations from GD and leaves the main manual efforts in GD&#8217;s incorrect or missed annotations. From our experiences, this HITL annotation adjustment method decreases the annotation time by half, e.g., on average, from 10 to 5 minutes for every 100 images.</p>\n\n",
                "matched_terms": [
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "text",
                    "emsnet",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "models",
                    "lstm",
                    "rnn",
                    "gru",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key idea of EMSServe&#8211;feature cache</span>: To eliminate redundant text submodule inference, EMSServe employs feature cache (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, right). When voice data arrives, instead of only running the text model, we simultaneously compute and cache the text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> in the text-vital model. When vitals1 arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, unlike conventional frameworks that rerun the costly text submodule, EMSServe reuses the cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and processes only the vitals module (vitals encoder) to obtain <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math>, which significantly reduces inference costs. The cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and newly computed <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math> are then concatenated as inputs to model headers. By addressing asynchronous EMS data arrival times, EMSServe mitigates redundant and costly submodule computation to improve inference efficiency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the modality-aware splitter decomposes the multimodal model <math alttext=\"M2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">M2</annotation></semantics></math> (<math alttext=\"M3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">M3</annotation></semantics></math>) into single-mode modules <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math> (<math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math>, and <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>), enabling precomputation and cache of the text module output features <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> (<math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math>) before vitals data arrive.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Low Latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> is low): offload compute-heavy tasks (e.g., text submodule inference, object detection) to the edge server for its higher processing power.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel cache computation</span>: We utilize multiple parallel threads to reduce cache computation overheads. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (right) shows the compute time for the text feature cache (step 1) and vitals cache (step 2) across all hardware platforms, including Google Glass. For the text module cache (top), parallel computation takes nearly the same time as running the text model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> alone, effectively hiding the text feature cache computation from users since <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> must be computed regardless. However, for the vitals module (bottom), which inherently takes much less time to compute by up to four orders of magnitude, launching multiple parallel threads often incurs higher costs than serial execution. Hence, we adopt serial feature cache computations for vitals.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seamless UI design is crucial for EMTs to adopt networked smart glasses, given their mobility and resource constraints (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> illustrates the on-display UI: the left screen continuously updates recommendations for EMS tasks while tapping the frame (right bottom) triggers EMSGlass to capture EMTs&#8217; symptoms speech, patients&#8217; vitals, and scene images. Synchronously arrived data is offloaded adaptively (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.SS2.SSS3\" title=\"4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2.3</span></a>) via WiFi to the Edge-4C server carried in a manpack, ensuring low-latency inference and real-time recommendation on-display updates in high mobility EMS scenarios.\nEMSGlass is implemented as an Android app on Google Glass Enterprise Edition 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">glassSpecs2023</span>)</cite>, comprising over 3,000 lines of Java. EMSServe, built on Torch Serve&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">torchserve</span>)</cite>, runs on Edge-4C with &#160;2,000 lines of Python. HTTPS communication enables multimodal inference offloading, ensuring efficient processing despite smart glasses&#8217; resource limitations.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "pmq",
                    "tasks",
                    "backbone",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, with PMI-enabled fine-tuning, 3-modal EMSNet achieves the consistently higher accuracy in recommending protocols (task1) and medicine types (task2). For example, the PMI-enabled BERTBase-LSTM-FC and BERTBase-GRU-FC achieve top-3 accuracy of 0.91 and 0.96, respectively, on the single protocol (P) and medicine type (M) tasks. For the medicine quantity recommendation task (Q and P-M-Q), although fine-tuning w/o PMI seems more performant, fine-tuning w/ PMI achieves comparable performance. For example, fine-tuning with PMI enables TinyBERT-LSTM-FC to achieve the MSE of 1.89 on the single Q task, close to the lowest MSE of 1.87. This also implies a future work direction on how PMI could further improve the regression accuracy task, like medicine quantity prescription in this paper.</p>\n\n",
                "matched_terms": [
                    "pmq",
                    "tasks",
                    "type",
                    "emsnet",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mobilebert",
                    "tinybert",
                    "models",
                    "pmq",
                    "tasks",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> demonstrates EMSWhisper&#8217;s superior transcription accuracy and robustness. Our Whisper-s and Whisper-m achieve significantly lower WER and CER than three smaller SOTA speech-to-text models across validation and test sets. For instance, on user5&#8217;s Google Glass (GG) test set, Whisper-m attains a WER(CER) of 0.056(0.027), whereas Whisper-t&#8217;s WER(CER) rises to 0.315(0.242), about 5.6(9) times higher. As a WER of 0.1 is generally regarded as the error standard for usable voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MSTestAccMSLearn2024</span>)</cite>, SOTA models struggle with data distribution shifts, with the test set WER exceeding 0.1 due to varying accents and microphone hardware. This challenge is even more critical in EMS settings, where stricter WER requirements render SOTA models impractical for EMS scenarios, further accrediting EMSWhisper&#8217;s ultra-low error rates. Moreover, EMSWhisper, including the Whisper-s and Whisper-m motivated by the scaling law, exhibits exceptional generalization with lower WER variance across users (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)-(b)) and microphones(Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(c)-(d)), underscoring its robustness to real-world distribution shifts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F12\" title=\"Figure 12 &#8227; 5.1.4. Accuracy of Objection Detection &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> compares the object detection performance of fine-tuning YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite> on the image dataset D4(image) labeled by Grounding Dino and labeled by our human-in-the-loop (HITL) annotation adjustment. Our HITL annotation strategy outperforms Grounding Dino&#8217;s annotations, bringing the test mAP value close to 0.8. In contrast, the mAP of all YOLO11 models on images annotated with Grounding Dino is below 0.6, which is unacceptable. The high mAP on the validate set but low mAP and recall on the test set means fine-tuning YOLO11 with Grounding Dino&#8217;s annotations causes the overfitting during the fine-tuning process. For example, using Grounding Dino with the hard text prompts from Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> can achieve an mAP above 0.8, but the test mAP of all YOLO11 models is nearly 0. The recall metric is similar: high validate recall while much lower test recall. This results from Grounding Dino&#8217;s high false positive predictions as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2\" title=\"2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our HITL avoids this problem because we add the human adjustments to correct Grounding Dino&#8217;s false positive annotations, enabling much higher test mAP and recall. It&#8217;s important to note that when compared to direct manual annotations,\nthe advantage of our HITL is that we exploit Grounding Dino&#8217;s accurate (i.e., true positives) annotations to save human labor time on annotations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the OCR module in EMSGlass, we evaluate an image dataset of size 204 captured by Google Glass&#8217;s 8MP camera, featuring a user holding a labeled medicine bottle at three distances&#8212;full arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.6m), half arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> 0.3m), and quarter arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.15m)&#8212;to account for varying EMT arm lengths. Each image has two versions: the original image without cropping and the cropped bottle segment. We use the word error rate (WER) and character error rate (CER) to measure OCR accuracy. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, EMSGlass OCR performance was evaluated using four state-of-the-art (SOTA) models listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: EasyOCR, TesseractOCR, PaddleOCR, CRNN. In general, EasyOCR achieves the lowest WER and CER. Our edit distance (ED)-based matching discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS4\" title=\"3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> is applied after each OCR prediction. As shown, the OCR models with ED-match can achieve significantly decreased WER and CER, indicating the effectiveness of our proposed ED-match method. ED-match helps EasyOCR to decrease WER and CER by 89% and 83%, respectively. EMSGlass employs Easy-Match as its OCR framework, which consistently outperforms other models with WER (CER) below 0.12 (0.05) across all arm distance and cropping conditions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "models",
                    "stateoftheart",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T5\" title=\"Table 5 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows EMSNet&#8217;s end-to-end (E2E) accuracy on the 3-modal dataset D2, i.e., from the end of original 3-modal inputs to the end of tasks 1-3. Here we use our Whisper-s and Whisper-m for the speech recognition (SR) module, YOLO11n for the object detection (OD) module, and 3 models in the backbone: unimodal MobileBERT, 2-modal MobileBERT-GRU, and 3-modal MobileBERT-GRU-FC. We make two observations from the table: 1) the multimodal models, including the 2-modal and 3-modal models, achieve higher accuracy, 2) the addition of speech recognition and object detection models don&#8217;t degrade the E2E accuracy, indicating seamless integrations with the backbone models.</p>\n\n",
                "matched_terms": [
                    "mobilebert",
                    "2modal",
                    "models",
                    "tasks",
                    "backbone",
                    "dataset",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EMSServe is the first multimodal model serving framework addressing the issue of asynchronous arrival times, which is inherently introduced by using smart glasses in multimodal EMS scenarios with high mobility. Here, we make the following evaluation plan to show EMSServe&#8217;s advantage in serving such asynchronously arrived multimodal data. Note that, when evaluating EMSServe, the BERTBase-GRU-FC backbone is used with Whisper-tiny and YOLO11n as the multimodal EMSNet model.</p>\n\n",
                "matched_terms": [
                    "backbone",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate EMSGlass&#8217;s usability and demonstrate its real-world end-to-end usage feasibility, we conducted a user study with 6 certified Emergency Medical Technicians (EMTs). Each EMT uses the full EMSGlass system (i.e., EMSNet and EMSServe) in a simulated emergency room illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. The user study process involves data perception and real-time decision-making in a simulated end-to-end EMS event, including protocol selection and medication prescription tasks. Following the user study, participants completed a 15-item Likert scale questionnaire (1 = strongly disagree, 5 = strongly agree) covering five dimensions:</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "selection",
                    "emsnet",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Padding: for the missed values in raw NEMSIS vitals, we simply add zero at the beginning of the vitals. Our experiments show this practice enables EMSNet to achive higher accuracy.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-sample normalization: NEMSIS dictates different scale ranges for different vitals, e.g., pulse oximetry (PO) in [0, 100] while blood glucose in [0, 2000]. These large numerical values produces the notorious &#8220;NaN&#8221; problems during training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>]</cite>. In addition, different scales of vitals values prevent deep learning models to effectively combine the information from different vitals. To address this problem, we adopt three common normalization methods: z-score,min-max and min-max over z-score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rahmad2024comparativenorm</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.SS1.SSS2\" title=\"5.1.2. 3-modal EMSNet backbone accuracy on D2 &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the full details of evaluating the EMSNet on 3-modal dataset D2 (text, vitals, image).</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "dataset",
                    "emsnet"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 4. Compare the 3-modal EMSNet w/ and w/o progressive modality integration in tasks 1-3 on the dataset 3-modal D2 (text, vitals, image).",
        "body": "Backbone(Text/Vitals/Image)\nTask1-Protocol (top-1/3/5)\nTask2-MedType (top-1/3/5)\nTask3-MedQuant (mse/pearsonr/spearmanr)\n\n\nT\nV\nI\nP\nP-M-Q\nM\nP-M-Q\nQ\nP-M-Q\n\n\nFine-tuning w/o PMI\nBERTBase\nLSTM\nFC\n0.69/0.89/0.93\n0.67/0.86/0.89\n0.66/0.93/0.98\n0.69/0.95/0.97\n1.94/0.04/-0.08\n2.42/0.11/0.18\n\n\nRNN\nFC\n0.67/0.86/0.91\n0.67/0.87/0.90\n0.67/0.94/0.98\n0.67/0.93/0.98\n1.90/0.15/0.21\n2.50/0.13/0.16\n\n\nGRU\nFC\n0.69/0.89/0.93\n0.65/0.86/0.89\n0.62/0.93/0.96\n0.67/0.94/0.97\n1.92/0.09/0.09\n\n2.15/0.16/0.24\n\n\nTinyBERT\nLSTM\nFC\n0.65/0.82/0.89\n0.54/0.76/0.84\n0.66/0.93/0.97\n0.67/0.94/0.96\n1.90/0.16/0.17\n1.95/0.19/0.21\n\n\nRNN\nFC\n0.63/0.83/0.89\n0.53/0.76/0.82\n0.65/0.95/0.97\n0.65/0.92/0.96\n1.87/0.18/0.18\n\n1.93/0.19/0.20\n\n\n\nGRU\nFC\n0.63/0.82/0.88\n0.54/0.74/0.81\n0.64/0.94/0.97\n0.67/0.93/0.97\n1.94/0.14/0.16\n\n1.93/0.20/0.21\n\n\n\nMobileBERT\nLSTM\nFC\n0.49/0.72/0.84\n0.02/0.05/0.09\n0.56/0.91/0.94\n0.03/0.07/0.45\n4e4/-0.02/0.03\n1e3/0.00/-0.02\n\n\nRNN\nFC\n0.47/0.68/0.78\n0.43/0.59/0.64\n0.43/0.91/0.94\n0.45/0.54/0.68\n1e1/0.00/-0.03\n2.15/-0.03/-0.06\n\n\nGRU\nFC\n0.48/0.69/0.78\n0.00/0.22/0.23\n0.52/0.89/0.93\n0.02/0.03/.048\n3e4/0.01/-0.05\n5e5/0.03/0.01\n\n\nFine-tuning w/ PMI (our)\nBERTBase\nLSTM\nFC\n\n0.72/0.91/0.95\n\n\n0.69/0.88/0.93\n0.68/0.95/0.98\n0.68/0.93/0.97\n1.91/0.12/0.11\n2.41/0.16/0.17\n\n\nRNN\nFC\n0.72/0.90/0.94\n0.67/0.88/0.92\n0.66/0.95/0.98\n0.69/0.94/0.97\n\n1.91/0.12/0.11\n2.56/0.14/0.16\n\n\nGRU\nFC\n0.72/0.89/0.95\n\n0.68/0.88/0.92\n\n\n0.67/0.96/0.98\n\n0.67/0.95/0.98\n1.91/0.11/0.10\n2.30/0.16/0.21\n\n\nTinyBERT\nLSTM\nFC\n0.67/0.83/0.89\n0.61/0.80/0.83\n0.66/0.95/0.98\n0.67/0.93/0.97\n\n1.89/0.18/0.22\n1.97/0.19/0.21\n\n\nRNN\nFC\n0.68/0.85/0.90\n0.59/0.79/0.84\n0.69/0.95/0.97\n0.66/0.95/0.97\n1.89/0.17/0.19\n1.96/0.19/0.23\n\n\nGRU\nFC\n0.67/0.83/0.90\n0.56/0.78/0.86\n0.70/0.96/0.98\n\n0.67/0.95/0.97\n1.96/0.14/0.18\n1.99/0.19/0.23\n\n\nMobileBERT\nLSTM\nFC\n0.72/0.89/0.94\n0.64/0.86/0.89\n0.66/0.95/0.98\n0.68/0.93/0.97\n1.93/0.13/0.14\n2.12/0.15/0.18\n\n\nRNN\nFC\n0.68/0.90/0.95\n0.66/0.89/0.91\n0.70/0.95/0.98\n0.67/0.93/0.96\n1.93/0.15/0.18\n2.19/0.15/0.18\n\n\nGRU\nFC\n0.70/0.88/0.93\n0.70/0.88/0.91\n\n0.68/0.95/0.98\n0.69/0.94/0.98\n1.96/0.13/0.17\n2.28/0.18/0.23",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">Backbone(Text/Vitals/Image)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Task1-Protocol (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Task2-MedType (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Task3-MedQuant (mse/pearsonr/spearmanr)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">T</td>\n<td class=\"ltx_td ltx_align_center\">V</td>\n<td class=\"ltx_td ltx_align_center\">I</td>\n<td class=\"ltx_td ltx_align_center\">P</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">M</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"9\">Fine-tuning w/o PMI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">BERTBase</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.04/-0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.42/0.11/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.87/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.15/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.50/0.13/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.62/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.09/0.09</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.15/0.16/</span>0.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">TinyBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.82/0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.76/0.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.16/0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.19/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.83/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.53/0.76/0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">1.87<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.18/0.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">1.93<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.19/0.20</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.82/0.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.74/0.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.14/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/</span>0.20<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.21</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">MobileBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.72/0.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.02/0.05/0.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.91/0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.03/0.07/0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">4e4/-0.02/0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1e3/0.00/-0.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.47/0.68/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.59/0.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.91/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.54/0.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1e1/0.00/-0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.15/-0.03/-0.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.69/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.52/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.02/0.03/.048</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">3e4/0.01/-0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">5e5/0.03/0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" rowspan=\"9\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning w/ PMI (our)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">BERTBase</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/</span>0.91<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.88/</span>0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.12/0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.41/0.16/0.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">0.69<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.94/0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.12/0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.56/0.14/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\">0.72<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.89/</span>0.95</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/</span>0.88<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/</span>0.96<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/</span>0.95<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.11/0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.30/0.16/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">TinyBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.83/0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.80/0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.89/</span>0.18<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.19/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.85/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.59/0.79/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.89/0.17/0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.19/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.83/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.78/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.96/0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.14/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.19/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"3\">MobileBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.13/0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.12/0.15/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_center\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.89/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.15/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.19/0.15/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\">GRU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.88/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.70<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.88/0.91</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.13/0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.28/0.18/0.23</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "3modal",
            "lstm",
            "integration",
            "text",
            "task3medquant",
            "mobilebert",
            "pmq",
            "image",
            "1e1000003",
            "task1protocol",
            "progressive",
            "top135",
            "emsnet",
            "modality",
            "3e4001005",
            "vitals",
            "1e3000002",
            "msepearsonrspearmanr",
            "gru",
            "dataset",
            "5e5003001",
            "bertbase",
            "4e4002003",
            "pmi",
            "tinybert",
            "finetuning",
            "task2medtype",
            "compare",
            "tasks",
            "rnn",
            "our",
            "backbonetextvitalsimage"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "emsnet",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> We build <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model trained on massive, real-world multimodal EMS datasets to simultaneously accomplish five critical EMS tasks: protocol selection, recommendation for medicine type, quantity, dosage, and disease history inference.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
                "matched_terms": [
                    "text",
                    "emsnet",
                    "vitals",
                    "image",
                    "tasks",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F3\" title=\"Figure 3 &#8227; 3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the data processing pipeline used to prepare four multimodal datasets (D1-D4) to train and test EMSNet. This pipeline is designed as a general plug-n-play tool for preparing multimodal datasets from NEMSIS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>, a publicly available (upon request) key-value tabular database that includes all real-world national emergency event reports in the US. Notably, <span class=\"ltx_text ltx_font_italic\">our data processor is the first tool of its kind to prepare multimodal EMS datasets</span>. The NEMSIS database uses a unique 9-digit &#8220;PCR key&#8221; as each emergency event ID. In this paper, we utilize the NEMSIS 2023 database, which includes over 54 million EMS events recorded by 14,369 EMS agencies across 54 states and territories in 2023.</p>\n\n",
                "matched_terms": [
                    "our",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "dataset",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D2(3-modal: text, vitals,scene)</span>: Similarly, we extract D2&#8217;s text and vitals with D2&#8217;s PCR Keys, a disjoint set of D1 corresponding to events with 3-modal input: text, vitals, and scene data. We also extract the EMS scene information indicating the existence of alcohol and pills. The scene information is one-hot encoded, giving a 3-modal dataset D2(text, vitals, scene) with 3,005 samples.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "dataset",
                    "vitals",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "lstm",
                    "image",
                    "rnn",
                    "gru",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "vitals",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "integration",
                    "text",
                    "vitals",
                    "pmi",
                    "our",
                    "progressive",
                    "dataset",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Close-set detection models (e.g., YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite>), in contrast to the aforementioned open-set Grounding Dino (GD) model, require manual dataset annotation and model training when generalized onto a new domain, e.g., EMS. To relieve the demanding annotation workloads, we propose <span class=\"ltx_text ltx_font_italic\">human-in-the-loop (HITL) annotation adjustment</span>, leveraging the open-set strength of the GD into the human annotation process. There are two rounds of annotations: in the first round, we feed unlabeled EMS scene images D4(image) to GD to get bounding box labels. In the second round, instead of annotating from scratch, we adjust GD&#8217;s annotations from the first round (e.g., relabel GD&#8217;s incorrect annotations). After two rounds, we fine-tune the close-set YOLO11. When compared to conventional manual annotation without GD&#8217;s auto-labeling process, our HITL annotation adjustment saves the annotation time by exploiting the correct annotations from GD and leaves the main manual efforts in GD&#8217;s incorrect or missed annotations. From our experiences, this HITL annotation adjustment method decreases the annotation time by half, e.g., on average, from 10 to 5 minutes for every 100 images.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "lstm",
                    "rnn",
                    "gru",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key idea of EMSServe&#8211;feature cache</span>: To eliminate redundant text submodule inference, EMSServe employs feature cache (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, right). When voice data arrives, instead of only running the text model, we simultaneously compute and cache the text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> in the text-vital model. When vitals1 arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, unlike conventional frameworks that rerun the costly text submodule, EMSServe reuses the cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and processes only the vitals module (vitals encoder) to obtain <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math>, which significantly reduces inference costs. The cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and newly computed <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math> are then concatenated as inputs to model headers. By addressing asynchronous EMS data arrival times, EMSServe mitigates redundant and costly submodule computation to improve inference efficiency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the modality-aware splitter decomposes the multimodal model <math alttext=\"M2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">M2</annotation></semantics></math> (<math alttext=\"M3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">M3</annotation></semantics></math>) into single-mode modules <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math> (<math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math>, and <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>), enabling precomputation and cache of the text module output features <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> (<math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math>) before vitals data arrive.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates how EMSServe dynamically serves multimodal inference in an EMS scenario where symptom speech, vitals, and image data arrive asynchronously. The EMT wearing Google Glass and the EMT carrying the manpack-mounted Edge-4C move independently, causing bandwidth (BW) fluctuations in Glass-edge communication. To monitor this variation in real-time, we implement a lightweight heartbeat monitor on the smart glasses, periodically (e.g., every second) measuring the file transmission bandwidth <math alttext=\"\\Delta t=BW*filesize\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>W</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta t=BW*filesize</annotation></semantics></math>. Unlike RTT, <math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> represents the actual file transfer time. Based on this measurement, EMSServe optimizes offloading decisions:</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Low Latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> is low): offload compute-heavy tasks (e.g., text submodule inference, object detection) to the edge server for its higher processing power.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel cache computation</span>: We utilize multiple parallel threads to reduce cache computation overheads. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (right) shows the compute time for the text feature cache (step 1) and vitals cache (step 2) across all hardware platforms, including Google Glass. For the text module cache (top), parallel computation takes nearly the same time as running the text model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> alone, effectively hiding the text feature cache computation from users since <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> must be computed regardless. However, for the vitals module (bottom), which inherently takes much less time to compute by up to four orders of magnitude, launching multiple parallel threads often incurs higher costs than serial execution. Hence, we adopt serial feature cache computations for vitals.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seamless UI design is crucial for EMTs to adopt networked smart glasses, given their mobility and resource constraints (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> illustrates the on-display UI: the left screen continuously updates recommendations for EMS tasks while tapping the frame (right bottom) triggers EMSGlass to capture EMTs&#8217; symptoms speech, patients&#8217; vitals, and scene images. Synchronously arrived data is offloaded adaptively (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.SS2.SSS3\" title=\"4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2.3</span></a>) via WiFi to the Edge-4C server carried in a manpack, ensuring low-latency inference and real-time recommendation on-display updates in high mobility EMS scenarios.\nEMSGlass is implemented as an Android app on Google Glass Enterprise Edition 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">glassSpecs2023</span>)</cite>, comprising over 3,000 lines of Java. EMSServe, built on Torch Serve&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">torchserve</span>)</cite>, runs on Edge-4C with &#160;2,000 lines of Python. HTTPS communication enables multimodal inference offloading, ensuring efficient processing despite smart glasses&#8217; resource limitations.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T3\" title=\"Table 3 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we compare the accuracy of the multimodal backbone with SOTA unimodal options in accomplishing tasks 1-3 on the 2-modal dataset D1. The P, M, and Q mean the backbone is trained to separately accomplish a single Task 1, 2, and 3. P-M means the backbone is trained to accomplish two tasks 1 and 2 simultaneously while P-M-Q means accomplishing three tasks at the same time. The lower mse and higher pearsonr/spearsonr indicate better performance in task3. As demonstrated, our multimodal backbone consistently outperforms unimodal options adopted in SOTA assistant systems. Specifically, it improves the top-1/3/5 accuracy for task2 by 2-3%, reduces the MSE by <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.1, and enhances Pearsonr and Spearmanr for Task 3, which are newly proposed EMS tasks in this work.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "top135",
                    "pmq",
                    "tasks",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, with PMI-enabled fine-tuning, 3-modal EMSNet achieves the consistently higher accuracy in recommending protocols (task1) and medicine types (task2). For example, the PMI-enabled BERTBase-LSTM-FC and BERTBase-GRU-FC achieve top-3 accuracy of 0.91 and 0.96, respectively, on the single protocol (P) and medicine type (M) tasks. For the medicine quantity recommendation task (Q and P-M-Q), although fine-tuning w/o PMI seems more performant, fine-tuning w/ PMI achieves comparable performance. For example, fine-tuning with PMI enables TinyBERT-LSTM-FC to achieve the MSE of 1.89 on the single Q task, close to the lowest MSE of 1.87. This also implies a future work direction on how PMI could further improve the regression accuracy task, like medicine quantity prescription in this paper.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "pmi",
                    "finetuning",
                    "pmq",
                    "tasks",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "text",
                    "mobilebert",
                    "pmi",
                    "tinybert",
                    "pmq",
                    "tasks",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F12\" title=\"Figure 12 &#8227; 5.1.4. Accuracy of Objection Detection &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> compares the object detection performance of fine-tuning YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite> on the image dataset D4(image) labeled by Grounding Dino and labeled by our human-in-the-loop (HITL) annotation adjustment. Our HITL annotation strategy outperforms Grounding Dino&#8217;s annotations, bringing the test mAP value close to 0.8. In contrast, the mAP of all YOLO11 models on images annotated with Grounding Dino is below 0.6, which is unacceptable. The high mAP on the validate set but low mAP and recall on the test set means fine-tuning YOLO11 with Grounding Dino&#8217;s annotations causes the overfitting during the fine-tuning process. For example, using Grounding Dino with the hard text prompts from Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> can achieve an mAP above 0.8, but the test mAP of all YOLO11 models is nearly 0. The recall metric is similar: high validate recall while much lower test recall. This results from Grounding Dino&#8217;s high false positive predictions as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2\" title=\"2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our HITL avoids this problem because we add the human adjustments to correct Grounding Dino&#8217;s false positive annotations, enabling much higher test mAP and recall. It&#8217;s important to note that when compared to direct manual annotations,\nthe advantage of our HITL is that we exploit Grounding Dino&#8217;s accurate (i.e., true positives) annotations to save human labor time on annotations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "finetuning",
                    "image",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the OCR module in EMSGlass, we evaluate an image dataset of size 204 captured by Google Glass&#8217;s 8MP camera, featuring a user holding a labeled medicine bottle at three distances&#8212;full arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.6m), half arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> 0.3m), and quarter arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.15m)&#8212;to account for varying EMT arm lengths. Each image has two versions: the original image without cropping and the cropped bottle segment. We use the word error rate (WER) and character error rate (CER) to measure OCR accuracy. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, EMSGlass OCR performance was evaluated using four state-of-the-art (SOTA) models listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: EasyOCR, TesseractOCR, PaddleOCR, CRNN. In general, EasyOCR achieves the lowest WER and CER. Our edit distance (ED)-based matching discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS4\" title=\"3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> is applied after each OCR prediction. As shown, the OCR models with ED-match can achieve significantly decreased WER and CER, indicating the effectiveness of our proposed ED-match method. ED-match helps EasyOCR to decrease WER and CER by 89% and 83%, respectively. EMSGlass employs Easy-Match as its OCR framework, which consistently outperforms other models with WER (CER) below 0.12 (0.05) across all arm distance and cropping conditions.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the barcode scanner, we only use the original image without cropping. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> (right), we use the success rate as the metric to measure the performance of barcode scanner (developed based on ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> in Android). At the 1/4 Arm, our scanner can achieve a 100% accuracy, indicating its effectiveness. On average, it takes less than 0.5 seconds to process one image, ensuring minimal latency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T5\" title=\"Table 5 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows EMSNet&#8217;s end-to-end (E2E) accuracy on the 3-modal dataset D2, i.e., from the end of original 3-modal inputs to the end of tasks 1-3. Here we use our Whisper-s and Whisper-m for the speech recognition (SR) module, YOLO11n for the object detection (OD) module, and 3 models in the backbone: unimodal MobileBERT, 2-modal MobileBERT-GRU, and 3-modal MobileBERT-GRU-FC. We make two observations from the table: 1) the multimodal models, including the 2-modal and 3-modal models, achieve higher accuracy, 2) the addition of speech recognition and object detection models don&#8217;t degrade the E2E accuracy, indicating seamless integrations with the backbone models.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "mobilebert",
                    "tasks",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three episodes</span>: As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we evaluate EMSServe by running three episodes of multimodal data arriving at different timestamps. Episode 1 includes one speech data, followed by ten continuous vitals data, and then ends up with ten continuous image data. Episode 1 echoes the typical data arrival sequence illustrated in the aforementioned Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Episodes 2 and 3 randomly shuffle the data sequence in episode 1 with two different random seeds. These three episodes collectively cover different scene data arrival times of multimodal data in real-world EMS events.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate EMSGlass&#8217;s usability and demonstrate its real-world end-to-end usage feasibility, we conducted a user study with 6 certified Emergency Medical Technicians (EMTs). Each EMT uses the full EMSGlass system (i.e., EMSNet and EMSServe) in a simulated emergency room illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. The user study process involves data perception and real-time decision-making in a simulated end-to-end EMS event, including protocol selection and medication prescription tasks. Following the user study, participants completed a 15-item Likert scale questionnaire (1 = strongly disagree, 5 = strongly agree) covering five dimensions:</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "emsnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Integration with communication and documentation systems.</span> Multiple participants identified integration with existing EMS tools (e.g., Pulsara for documentation, Zoll for vitals, incident command dashboards) as a key driver of real-world utility and adoption:</p>\n\n",
                "matched_terms": [
                    "integration",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS1\" title=\"3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, our multimodal data processor includes the following steps for vitals data:</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Padding: for the missed values in raw NEMSIS vitals, we simply add zero at the beginning of the vitals. Our experiments show this practice enables EMSNet to achive higher accuracy.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "emsnet",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.SS1.SSS2\" title=\"5.1.2. 3-modal EMSNet backbone accuracy on D2 &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the full details of evaluating the EMSNet on 3-modal dataset D2 (text, vitals, image).</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "text",
                    "emsnet",
                    "vitals",
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS1\" title=\"3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(a)-(e) are the room setups to collect the image dataset D4(image): 908 train set images are collected from (a)-(d), and 200 test set images from (e). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) is also the room setup used for the user study.</p>\n\n",
                "matched_terms": [
                    "image",
                    "dataset"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 5. End2End EMSFoundation model accuracy with different speech recognition (SR) and objection detection (OD) models. We use MobileBERT, GRU, and FC in the backbone model.",
        "body": "Task1: Protocol\nTask2: Med-type\nTask3: Med-quant\n\n\nSR\nOD\nTop-1/3/5\nTop-1/3-5\nmse/pearsonr/spearmanr\n\n\nMobileBERT\n\n\n\n\n\nTruth\n\n0.70/0.90/0.94\n0.54/0.94/0.96\n1.67/0.29/0.23\n\n\nWhisper-s\n\n0.70/0.92/0.98+0.00/+0.02/+0.04\n0.55/0.95/0.98+0.01/+0.01/+0.02\n1.66/0.28/0.29-0.01/-0.01/+0.06\n\n\nWhisper-m\n\n0.70/0.90/0.95+0.00/+0.00/-0.01\n0.56/0.95/0.98+0.02/+0.01/+0.02\n1.66/0.28/0.27-0.01/-0.01/+0.04\n\n\nMobileBERT-GRU\n\n\n\n\n\nTruth\n\n0.72/0.90/0.96\n 0.50/0.94/0.96\n1.66/0.32/0.22\n\n\nWhisper-s\n\n0.71/0.91/0.97-0.01/+0.01/+0.01\n0.51/0.94/0.98+0.01/+0.00/+0.02\n1.56/0.39/0.32-0.1/+0.07/+0.10\n\n\nWhisper-m\n\n0.71/0.90/0.96-0.01/+0.00/+0.00\n0.52/0.94/0.97+0.02/+0.00/+0.01\n1.56/0.39/0.30-0.1/+0.07/+0.08\n\n\nMobileBERT-GRU-FC\n\n\n\n\n\nTruth\n\n0.76/0.96/0.98\n0.58/0.92/0.98\n1.61/0.36/0.34\n\n\nWhisper-s\n\n0.74/0.97/1.00-0.02/+0.01/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n 1.60/0.36/0.33-0.01/+0.00/-0.01\n\n\nWhisper-m\nTruth\n0.73/0.98/1.00-0.03/+0.02/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n1.59/0.36/0.32-0.02/+0.00/-0.02\n\n\nMobileBERT-GRU-FC\n\n\n\n\n\nTruth\n\n0.76/0.96/0.98\n0.58/0.92/0.98\n1.61/0.36/0.34\n\n\nWhisper-s\n\n0.74/0.97/1.00-0.02/+0.01/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n1.60/0.36/0.33-0.01/+0.00/-0.01\n\n\nWhisper-m\nyolo11n\n0.73/0.98/1.00-0.03/+0.02/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n1.59/0.36/0.32-0.02/+0.00/-0.02\n\n\nMobileBERT-GRU-FC\n\n\n\n\n\nTruth\n\n0.76/0.96/0.98\n0.58/0.92/0.98\n1.61/0.36/0.34\n\n\nWhisper-s\n\n0.74/0.97/1.00-0.02/+0.01/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n1.60/0.36/0.33-0.01/+0.00/-0.01\n\n\nWhisper-m\nyolo11x\n0.73/0.98/1.00-0.03/+0.02/+0.02\n0.61/0.92/0.99+0.03/+0.00/+0.01\n1.59/0.36/0.32-0.02/+0.00/-0.02",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Task1: Protocol</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Task2: Med-type</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Task3: Med-quant</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">SR</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">OD</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Top-1/3/5</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Top-1/3-5</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">mse/pearsonr/spearmanr</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">MobileBERT</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.94/0.96</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.67/0.29/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-s</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.92/0.98<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.00/+0.02/+0.04</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.55/0.95/0.98<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.01/+0.01/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.66/0.28/0.29<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/-0.01/+0.06</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-m</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.90/0.95<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.00/+0.00/-0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.95/0.98<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.02/+0.01/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.66/0.28/0.27<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/-0.01/+0.04</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">MobileBERT-GRU</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.90/0.96</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.50/0.94/0.96</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.66/0.32/0.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-s</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.91/0.97<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/+0.01/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.51/0.94/0.98<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.01/+0.00/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.56/0.39/0.32<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.1/+0.07/+0.10</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-m</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.90/0.96<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/+0.00/+0.00</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.52/0.94/0.97<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.02/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.56/0.39/0.30<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.1/+0.07/+0.08</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">MobileBERT-GRU-FC</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.76/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.58/0.92/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.61/0.36/0.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-s</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.97/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.01/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 1.60/0.36/0.33<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/+0.00/-0.01</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-m</td>\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.98/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.03/+0.02/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.59/0.36/0.32<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.00/-0.02</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">MobileBERT-GRU-FC</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.76/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.58/0.92/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.61/0.36/0.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-s</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.97/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.01/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.60/0.36/0.33<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/+0.00/-0.01</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-m</td>\n<td class=\"ltx_td ltx_align_left\">yolo11n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.98/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.03/+0.02/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.59/0.36/0.32<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.00/-0.02</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">MobileBERT-GRU-FC</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Truth</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.76/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.58/0.92/0.98</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.61/0.36/0.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Whisper-s</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.74/0.97/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.01/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.60/0.36/0.33<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.01/+0.00/-0.01</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Whisper-m</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">yolo11x</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.98/1.00<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.03/+0.02/+0.02</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.92/0.99<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">+0.03/+0.00/+0.01</span></sup></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.59/0.36/0.32<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#000000;\">-0.02/+0.00/-0.02</span></sup></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "recognition",
            "emsfoundation",
            "speech",
            "whisperm",
            "mobilebert",
            "accuracy",
            "task2",
            "yolo11n",
            "different",
            "task1",
            "use",
            "truth",
            "mobilebertgru",
            "whispers",
            "top135",
            "detection",
            "msepearsonrspearmanr",
            "medquant",
            "end2end",
            "backbone",
            "gru",
            "mobilebertgrufc",
            "medtype",
            "task3",
            "models",
            "objection",
            "model",
            "yolo11x",
            "protocol"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T5\" title=\"Table 5 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows EMSNet&#8217;s end-to-end (E2E) accuracy on the 3-modal dataset D2, i.e., from the end of original 3-modal inputs to the end of tasks 1-3. Here we use our Whisper-s and Whisper-m for the speech recognition (SR) module, YOLO11n for the object detection (OD) module, and 3 models in the backbone: unimodal MobileBERT, 2-modal MobileBERT-GRU, and 3-modal MobileBERT-GRU-FC. We make two observations from the table: 1) the multimodal models, including the 2-modal and 3-modal models, achieve higher accuracy, 2) the addition of speech recognition and object detection models don&#8217;t degrade the E2E accuracy, indicating seamless integrations with the backbone models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "protocol",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mobile assistant systems for EMS have been evolving to harness both cloud and edge (e.g., smart glasses) resources for real-time cognitive assistance and protocol recommendations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite>. For instance, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> allow EMTs to wear smart glasses for capturing the voice of patient symptoms. They utilize domain-specific speech-to-text models (e.g., Whisper-tiny) to transcribe the speech and data-driven models (e.g., MobileBERT) to help select protocols. These models are trained in the cloud and deployed on hands-free smart glasses (e.g., Google Glass, Vuzix M4000) and edge servers to address challenges such as unreliable cloud connectivity in disaster scenarios while also enhancing user privacy. Despite these advancements, several clinical and technical challenges remain unresolved:</p>\n\n",
                "matched_terms": [
                    "mobilebert",
                    "models",
                    "speech",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #2: High-latency serving frameworks for multimodal models in EMS scenarios.</span> Existing multimodal model serving frameworks, such as PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite> and TensorFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tensorflow2016</span>)</cite> used in SOTA assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, assume simultaneous availability of all data modalities. However, in the EMS scenario shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, smart glasses receive different data modes asynchronously (i.e., asynchronous arrival times) as EMTs move through the scene. Without caching intermediate processing results for early arrived modes of data, directly using multimodal model serving frameworks has to repeatedly process early arrived voice data when later-arriving data like vitals and images become available, leading to redundant computations and higher latency. To achieve timely EMS delivery given the asynchronous arrival of multimodal data, a low-latency multimodal serving framework in EMS scenarios is essential.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> We build <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model trained on massive, real-world multimodal EMS datasets to simultaneously accomplish five critical EMS tasks: protocol selection, recommendation for medicine type, quantity, dosage, and disease history inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(2)</span> We develop <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, the first serving framework for multimodal models to address the challenges introduced by different data arrival times in EMS scenarios. With the adaptive edge-assisted offloading, EMSServe harnesses the computing resources on both the glass and edge servers. Comprehensive evaluations show EMSServe generally outperforms direct usage of PyTorch by 1.9&#215; &#8211; 11.7&#215; speedup across diverse edge devices (Google Glass) and edge servers.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The national EMS dataset (NEMSIS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite> mandates reporting pill and alcohol presence at EMS scenes due to their strong association with overdose and respiratory emergencies. Accurate detection of these objects can narrow protocol options (e.g., from over 100 to fewer than 5), yet EMS-specific object detectors remain underdeveloped. In parallel, accurate medication administration requires precise Med-Math (e.g., administering <math alttext=\"21mg\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>21</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">21mg</annotation></semantics></math> of Adrenaline from a <math alttext=\"4.2mg/ml\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><mn>4.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">4.2mg/ml</annotation></semantics></math> solution: <math alttext=\"\\frac{21mg}{4.2mg/ml}=5ml\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mfrac><mrow><mn>21</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mrow><mrow><mn>4.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></mfrac><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{21mg}{4.2mg/ml}=5ml</annotation></semantics></math>), but complex unit conversions and vendor-specific concentration increase cognitive load and delay critical care. Smart-glass cameras can automate medication recognition through OCR and barcode scanning, enabling instant extraction of drug names and concentrations, linking to patient history, and reducing errors. Despite its clinical importance, no prior work has focused on developing such integrated object detection and OCR pipelines for EMS.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "protocol",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
                "matched_terms": [
                    "protocol",
                    "model",
                    "backbone",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "use",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D3(audio)</span>: To train an accurate speech-to-text model, we use the open-sourced audio dataset (1123 for training and 600 for validation) from EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>. For testing, with 50 randomly sampled symptom texts from D2, five users recorded 50 audio samples using HyperX Solocast microphone&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HyperXSolocast2024</span>)</cite> and Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite>, giving a set of 500 audio samples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
                "matched_terms": [
                    "mobilebert",
                    "detection",
                    "models",
                    "accuracy",
                    "backbone",
                    "use",
                    "gru",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "backbone",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "accuracy",
                    "different",
                    "backbone",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce the manual effort required for data annotation and model training in developing an EMS-specific object detector, we assessed the SOTA open-set detection model Grounding DINO<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024groundingdinomarryingdino</span>)</cite> (GD) using prompts of varying difficulty for humans to come up with (See Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> (left): easy, medium, and hard).</p>\n\n",
                "matched_terms": [
                    "model",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> (right) shows the mean average precision (mAP) and recall performance for two GD models (Swin-T: 172m, Swin-B: 341m), where higher mAP and recall indicate better performance. The observed disparity between low mAP (&#161;0.2) and high recall (&#191;0.55) suggests that open-set object detection models like GD generate excessive false positives, limiting its suitability in generalizing EMS objects detection, where precision is crucial. Therefore, a dedicated EMS-specific detector remains necessary.</p>\n\n",
                "matched_terms": [
                    "models",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Close-set detection models (e.g., YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite>), in contrast to the aforementioned open-set Grounding Dino (GD) model, require manual dataset annotation and model training when generalized onto a new domain, e.g., EMS. To relieve the demanding annotation workloads, we propose <span class=\"ltx_text ltx_font_italic\">human-in-the-loop (HITL) annotation adjustment</span>, leveraging the open-set strength of the GD into the human annotation process. There are two rounds of annotations: in the first round, we feed unlabeled EMS scene images D4(image) to GD to get bounding box labels. In the second round, instead of annotating from scratch, we adjust GD&#8217;s annotations from the first round (e.g., relabel GD&#8217;s incorrect annotations). After two rounds, we fine-tune the close-set YOLO11. When compared to conventional manual annotation without GD&#8217;s auto-labeling process, our HITL annotation adjustment saves the annotation time by exploiting the correct annotations from GD and leaves the main manual efforts in GD&#8217;s incorrect or missed annotations. From our experiences, this HITL annotation adjustment method decreases the annotation time by half, e.g., on average, from 10 to 5 minutes for every 100 images.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
                "matched_terms": [
                    "gru",
                    "models",
                    "mobilebert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "yolo11n",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on our insights, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> abstracts EMSServe, the first serving framework for multimodal model inference in EMS. EMSServe comprises three key components:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> a modality-aware splitter that decomposes multimodal models into single-mode modules,  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> model inference time profiling to measure inference latency on the smart glasses and the edge server, and  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic3\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> adaptive offloading with feature caching to minimize latency in dynamic EMS scenarios. While we evaluate EMSGlass on PH1 and Edge-64X in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for generality, our primary setup uses Google Glass as the mobile device and a manpack-mounted Edge-4C as the edge server.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T3\" title=\"Table 3 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we compare the accuracy of the multimodal backbone with SOTA unimodal options in accomplishing tasks 1-3 on the 2-modal dataset D1. The P, M, and Q mean the backbone is trained to separately accomplish a single Task 1, 2, and 3. P-M means the backbone is trained to accomplish two tasks 1 and 2 simultaneously while P-M-Q means accomplishing three tasks at the same time. The lower mse and higher pearsonr/spearsonr indicate better performance in task3. As demonstrated, our multimodal backbone consistently outperforms unimodal options adopted in SOTA assistant systems. Specifically, it improves the top-1/3/5 accuracy for task2 by 2-3%, reduces the MSE by <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.1, and enhances Pearsonr and Spearmanr for Task 3, which are newly proposed EMS tasks in this work.</p>\n\n",
                "matched_terms": [
                    "task3",
                    "accuracy",
                    "task2",
                    "backbone",
                    "top135"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "backbone",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, with PMI-enabled fine-tuning, 3-modal EMSNet achieves the consistently higher accuracy in recommending protocols (task1) and medicine types (task2). For example, the PMI-enabled BERTBase-LSTM-FC and BERTBase-GRU-FC achieve top-3 accuracy of 0.91 and 0.96, respectively, on the single protocol (P) and medicine type (M) tasks. For the medicine quantity recommendation task (Q and P-M-Q), although fine-tuning w/o PMI seems more performant, fine-tuning w/ PMI achieves comparable performance. For example, fine-tuning with PMI enables TinyBERT-LSTM-FC to achieve the MSE of 1.89 on the single Q task, close to the lowest MSE of 1.87. This also implies a future work direction on how PMI could further improve the regression accuracy task, like medicine quantity prescription in this paper.</p>\n\n",
                "matched_terms": [
                    "task1",
                    "accuracy",
                    "task2",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "mobilebert",
                    "models",
                    "accuracy",
                    "different",
                    "mobilebertgrufc",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> demonstrates EMSWhisper&#8217;s superior transcription accuracy and robustness. Our Whisper-s and Whisper-m achieve significantly lower WER and CER than three smaller SOTA speech-to-text models across validation and test sets. For instance, on user5&#8217;s Google Glass (GG) test set, Whisper-m attains a WER(CER) of 0.056(0.027), whereas Whisper-t&#8217;s WER(CER) rises to 0.315(0.242), about 5.6(9) times higher. As a WER of 0.1 is generally regarded as the error standard for usable voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MSTestAccMSLearn2024</span>)</cite>, SOTA models struggle with data distribution shifts, with the test set WER exceeding 0.1 due to varying accents and microphone hardware. This challenge is even more critical in EMS settings, where stricter WER requirements render SOTA models impractical for EMS scenarios, further accrediting EMSWhisper&#8217;s ultra-low error rates. Moreover, EMSWhisper, including the Whisper-s and Whisper-m motivated by the scaling law, exhibits exceptional generalization with lower WER variance across users (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)-(b)) and microphones(Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(c)-(d)), underscoring its robustness to real-world distribution shifts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "accuracy",
                    "whispers",
                    "whisperm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F12\" title=\"Figure 12 &#8227; 5.1.4. Accuracy of Objection Detection &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> compares the object detection performance of fine-tuning YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite> on the image dataset D4(image) labeled by Grounding Dino and labeled by our human-in-the-loop (HITL) annotation adjustment. Our HITL annotation strategy outperforms Grounding Dino&#8217;s annotations, bringing the test mAP value close to 0.8. In contrast, the mAP of all YOLO11 models on images annotated with Grounding Dino is below 0.6, which is unacceptable. The high mAP on the validate set but low mAP and recall on the test set means fine-tuning YOLO11 with Grounding Dino&#8217;s annotations causes the overfitting during the fine-tuning process. For example, using Grounding Dino with the hard text prompts from Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> can achieve an mAP above 0.8, but the test mAP of all YOLO11 models is nearly 0. The recall metric is similar: high validate recall while much lower test recall. This results from Grounding Dino&#8217;s high false positive predictions as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2\" title=\"2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our HITL avoids this problem because we add the human adjustments to correct Grounding Dino&#8217;s false positive annotations, enabling much higher test mAP and recall. It&#8217;s important to note that when compared to direct manual annotations,\nthe advantage of our HITL is that we exploit Grounding Dino&#8217;s accurate (i.e., true positives) annotations to save human labor time on annotations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the OCR module in EMSGlass, we evaluate an image dataset of size 204 captured by Google Glass&#8217;s 8MP camera, featuring a user holding a labeled medicine bottle at three distances&#8212;full arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.6m), half arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> 0.3m), and quarter arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.15m)&#8212;to account for varying EMT arm lengths. Each image has two versions: the original image without cropping and the cropped bottle segment. We use the word error rate (WER) and character error rate (CER) to measure OCR accuracy. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, EMSGlass OCR performance was evaluated using four state-of-the-art (SOTA) models listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: EasyOCR, TesseractOCR, PaddleOCR, CRNN. In general, EasyOCR achieves the lowest WER and CER. Our edit distance (ED)-based matching discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS4\" title=\"3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> is applied after each OCR prediction. As shown, the OCR models with ED-match can achieve significantly decreased WER and CER, indicating the effectiveness of our proposed ED-match method. ED-match helps EasyOCR to decrease WER and CER by 89% and 83%, respectively. EMSGlass employs Easy-Match as its OCR framework, which consistently outperforms other models with WER (CER) below 0.12 (0.05) across all arm distance and cropping conditions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "accuracy",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the barcode scanner, we only use the original image without cropping. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> (right), we use the success rate as the metric to measure the performance of barcode scanner (developed based on ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> in Android). At the 1/4 Arm, our scanner can achieve a 100% accuracy, indicating its effectiveness. On average, it takes less than 0.5 seconds to process one image, ensuring minimal latency.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EMSServe is the first multimodal model serving framework addressing the issue of asynchronous arrival times, which is inherently introduced by using smart glasses in multimodal EMS scenarios with high mobility. Here, we make the following evaluation plan to show EMSServe&#8217;s advantage in serving such asynchronously arrived multimodal data. Note that, when evaluating EMSServe, the BERTBase-GRU-FC backbone is used with Whisper-tiny and YOLO11n as the multimodal EMSNet model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "backbone",
                    "yolo11n"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three episodes</span>: As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we evaluate EMSServe by running three episodes of multimodal data arriving at different timestamps. Episode 1 includes one speech data, followed by ten continuous vitals data, and then ends up with ten continuous image data. Episode 1 echoes the typical data arrival sequence illustrated in the aforementioned Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Episodes 2 and 3 randomly shuffle the data sequence in episode 1 with two different random seeds. These three episodes collectively cover different scene data arrival times of multimodal data in real-world EMS events.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #2: offloading without user mobility</span>. When compared to Google Glass, Edge-4C&#8217;s lower inference latency illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> expose opportunities to offload compute-expensive inference workloads for the benefit of faster inference. We use episode 1 to evaluate the impact of non-line-of-sight (NLOS) distances in inference workloads offloading from Google Glass to Edge-4C, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. We put the Google Glass at static distances (0-30 meters) to the Edge-4C in a building. This scenario #2 reflects a typical EMS realism, where the EMTs wearing smart glasses and the EMTs carrying the manpack collaboratively search for patients and perform EMS interventions in different indoor rooms. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(a) provides the evaluation results, where 5m indicates an NLOS room between Google Glass and Edge-4C while 30m indicates 6 rooms. As we increase the distances, offloading the symptom speech or vitals produces similar cumulative inference latencies while offloading the 10 images tells the essence of offloading in EMS scenarios: it&#8217;s more advantageous to offload images to Edge-4C when the distances are long(e.g., &#191;5m). This is mainly because the size of images used in the episode is relatively large, making offloading images sensitive to the glass-edge distances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "use",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel2\" title=\"Figure 16 &#8227; Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A4.SS2\" title=\"D.2. EMT user study assessment &#8227; Appendix D User study process and quantitative scores &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a> presents detailed scores. Overall, EMSGlass achieved consistently high ratings across all five evaluation dimensions, demonstrating its strong usability and technical reliability in simulated end-to-end EMS scenarios. Participants rated usability and interaction positively, with average scores between 4.0 and 4.3, indicating that the system was easy to operate during time-sensitive tasks. Multimodal perception accuracy emerged as a particular strength: voice transcription, pill and alcohol detection, and especially medicine label recognition achieved mean scores between 4.3 and 5, reflecting highly reliable system performance. The clarity and correctness of system recommendations throughout the scenario also received favorable ratings (average above 4.0), confirming that participants were able to follow and trust EMSGlass&#8217;s evolving guidance during emergency response tasks. Taken together, these quantitative results highlight EMSGlass&#8217;s potential to deliver accurate, real-time, and user-friendly decision support for emergency medical care.</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "accuracy",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">&#8220;Every area has different meds. EMTs and local doctors control what&#8217;s available &#8212; one size won&#8217;t fit all.&#8221; (User2) This points to the need for flexible, user-editable protocol and medication support for widespread adoption.</p>\n\n",
                "matched_terms": [
                    "protocol",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">5) Adoption drivers: accuracy, speed, reliability.</span> Across interviews, participants consistently identified accuracy, real-time speed, and reliability as the three core adoption drivers. Medication recognition accuracy was viewed positively, but slow scene processing and occasional recommendation mismatches were cited as the direction for hardware and software improvements in the future:</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "emsfoundation",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-sample normalization: NEMSIS dictates different scale ranges for different vitals, e.g., pulse oximetry (PO) in [0, 100] while blood glucose in [0, 2000]. These large numerical values produces the notorious &#8220;NaN&#8221; problems during training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>]</cite>. In addition, different scales of vitals values prevent deep learning models to effectively combine the information from different vitals. To address this problem, we adopt three common normalization methods: z-score,min-max and min-max over z-score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rahmad2024comparativenorm</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "task1",
                    "task3",
                    "task2",
                    "protocol"
                ]
            }
        ]
    },
    "S5.T6": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 6. Three episodes contain three asynchronously arrived multimodal data sequences, reflecting different arrival times of real-world EMS scene data.",
        "body": "Episode\nAsynchronously arrived multimodal data sequence (S-Speech, V-Vital, I-Image)\n\n\n1\nS\nV\nV\nV\nV\nV\nV\nV\nV\nV\nV\nI\nI\nI\nI\nI\nI\nI\nI\nI\nI\n\n\n2\nI\nV\nI\nV\nI\nV\nI\nS\nV\nI\nV\nI\nI\nV\nV\nI\nV\nV\nI\nV\nI\n\n\n3\nV\nV\nV\nV\nV\nV\nI\nI\nI\nI\nI\nI\nV\nI\nV\nV\nI\nI\nS\nV\nI",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Episode</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"21\">Asynchronously arrived multimodal data sequence (S-Speech, V-Vital, I-Image)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFCCC9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFCCC9;\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFCCC9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFCCC9;\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">3</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#FFCCC9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFCCC9;\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#ECF4FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF4FF;\">V</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#D9D2E9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D9D2E9;\">I</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sequence",
            "data",
            "iimage",
            "vvital",
            "realworld",
            "sequences",
            "asynchronously",
            "scene",
            "different",
            "contain",
            "multimodal",
            "episodes",
            "episode",
            "ems",
            "times",
            "sspeech",
            "reflecting",
            "arrival",
            "three",
            "arrived"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three episodes</span>: As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we evaluate EMSServe by running three episodes of multimodal data arriving at different timestamps. Episode 1 includes one speech data, followed by ten continuous vitals data, and then ends up with ten continuous image data. Episode 1 echoes the typical data arrival sequence illustrated in the aforementioned Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Episodes 2 and 3 randomly shuffle the data sequence in episode 1 with two different random seeds. These three episodes collectively cover different scene data arrival times of multimodal data in real-world EMS events.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems",
                    "multimodal",
                    "realworld",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "sequence",
                    "ems",
                    "data",
                    "different",
                    "arrived",
                    "multimodal",
                    "asynchronously"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "multimodal",
                    "data",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #2: High-latency serving frameworks for multimodal models in EMS scenarios.</span> Existing multimodal model serving frameworks, such as PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite> and TensorFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tensorflow2016</span>)</cite> used in SOTA assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, assume simultaneous availability of all data modalities. However, in the EMS scenario shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, smart glasses receive different data modes asynchronously (i.e., asynchronous arrival times) as EMTs move through the scene. Without caching intermediate processing results for early arrived modes of data, directly using multimodal model serving frameworks has to repeatedly process early arrived voice data when later-arriving data like vitals and images become available, leading to redundant computations and higher latency. To achieve timely EMS delivery given the asynchronous arrival of multimodal data, a low-latency multimodal serving framework in EMS scenarios is essential.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems",
                    "times",
                    "data",
                    "different",
                    "arrived",
                    "multimodal",
                    "asynchronously",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #3: Lacking effective user interaction designs and real-world user studies.</span>\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, while EMTs move dynamically through emergency scenes, smart glasses continuously receive asynchronous data streams. For each newly arrived data, EMTs expect prompt and reliable recommendation updates to support time-critical decisions. However, the user interface designs in existing EMS assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> fail to account for either EMT mobility or the variability of data arrival times in practice. Although the behavior tree assistant &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite> supports continuous recommendation updates, its laptop-based interface restricts hands-free operation and ignores the stringent resource constraints of smart glasses. Furthermore, prior works lack real-world user studies with EMTs&#8217; end-to-end usage, leaving their effectiveness and usability of interaction designs in real-world EMS scenarios largely unexamined.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "arrived",
                    "realworld",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we present EMSGlass, the first multimodal multitask model-enabled smart glasses system for EMS, with the following threefold contributions:</p>\n\n",
                "matched_terms": [
                    "ems",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> We build <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model trained on massive, real-world multimodal EMS datasets to simultaneously accomplish five critical EMS tasks: protocol selection, recommendation for medicine type, quantity, dosage, and disease history inference.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "multimodal",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(2)</span> We develop <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, the first serving framework for multimodal models to address the challenges introduced by different data arrival times in EMS scenarios. With the adaptive edge-assisted offloading, EMSServe harnesses the computing resources on both the glass and edge servers. Comprehensive evaluations show EMSServe generally outperforms direct usage of PyTorch by 1.9&#215; &#8211; 11.7&#215; speedup across diverse edge devices (Google Glass) and edge servers.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "different",
                    "multimodal",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">visionFoundation2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DISTMM2024NSDI</span>)</cite> are designed to understand and fuse complementary information from multiple sources. These models rely on one or more deep learning submodules to process and integrate data effectively. They aim to generate intermediate unified numeric representations from multimodal inputs for multiple simultaneous tasks across diverse domains, e.g., general healthcare&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2020fusion</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lian2024npj</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2019early</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharazmi2018feature</span>)</cite>. However, few has explored the feasibility and advantage of such multimodal multitask capability in the EMS domain.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "data",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the release of Google Glass Explorer Edition as a ubiquitous computing platform&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleGlassExplorer2012</span>)</cite>, smart glasses have been integrated into various medical applications, including surgical assistance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pedisurgery2014</span>)</cite>, eating behavior monitoring&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eating2015PervasiveHealth</span>)</cite>, physiological vitals sensing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SpiderNie2020</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SPIDERSNie2021</span>)</cite>, and cognitive state and ocular health tracking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BlinkMental2025</span>)</cite>. Despite their promising hands-free interaction and heads-up display capabilities, the potential of on-glass assistants to enhance EMS delivery remains largely underexplored. Recent systems such as EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> have made initial attempts to assist EMTs with protocol selection on smart glasses. However, they either lack multimodal multitasking capabilities or comprehensive real-world user studies, leaving a critical gap between technological advancement and practical adoption in EMS.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "multimodal",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "three",
                    "ems",
                    "contain",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F3\" title=\"Figure 3 &#8227; 3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the data processing pipeline used to prepare four multimodal datasets (D1-D4) to train and test EMSNet. This pipeline is designed as a general plug-n-play tool for preparing multimodal datasets from NEMSIS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>, a publicly available (upon request) key-value tabular database that includes all real-world national emergency event reports in the US. Notably, <span class=\"ltx_text ltx_font_italic\">our data processor is the first tool of its kind to prepare multimodal EMS datasets</span>. The NEMSIS database uses a unique 9-digit &#8220;PCR key&#8221; as each emergency event ID. In this paper, we utilize the NEMSIS 2023 database, which includes over 54 million EMS events recorded by 14,369 EMS agencies across 54 states and territories in 2023.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "multimodal",
                    "data",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "contain",
                    "data",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D2(3-modal: text, vitals,scene)</span>: Similarly, we extract D2&#8217;s text and vitals with D2&#8217;s PCR Keys, a disjoint set of D1 corresponding to events with 3-modal input: text, vitals, and scene data. We also extract the EMS scene information indicating the existence of alcohol and pills. The scene information is one-hot encoded, giving a 3-modal dataset D2(text, vitals, scene) with 3,005 samples.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "three",
                    "data",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "multimodal",
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "ems",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These challenges stem from data distribution shift&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HwangDistributionECCV2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChipDataDistributionOnline2024</span>)</cite>, where real-world inputs&#8212;affected by diverse accents and microphones&#8212;differ from training data, yet the labels remain unchanged. The root cause is the small size of existing models, all under 100 million parameters (EMSConformer: 10m, Whisper-tiny: 74m, Whisper-base: 74m), limiting both learning and generalization. Empirical evidence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaplan2020scalinglawsneurallanguage</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">alabdulmohsin2022revisitingscalinglaws</span>)</cite> supports this approach, showing that larger models achieve lower training loss. Specifically, we propose training Whisper-small (242m) and Whisper-medium (764m) on dataset D3(audio) to enhance generalization (as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Here we select one order of magnitude higher than the SOTA to demonstrate the effectiveness of EMSWhisper&#8217;s idea in enhancing generalization capability. Further explorations of alternative scaling options could be considered future work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Close-set detection models (e.g., YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite>), in contrast to the aforementioned open-set Grounding Dino (GD) model, require manual dataset annotation and model training when generalized onto a new domain, e.g., EMS. To relieve the demanding annotation workloads, we propose <span class=\"ltx_text ltx_font_italic\">human-in-the-loop (HITL) annotation adjustment</span>, leveraging the open-set strength of the GD into the human annotation process. There are two rounds of annotations: in the first round, we feed unlabeled EMS scene images D4(image) to GD to get bounding box labels. In the second round, instead of annotating from scratch, we adjust GD&#8217;s annotations from the first round (e.g., relabel GD&#8217;s incorrect annotations). After two rounds, we fine-tune the close-set YOLO11. When compared to conventional manual annotation without GD&#8217;s auto-labeling process, our HITL annotation adjustment saves the annotation time by exploiting the correct annotations from GD and leaves the main manual efforts in GD&#8217;s incorrect or missed annotations. From our experiences, this HITL annotation adjustment method decreases the annotation time by half, e.g., on average, from 10 to 5 minutes for every 100 images.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "different",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "different",
                    "multimodal",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key idea of EMSServe&#8211;feature cache</span>: To eliminate redundant text submodule inference, EMSServe employs feature cache (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, right). When voice data arrives, instead of only running the text model, we simultaneously compute and cache the text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> in the text-vital model. When vitals1 arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, unlike conventional frameworks that rerun the costly text submodule, EMSServe reuses the cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and processes only the vitals module (vitals encoder) to obtain <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math>, which significantly reduces inference costs. The cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and newly computed <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math> are then concatenated as inputs to model headers. By addressing asynchronous EMS data arrival times, EMSServe mitigates redundant and costly submodule computation to improve inference efficiency.</p>\n\n",
                "matched_terms": [
                    "arrival",
                    "ems",
                    "times",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on our insights, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> abstracts EMSServe, the first serving framework for multimodal model inference in EMS. EMSServe comprises three key components:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> a modality-aware splitter that decomposes multimodal models into single-mode modules,  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> model inference time profiling to measure inference latency on the smart glasses and the edge server, and  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic3\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> adaptive offloading with feature caching to minimize latency in dynamic EMS scenarios. While we evaluate EMSGlass on PH1 and Edge-64X in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for generality, our primary setup uses Google Glass as the mobile device and a manpack-mounted Edge-4C as the edge server.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "three",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the modality-aware splitter decomposes the multimodal model <math alttext=\"M2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">M2</annotation></semantics></math> (<math alttext=\"M3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">M3</annotation></semantics></math>) into single-mode modules <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math> (<math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math>, and <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>), enabling precomputation and cache of the text module output features <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> (<math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math>) before vitals data arrive.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After splitting, we profile all models and modules to obtain their inference times <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> on Google Glass and Edge-4C. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the profiling results. Both modality-aware splitting and profiling are one-time offline efforts, providing <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> as inputs for EMSServe&#8217;s real-time multimodal request serving.</p>\n\n",
                "matched_terms": [
                    "times",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates how EMSServe dynamically serves multimodal inference in an EMS scenario where symptom speech, vitals, and image data arrive asynchronously. The EMT wearing Google Glass and the EMT carrying the manpack-mounted Edge-4C move independently, causing bandwidth (BW) fluctuations in Glass-edge communication. To monitor this variation in real-time, we implement a lightweight heartbeat monitor on the smart glasses, periodically (e.g., every second) measuring the file transmission bandwidth <math alttext=\"\\Delta t=BW*filesize\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>W</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta t=BW*filesize</annotation></semantics></math>. Unlike RTT, <math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> represents the actual file transfer time. Based on this measurement, EMSServe optimizes offloading decisions:</p>\n\n",
                "matched_terms": [
                    "asynchronously",
                    "multimodal",
                    "data",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make these decisions efficiently in real-world EMS scenarios with high mobility, EMSServe employs adaptive offloading with a feature cache, executed in three key steps:</p>\n\n",
                "matched_terms": [
                    "ems",
                    "three",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "arrival",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seamless UI design is crucial for EMTs to adopt networked smart glasses, given their mobility and resource constraints (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> illustrates the on-display UI: the left screen continuously updates recommendations for EMS tasks while tapping the frame (right bottom) triggers EMSGlass to capture EMTs&#8217; symptoms speech, patients&#8217; vitals, and scene images. Synchronously arrived data is offloaded adaptively (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.SS2.SSS3\" title=\"4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2.3</span></a>) via WiFi to the Edge-4C server carried in a manpack, ensuring low-latency inference and real-time recommendation on-display updates in high mobility EMS scenarios.\nEMSGlass is implemented as an Android app on Google Glass Enterprise Edition 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">glassSpecs2023</span>)</cite>, comprising over 3,000 lines of Java. EMSServe, built on Torch Serve&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">torchserve</span>)</cite>, runs on Edge-4C with &#160;2,000 lines of Python. HTTPS communication enables multimodal inference offloading, ensuring efficient processing despite smart glasses&#8217; resource limitations.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems",
                    "data",
                    "arrived",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T3\" title=\"Table 3 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we compare the accuracy of the multimodal backbone with SOTA unimodal options in accomplishing tasks 1-3 on the 2-modal dataset D1. The P, M, and Q mean the backbone is trained to separately accomplish a single Task 1, 2, and 3. P-M means the backbone is trained to accomplish two tasks 1 and 2 simultaneously while P-M-Q means accomplishing three tasks at the same time. The lower mse and higher pearsonr/spearsonr indicate better performance in task3. As demonstrated, our multimodal backbone consistently outperforms unimodal options adopted in SOTA assistant systems. Specifically, it improves the top-1/3/5 accuracy for task2 by 2-3%, reduces the MSE by <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.1, and enhances Pearsonr and Spearmanr for Task 3, which are newly proposed EMS tasks in this work.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "three",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "three",
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> demonstrates EMSWhisper&#8217;s superior transcription accuracy and robustness. Our Whisper-s and Whisper-m achieve significantly lower WER and CER than three smaller SOTA speech-to-text models across validation and test sets. For instance, on user5&#8217;s Google Glass (GG) test set, Whisper-m attains a WER(CER) of 0.056(0.027), whereas Whisper-t&#8217;s WER(CER) rises to 0.315(0.242), about 5.6(9) times higher. As a WER of 0.1 is generally regarded as the error standard for usable voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MSTestAccMSLearn2024</span>)</cite>, SOTA models struggle with data distribution shifts, with the test set WER exceeding 0.1 due to varying accents and microphone hardware. This challenge is even more critical in EMS settings, where stricter WER requirements render SOTA models impractical for EMS scenarios, further accrediting EMSWhisper&#8217;s ultra-low error rates. Moreover, EMSWhisper, including the Whisper-s and Whisper-m motivated by the scaling law, exhibits exceptional generalization with lower WER variance across users (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)-(b)) and microphones(Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(c)-(d)), underscoring its robustness to real-world distribution shifts.</p>\n\n",
                "matched_terms": [
                    "three",
                    "ems",
                    "times",
                    "data",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EMSServe is the first multimodal model serving framework addressing the issue of asynchronous arrival times, which is inherently introduced by using smart glasses in multimodal EMS scenarios with high mobility. Here, we make the following evaluation plan to show EMSServe&#8217;s advantage in serving such asynchronously arrived multimodal data. Note that, when evaluating EMSServe, the BERTBase-GRU-FC backbone is used with Whisper-tiny and YOLO11n as the multimodal EMSNet model.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "arrived",
                    "multimodal",
                    "asynchronously",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three scenarios</span>: We run episodes in the following three EMS scenarios and measure the cumulative time spent in inference. To avoid measurement interferences like cold-start, we perform all experiments 15 times, and take the average results from the last 10 runs. Scenarios 2) and 3) below employ the feature cache in EMSServe by default.</p>\n\n",
                "matched_terms": [
                    "three",
                    "episodes",
                    "ems",
                    "times"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #2: offloading without user mobility</span>. When compared to Google Glass, Edge-4C&#8217;s lower inference latency illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> expose opportunities to offload compute-expensive inference workloads for the benefit of faster inference. We use episode 1 to evaluate the impact of non-line-of-sight (NLOS) distances in inference workloads offloading from Google Glass to Edge-4C, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. We put the Google Glass at static distances (0-30 meters) to the Edge-4C in a building. This scenario #2 reflects a typical EMS realism, where the EMTs wearing smart glasses and the EMTs carrying the manpack collaboratively search for patients and perform EMS interventions in different indoor rooms. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(a) provides the evaluation results, where 5m indicates an NLOS room between Google Glass and Edge-4C while 30m indicates 6 rooms. As we increase the distances, offloading the symptom speech or vitals produces similar cumulative inference latencies while offloading the 10 images tells the essence of offloading in EMS scenarios: it&#8217;s more advantageous to offload images to Edge-4C when the distances are long(e.g., &#191;5m). This is mainly because the size of images used in the episode is relatively large, making offloading images sensitive to the glass-edge distances.</p>\n\n",
                "matched_terms": [
                    "episode",
                    "ems",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate EMSGlass&#8217;s usability and demonstrate its real-world end-to-end usage feasibility, we conducted a user study with 6 certified Emergency Medical Technicians (EMTs). Each EMT uses the full EMSGlass system (i.e., EMSNet and EMSServe) in a simulated emergency room illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. The user study process involves data perception and real-time decision-making in a simulated end-to-end EMS event, including protocol selection and medication prescription tasks. Following the user study, participants completed a 15-item Likert scale questionnaire (1 = strongly disagree, 5 = strongly agree) covering five dimensions:</p>\n\n",
                "matched_terms": [
                    "ems",
                    "data",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel2\" title=\"Figure 16 &#8227; Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A4.SS2\" title=\"D.2. EMT user study assessment &#8227; Appendix D User study process and quantitative scores &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a> presents detailed scores. Overall, EMSGlass achieved consistently high ratings across all five evaluation dimensions, demonstrating its strong usability and technical reliability in simulated end-to-end EMS scenarios. Participants rated usability and interaction positively, with average scores between 4.0 and 4.3, indicating that the system was easy to operate during time-sensitive tasks. Multimodal perception accuracy emerged as a particular strength: voice transcription, pill and alcohol detection, and especially medicine label recognition achieved mean scores between 4.3 and 5, reflecting highly reliable system performance. The clarity and correctness of system recommendations throughout the scenario also received favorable ratings (average above 4.0), confirming that participants were able to follow and trust EMSGlass&#8217;s evolving guidance during emergency response tasks. Taken together, these quantitative results highlight EMSGlass&#8217;s potential to deliver accurate, real-time, and user-friendly decision support for emergency medical care.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "ems",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2) Integration with communication and documentation systems.</span> Multiple participants identified integration with existing EMS tools (e.g., Pulsara for documentation, Zoll for vitals, incident command dashboards) as a key driver of real-world utility and adoption:</p>\n\n",
                "matched_terms": [
                    "ems",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">&#8220;If it could fill out Pulsara automatically, it&#8217;d save a lot of time during handover.&#8221; (User2) &#8220;Streaming scene video to incident command would help coordinate resources during big events.&#8221; (User1) These insights highlight EMSGlass&#8217;s potential to reduce communication overhead and automate cumbersome documentation, which are both critical pain points in EMS operations.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">5) Adoption drivers: accuracy, speed, reliability.</span> Across interviews, participants consistently identified accuracy, real-time speed, and reliability as the three core adoption drivers. Medication recognition accuracy was viewed positively, but slow scene processing and occasional recommendation mismatches were cited as the direction for hardware and software improvements in the future:</p>\n\n",
                "matched_terms": [
                    "scene",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "times",
                    "data",
                    "multimodal",
                    "realworld",
                    "arrival"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS1\" title=\"3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, our multimodal data processor includes the following steps for vitals data:</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Outlier removal: due to unattended mistakes during recording the patient vitals in real-world EMS events, raw vitals data in NEMSIS often contain default maximum or minimum values, e.g., heart rate at 500 per minute. To avoid the influence of these unrealistic extreme vitals, e.g., maximum default values, we apply the 2nd and 98th percentile clipping, removing the vitals outside the 2%-98% percentile range.</p>\n\n",
                "matched_terms": [
                    "ems",
                    "contain",
                    "data",
                    "realworld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-sample normalization: NEMSIS dictates different scale ranges for different vitals, e.g., pulse oximetry (PO) in [0, 100] while blood glucose in [0, 2000]. These large numerical values produces the notorious &#8220;NaN&#8221; problems during training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>]</cite>. In addition, different scales of vitals values prevent deep learning models to effectively combine the information from different vitals. To address this problem, we adopt three common normalization methods: z-score,min-max and min-max over z-score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rahmad2024comparativenorm</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "three",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "contain",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before each user study, each EMT participant signed a consent form and received a brief orientation to the EMSGlass system and Google Glass hardware. The study began with a short hands-on tutorial introducing key hardware components (display prism, camera, microphone, and touchpad frame), and training participants to operate EMSGlass through simple tap and voice interactions. Participants were then guided through a demonstration scenario showing how EMSGlass transcribes symptoms, displays real-time vitals, detects scene objects (e.g., pills, alcohol bottles), and recommends corresponding EMS protocols, medicines, and dosages. Following the demonstration, participants performed a full simulation independently, interacting with the app to assess a manikin patient by describing symptoms, observing dynamic updates in vitals and recommendations, and completing the scenario by scanning medication labels. Each participant could repeat dry runs until comfortable with the workflow. This structured process ensured that all participants understood EMSGlass&#8217;s multimodal functionalities before proceeding to the formal evaluation.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "ems",
                    "multimodal"
                ]
            }
        ]
    },
    "A2.T7": {
        "source_file": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "caption": "Table 7. Compare EMSGlasss multimodal models with SOTA unimodal models in tasks 1-3 on the dataset D2 (text, vitals, image).",
        "body": "Backbone(Text/Vitals/Image)\nTask1-Protocol Selection (top-1/3/5)\nTask2-Medicine Type Prescription (top-1/3/5)\nTask3-Medicine Quant. Prescription (mse/pearsonr/spearmanr)\n\n\n\n\nT\nV\nI\nP\nP-M\nP-Q\nP-M-Q\nM\nP-M\nM-Q\nP-M-Q\nQ\nP-Q\nM-Q\nP-M-Q\n\n\n\n\n\nLSTM\n\n0.09/0.40/0.67\n0.09/0.24/0.59\n0.10/0.23/0.48\n0.09/0.24/0.51\n0.42/0.66/0.85\n0.39/0.60/0.86\n0.42/0.60/0.83\n0.39/0.51/0.82\n1.94/0.14/0.11\n 1.96/0.17/0.12\n1.95/0.13/0.10\n1.97/0.17/0.10\n\n\n\n\n\nRNN\n\n0.20/0.51/0.67\n0.14/0.46/0.61\n0.12/0.40/0.57\n0.11/0.40/0.57\n0.41/0.71/0.84\n0.40/0.72/0.84\n0.42/0.70/0.85\n0.39/0.69/0.84\n1.90/0.16/0.12\n1.93/0.20/0.14\n1.95/0.13/0.12\n1.97/0.17/0.13\n\n\n\n\n\nGRU\n\n 0.48/0.68/0.74\n0.43/0.66/0.75\n0.43/0.66/0.75\n0.11/0.38/0.61\n0.43/0.92/0.94\n0.45/0.91/0.95\n0.45/0.92/0.95\n0.39/0.68/0.84\n1.93/0.03/0.00\n2.03/0.06/0.01\n1.95/-0.05/-0.02\n1.98/0.16/0.12\n\n\n\n\nBERTBase\n\n\n0.72/0.93/0.96\n0.69/0.92/0.97\n0.70/0.90/0.96\n0.71 /0.92/0.96\n\n0.67/0.94/0.98\n0.69/0.95/0.97\n0.69/0.94/0.98\n0.68/0.95/0.98\n 1.94/0.01/-0.02\n1.92 /0.26/0.27\n1.95/0.24/0.22\n1.93/0.25/0.25\n\n\n\n\nTinyBERT\n\n\n0.72/0.92/0.96\n0.69/0.91/0.96\n0.69/0.91/0.96\n0.69/0.89/0.96\n0.67/0.96/0.98\n0.66/0.95/0.98\n0.65/0.96/0.98\n0.66/0.95/0.98\n1.96/0.13/0.17\n1.96/0.21/0.23\n1.95/0.17/0.21\n1.91/0.24/0.27\n\n\n\n\nMobileBERT\n\n\n 0.73/0.93/0.97\n0.73/0.91/0.96\n0.66/0.86/0.93\n0.66/0.90/0.95\n0.67/0.95/0.98\n0.69/0.95/0.97\n0.67/0.96/0.98\n0.68/0.95/0.98\n1.91/0.17/0.22\n1.99/0.16/0.22\n1.89 /0.20/0.24\n\n1.94/0.21/0.25\n\n\n\n\nUnimodal (SOTA)\n\n\n\n\nNo FT\n\n\n\n\nFC\n0.00/0.02/0.02\n0.00/0.01/0.01\n0.00/0.01/0.01\n0.00/0.01/0.01\n0.00/0.03/0.05\n0.01/0.41/0.48\n0.00/0.03/0.05\n 0.01/0.41/0.48\n2.26/0.10/0.12\n2.10/0.16/0.16\n1.97/-0.11/-0.12\n2.08/0.16/0.16\n\n\n\n\n\nLSTM\n\n0.71/0.92/0.97\n0.66/0.90/0.95\n0.10/0.27/0.52\n0.70/0.91/0.96\n0.71/0.96/0.99\n0.70/0.94/0.98\n0.73 /0.96/0.97\n\n0.68/0.94/0.97\n1.94/0.13/0.10\n1.97/0.17/0.12\n\n1.91/0.26/0.25\n1.98/0.27/0.25\n\n\n\n\n\nRNN\n\n\n 0.72/0.94/0.98\n0.68/0.91/0.96\n\n0.70/0.90/0.97\n\n0.70/0.91/0.97\n0.68/0.95/0.98\n0.69/0.95/0.98\n0.43/0.68/0.84\n0.70 /0.95/0.98\n\n1.90/0.16/0.12\n2.00/0.24/0.24\n1.96/0.11/0.10\n1.88 /0.27/0.28\n\n\n\n\nBERTBase\nGRU\n\n0.72/0.92/0.97\n0.72/0.92/0.97\n\n0.70/0.92/0.96\n\n0.66/0.89/0.94\n0.66/0.95/0.98\n0.70/0.95/0.98\n 0.69/0.95/0.99\n0.68/0.93/0.98\n1.95/0.13/0.13\n\n1.95/0.26/0.24\n\n1.93/0.22/0.22\n1.94/0.24/0.24\n\n\n\n\n\nLSTM\n\n0.74 /0.93/0.96\n\n0.69/0.91/0.95\n0.66/0.90/0.95\n0.67/0.91/0.95\n0.72/0.95/0.98\n0.66/0.95/0.97\n0.69/0.96/0.97\n0.67/0.95/0.98\n1.94/0.20/0.22\n1.94/0.23/0.23\n1.95/0.21/0.23\n1.91/0.25/0.26\n\n\n\n\n\nRNN\n\n 0.71/0.93/0.95\n0.67/0.91/0.95\n0.69/0.91/0.96\n0.68/0.91/0.95\n0.69/0.95/0.99\n\n0.68/0.96/0.98\n\n0.67/0.96/0.98\n0.67/0.95/0.98\n1.91/0.20/0.23\n1.95/0.24/0.25\n1.92/0.22/0.23\n1.95/0.23/0.24\n\n\n\n\nTinyBERT\nGRU\n\n0.72/0.92/0.96\n0.68/0.91/0.95\n0.69/0.90/0.96\n0.68/0.91/0.95\n0.69/0.95/0.98\n 0.68/0.95/0.98\n\n0.70/0.95/0.99\n0.68/0.94/0.98\n1.97/0.18/0.21\n1.97/0.22/0.24\n1.98/0.18/0.20\n1.97/0.22/0.22\n\n\n\n\n\nLSTM\n\n0.72/0.93/0.96\n0.71/0.91/0.95\n0.66/0.88/0.95\n0.68/0.89/0.95\n0.68/0.95/0.98\n0.69/0.95/0.98\n0.69/0.95/0.98\n0.67/0.95/0.98\n1.93/0.19/0.20\n1.95/0.21/0.23\n1.92/0.21/0.24\n1.94/0.22/0.24\n\n\n\n\n\nRNN\n\n\n 0.72/0.94/0.97\n\n0.73 /0.91/0.97\n0.67/0.89/0.96\n0.66/0.88/0.95\n0.71/0.94/0.98\n0.70/0.95/0.97\n0.67/0.95/0.98\n0.66/0.95/0.98\n\n1.92/0.20/0.23\n1.95/0.22/0.25\n1.93/0.20/0.22\n2.00/0.17/0.21\n\n\n\n\n\nNo FT\n\n\nMobileBERT\nGRU\n\n0.72/0.92/0.97\n\n0.70/0.92/0.96\n\n0.65/0.89/0.95\n0.69/0.90/0.95\n0.72 /0.95/0.99\n\n0.69/0.94/0.99\n0.68/0.96/0.98\n0.68/0.95/0.98\n1.93/0.19/0.22\n1.94/0.22/0.23\n1.91/0.21/0.22\n1.96/0.22/0.23\n\n\n\n\n\nLSTM\nFC\n0.48/0.68/0.77\n0.43/0.66/0.75\n0.43/0.66/0.75\n0.43/0.66/0.75\n0.43/0.92/0.93\n0.45/0.91/0.95\n0.43/0.92/0.93\n0.45/0.91/0.95\n1.93/0.09/0.03\n2.00/0.02/0.04\n2.07/0.03/0.01\n2.00/0.09/0.07\n\n\n\n\n\nRNN\nFC\n0.48/0.68/0.77\n0.43/0.66/0.74\n0.43/0.66/0.72\n0.43/0.66/0.72\n0.43/0.92/0.95\n0.45/0.91/0.93\n0.43/0.92/0.95\n0.45/0.91/0.93\n1.93/0.06/0.06\n1.99/0.13/0.06\n1.94/0.07/0.06\n2.00/0.06/0.01\n\n\n\n\n\nGRU\nFC\n0.48/0.68/0.77\n0.43/0.66/0.75\n0.43/0.66/0.75\n 0.43/0.66/0.75\n0.43/0.92/0.93\n0.45/0.91/0.93\n0.43/0.92/0.93\n0.45/0.91/0.94\n1.94/-0.02/-0.01\n2.00/0.11/0.07\n1.95/-0.02/-0.03\n2.00/0.01/0.01\n\n\n\n\nBERTBase\n\nFC\n0.68/0.88/0.93\n0.67/0.87/0.92\n0.52/0.75/0.82\n0.65/0.83/0.89\n0.68/0.94/0.97\n0.66/0.93/0.97\n0.65/0.95/0.97\n0.65/0.92/0.96\n2.01/0.09/0.14\n1.97/0.14/0.17\n2.10/0.13/0.15\n 2.55/0.12/0.14\n\n\n\n\nTinyBERT\n\nFC\n0.63/0.80/0.88\n0.58/0.70/0.79\n0.43/0.66/0.73\n0.51/0.69/0.79\n0.66/0.94/0.97\n0.67/0.92/0.95\n0.67/0.93/0.96\n0.66/0.90/0.95\n1.87/0.19/0.20\n1.96/0.15/0.18\n1.91/0.17/0.17\n1.98/0.16/0.18\n\n\n\n\n\nFT w/o PMI\n\n\nMobileBERT\n\nFC\n0.49/0.67/0.72\n0.42/0.60/0.65\n 0.16/0.59/0.64\n0.43/0.47/0.63\n0.51/0.89/0.93\n0.44/0.88/0.91\n0.41/0.46/0.48\n0.45/0.48/0.54\n6e2/-0.04/0.01\n1e6/0.06/0.06\n2e5/0.03/0.04\n10e6/0.05/0.07\n\n\n\n\n\nLSTM\nFC\n0.48/0.68/0.76\n0.43/0.65/0.75\n0.43/0.65/0.74\n0.43/0.66/0.76\n0.50/0.90/0.94\n0.40/0.91/0.93\n0.43/0.90/0.94\n0.49/0.88/0.95\n1.93/0.09/0.06\n2.00/0.02/-0.02\n 1.93/0.06/-0.02\n1.99/0.11/0.08\n\n\n\n\n\nRNN\nFC\n0.48/0.69/0.77\n0.43/0.66/0.74\n0.43/0.66/0.75\n0.43/0.66/0.75\n0.45/0.91/0.95\n0.54/0.91/0.95\n0.48/0.91/0.95\n0.45/0.89/0.94\n1.92/0.08/0.07\n2.00/0.06/0.06\n1.96/0.03/0.04\n1.99/0.07/0.09\n\n\n\n\n\nGRU\nFC\n0.48/0.68/0.74\n 0.43/0.66/0.75\n0.43/0.65/0.75\n0.43/0.66/0.73\n0.50/0.92/0.95\n0.48/0.91/0.95\n0.50/0.92/0.95\n0.40/0.88/0.94\n1.91/0.12/0.12\n1.97/0.11/0.08\n1.92/0.09/0.03\n2.00/0.05/0.05\n\n\n\n\nBERTBase\n\nFC\n0.69/0.90/0.94\n0.69/0.88/0.94\n0.66/0.89/0.93\n0.70/0.89/0.93\n0.65/0.95/0.98\n0.68/0.94/0.98\n0.64/0.96/0.98\n0.67/0.94/0.97\n1.94/0.10/0.10\n 2.31/0.20/0.22\n1.95/0.21/0.22\n2.53/0.17/0.16\n\n\n\n\nTinyBERT\n\nFC\n0.67/0.86/0.91\n0.64/0.81/0.87\n0.56/0.81/0.86\n0.58/0.78/0.85\n0.66/0.95/0.97\n0.68/0.94/0.97\n0.68/0.95/0.97\n0.68/0.94/0.98\n1.91/0.17/0.20\n2.02/0.17/0.20\n1.97/0.16/0.18\n2.01/0.16/0.18\n\n\n\n\nMultimodal: 2-modal (our)\n\n\n\n\nFT w/ PMI\n\n\nMobileBERT\n\nFC\n 0.71/0.92/0.95\n0.69/0.90/0.93\n0.63/0.83/0.88\n0.69/0.88/0.94\n0.69/0.94/0.97\n0.68/0.95/0.98\n0.68/0.95/0.98\n0.68/0.94/0.97\n2.05/0.13/0.16\n2.18/0.08/0.11\n1.97/0.12/0.15\n2.20/0.14/0.18\n\n\n\n\n\nLSTM\nFC\n0.69/0.89/0.93\n0.68/0.88/0.92\n0.64/0.83/0.86\n0.67/0.86/0.89\n0.66/0.93/0.98\n0.68/0.93/0.97\n0.6//0.94/0.96\n0.69/0.95/0.97\n 1.94/0.04/-0.08\n2.35/0.10/0.17\n2.05/0.15/0.16\n2.42/0.11/0.18\n\n\n\n\n\nRNN\nFC\n0.67/0.86/0.91\n0.67/0.87/0.91\n0.67/0.86/0.91\n0.67/0.87/0.90\n0.67/0.94/0.98\n0.67/0.94/0.98\n0.64/0.92/0.96\n0.67/0.93/0.98\n1.90/0.15/0.21\n2.05/0.20/0.23\n1.93/0.13/0.16\n2.50/0.13/0.16\n\n\n\n\nBERTBase\nGRU\nFC\n 0.69/0.89/0.93\n0.67/0.89/0.93\n0.66/0.84/0.89\n0.65/0.86/0.89\n0.62/0.93/0.96\n0.68/0.94/0.98\n0.67/0.93/0.97\n0.67/0.94/0.97\n1.92/0.09/0.09\n2.01/0.19/0.25\n2.18/0.18/0.20\n2.15/0.16/0.24\n\n\n\n\n\nLSTM\nFC\n0.65/0.82/0.89\n0.59/0.78/0.84\n0.55/0.77/0.82\n0.54/0.76/0.84\n0.66/0.93/0.97\n0.66/0.93/0.96\n0.66/0.93/0.97\n 0.67/0.94/0.96\n1.90/0.16/0.17\n1.94/0.20/0.22\n1.97/0.14/0.16\n1.95/0.19/0.21\n\n\n\n\n\nRNN\nFC\n0.63/0.83/0.89\n0.61/0.80/0.84\n0.55/0.74/0.82\n0.53/0.76/0.82\n0.65/0.95/0.97\n0.65/0.92/0.97\n0.66/0.92/0.95\n0.65/0.92/0.96\n1.87 /0.18/0.18\n\n1.98/0.17/0.19\n1.92/0.13/0.17\n1.93/0.19/0.20\n\n\n\n\nTinyBERT\nGRU\nFC\n 0.63/0.82/0.88\n0.61/0.81/0.85\n0.52/0.74/0.83\n0.54/0.74/0.81\n0.64/0.94/0.97\n0.68/0.93/0.97\n0.66/0.93/0.97\n0.67/0.93/0.97\n1.94/0.14/0.16\n1.97/0.18/0.21\n1.92/0.17/0.19\n1.93/0.20/0.21\n\n\n\n\n\nLSTM\nFC\n0.49/0.72/0.84\n0.44/0.68/0.77\n0.02/0.09/0.11\n0.02/0.05/0.09\n0.56/0.91/0.94\n0.45/0.89/0.95\n 0.43/0.76/0.89\n0.03/0.07/0.45\n4e4/-0.02/0.03\n4e5/0.01/0.00\n2.04/0.08/0.12\n1e3/0.00/-0.02\n\n\n\n\n\nRNN\nFC\n0.47/0.68/0.78\n0.41/0.64/0.78\n0.43/0.48/0.56\n0.43/0.59/0.64\n0.43/0.91/0.94\n0.53/0.89/0.94\n0.49/0.88/0.91\n0.45/0.54/0.68\n1e1/0.00/-0.03\n5e4/0.00/-0.07\n5e3/0.00/-0.06\n2.15/-0.03/-0.06\n\n\n\n\n\nFT w/o PMI\n\n\nMobileBERT\nGRU\nFC\n0.48/0.69/0.78\n0.43/0.64/0.74\n0.00/0.49/0.49\n0.00/0.22/0.23\n0.52/0.89/0.93\n0.55/0.89/0.94\n0.40/0.87/0.92\n0.02/0.03/.048\n3e4/0.01/-0.05\n6e5/-0.04/-0.03\n6e2/-0.03/-0.10\n5e5/0.03/0.01\n\n\n\n\n\nLSTM\nFC\n0.72/0.91/0.95\n0.68/0.89/0.93\n0.43/0.66/0.75\n0.69/0.88/0.93\n0.68/0.95/0.98\n 0.68/0.94/0.98\n0.70/0.95/0.98\n0.68/0.93/0.97\n1.91/0.12/0.11\n1.99/0.09/0.04\n2.45/0.16/0.17\n2.41/0.16/0.17\n\n\n\n\n\nRNN\nFC\n0.72/0.90/0.94\n0.69/0.89/0.92\n0.66/0.88/0.94\n0.67/0.88/0.92\n0.66/0.95/0.98\n0.70 /0.95/0.98\n\n0.54/0.91/0.95\n0.69/0.94/0.97\n1.91/0.12/0.11\n2.79/0.14/0.16\n1.95/0.01/0.00\n2.56/0.14/0.16\n\n\n\n\nBERTBase\nGRU\nFC\n0.72/0.89/0.95\n0.70/0.90/0.95\n0.70 /0.89/0.92\n\n0.68/0.88/0.92\n0.67/0.96/0.98\n0.69/0.93/0.98\n0.68/0.95/0.98\n0.67/0.95/0.98\n1.91/0.11/0.10\n2.14/0.21/0.21\n2.02/0.22/0.23\n2.30/0.16/0.21\n\n\n\n\n\nLSTM\nFC\n0.67/0.83/0.89\n0.64/0.80/0.87\n0.60/0.80/0.84\n0.61/0.80/0.83\n 0.66/0.95/0.98\n0.69/0.95/0.97\n0.67/0.95/0.97\n0.67/0.93/0.97\n1.89/0.18/0.22\n1.99/0.19/0.22\n1.91/0.20/0.21\n1.97/0.19/0.21\n\n\n\n\n\nRNN\nFC\n0.68/0.85/0.90\n0.63/0.80/0.86\n0.59/0.80/0.86\n0.59/0.79/0.84\n0.69/0.95/0.97\n0.68/0.95/0.97\n0.69/0.95/0.97\n0.66/0.95/0.97\n1.89/0.17/0.19\n2.03/0.16/0.19\n1.94/0.18/0.20\n1.96/0.19/0.23\n\n\n\n\nTinyBERT\nGRU\nFC\n0.67/0.83/0.90\n0.62/0.79/0.85\n0.61/0.81/0.86\n0.56/0.78/0.86\n\n0.70/0.96/0.98\n\n0.70/0.94/0.97\n0.67/0.95/0.98\n\n0.67/0.95/0.97\n\n1.96/0.14/0.18\n1.95/0.20/0.23\n1.96/0.16/0.18\n1.99/0.19/0.23\n\n\n\n\n\nLSTM\nFC\n0.72/0.89/0.94\n0.68/0.88/0.93\n0.66/0.88/0.92\n 0.64/0.86/0.89\n0.66/0.95/0.98\n0.68/0.95/0.97\n0.68/0.95/0.98\n0.68/0.93/0.97\n1.93/0.13/0.14\n2.32/0.19/0.22\n2.24/0.15/0.18\n2.12/0.15/0.18\n\n\n\n\n\nRNN\nFC\n0.68/0.90/0.95\n0.67/0.86/0.91\n0.67/0.87/0.92\n0.66/0.89/0.91\n0.70/0.95/0.98\n0.66/0.95/0.98\n\n0.65/0.96/0.97\n\n0.67/0.93/0.96\n1.93/0.15/0.18\n2.11/0.18/0.23\n2.03/0.14/0.14\n 2.19/0.15/0.18\n\n\n\n\nMultimodal: 3-modal (our)\n\n\n\n\nFT w/ PMI\n\n\nMobileBERT\nGRU\nFC\n0.70/0.88/0.93\n0.69/0.90/0.93\n0.65/0.88/0.92\n0.70/0.88/0.91\n0.68/0.95/0.98\n0.68/0.95/0.97\n0.66/0.94/0.98\n\n0.69/0.94/0.98\n1.96/0.13/0.17\n2.27/0.16/0.20\n2.36/0.13/0.15\n2.28/0.18/0.23",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">Backbone(Text/Vitals/Image)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task1-Protocol Selection (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task2-Medicine Type Prescription (top-1/3/5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">Task3-Medicine Quant. Prescription (mse/pearsonr/spearmanr)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">T</td>\n<td class=\"ltx_td ltx_align_left\">V</td>\n<td class=\"ltx_td ltx_align_left\">I</td>\n<td class=\"ltx_td ltx_align_center\">P</td>\n<td class=\"ltx_td ltx_align_center\">P-M</td>\n<td class=\"ltx_td ltx_align_center\">P-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">M</td>\n<td class=\"ltx_td ltx_align_center\">P-M</td>\n<td class=\"ltx_td ltx_align_center\">M-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n<td class=\"ltx_td ltx_align_center\">Q</td>\n<td class=\"ltx_td ltx_align_center\">P-Q</td>\n<td class=\"ltx_td ltx_align_center\">M-Q</td>\n<td class=\"ltx_td ltx_align_center\">P-M-Q</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.09/0.40/0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.09/0.24/0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.10/0.23/0.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.09/0.24/0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.42/0.66/0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.60/0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.42/0.60/0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.51/0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.14/0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 1.96/0.17/0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.13/0.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.17/0.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">RNN</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.20/0.51/0.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.14/0.46/0.61</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.12/0.40/0.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.11/0.40/0.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.41/0.71/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.72/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.42/0.70/0.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.69/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.16/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.20/0.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.13/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.17/0.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">GRU</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.48/0.68/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.11/0.38/0.61</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.39/0.68/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.03/0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.03/0.06/0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/-0.05/-0.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.98/0.16/0.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">BERTBase</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.90/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">0.71<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /</span>0.92<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 1.94/0.01/-0.02</span></td>\n<td class=\"ltx_td ltx_align_center\">1.92<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.26/</span>0.27</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.24/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.25/0.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">TinyBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.89/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.13/0.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.21/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.17/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.24/0.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">MobileBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.73/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.73/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.86/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.17/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.16/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\">1.89<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.20/0.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.21/0.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:81.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:81.1pt;transform:translate(-35.6pt,-35.6pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Unimodal (SOTA)</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:29.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:29.6pt;transform:translate(-11.4pt,-11.4pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">No FT</p>\n</span></div>\n</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.02/0.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.01/0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.01/0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.01/0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.03/0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.01/0.41/0.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.03/0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.01/0.41/0.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.26/0.10/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.10/0.16/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/-0.11/-0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.08/0.16/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.10/0.27/0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.96/0.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.73<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.96/0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.13/0.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.17/0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/</span>0.26<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.98/0.27/0.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">RNN</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.72/0.94/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.90/</span>0.97</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.91/</span>0.97</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.68/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.95/0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.16/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.24/0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.11/0.10</span></td>\n<td class=\"ltx_td ltx_align_center\">1.88<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /</span>0.27<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">BERTBase</td>\n<td class=\"ltx_td ltx_align_left\">GRU</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/</span>0.92<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.69/0.95/0.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.13/0.13</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/</span>0.26<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.24</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.22/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.24/0.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">LSTM</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">0.74<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.93/0.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.96/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.20/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.23/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.21/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.25/0.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">RNN</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.71/0.93/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.91/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.99</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/</span>0.96<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.20/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.24/0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.23/0.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">TinyBERT</td>\n<td class=\"ltx_td ltx_align_left\">GRU</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.90/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/</span>0.99</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.18/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.22/0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.98/0.18/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.22/0.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">LSTM</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.88/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.89/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.19/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.21/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.21/0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.22/0.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">RNN</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.72/</span>0.94<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.73<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.91/</span>0.97</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.89/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.88/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.71/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/</span>0.20<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/</span>0.23</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.22/0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.20/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.17/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:29.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:29.6pt;transform:translate(-11.4pt,-11.4pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">No FT</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left\">MobileBERT</td>\n<td class=\"ltx_td ltx_align_left\">GRU</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/</span>0.92<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.89/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\">0.72<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.95/</span>0.99</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/</span>0.99</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.19/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.22/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.68/0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.09/0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.02/0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.07/0.03/0.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.09/0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.68/0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.06/0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.13/0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.07/0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.06/0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.68/0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.92/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/-0.02/-0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.11/0.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/-0.02/-0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.01/0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">BERTBase</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.88/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.87/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.52/0.75/0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.83/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.01/0.09/0.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.14/0.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.10/0.13/0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.55/0.12/0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.80/0.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.58/0.70/0.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.51/0.69/0.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.87/0.19/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.15/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.17/0.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.98/0.16/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:57.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:57.2pt;transform:translate(-23.6pt,-23.6pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">FT w/o PMI</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center\">MobileBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.67/0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.42/0.60/0.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.16/0.59/0.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.47/0.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.51/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.44/0.88/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.41/0.46/0.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.48/0.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">6e2/-0.04/0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1e6/0.06/0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2e5/0.03/0.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">10e6/0.05/0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.68/0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.65/0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.65/0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.91/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.88/0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.09/0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.02/-0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 1.93/0.06/-0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.11/0.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.69/0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.08/0.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.06/0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.03/0.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.07/0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.68/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.65/0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.50/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.88/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.12/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.11/0.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.09/0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.00/0.05/0.05</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">BERTBase</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.88/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.10/0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.31/0.20/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.21/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.53/0.17/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.81/0.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.81/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.58/0.78/0.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.17/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.02/0.17/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.16/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.01/0.16/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:136.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:136.1pt;transform:translate(-63.0pt,-63.0pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal: 2-modal (our)</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:60.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:60.1pt;transform:translate(-25.0pt,-25.0pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FT w/ PMI</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center\">MobileBERT</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.71/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.90/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.83/0.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.88/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.05/0.13/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.18/0.08/0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.12/0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.20/0.14/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.83/0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.6//0.94/0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 1.94/0.04/-0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.35/0.10/0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.05/0.15/0.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.42/0.11/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.87/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.87/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.15/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.05/0.20/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.13/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.50/0.13/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">BERTBase</td>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.69/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.84/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.62/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.09/0.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.01/0.19/0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.18/0.18/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.15/0.16/0.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">LSTM</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.82/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.59/0.78/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.55/0.77/0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.76/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.67/0.94/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.90/0.16/0.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.20/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.14/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.19/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.83/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.80/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.55/0.74/0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.53/0.76/0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.92/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.92/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.92/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\">1.87<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.18/0.18</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.98/0.17/0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.13/0.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.19/0.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyBERT</td>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.63/0.82/0.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.81/0.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.52/0.74/0.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.74/0.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.14/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.18/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.92/0.17/0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.20/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">LSTM</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.72/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.44/0.68/0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.02/0.09/0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.02/0.05/0.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.91/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.89/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.43/0.76/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.03/0.07/0.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">4e4/-0.02/0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">4e5/0.01/0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.04/0.08/0.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1e3/0.00/-0.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.47/0.68/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.41/0.64/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.48/0.56</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.59/0.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.91/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.53/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.49/0.88/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.45/0.54/0.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1e1/0.00/-0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">5e4/0.00/-0.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">5e3/0.00/-0.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.15/-0.03/-0.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:57.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:57.2pt;transform:translate(-23.6pt,-23.6pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">FT w/o PMI</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center\">MobileBERT</td>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.48/0.69/0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.64/0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.49/0.49</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.00/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.52/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.55/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.40/0.87/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.02/0.03/.048</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">3e4/0.01/-0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">6e5/-0.04/-0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">6e2/-0.03/-0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">5e5/0.03/0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.89/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.43/0.66/0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.88/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.68/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.12/0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.09/0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.45/0.16/0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.41/0.16/0.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.90/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.89/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.88/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.95/0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.54/0.91/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.12/0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.79/0.14/0.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.01/0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.56/0.14/0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">BERTBase</td>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.89/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> /0.89/0.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.96/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.93/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.11/0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.14/0.21/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.02/0.22/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.30/0.16/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">LSTM</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.83/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.64/0.80/0.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.60/0.80/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.80/0.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.89/0.18/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.19/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.91/0.20/0.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.97/0.19/0.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.85/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.63/0.80/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.59/0.80/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.59/0.79/0.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.89/0.17/0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.03/0.16/0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.94/0.18/0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.19/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyBERT</td>\n<td class=\"ltx_td ltx_align_center\">GRU</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.83/0.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.62/0.79/0.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.61/0.81/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.56/0.78/0.86</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/</span>0.96<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.94/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/</span>0.95<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.14/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.95/0.20/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.16/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.99/0.19/0.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">LSTM</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.72/0.89/0.94</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.88/0.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 0.64/0.86/0.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.93/0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.13/0.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.32/0.19/0.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.24/0.15/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.12/0.15/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">RNN</td>\n<td class=\"ltx_td ltx_align_left\">FC</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.90/0.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.86/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.87/0.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.89/0.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/</span>0.96<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">/0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.67/0.93/0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.93/0.15/0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.11/0.18/0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.03/0.14/0.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\"> 2.19/0.15/0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:136.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:136.1pt;transform:translate(-63.0pt,-63.0pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal: 3-modal (our)</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:10.0pt;height:60.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:60.1pt;transform:translate(-25.0pt,-25.0pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FT w/ PMI</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">MobileBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">GRU</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">FC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.88/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.90/0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.65/0.88/0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.70/0.88/0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.68/0.95/0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.66/0.94/0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">0.69/0.94/</span>0.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">1.96/0.13/0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.27/0.16/0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.36/0.13/0.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#9B9B9B;\">2.28/0.18/0.23</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "3modal",
            "6e2004001",
            "lstm",
            "emsglasss",
            "text",
            "mobilebert",
            "5e4000007",
            "2modal",
            "our",
            "selection",
            "2e5003004",
            "pmq",
            "image",
            "1e1000003",
            "multimodal",
            "quant",
            "task1protocol",
            "1e6006006",
            "6e2003010",
            "top135",
            "prescription",
            "3e4001005",
            "5e3000006",
            "vitals",
            "1e3000002",
            "10e6005007",
            "msepearsonrspearmanr",
            "sota",
            "gru",
            "dataset",
            "unimodal",
            "5e5003001",
            "bertbase",
            "4e4002003",
            "task2medicine",
            "6e5004003",
            "pmi",
            "tinybert",
            "models",
            "compare",
            "tasks",
            "rnn",
            "task3medicine",
            "4e5001000",
            "type",
            "backbonetextvitalsimage"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS2\" title=\"3.2. Backbone and headers &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, D2(3-modal: text, vitals, scene) is a 3-modal dataset whose size (3005 samples) is about 2 orders of magnitude smaller than the dataset D1(2-modal: text, vitals). To achieve effective digestion of small-sized D2 in EMSNet&#8217;s backbone, we use the progressive modality integration (PMI) to fine-tune the 3-modal backbone module on D2. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the 3-modal backbone&#8217;s accuracy on D2. It&#8217;s good to note that, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T4\" title=\"Table 4 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we only highlight 3-modal EMSNet&#8217;s accuracy with and without PMI on three single tasks (P, M, Q) and three simultaneous tasks (P-M-Q), full details for two simultaneous tasks (P-M, P-Q, M-Q) can be found in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2\" title=\"Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.SS1.SSS2\" title=\"5.1.2. 3-modal EMSNet backbone accuracy on D2 &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the full details of evaluating the EMSNet on 3-modal dataset D2 (text, vitals, image).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present <span class=\"ltx_text ltx_font_bold\">EMSGlass</span>, a smart-glasses system powered by <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model for Emergency Medical Services (EMS), and <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves <span class=\"ltx_text ltx_font_bold\">1.9&#215; &#8211; 11.7&#215;</span> speedup over direct PyTorch multimodal inference.\nA user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks",
                    "multimodal",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the workflow of an EMT in a typical EMS scenario. The EMT first assesses the patient&#8217;s symptoms, then uses medical equipment to obtain vital signs, and finally evaluates the surrounding environment. These multimodal data&#8211;symptoms, vitals, and images in the scene contexts including alcohol and pills&#8211;are asynchronously collected and processed in sequence to inform real-time clinical decision-making. As these multimodal data arrived (i.e., perceived by EMTs) at different timestamps, one of the critical tasks for EMTs is to select the appropriate EMS protocol from over 100 available options, which requires matching the asynchronously observed multimodal data with that defined in each protocol. For example, in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, after analyzing the asynchronously perceived multimodal data, the EMT selects the protocol &#8220;Medical-Respiratory,&#8221; which prescribes general clinical interventions including the administration of Atrovent medicine at a quantity of 0.5mg. Making such high-stakes clinical decisions under time constraints is critical for optimal EMS delivery but remains a significant challenge for EMTs.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mobile assistant systems for EMS have been evolving to harness both cloud and edge (e.g., smart glasses) resources for real-time cognitive assistance and protocol recommendations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>)</cite>. For instance, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> allow EMTs to wear smart glasses for capturing the voice of patient symptoms. They utilize domain-specific speech-to-text models (e.g., Whisper-tiny) to transcribe the speech and data-driven models (e.g., MobileBERT) to help select protocols. These models are trained in the cloud and deployed on hands-free smart glasses (e.g., Google Glass, Vuzix M4000) and edge servers to address challenges such as unreliable cloud connectivity in disaster scenarios while also enhancing user privacy. Despite these advancements, several clinical and technical challenges remain unresolved:</p>\n\n",
                "matched_terms": [
                    "models",
                    "mobilebert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #1: Lacking a multimodal model for multiple simultaneous EMS tasks.</span> On the input side, state-of-the-art (SOTA) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> rely solely on patient symptoms but ignore vitals and scene images (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), which are all necessary modalities for comprehensive situational awareness and contextual understanding. This limitation leads to suboptimal EMS delivery by failing to account for multimodal data. On the output side, while EMS protocols dictate general clinical guidance, EMTs must handle diverse tasks including real-time treatment personalization, e.g., extra medication or additional dosage based on patients&#8217; rapidly evolving conditions. Restricting focus on the protocol selection in existing systems reduces their practicality and usability, highlighting the need for a multimodal model that supports multiple simultaneous EMS tasks.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "selection",
                    "tasks",
                    "sota",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge #2: High-latency serving frameworks for multimodal models in EMS scenarios.</span> Existing multimodal model serving frameworks, such as PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite> and TensorFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tensorflow2016</span>)</cite> used in SOTA assistant systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, assume simultaneous availability of all data modalities. However, in the EMS scenario shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, smart glasses receive different data modes asynchronously (i.e., asynchronous arrival times) as EMTs move through the scene. Without caching intermediate processing results for early arrived modes of data, directly using multimodal model serving frameworks has to repeatedly process early arrived voice data when later-arriving data like vitals and images become available, leading to redundant computations and higher latency. To achieve timely EMS delivery given the asynchronous arrival of multimodal data, a low-latency multimodal serving framework in EMS scenarios is essential.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "models",
                    "sota",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> We build <span class=\"ltx_text ltx_font_bold\">EMSNet</span>, the first multimodal multitask model trained on massive, real-world multimodal EMS datasets to simultaneously accomplish five critical EMS tasks: protocol selection, recommendation for medicine type, quantity, dosage, and disease history inference.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "selection",
                    "type",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(2)</span> We develop <span class=\"ltx_text ltx_font_bold\">EMSServe</span>, the first serving framework for multimodal models to address the challenges introduced by different data arrival times in EMS scenarios. With the adaptive edge-assisted offloading, EMSServe harnesses the computing resources on both the glass and edge servers. Comprehensive evaluations show EMSServe generally outperforms direct usage of PyTorch by 1.9&#215; &#8211; 11.7&#215; speedup across diverse edge devices (Google Glass) and edge servers.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">visionFoundation2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DISTMM2024NSDI</span>)</cite> are designed to understand and fuse complementary information from multiple sources. These models rely on one or more deep learning submodules to process and integrate data effectively. They aim to generate intermediate unified numeric representations from multimodal inputs for multiple simultaneous tasks across diverse domains, e.g., general healthcare&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2020fusion</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lian2024npj</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2019early</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharazmi2018feature</span>)</cite>. However, few has explored the feasibility and advantage of such multimodal multitask capability in the EMS domain.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the release of Google Glass Explorer Edition as a ubiquitous computing platform&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleGlassExplorer2012</span>)</cite>, smart glasses have been integrated into various medical applications, including surgical assistance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pedisurgery2014</span>)</cite>, eating behavior monitoring&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eating2015PervasiveHealth</span>)</cite>, physiological vitals sensing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SpiderNie2020</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SPIDERSNie2021</span>)</cite>, and cognitive state and ocular health tracking&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BlinkMental2025</span>)</cite>. Despite their promising hands-free interaction and heads-up display capabilities, the potential of on-glass assistants to enhance EMS delivery remains largely underexplored. Recent systems such as EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssistDemo2023</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> have made initial attempts to assist EMTs with protocol selection on smart glasses. However, they either lack multimodal multitasking capabilities or comprehensive real-world user studies, leaving a critical gap between technological advancement and practical adoption in EMS.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "selection",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the modular design of EMSNet, the first on-glass multimodal multitask model for EMS. When an EMT wearing smart glasses encounters a collapsed patient at the EMS scene, they verbally report the patient&#8217;s symptoms &#8220;unconscious seizures, abnormal breath&#8221;. The microphone on the smart glasses captures voice, processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">text module</span>, i.e., speech-to-text module and text encoder. The patient&#8217;s numeric time-series vitals, obtained from medical equipment, are processed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">vitals module</span>, i.e., vitals encoder. The smart glasses camera captures scene images, analyzed by the&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">image module</span>, i.e., object detection and object encoder. We call the three encoder modules&#160;<span class=\"ltx_text ltx_framed ltx_framed_underline\">backbone</span> as their output features (<math alttext=\"F_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">F_{T}</annotation></semantics></math>, <math alttext=\"F_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">F_{V}</annotation></semantics></math>, and <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>) are concatenated, and then form a unified feature representation <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. <math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>I</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|+|F_{I}|}</annotation></semantics></math> serves as input to three distinct header modules producing recommendations for three tasks: Task 1) protocol selection recommendation, Task 2) medicine type prescription for recommending which type of medicine to administer, and Task 3) medicine quantity prescription for recommending specific medication quantities.\nIf the object detection module locates a medicine bottle from the scene, the original scene image is processed by the OCR and barcode scanner for medicine name and concentration from the label. The Med-Math module then takes the quantity from header 3, using concentration to determine Task 4) dosage prescription. Medicine names undergo a dictionary check to infer Task 5) patient&#8217;s disease history. Both OCR and barcode scanner are needed because we find medicine labels may not always contain both names and barcodes. Complementing the outputs from both pipelines ensures EMSGlass can always extract the desired information. Finally, recommendations of all five tasks are continuously updated on the display of the glasses. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> lists model candidates for all component modules of EMSNet. Before we dive into the design details of these module candidates, we first describe how we prepare the multimodal EMS dataset to train and test EMSNet.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "selection",
                    "image",
                    "tasks",
                    "multimodal",
                    "type",
                    "dataset",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F3\" title=\"Figure 3 &#8227; 3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates the data processing pipeline used to prepare four multimodal datasets (D1-D4) to train and test EMSNet. This pipeline is designed as a general plug-n-play tool for preparing multimodal datasets from NEMSIS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>, a publicly available (upon request) key-value tabular database that includes all real-world national emergency event reports in the US. Notably, <span class=\"ltx_text ltx_font_italic\">our data processor is the first tool of its kind to prepare multimodal EMS datasets</span>. The NEMSIS database uses a unique 9-digit &#8220;PCR key&#8221; as each emergency event ID. In this paper, we utilize the NEMSIS 2023 database, which includes over 54 million EMS events recorded by 14,369 EMS agencies across 54 states and territories in 2023.</p>\n\n",
                "matched_terms": [
                    "our",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D1(2-modal: text,vitals)</span>: We get D1&#8217;s PCR keys from NEMSIS, extracting events that contain symptom data (primary symptom, primary impression, associate symptom, and secondary impression). Since EMTs typically use symptoms to describe patients&#8217; medical status&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite>, we concatenate the four symptom texts into a single sentence as the text-mode input to EMSNet. Similarly, we use the same PCR keys for time-series vitals (BP-blood pressure, HR-heart rate, PO-pulse oximetry, RR-respiratory rate, CO<sub class=\"ltx_sub\">2</sub>-end-tidal carbon dioxide, BG-blood glucose) inputs. The extracted vitals are preprocessed with a few steps detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A1\" title=\"Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, e.g., outlier removal, padding missing values, and cross-sample normalization.\nOn the right, we use the same keys to extract labels for each sample: protocol (abdominal pain) for Task 1 (protocol selection), medicine type (naloxone) for Task 2 (medicine prescription), and quantity (3.25 mg) for Task 3 (medicine quantity). <span class=\"ltx_text ltx_font_italic\">This data processor is the first to prepare and enable the Task 2 and 3 for EMS</span>. The final dataset is a 2-modal (text,vitals) set with 123,803 samples.</p>\n\n",
                "matched_terms": [
                    "2modal",
                    "vitals",
                    "selection",
                    "type",
                    "dataset",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D2(3-modal: text, vitals,scene)</span>: Similarly, we extract D2&#8217;s text and vitals with D2&#8217;s PCR Keys, a disjoint set of D1 corresponding to events with 3-modal input: text, vitals, and scene data. We also extract the EMS scene information indicating the existence of alcohol and pills. The scene information is one-hot encoded, giving a 3-modal dataset D2(text, vitals, scene) with 3,005 samples.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "dataset",
                    "vitals",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">D4(image)</span>: We place 3 types of EMS objects (alcohol bottles, pills, and medicine bottles) on a table in 5 different rooms shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>: (a) bedroom, (b) living room, (c) student office, (d) conference room, and (e) user study room. We recorded videos of objects using Google Glass&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleEEII2024</span>)</cite> in these 5 rooms. From these video recordings, we sample 1,240 images from rooms (a)-(d) for training (908) and validation (232) sets, and 200 images from room (e) for testing. The final image dataset contains 1,340 samples for training and testing EMS-specific object detection models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the backbone includes three encoder models: 1) a symptom text encoder, 2) a vitals encoder, and 3) an image encoder. We use TinyBERT, MobileBERT, and BERTBase for the symptom text encoder as they have demonstrated success in encoding text data, including EMS texts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2019bert</span>)</cite>. RNN, LSTM, and GRU are employed for time-series vitals data. A fully-connect (FC) operator is applied to encode the one-hot vector from the object detection model. To fuse multimodal features, we use a feature concatenator to concatenate three encoders&#8217; output vectors into one longer vector. We also tried other feature fusion methods, including dot product, weighted sum, weighted concatenation, and attention. We ultimately choose concatenation due to its higher accuracy on the multimodal EMS dataset. Headers 1 and 2 are standard one-layer classification headers while header 3 is a standard regression header. For simplicity, in this paper, we call the collection of the feature concatenator and three headers as headers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "models",
                    "lstm",
                    "image",
                    "multimodal",
                    "rnn",
                    "gru",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use D1 (2-modal: text, vitals) and D2 (3-modal: text, vitals, scene) to train the multimodal backbone with PyTorch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pytorch2019</span>)</cite>. During training, we employ top 1/3/5 categorical loss for headers 1 and 2, while mean square error (mse), pearson coefficient (pearsonr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite>, and spearmanr coefficient (spearmanr<math alttext=\"\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\in[0,1]</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Pearsonr</span>)</cite> for the header 3.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "text",
                    "2modal",
                    "vitals",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal training with progressive modality integration (PMI)</span>: Multimodal training generally suffers from the imbalanced dataset size among different modalities. For example, the 2-modal D1 has a much larger dataset size (123,803) than 3-modal D2 (3005). Although using D1 alone may get a performant 2-modal model due to sufficient amount of 2-modal data samples,\ndirectly training the 3-modal model on D2 from scratch will significantly degrade the 3-modal accuracy. To address this, we propose progressive modality integration (PMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shankar2022progressivefusionmultimodalintegration</span>)</cite>. Instead of training the 3-modal model from scratch, we leverage the pre-trained 2-modal model. Specifically, in each epoch of 3-modal training, instead of passing the batched samples to a newly initialized 3-modal model, we feed the 2-modal parts (text, vitals) of the 3-modal sample into the trained 2-modal model to get feature outputs (<math alttext=\"F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>F</mi><mi>C</mi></msub><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>T</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>V</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><annotation encoding=\"application/x-tex\">F_{C}\\in\\mathcal{R}^{|F_{T}|+|F_{V}|}</annotation></semantics></math>), while passing the scene part of the batched samples to the newly initialized scene encoder to get <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>. We combine it with the 2-modal&#8217;s <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> for a new <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math>. Since the 2-modal <math alttext=\"F_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">F_{C}</annotation></semantics></math> is an order of magnitude larger than <math alttext=\"F_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>I</mi></msub><annotation encoding=\"application/x-tex\">F_{I}</annotation></semantics></math>, it retains most of the knowledge learned from D1 while integrating additional scene information from D2. Our evaluation confirms that PMI effectively enables competent 3-modal backbone models.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "text",
                    "2modal",
                    "vitals",
                    "pmi",
                    "models",
                    "multimodal",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art EMS assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumTowardsICCPS2018</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">PreumCognitiveEMSSigbed2019</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ShuBehaviorIROS2019</span>)</cite> rely on cloud-based speech-to-text services like Google Speech-to-Text, which require stable internet access&#8212;often unavailable in disaster-stricken areas. To address this, EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>)</cite> and CognitiveEMS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CognitiveEMS2024IoTDI</span>)</cite> fine-tuned small models (e.g., EMSConformer, Whisper-tiny, and Whisper-base) with custom audio datasets for reliable on-device transcription. However, these models exhibit poor generalization across user accents and microphone hardware. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a-b), accuracy degradation is evident in dataset D3(audio), where HyperX-recorded validation data achieves lower word error rates (WER) and character error rates (CER) than user5&#8217;s test set. Additionally, pervasive microphone frequency cutoffs on mobile devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">NFON2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FlexRadio2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Audio-Technica2024</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asmar2019phone4khz</span>)</cite>, such as Google Glass&#8217;s 8kHz limitation, cause substantial speech information loss. Spectrograms in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c-e) confirms this, showing lost high-frequency signals in Google Glass recordings. Simultaneously recorded audio tests reveal Whisper-t&#8217;s WER rises from 0.139 (HyperX) to 0.315 (Google Glass).</p>\n\n",
                "matched_terms": [
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These challenges stem from data distribution shift&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HwangDistributionECCV2022</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChipDataDistributionOnline2024</span>)</cite>, where real-world inputs&#8212;affected by diverse accents and microphones&#8212;differ from training data, yet the labels remain unchanged. The root cause is the small size of existing models, all under 100 million parameters (EMSConformer: 10m, Whisper-tiny: 74m, Whisper-base: 74m), limiting both learning and generalization. Empirical evidence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaplan2020scalinglawsneurallanguage</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">alabdulmohsin2022revisitingscalinglaws</span>)</cite> supports this approach, showing that larger models achieve lower training loss. Specifically, we propose training Whisper-small (242m) and Whisper-medium (764m) on dataset D3(audio) to enhance generalization (as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F4\" title=\"Figure 4 &#8227; 3.3.1. Speech-to-text &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Here we select one order of magnitude higher than the SOTA to demonstrate the effectiveness of EMSWhisper&#8217;s idea in enhancing generalization capability. Further explorations of alternative scaling options could be considered future work.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "models",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Close-set detection models (e.g., YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite>), in contrast to the aforementioned open-set Grounding Dino (GD) model, require manual dataset annotation and model training when generalized onto a new domain, e.g., EMS. To relieve the demanding annotation workloads, we propose <span class=\"ltx_text ltx_font_italic\">human-in-the-loop (HITL) annotation adjustment</span>, leveraging the open-set strength of the GD into the human annotation process. There are two rounds of annotations: in the first round, we feed unlabeled EMS scene images D4(image) to GD to get bounding box labels. In the second round, instead of annotating from scratch, we adjust GD&#8217;s annotations from the first round (e.g., relabel GD&#8217;s incorrect annotations). After two rounds, we fine-tune the close-set YOLO11. When compared to conventional manual annotation without GD&#8217;s auto-labeling process, our HITL annotation adjustment saves the annotation time by exploiting the correct annotations from GD and leaves the main manual efforts in GD&#8217;s incorrect or missed annotations. From our experiences, this HITL annotation adjustment method decreases the annotation time by half, e.g., on average, from 10 to 5 minutes for every 100 images.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once a medicine bottle object is detected in the scene image, EMSNet in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F2\" title=\"Figure 2 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> uses optical character recognition (OCR) and barcode scanner to extract medicine details from the original image. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F6\" title=\"Figure 6 &#8227; 3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we use Easy-OCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EasyOCR2023</span>)</cite> to extract medicine names and concentrations on the labels of the medicine bottle. To filter out wrong outputs, we employ an edit distance (ED)-based post-processing module for matching all outputs with a predefined true list of EMS medicines and corresponding concentration. In cases where the text information on the medicine bottle label is missing but the barcode is available, we use ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> to build a barcode scanner to complement the OCR by extracting the medicine names and concentration from the barcode. As exemplified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2.SS3\" title=\"2.3. Object detection and OCR in EMS &#8227; 2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>, OCR/scanner&#8217;s medicine concentration output, combined with the medicine quantity prescription from header 3, is input to med math&#8211;a division operator&#8211;to compute the dosage of the medicine solution for task 4. The medicine name output is mapped to a list of 82 common EMS diseases for accomplishing task 5.</p>\n\n",
                "matched_terms": [
                    "text",
                    "image",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">EMSServe is the first multimodal model serving framework for EMS scenarios with multimodal data arriving at different times</span>. Before we dive into the design details, we would like to clarify: as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3\" title=\"3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, a &#8220;module&#8221; (e.g., text module, vitals module) refers to EMSNet components; in contrast, a &#8220;text-vital&#8221; model includes both text and vitals modules with additional headers for accomplishing EMS tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 1 &#8211; Avoid redundant text module inference</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an EMS scenario where an EMT&#8217;s speech describing symptoms and a patient&#8217;s vitals arrive at the smart glasses at different times. In conventional multimodal frameworks like PyTorch (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, left), the speech is first processed by a text model (i.e., speech-to-text + text encoder + headers) at <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>. When the first vitals data (vitals1) arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, the system recomputes the text submodule output by feeding both speech and vitals1 into a text-vital model. This process repeats when additional vitals (vitals2) arrive at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, leading to the redundant inference of the text module (speech-to-text + text encoder). Real-world EMS data (NEMSIS 2023&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nemsis2015</span>)</cite>) records up to 30 vitals per event, potentially causing the text submodule to run up to 30 times.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 2 &#8211; Text modules dominate EMSNet inference time</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that text modules, including the speech-to-text and text encoder, are the primary inference bottleneck across four hardware platforms: Edge-64X, Edge-4C, PH1 Phone, and Google Glass Enterprise Edition 2 (hardware details in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Whisper variants, the speech-to-text models, require significantly more inference time than other components. Whisper-small and Whisper-medium, the largest models, are the slowest on edge servers and are entirely unusable on PH1 and Google Glass due to memory limitations. Additionally, text encoders further exacerbate latency. For instance, components incorporating BERTBase exhibit inference times nearly identical to BERTBase itself, with similar trends for TinyBERT and MobileBERT. In contrast, vitals submodules (RNN, GRU, LSTM) and headers require substantially less processing time, underscoring the inefficiency challenge of repeated text submodule computations highlighted in Insight 1.</p>\n\n",
                "matched_terms": [
                    "text",
                    "bertbase",
                    "mobilebert",
                    "vitals",
                    "tinybert",
                    "models",
                    "lstm",
                    "rnn",
                    "gru"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key idea of EMSServe&#8211;feature cache</span>: To eliminate redundant text submodule inference, EMSServe employs feature cache (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F7\" title=\"Figure 7 &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, right). When voice data arrives, instead of only running the text model, we simultaneously compute and cache the text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> in the text-vital model. When vitals1 arrives at <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, unlike conventional frameworks that rerun the costly text submodule, EMSServe reuses the cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and processes only the vitals module (vitals encoder) to obtain <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math>, which significantly reduces inference costs. The cached <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and newly computed <math alttext=\"F_{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{V}^{2}</annotation></semantics></math> are then concatenated as inputs to model headers. By addressing asynchronous EMS data arrival times, EMSServe mitigates redundant and costly submodule computation to improve inference efficiency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Insight 3 &#8212; inference latency varies rapidly across hardware</span>: Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows that inference latency differs significantly across hardware. Google Glass, with limited computing power, presents the highest latency, while Edge-64X, with ample resources, is the fastest. For instance, YOLO11n inference takes 3.2s on Google Glass, 0.7s on PH1, 0.08s on Edge-4C, and just 0.03s on Edge-64X. This disparity implies an opportunity for latency reduction through edge-assisted workload offloading. Offloading YOLO11n from Google Glass to Edge-64X achieves over 100&#215; speedup. EMSServe exploits this by offloading high-latency tasks, such as text module inference and object detection, to edge servers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on our insights, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> abstracts EMSServe, the first serving framework for multimodal model inference in EMS. EMSServe comprises three key components:  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic1\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">1</span></span></foreignobject></g></g></svg></span> a modality-aware splitter that decomposes multimodal models into single-mode modules,  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic2\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">2</span></span></foreignobject></g></g></svg></span> model inference time profiling to measure inference latency on the smart glasses and the edge server, and  <span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"16.46\" id=\"S4.SS2.p1.pic3\" overflow=\"visible\" style=\"vertical-align:-3.77px\" version=\"1.1\" viewbox=\"0 0 16.46 16.46\" width=\"16.46\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,16.46) matrix(1 0 0 -1 0 0) translate(8.23,0) translate(0,8.23)\"><g color=\"#FFFFFF\" fill=\"#000000\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#000000;--ltx-fg-color:#FFFFFF;\"><path d=\"M 7.95 0 C 7.95 4.39 4.39 7.95 0 7.95 C -4.39 7.95 -7.95 4.39 -7.95 0 C -7.95 -4.39 -4.39 -7.95 0 -7.95 C 4.39 -7.95 7.95 -4.39 7.95 0 Z M 0 0\"/></g><g fill=\"#FFFFFF\" stroke=\"#FFFFFF\" style=\"--ltx-stroke-color:#FFFFFF;--ltx-fill-color:#FFFFFF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)\"><foreignobject color=\"#FFFFFF\" height=\"8.92\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.92)\" width=\"6.92\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">3</span></span></foreignobject></g></g></svg></span> adaptive offloading with feature caching to minimize latency in dynamic EMS scenarios. While we evaluate EMSGlass on PH1 and Edge-64X in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for generality, our primary setup uses Google Glass as the mobile device and a manpack-mounted Edge-4C as the edge server.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the modality-aware splitter decomposes the multimodal model <math alttext=\"M2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">M2</annotation></semantics></math> (<math alttext=\"M3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">M3</annotation></semantics></math>) into single-mode modules <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math> (<math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math>, and <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>), enabling precomputation and cache of the text module output features <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> (<math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math>) before vitals data arrive.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After splitting, we profile all models and modules to obtain their inference times <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> on Google Glass and Edge-4C. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> details the profiling results. Both modality-aware splitting and profiling are one-time offline efforts, providing <math alttext=\"t^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>g</mi></msup><annotation encoding=\"application/x-tex\">t^{g}</annotation></semantics></math> and <math alttext=\"t^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><msup><mi>t</mi><mi>e</mi></msup><annotation encoding=\"application/x-tex\">t^{e}</annotation></semantics></math> as inputs for EMSServe&#8217;s real-time multimodal request serving.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> illustrates how EMSServe dynamically serves multimodal inference in an EMS scenario where symptom speech, vitals, and image data arrive asynchronously. The EMT wearing Google Glass and the EMT carrying the manpack-mounted Edge-4C move independently, causing bandwidth (BW) fluctuations in Glass-edge communication. To monitor this variation in real-time, we implement a lightweight heartbeat monitor on the smart glasses, periodically (e.g., every second) measuring the file transmission bandwidth <math alttext=\"\\Delta t=BW*filesize\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>=</mo><mrow><mrow><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>W</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>z</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta t=BW*filesize</annotation></semantics></math>. Unlike RTT, <math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> represents the actual file transfer time. Based on this measurement, EMSServe optimizes offloading decisions:</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Low Latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math> is low): offload compute-heavy tasks (e.g., text submodule inference, object detection) to the edge server for its higher processing power.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1) When the EMT&#8217;s symptom speech arrives at time <math alttext=\"t_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">t_{1}</annotation></semantics></math>, EMSServe evaluates the real-time Glass-edge latency (<math alttext=\"\\Delta t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">\\Delta t</annotation></semantics></math>) and the profiled inference time of model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> on the edge server (<math alttext=\"t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{e}</annotation></semantics></math>). Since the total offloading time (<math alttext=\"\\Delta t+t_{M1}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t_{M1}^{e}</annotation></semantics></math>) is lower than the inference time on Google Glass (<math alttext=\"t_{M1}^{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m6\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t_{M1}^{g}</annotation></semantics></math>), the system offloads the speech processing to the edge server. The edge server returns recommendations <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math> and precomputed text module feature cache <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m8\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math> and <math alttext=\"F_{T}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m9\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{3}</annotation></semantics></math> from models <math alttext=\"M2_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{T}</annotation></semantics></math> and <math alttext=\"M3_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p4.m11\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{T}</annotation></semantics></math>, respectively, reducing redundant inference for future multimodal tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "tasks",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2) Upon the arrival of the first vitals data at time <math alttext=\"t_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">t_{2}</annotation></semantics></math>, on-glass inference is chosen because the estimated offloading delay (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}</annotation></semantics></math>) exceeds the inference time on Google Glass (<math alttext=\"t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup><annotation encoding=\"application/x-tex\">t^{g}_{M2_{V}}</annotation></semantics></math>). The vitals data is processed by the pre-split vitals module <math alttext=\"M2_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M2_{V}</annotation></semantics></math>, generating <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>. Using the cached text module output <math alttext=\"F_{T}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m6\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F_{T}^{2}</annotation></semantics></math>, EMSServe efficiently computes the recommendation <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m7\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> on Google Glass without running the full multimodal model. When a second vitals data point arrives at <math alttext=\"t_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">t_{3}</annotation></semantics></math>, the system instead offloads processing to the edge server (<math alttext=\"\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m9\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mo>+</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>e</mi></msubsup></mrow><mo>&lt;</mo><msubsup><mi>t</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>2</mn><mi>V</mi></msub></mrow><mi>g</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Delta t+t^{e}_{M2_{V}}&lt;t^{g}_{M2_{V}}</annotation></semantics></math>). In parallel, the edge server also processes the vitals module <math alttext=\"M3_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m10\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{V}</annotation></semantics></math> to generate feature cache <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m11\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math>, which is returned alongside <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m12\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">R^{2}</annotation></semantics></math> and <math alttext=\"F^{2}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p5.m13\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">F^{2}_{V}</annotation></semantics></math>, ensuring cache consistency.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3) Similarly, for EMS scene images, EMSServe offloads processing when the offloading delay is lower than the on-glass inference time. When the first image arrives at <math alttext=\"t_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>4</mn></msub><annotation encoding=\"application/x-tex\">t_{4}</annotation></semantics></math>, it is offloaded to the edge server, where the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math> extracts feature <math alttext=\"F^{3}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>I</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{I}</annotation></semantics></math>. This is combined with cached features <math alttext=\"F^{3}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>T</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{T}</annotation></semantics></math> and <math alttext=\"F^{3}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m5\" intent=\":literal\"><semantics><msubsup><mi>F</mi><mi>V</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">F^{3}_{V}</annotation></semantics></math> precomputed in previous steps to generate recommendation <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m6\" intent=\":literal\"><semantics><msup><mi>R</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">R^{3}</annotation></semantics></math>, which is sent back to Google Glass. When the second image arrives at <math alttext=\"t_{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mn>5</mn></msub><annotation encoding=\"application/x-tex\">t_{5}</annotation></semantics></math>, the on-glass inference is chosen due to high offloading latency. With previously cached text and vitals features, the system processes only the image submodule <math alttext=\"M3_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p6.m8\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mn>3</mn><mi>I</mi></msub></mrow><annotation encoding=\"application/x-tex\">M3_{I}</annotation></semantics></math>, avoiding redundant inference of other modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel cache computation</span>: We utilize multiple parallel threads to reduce cache computation overheads. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F8\" title=\"Figure 8 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (right) shows the compute time for the text feature cache (step 1) and vitals cache (step 2) across all hardware platforms, including Google Glass. For the text module cache (top), parallel computation takes nearly the same time as running the text model <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> alone, effectively hiding the text feature cache computation from users since <math alttext=\"M1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p7.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M1</annotation></semantics></math> must be computed regardless. However, for the vitals module (bottom), which inherently takes much less time to compute by up to four orders of magnitude, launching multiple parallel threads often incurs higher costs than serial execution. Hence, we adopt serial feature cache computations for vitals.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seamless UI design is crucial for EMTs to adopt networked smart glasses, given their mobility and resource constraints (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F10\" title=\"Figure 10 &#8227; 4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> illustrates the on-display UI: the left screen continuously updates recommendations for EMS tasks while tapping the frame (right bottom) triggers EMSGlass to capture EMTs&#8217; symptoms speech, patients&#8217; vitals, and scene images. Synchronously arrived data is offloaded adaptively (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.SS2.SSS3\" title=\"4.2.3. Adaptive offloading with feature cache &#8227; 4.2. EMSServe design &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">4.2.3</span></a>) via WiFi to the Edge-4C server carried in a manpack, ensuring low-latency inference and real-time recommendation on-display updates in high mobility EMS scenarios.\nEMSGlass is implemented as an Android app on Google Glass Enterprise Edition 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">glassSpecs2023</span>)</cite>, comprising over 3,000 lines of Java. EMSServe, built on Torch Serve&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">torchserve</span>)</cite>, runs on Edge-4C with &#160;2,000 lines of Python. HTTPS communication enables multimodal inference offloading, ensuring efficient processing despite smart glasses&#8217; resource limitations.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "tasks",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T3\" title=\"Table 3 &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we compare the accuracy of the multimodal backbone with SOTA unimodal options in accomplishing tasks 1-3 on the 2-modal dataset D1. The P, M, and Q mean the backbone is trained to separately accomplish a single Task 1, 2, and 3. P-M means the backbone is trained to accomplish two tasks 1 and 2 simultaneously while P-M-Q means accomplishing three tasks at the same time. The lower mse and higher pearsonr/spearsonr indicate better performance in task3. As demonstrated, our multimodal backbone consistently outperforms unimodal options adopted in SOTA assistant systems. Specifically, it improves the top-1/3/5 accuracy for task2 by 2-3%, reduces the MSE by <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.1, and enhances Pearsonr and Spearmanr for Task 3, which are newly proposed EMS tasks in this work.</p>\n\n",
                "matched_terms": [
                    "2modal",
                    "compare",
                    "pmq",
                    "tasks",
                    "multimodal",
                    "sota",
                    "our",
                    "dataset",
                    "top135",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, with PMI-enabled fine-tuning, 3-modal EMSNet achieves the consistently higher accuracy in recommending protocols (task1) and medicine types (task2). For example, the PMI-enabled BERTBase-LSTM-FC and BERTBase-GRU-FC achieve top-3 accuracy of 0.91 and 0.96, respectively, on the single protocol (P) and medicine type (M) tasks. For the medicine quantity recommendation task (Q and P-M-Q), although fine-tuning w/o PMI seems more performant, fine-tuning w/ PMI achieves comparable performance. For example, fine-tuning with PMI enables TinyBERT-LSTM-FC to achieve the MSE of 1.89 on the single Q task, close to the lowest MSE of 1.87. This also implies a future work direction on how PMI could further improve the regression accuracy task, like medicine quantity prescription in this paper.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "pmi",
                    "pmq",
                    "tasks",
                    "type",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, smaller 3-modal backbones that have TinyBERT or MobileBERT as the text modules are critical for on-device deployment in disaster emergency scenarios where internet access to cloud resources is destroyed. However, without PMI, these smaller models struggle with achieving high accuracy. For example, the TinyBERT-GRU-FC&#8217;s top-1 accuracy is as low as 0.64 while its PMI-enabled version reaches the highest 0.7. Furthermore, given the small 3-modal dataset size in D2, the MobileBERT-GRU-FC generates random numbers in its top-3 protocol recommendation outputs, resulting in zero accuracy for three simultaneous tasks (P-M-Q). In contrast, PMI helps MobileBERT-GRU-FC&#8217;s accuracy reach 0.7, which is the highest for the same metric. This indicates PMI&#8217;s potential in increasing small models&#8217; multimodal capability, especially when the dataset size of different modalities is highly imbalanced (123,803 samples in D1, 3005 in D2). As one of the future works, this potential can be further explored and generalized when smaller multimodal on-device models are needed while the size of data modalities is imbalanced.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "text",
                    "mobilebert",
                    "pmi",
                    "tinybert",
                    "models",
                    "pmq",
                    "tasks",
                    "multimodal",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> demonstrates EMSWhisper&#8217;s superior transcription accuracy and robustness. Our Whisper-s and Whisper-m achieve significantly lower WER and CER than three smaller SOTA speech-to-text models across validation and test sets. For instance, on user5&#8217;s Google Glass (GG) test set, Whisper-m attains a WER(CER) of 0.056(0.027), whereas Whisper-t&#8217;s WER(CER) rises to 0.315(0.242), about 5.6(9) times higher. As a WER of 0.1 is generally regarded as the error standard for usable voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MSTestAccMSLearn2024</span>)</cite>, SOTA models struggle with data distribution shifts, with the test set WER exceeding 0.1 due to varying accents and microphone hardware. This challenge is even more critical in EMS settings, where stricter WER requirements render SOTA models impractical for EMS scenarios, further accrediting EMSWhisper&#8217;s ultra-low error rates. Moreover, EMSWhisper, including the Whisper-s and Whisper-m motivated by the scaling law, exhibits exceptional generalization with lower WER variance across users (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)-(b)) and microphones(Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F11\" title=\"Figure 11 &#8227; 5.1.3. Accuracy of Speech Recognition &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(c)-(d)), underscoring its robustness to real-world distribution shifts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F12\" title=\"Figure 12 &#8227; 5.1.4. Accuracy of Objection Detection &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> compares the object detection performance of fine-tuning YOLO11&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">YOLO11Ultralytics2024</span>)</cite> on the image dataset D4(image) labeled by Grounding Dino and labeled by our human-in-the-loop (HITL) annotation adjustment. Our HITL annotation strategy outperforms Grounding Dino&#8217;s annotations, bringing the test mAP value close to 0.8. In contrast, the mAP of all YOLO11 models on images annotated with Grounding Dino is below 0.6, which is unacceptable. The high mAP on the validate set but low mAP and recall on the test set means fine-tuning YOLO11 with Grounding Dino&#8217;s annotations causes the overfitting during the fine-tuning process. For example, using Grounding Dino with the hard text prompts from Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.F5\" title=\"Figure 5 &#8227; 3.3.2. Object detection &#8227; 3.3. Speech-to-text and object detection &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> can achieve an mAP above 0.8, but the test mAP of all YOLO11 models is nearly 0. The recall metric is similar: high validate recall while much lower test recall. This results from Grounding Dino&#8217;s high false positive predictions as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S2\" title=\"2. Background and Motivation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our HITL avoids this problem because we add the human adjustments to correct Grounding Dino&#8217;s false positive annotations, enabling much higher test mAP and recall. It&#8217;s important to note that when compared to direct manual annotations,\nthe advantage of our HITL is that we exploit Grounding Dino&#8217;s accurate (i.e., true positives) annotations to save human labor time on annotations.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "image",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the OCR module in EMSGlass, we evaluate an image dataset of size 204 captured by Google Glass&#8217;s 8MP camera, featuring a user holding a labeled medicine bottle at three distances&#8212;full arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.6m), half arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> 0.3m), and quarter arm (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS5.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.15m)&#8212;to account for varying EMT arm lengths. Each image has two versions: the original image without cropping and the cropped bottle segment. We use the word error rate (WER) and character error rate (CER) to measure OCR accuracy. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, EMSGlass OCR performance was evaluated using four state-of-the-art (SOTA) models listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.T1\" title=\"Table 1 &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: EasyOCR, TesseractOCR, PaddleOCR, CRNN. In general, EasyOCR achieves the lowest WER and CER. Our edit distance (ED)-based matching discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS4\" title=\"3.4. OCR, barcode scanner, and med math &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> is applied after each OCR prediction. As shown, the OCR models with ED-match can achieve significantly decreased WER and CER, indicating the effectiveness of our proposed ED-match method. ED-match helps EasyOCR to decrease WER and CER by 89% and 83%, respectively. EMSGlass employs Easy-Match as its OCR framework, which consistently outperforms other models with WER (CER) below 0.12 (0.05) across all arm distance and cropping conditions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "image",
                    "sota",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the barcode scanner, we only use the original image without cropping. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F13\" title=\"Figure 13 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> (right), we use the success rate as the metric to measure the performance of barcode scanner (developed based on ML Kit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MLKit</span>)</cite> in Android). At the 1/4 Arm, our scanner can achieve a 100% accuracy, indicating its effectiveness. On average, it takes less than 0.5 seconds to process one image, ensuring minimal latency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T5\" title=\"Table 5 &#8227; 5.1.5. Accuracy of OCR and Barcode scanner &#8227; 5.1. Evaluation of EMSNet &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows EMSNet&#8217;s end-to-end (E2E) accuracy on the 3-modal dataset D2, i.e., from the end of original 3-modal inputs to the end of tasks 1-3. Here we use our Whisper-s and Whisper-m for the speech recognition (SR) module, YOLO11n for the object detection (OD) module, and 3 models in the backbone: unimodal MobileBERT, 2-modal MobileBERT-GRU, and 3-modal MobileBERT-GRU-FC. We make two observations from the table: 1) the multimodal models, including the 2-modal and 3-modal models, achieve higher accuracy, 2) the addition of speech recognition and object detection models don&#8217;t degrade the E2E accuracy, indicating seamless integrations with the backbone models.</p>\n\n",
                "matched_terms": [
                    "3modal",
                    "mobilebert",
                    "2modal",
                    "models",
                    "tasks",
                    "multimodal",
                    "our",
                    "dataset",
                    "unimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Three episodes</span>: As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we evaluate EMSServe by running three episodes of multimodal data arriving at different timestamps. Episode 1 includes one speech data, followed by ten continuous vitals data, and then ends up with ten continuous image data. Episode 1 echoes the typical data arrival sequence illustrated in the aforementioned Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.F9\" title=\"Figure 9 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Episodes 2 and 3 randomly shuffle the data sequence in episode 1 with two different random seeds. These three episodes collectively cover different scene data arrival times of multimodal data in real-world EMS events.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "image",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #1: static running without offloading</span>. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F14\" title=\"Figure 14 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we statically run three episodes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.T6\" title=\"Table 6 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> on four real-world hardware specified in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Motivations and insights &#8227; 4. EMSServe design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, including Google Glass, PH1 Phone, and 2 edge servers: Edge-4C and Edge-64X. Directly using PyTorch for multimodal inference on mobile devices and edge servers linearly increases cumulative inference time costs. In contrast, our EMSServe demonstrates much better scalability, incurring minimal cumulative inference time increments in all episodes. This is because the conventional multimodal serving framework repeats text module computation across a whole episode. With the modality-aware splitter splitting the text module and other modal modules apart, EMSServe can cache the text features after processing early arrived symptom speech, and reuse the text feature cache with lately arrived data for subsequent multimodal inference. This avoids repeated text feature cache computations in EMS scenarios where different data arrives asynchronously, achieving 1.9&#215; &#8211; 11.7&#215; speedup on all hardware platforms. In real-world deployments where the length of an episode is longer (e.g., unlimited images in video streaming), EMSServe&#8217;s advantage will be more prominent.</p>\n\n",
                "matched_terms": [
                    "text",
                    "multimodal",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scenario #3: offloading with user mobility</span>. Following up with scenario #2 discussed above, a user wearing Google Glass walks from 0m to 30m and then walks back from 30m to 0m, evaluating the adaptive offloading enabled by the latency monitor thread in EMSServe. This is important because, as discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S1\" title=\"1. Introduction &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, seamless EMS recommendations on the UI display require lower inference latencies while the user moves in EMS scenes. The black triangles in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S5.F15\" title=\"Figure 15 &#8227; 5.2.1. Evaluation plan &#8227; 5.2. Evaluation of EMSServe &#8227; 5. EMSGlass Evaluation &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>(b-c) indicate the on-glass inference when adaptive offloading is employed. Both figures illustrate that decisions on offloading vitals are not sensitive to mobility due to the fast vitals inference on Google Glass. In (b), with automatic on-glass inference enabled by our adaptive offloading design, where image8 distinguishes the adaptive and nonadaptive offloading options, where the adaptive offloading provides lower model serving latency. In (c), the speech arrives at 30m. While the EMT walks towards Edge-4C, the vitals and images arrive in sequence. When the distance is relatively long (image1-image4), the offloading options achieve comparable cumulative serving latency to on-glass options. When the wireless network becomes unstable (image5), the advantage of adaptively choosing on-glass serving becomes prominent, leaving an obvious gap between non-adaptive and adaptive serving.</p>\n\n",
                "matched_terms": [
                    "our",
                    "vitals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate EMSGlass&#8217;s usability and demonstrate its real-world end-to-end usage feasibility, we conducted a user study with 6 certified Emergency Medical Technicians (EMTs). Each EMT uses the full EMSGlass system (i.e., EMSNet and EMSServe) in a simulated emergency room illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A3\" title=\"Appendix C Setup of image data collection room and user study room &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. The user study process involves data perception and real-time decision-making in a simulated end-to-end EMS event, including protocol selection and medication prescription tasks. Following the user study, participants completed a 15-item Likert scale questionnaire (1 = strongly disagree, 5 = strongly agree) covering five dimensions:</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "emsglasss",
                    "selection",
                    "prescription"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel2\" title=\"Figure 16 &#8227; Appendix A Multimodal Dataset Processor &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#A4.SS2\" title=\"D.2. EMT user study assessment &#8227; Appendix D User study process and quantitative scores &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a> presents detailed scores. Overall, EMSGlass achieved consistently high ratings across all five evaluation dimensions, demonstrating its strong usability and technical reliability in simulated end-to-end EMS scenarios. Participants rated usability and interaction positively, with average scores between 4.0 and 4.3, indicating that the system was easy to operate during time-sensitive tasks. Multimodal perception accuracy emerged as a particular strength: voice transcription, pill and alcohol detection, and especially medicine label recognition achieved mean scores between 4.3 and 5, reflecting highly reliable system performance. The clarity and correctness of system recommendations throughout the scenario also received favorable ratings (average above 4.0), confirming that participants were able to follow and trust EMSGlass&#8217;s evolving guidance during emergency response tasks. Taken together, these quantitative results highlight EMSGlass&#8217;s potential to deliver accurate, real-time, and user-friendly decision support for emergency medical care.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "emsglasss",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EMSGlass, a networked smart glass system designed to optimize the EMTs&#8217; workflow by leveraging a multimodal foundation model, EMSFoundation, and an\nadaptive edge-assisted multimodal model serving framework, EMSServe. EMSFoundation is trained on a real-world multimodal EMS dataset to accomplish up to five critical EMS tasks. EMSServe exploits the feature cache during serving multimodal models to handle variable multimodal data arrival times. Extensive experiments and evaluations demonstrate EMSGlass&#8217;s effectiveness, showing significant improvements in both decision accuracy and processing speed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks",
                    "multimodal",
                    "emsglasss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS1\" title=\"3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, our multimodal data processor includes the following steps for vitals data:</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "multimodal",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Padding: for the missed values in raw NEMSIS vitals, we simply add zero at the beginning of the vitals. Our experiments show this practice enables EMSNet to achive higher accuracy.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-sample normalization: NEMSIS dictates different scale ranges for different vitals, e.g., pulse oximetry (PO) in [0, 100] while blood glucose in [0, 2000]. These large numerical values produces the notorious &#8220;NaN&#8221; problems during training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>]</cite>. In addition, different scales of vitals values prevent deep learning models to effectively combine the information from different vitals. To address this problem, we adopt three common normalization methods: z-score,min-max and min-max over z-score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">google_normalization</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rahmad2024comparativenorm</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the design in EMSAssist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">EMSAssist2023MobiSys</span>]</cite> to set the number of protocols at 46. We take all the samples and set the number of medicine at 18. We apply a one-hot encoding process to both protocol and medicine type labels. We apply the same outlier removal and normalizations steps of vitals to the medicine quantity value labels. As medicine quantity labels are single float numbers, we did not apply the padding step, which is only required by the time-series vitals. Thus, task1 and task2 are both essentially single-label multi-class classification problems while the task3 is a regression problem. In the end, we obtain 123,803 samples as D1 that contain 2-modes of inputs (text and vitals) and corresponding labels for three tasks. We further split it into train/validate/test sets with 74821/24761/24761 (3:1:1) samples, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vitals",
                    "tasks",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#S3.SS1\" title=\"3.1. Multimodal multitask data processor &#8227; 3. EMSNet Design &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(a)-(e) are the room setups to collect the image dataset D4(image): 908 train set images are collected from (a)-(d), and 200 test set images from (e). Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13078v1#acmlabel3\" title=\"Figure 17 &#8227; Appendix B Full evaluation of EMSNet on the 3-modal dataset &#8227; A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>(e) is also the room setup used for the user study.</p>\n\n",
                "matched_terms": [
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before each user study, each EMT participant signed a consent form and received a brief orientation to the EMSGlass system and Google Glass hardware. The study began with a short hands-on tutorial introducing key hardware components (display prism, camera, microphone, and touchpad frame), and training participants to operate EMSGlass through simple tap and voice interactions. Participants were then guided through a demonstration scenario showing how EMSGlass transcribes symptoms, displays real-time vitals, detects scene objects (e.g., pills, alcohol bottles), and recommends corresponding EMS protocols, medicines, and dosages. Following the demonstration, participants performed a full simulation independently, interacting with the app to assess a manikin patient by describing symptoms, observing dynamic updates in vitals and recommendations, and completing the scenario by scanning medication labels. Each participant could repeat dry runs until comfortable with the workflow. This structured process ensured that all participants understood EMSGlass&#8217;s multimodal functionalities before proceeding to the formal evaluation.</p>\n\n",
                "matched_terms": [
                    "vitals",
                    "emsglasss",
                    "multimodal"
                ]
            }
        ]
    }
}