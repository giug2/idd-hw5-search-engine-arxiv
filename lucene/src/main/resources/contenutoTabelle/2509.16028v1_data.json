{
    "S4.T1": {
        "caption": "Table 1: Criteria for human evaluation of spoken responses.\nDetailed descriptions are available in Appendix C.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Criteria</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Naturalness:</span> Whether the response sounds like something a human would naturally say in conversation.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness:</span> Whether the response delivers essential information without unnecessary verbosity.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Understandability:</span> How easily the response can be comprehended when spoken aloud.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> Overall impression of the response&#8217;s quality and suitability for spoken delivery.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "response’s",
            "say",
            "overall",
            "naturalness",
            "evaluation",
            "unnecessary",
            "when",
            "spoken",
            "information",
            "essential",
            "understandability",
            "appendix",
            "suitability",
            "delivery",
            "criteria",
            "naturally",
            "sounds",
            "conciseness",
            "like",
            "descriptions",
            "how",
            "impression",
            "responses",
            "something",
            "without",
            "response",
            "delivers",
            "easily",
            "would",
            "available",
            "conversation",
            "comprehended",
            "aloud",
            "verbosity",
            "human",
            "whether",
            "detailed",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
            "<p class=\"ltx_p\">Each output is evaluated along four key dimensions. We provide annotators with the following definitions for each factor, which offer additional guidance beyond the brief descriptions in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "delivery",
                    "naturalness",
                    "conciseness",
                    "spoken",
                    "available"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "without",
                    "human",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "without",
                    "when",
                    "spoken",
                    "how",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "available",
                    "appendix",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "without",
                    "evaluation",
                    "easily"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "response",
                    "delivery",
                    "overall",
                    "conciseness",
                    "whether"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "evaluation",
                    "human",
                    "conciseness",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "understandability",
                    "overall",
                    "naturalness",
                    "evaluation",
                    "criteria",
                    "conciseness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "human",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "spoken",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "information",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "essential",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "whether",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "easily",
                    "evaluation",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Naturalness:</span> measures whether the response sounds like something a real person would say in a conversation. This is NOT a measure of acoustic quality&#8212;focus on the wording and phrasing, not the voice.</p>\n\n",
                "matched_terms": [
                    "say",
                    "something",
                    "response",
                    "naturalness",
                    "sounds",
                    "whether",
                    "would",
                    "like",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness:</span> measures whether the response gets to the point without including unnecessary or excessive information. Focus on whether the response is brief and relevant, or if it feels too long or contains details that aren&#8217;t needed.</p>\n\n",
                "matched_terms": [
                    "without",
                    "response",
                    "unnecessary",
                    "conciseness",
                    "whether",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Understandability:</span> measures how clearly the response communicates its meaning. Focus on whether you can easily grasp what the response is trying to say, without getting lost or confused by the way the information is presented.</p>\n\n",
                "matched_terms": [
                    "say",
                    "understandability",
                    "without",
                    "response",
                    "easily",
                    "whether",
                    "how",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "impression",
                    "response",
                    "overall",
                    "naturalness",
                    "conciseness",
                    "how",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate each evaluation criterion on a 1&#8211;5 Likert scale, where 1 represents the lowest and 5 the highest quality.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "detailed",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "when",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "understandability",
                    "criteria",
                    "naturalness",
                    "evaluation",
                    "verbosity",
                    "human"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Main results comparing different baselines and our proposed methods (TVS (Seq) and (ReVerT) ) across three base Think models. We report (a) task accuracy on GSM8K, 2WikiMultiHopQA, and SciBench; (b) speech-suitability scores using word count (WC), Flesch Reading Ease (FRE), dependency depth (DD), and number of non-vocal characters (NV); and (c) generation latency (T1\\mathrm{T}_{1}, T2\\mathrm{T}_{2}) at the 50th percentile. Speech-suitability scores and latencies are computed on the GSM8K test set. By decoupling thinking and verbalizing (TVS), we substantially improve speech-friendliness while preserving reasoning capabilities of the Chain-of-Thought baseline. Furthermore, the use of the ReVerT model significantly reduces latency. Results of speech-suitability evaluation on additional datasets are presented in Appendix D.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(a) Accuracy (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(b) Speech-suitability</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(c) Latency (s)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Models</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GSM8K</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2MHQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">SciBench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">WC(&#8595;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">FRE(&#8593;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">DD(&#8595;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">NV(&#8595;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathrm{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathrm{T}_{1}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathrm{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathrm{T}_{2}</annotation></semantics></math></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Qwen2.5-Omni-7B</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">84.53</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">14.30</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">20.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">101.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">90.90</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.24</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.78</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Qwen2.5-7B-Instruct</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">92.72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">30.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">50.72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">153.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">67.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.64</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">87.57</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">26.60</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">45.09</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">87.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">84.97</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">11.04</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.46</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-Friendly Finetuning</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">68.69</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">32.70</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">21.97</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.90</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.32</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.28</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.035</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.47</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">93.18</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">29.75</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">47.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">42.15</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.71</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.005</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">8.08</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.43</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">92.65</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">30.00</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">47.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.21</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.45</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Llama-3.1-8B-Instruct</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">85.44</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">17.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">28.32</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">194.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.86</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.42</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">9.079</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.77</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">83.70</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">16.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">22.83</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">101.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">87.99</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.887</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.34</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-Friendly Finetuning</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">65.13</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">42.50</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">14.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">48.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.43</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.034</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.35</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">85.44</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">22.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">26.01</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43.94</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.72</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.026</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.19</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.44</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">85.29</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">19.10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">27.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.89</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.20</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.043</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.67</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.44</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">gpt-4o-mini-2024-07-18</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">94.84</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39.60</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">55.64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">175.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">67.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.69</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">87.26</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">34.40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">34.54</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">73.09</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">82.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.14</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.215</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">94.77</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39.75</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">53.26</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43.83</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.008</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">94.69</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39.55</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">53.32</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">45.92</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.39</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.019</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">-</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechfriendly",
            "nonvocal",
            "speechfriendliness",
            "evaluation",
            "percentile",
            "flesch",
            "wc↓",
            "accuracy",
            "capabilities",
            "2mhqa",
            "results",
            "reasoning",
            "tvs",
            "gpt4omini20240718",
            "datasets",
            "fre",
            "substantially",
            "use",
            "seq",
            "llama318binstruct",
            "scibench",
            "dd↓",
            "t2mathrmt2",
            "our",
            "revert",
            "2wikimultihopqa",
            "speechsuitability",
            "generation",
            "think",
            "preserving",
            "gsm8k",
            "furthermore",
            "scores",
            "dependency",
            "across",
            "count",
            "depth",
            "verbalizing",
            "proposed",
            "additional",
            "prompting",
            "qwen257binstruct",
            "improve",
            "decoupling",
            "models",
            "word",
            "baselines",
            "reading",
            "base",
            "computed",
            "methods",
            "test",
            "fre↑",
            "finetuning",
            "main",
            "thinking",
            "number",
            "set",
            "task",
            "model",
            "qwen25omni7b",
            "significantly",
            "baseline",
            "different",
            "while",
            "chainofthought",
            "latencies",
            "appendix",
            "ease",
            "comparing",
            "presented",
            "nv↓",
            "report",
            "characters",
            "reduces",
            "t1mathrmt1",
            "three",
            "50th",
            "latency"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
            "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "speechfriendly",
                    "while",
                    "revert",
                    "results",
                    "reasoning",
                    "capabilities",
                    "verbalizing",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "prompting",
                    "model",
                    "think",
                    "methods",
                    "reasoning",
                    "chainofthought",
                    "substantially"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "speechfriendly",
                    "revert",
                    "model",
                    "latency",
                    "reasoning",
                    "capabilities",
                    "different",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which enhances the speech-friendliness of generated utterances while preserving the problem-solving capabilities of the underlying reasoning model.</p>\n\n",
                "matched_terms": [
                    "while",
                    "speechfriendliness",
                    "model",
                    "preserving",
                    "reasoning",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reduces",
                    "revert",
                    "model",
                    "latency",
                    "reasoning",
                    "significantly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "prompting",
                    "model",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "reading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "generation",
                    "preserving",
                    "reasoning",
                    "capabilities",
                    "dependency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "model",
                    "think",
                    "reasoning",
                    "accuracy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "think",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "think",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "model",
                    "revert",
                    "think",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "set",
                    "while",
                    "thinking",
                    "revert",
                    "model",
                    "reasoning",
                    "verbalizing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "thinking",
                    "revert",
                    "additional",
                    "model",
                    "generation",
                    "reasoning",
                    "verbalizing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "model",
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "revert",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "speechfriendly",
                    "proposed",
                    "appendix",
                    "three",
                    "revert",
                    "model",
                    "use",
                    "reasoning",
                    "datasets",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "task",
                    "thinking",
                    "revert",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "appendix",
                    "across",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "seq",
                    "reasoning",
                    "tvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "prompting",
                    "model",
                    "use",
                    "qwen25omni7b",
                    "baselines",
                    "reasoning",
                    "baseline",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "use",
                    "revert",
                    "model",
                    "seq",
                    "qwen25omni7b",
                    "llama318binstruct",
                    "qwen257binstruct",
                    "think",
                    "gpt4omini20240718"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "while",
                    "scibench",
                    "three",
                    "2wikimultihopqa",
                    "evaluation",
                    "gsm8k",
                    "reasoning",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "set",
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "report",
                    "evaluation",
                    "reasoning",
                    "accuracy",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "flesch",
                    "count",
                    "word",
                    "depth",
                    "computed",
                    "reading",
                    "ease",
                    "number",
                    "fre",
                    "dependency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "latencies",
                    "report",
                    "use",
                    "revert",
                    "model",
                    "additional",
                    "50th",
                    "latency",
                    "gsm8k",
                    "verbalizing",
                    "percentile"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "scibench",
                    "speechfriendliness",
                    "prompting",
                    "2wikimultihopqa",
                    "qwen25omni7b",
                    "evaluation",
                    "speechsuitability",
                    "gsm8k",
                    "reasoning",
                    "significantly",
                    "chainofthought",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "set",
                    "speechfriendly",
                    "finetuning",
                    "speechfriendliness",
                    "additional",
                    "model",
                    "2wikimultihopqa",
                    "reasoning",
                    "capabilities",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight a fundamental trade-off within the two-stage paradigm: optimizing for reasoning capability tends to degrade speech-suitability, and vice versa.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechsuitability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "while",
                    "task",
                    "scibench",
                    "baselines",
                    "scores",
                    "think",
                    "accuracy",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "three",
                    "revert",
                    "model",
                    "2wikimultihopqa",
                    "seq",
                    "llama318binstruct",
                    "think",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "baselines",
                    "evaluation",
                    "speechsuitability",
                    "results",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "speechfriendliness",
                    "model",
                    "think",
                    "reasoning",
                    "capabilities",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "latency",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "while",
                    "revert",
                    "seq",
                    "latency",
                    "significantly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "models",
                    "task",
                    "scibench",
                    "three",
                    "revert",
                    "model",
                    "speechsuitability",
                    "results",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "reduces",
                    "across",
                    "speechfriendliness",
                    "revert",
                    "model",
                    "speechsuitability",
                    "latency",
                    "reasoning",
                    "accuracy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "while",
                    "reasoning",
                    "model",
                    "use",
                    "results",
                    "methods",
                    "improve",
                    "different",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "additional",
                    "model",
                    "generation",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "use",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "set",
                    "use",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "prompting",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because enforcing all constraints simultaneously in a single instruction yields suboptimal results, we adopt a progressive approach, providing the language model with one constraint at a time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "presented",
                    "finetuning",
                    "proposed",
                    "prompting",
                    "revert",
                    "model",
                    "seq",
                    "qwen25omni7b",
                    "methods",
                    "reasoning",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "model",
                    "seq",
                    "llama318binstruct",
                    "qwen257binstruct",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model",
                    "revert",
                    "generation",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "speechfriendliness",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scibench",
                    "thinking",
                    "three",
                    "use",
                    "2wikimultihopqa",
                    "model",
                    "qwen257binstruct",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "scibench",
                    "use",
                    "2wikimultihopqa",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "our",
                    "main",
                    "additional",
                    "speechsuitability",
                    "evaluation",
                    "results",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "scibench",
                    "prompting",
                    "model",
                    "speechsuitability",
                    "think",
                    "reasoning",
                    "datasets",
                    "chainofthought",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "significantly",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "speechfriendliness",
                    "use",
                    "revert",
                    "model",
                    "2wikimultihopqa",
                    "evaluation",
                    "qwen257binstruct",
                    "think",
                    "results",
                    "gsm8k",
                    "reasoning",
                    "our"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: \nHuman evaluation scores for spoken responses using a 5-point Likert scale. \"Natu.\", \"Conc.\", \"Unde.\", and \"Over.\" denote Naturalness, Conciseness, Understandability, and Overall Quality, respectively. Each cell contains the mean and standard error of the ratings across three datasets. Bold indicates the highest score in each column, and underline indicates the lowest.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2WikiMultiHopQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SciBench</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Natu.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Conc.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Unde.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Over.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Natu.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Conc.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Unde.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Over.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Natu.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Conc.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Unde.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Over.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CoT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.55<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.09</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.72<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.48<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.09</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.28<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.09</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.92</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.16</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.92</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.40</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.53</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.25<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.10<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.16</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.75<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.08<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SFP</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.32<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.67<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.52<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.17<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.47<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.10</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.23<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.12</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.65<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.08</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.35<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.10</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.20<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.02</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.90<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.11</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.03<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.09</sup></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SFF</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.62<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.33<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.52<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.33<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.47<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.12</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.17<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m30\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.60<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m31\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.32<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.23<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m33\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.00<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m34\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.80<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m35\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.10<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m36\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Qwen2.5-Omni-7B</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.40</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m37\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.50</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m38\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.72</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m39\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.50</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m40\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.03<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m41\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.18<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m42\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.47<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m43\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.10<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m44\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.00</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m45\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.16</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.48<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m46\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.70</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m47\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.93</span><sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m48\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.62<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m49\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.40<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m50\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.55<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m51\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.37<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m52\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.35<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m53\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.10<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m54\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.58<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m55\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.33<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m56\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.40<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m57\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.02<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m58\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.82<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m59\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.28<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m60\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.58<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m61\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.45<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m62\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.11</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.23<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m63\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.23<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m64\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.39<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m65\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.21<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m66\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.63<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m67\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.10</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.18<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m68\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.45<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m69\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.07<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m70\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.12</span></sup></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.87<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m71\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.28<sup class=\"ltx_sup\"><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m72\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_medium\">0.09</span></sup></span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "310±pm016",
            "432±pm012",
            "462±pm009",
            "overall",
            "evaluation",
            "440±pm010",
            "each",
            "spoken",
            "underline",
            "error",
            "unde",
            "440±pm011",
            "380±pm012",
            "over",
            "410±pm010",
            "responses",
            "460±pm010",
            "400±pm016",
            "bold",
            "437±pm009",
            "370±pm014",
            "439±pm013",
            "428±pm009",
            "tvs",
            "datasets",
            "433±pm011",
            "463±pm010",
            "433±pm010",
            "seq",
            "method",
            "350±pm014",
            "column",
            "scibench",
            "407±pm012",
            "417±pm013",
            "understandability",
            "425±pm012",
            "revert",
            "2wikimultihopqa",
            "gsm8k",
            "418±pm010",
            "scores",
            "across",
            "375±pm012",
            "5point",
            "mean",
            "382±pm013",
            "indicates",
            "natu",
            "conc",
            "452±pm010",
            "respectively",
            "ratings",
            "420±pm011",
            "standard",
            "human",
            "348±pm015",
            "372±pm015",
            "302±pm018",
            "445±pm011",
            "393±pm014",
            "448±pm009",
            "423±pm013",
            "435±pm010",
            "423±pm015",
            "423±pm012",
            "conciseness",
            "452±pm012",
            "465±pm008",
            "likert",
            "lowest",
            "447±pm010",
            "scale",
            "392±pm016",
            "denote",
            "387±pm014",
            "408±pm010",
            "score",
            "qwen25omni7b",
            "highest",
            "372±pm011",
            "contains",
            "cot",
            "402±pm011",
            "417±pm012",
            "403±pm009",
            "445±pm009",
            "naturalness",
            "447±pm012",
            "390±pm011",
            "435±pm012",
            "455±pm009",
            "400±pm012",
            "sfp",
            "410±pm011",
            "418±pm011",
            "458±pm010",
            "421±pm013",
            "292±pm018",
            "367±pm015",
            "340±pm014",
            "cell",
            "353±pm015",
            "three",
            "403±pm014",
            "sff",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "across",
                    "revert",
                    "naturalness",
                    "conciseness",
                    "method",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "across",
                    "revert",
                    "human",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "cot",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "naturalness",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "revert",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "each",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "standard",
                    "three",
                    "revert",
                    "each",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "each",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "tvs",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "cot",
                    "standard",
                    "qwen25omni7b",
                    "sff",
                    "spoken",
                    "sfp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "qwen25omni7b",
                    "revert",
                    "seq",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "three",
                    "2wikimultihopqa",
                    "evaluation",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "each",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "score",
                    "responses",
                    "overall",
                    "each",
                    "conciseness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "revert",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "5point",
                    "understandability",
                    "responses",
                    "scale",
                    "three",
                    "overall",
                    "naturalness",
                    "evaluation",
                    "each",
                    "conciseness",
                    "human",
                    "likert",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "lowest",
                    "highest",
                    "each",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "score",
                    "responses",
                    "2wikimultihopqa",
                    "highest",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "seq",
                    "three",
                    "revert",
                    "2wikimultihopqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "understandability",
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "overall",
                    "naturalness",
                    "evaluation",
                    "each",
                    "conciseness",
                    "human",
                    "gsm8k",
                    "quality",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "across",
                    "revert",
                    "seq",
                    "spoken",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "three",
                    "revert",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "across",
                    "responses",
                    "revert",
                    "human",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "each",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "over",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "cot",
                    "qwen25omni7b",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "sff",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "method",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scibench",
                    "three",
                    "2wikimultihopqa",
                    "each",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness:</span> measures whether the response gets to the point without including unnecessary or excessive information. Focus on whether the response is brief and relevant, or if it feels too long or contains details that aren&#8217;t needed.</p>\n\n",
                "matched_terms": [
                    "contains",
                    "conciseness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "naturalness",
                    "conciseness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "ratings",
                    "three",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate each evaluation criterion on a 1&#8211;5 Likert scale, where 1 represents the lowest and 5 the highest quality.</p>\n\n",
                "matched_terms": [
                    "likert",
                    "scale",
                    "highest",
                    "evaluation",
                    "each",
                    "lowest",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each criterion, provide a brief explanation to justify the assigned score.</p>\n\n",
                "matched_terms": [
                    "score",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "across",
                    "responses",
                    "scibench",
                    "2wikimultihopqa",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "each",
                    "human",
                    "method",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "scibench",
                    "standard",
                    "datasets",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "understandability",
                    "naturalness",
                    "evaluation",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "evaluation",
                    "each",
                    "gsm8k"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Comparison of verbalization abilities across different ReVerT model sizes. Speech-suitability scores, consisting of word count (WC) and Flesch Reading Ease (FRE), are calculated on GSM8K.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Accuracy (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Speech-suitability</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SciBench</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">WC (&#8595;)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">FRE (&#8593;)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">92.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">50.7</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8211;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8211;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">92.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.3</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.5B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">92.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">45.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.9</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">91.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.1</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.6</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "15b",
            "word",
            "scibench",
            "size",
            "calculated",
            "reading",
            "comparison",
            "verbalization",
            "flesch",
            "revert",
            "abilities",
            "speechsuitability",
            "sizes",
            "gsm8k",
            "consisting",
            "ease",
            "accuracy",
            "scores",
            "across",
            "count",
            "fre",
            "model",
            "05b",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "across",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "revert",
                    "sizes",
                    "different",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "model",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "abilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "reading"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "accuracy",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "abilities",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "flesch",
                    "count",
                    "word",
                    "reading",
                    "ease",
                    "fre"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "verbalization",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "speechsuitability",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "speechsuitability",
                    "scores",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "scores",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "speechsuitability",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "different",
                    "across",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "across",
                    "revert",
                    "model",
                    "speechsuitability",
                    "accuracy",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "revert",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "model",
                    "revert",
                    "verbalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "across",
                    "model",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "across",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "speechsuitability",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "speechsuitability",
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scibench",
                    "revert",
                    "gsm8k"
                ]
            }
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Prompts for dataset construction.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Prompts for Data Construction</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Solve</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Provide a step-by-step reasoning process before arriving at the final answer.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Summarize</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Write a clear, concise, and speech-friendly summary of the provided analysis. Ensure the summary flows naturally when read aloud, avoiding complex sentences, mathematical equations or awkward phrasing. Follow the analysis&#8217;s logical progression, presenting key points in the same order and context. Include only essential steps, omitting unnecessary details, boilerplate text, or repetitions. The general tone of the summary must match that of the original analysis. The summary must be appropriate as the response to the original question. Do not apply LaTeX or markdown formatting in your summary.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>First, extract the essential information from the analysis.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Now, make sure the logical progression of the summary matches the order of the analysis, without adding or removing information.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Now, remove all parts that are repeating the information from the original question.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Now, make sure the content is speech-friendly be breaking up complex sentences and avoiding long and hard-to-read words.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Scatter</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Combine an analysis with its corresponding summary by interleaving summary segments with relevant analysis portions.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Ensure the summary segments immediately follow the equivalent content within the analysis and are enclosed with &#8216;&lt;bov&gt;&#8216; and &#8216;&lt;eov&gt;&#8216; tags.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Steps\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>1. **Receive analysis and summary**: Identify sections in both the analysis and the summary.\n<br class=\"ltx_break\"/>2. **Match content segments**: Pair each summary segment with the corresponding analysis segment to ensure logical flow.\n<br class=\"ltx_break\"/>3. **Interleave content**: Insert summary segments after the matching sections of the analysis, surrounded by &#8216;&lt;bov&gt;&#8216; and &#8216;&lt;eov&gt;&#8216; tags.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Output Format\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>The output should maintain the structure of the analysis, with summary segments appropriately interleaved. Each summary segment must be enclosed in &#8216;&lt;bov&gt;&#8216; and &#8216;&lt;eov&gt;&#8216; tags immediately after the equivalent analysis content.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Notes\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>- Maintain the logical sequence of both the analysis and summary.\n<br class=\"ltx_break\"/>- Ensure clarity in how summary elements align with the analysis.\n<br class=\"ltx_break\"/>- Avoid rephrasing the given segments; use them verbatim for consistency and accuracy.\n<br class=\"ltx_break\"/>- The summary and analysis must not change from the original text.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arriving",
            "speechfriendly",
            "prompts",
            "include",
            "markdown",
            "details",
            "each",
            "words",
            "equations",
            "must",
            "scatter",
            "information",
            "equivalent",
            "concise",
            "from",
            "accuracy",
            "appropriate",
            "first",
            "how",
            "immediately",
            "elements",
            "clarity",
            "analysis",
            "response",
            "maintain",
            "both",
            "reasoning",
            "only",
            "matches",
            "points",
            "omitting",
            "use",
            "given",
            "steps",
            "logical",
            "format",
            "parts",
            "complex",
            "matching",
            "consistency",
            "final",
            "write",
            "follow",
            "portions",
            "unnecessary",
            "general",
            "not",
            "‘bov‘",
            "its",
            "construction",
            "provide",
            "before",
            "match",
            "context",
            "after",
            "presenting",
            "ensure",
            "‘eov‘",
            "text",
            "segment",
            "interleaving",
            "identify",
            "appropriately",
            "notes",
            "without",
            "repetitions",
            "insert",
            "clear",
            "provided",
            "phrasing",
            "tags",
            "segments",
            "avoiding",
            "relevant",
            "order",
            "sections",
            "stepbystep",
            "apply",
            "content",
            "key",
            "output",
            "make",
            "when",
            "rephrasing",
            "corresponding",
            "breaking",
            "remove",
            "naturally",
            "them",
            "sentences",
            "combine",
            "latex",
            "enclosed",
            "removing",
            "structure",
            "pair",
            "flows",
            "sure",
            "boilerplate",
            "extract",
            "verbatim",
            "same",
            "within",
            "summarize",
            "question",
            "all",
            "answer",
            "data",
            "summary",
            "repeating",
            "now",
            "interleave",
            "progression",
            "original",
            "formatting",
            "align",
            "adding",
            "receive",
            "essential",
            "change",
            "your",
            "read",
            "long",
            "avoid",
            "flow",
            "tone",
            "surrounded",
            "awkward",
            "analysis’s",
            "interleaved",
            "process",
            "sequence",
            "aloud",
            "solve",
            "mathematical",
            "dataset",
            "hardtoread"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "from",
                    "reasoning",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "corresponding",
                    "content",
                    "output",
                    "reasoning",
                    "not",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "process",
                    "without",
                    "output",
                    "response",
                    "concise",
                    "both",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "question",
                    "data",
                    "reasoning",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "dataset",
                    "them",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "points",
                    "within",
                    "each",
                    "equations",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "text",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "sections",
                    "without",
                    "content",
                    "output",
                    "mathematical",
                    "first",
                    "make",
                    "when",
                    "equations",
                    "text",
                    "how",
                    "information",
                    "latex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "content",
                    "output",
                    "response",
                    "both",
                    "reasoning",
                    "accuracy",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "output",
                    "reasoning",
                    "speechfriendly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "output",
                    "segments",
                    "after",
                    "text",
                    "each",
                    "reasoning",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "summary",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Speak</span> stage, we convert the utterances to speech using a TTS model. Specifically, we employ a TTS model that supports both streaming input and output, allowing the system to process streaming outputs from the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage and play the generated speech with minimal delay for the user.</p>\n\n",
                "matched_terms": [
                    "output",
                    "from",
                    "both",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "format",
                    "construction",
                    "data",
                    "reasoning",
                    "dataset",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "corresponding",
                    "reasoning",
                    "response",
                    "segments",
                    "steps",
                    "data",
                    "each",
                    "must"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "enclosed",
                    "speechfriendly",
                    "segments",
                    "text",
                    "each",
                    "reasoning",
                    "summary",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "format",
                    "speechfriendly",
                    "prompts",
                    "output",
                    "each",
                    "scatter",
                    "its",
                    "after",
                    "immediately",
                    "enclosed",
                    "provided",
                    "reasoning",
                    "process",
                    "use",
                    "summarize",
                    "solve",
                    "steps",
                    "all",
                    "data",
                    "dataset",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "process",
                    "within",
                    "sequence",
                    "segments",
                    "data",
                    "each",
                    "reasoning",
                    "only",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "within",
                    "segments",
                    "from",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th token in the response sequence <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>, and <math alttext=\"p_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">p_{\\theta}</annotation></semantics></math> is the model&#8217;s output probability.</p>\n\n",
                "matched_terms": [
                    "output",
                    "response",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "provided",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "same",
                    "speechfriendly",
                    "output",
                    "include",
                    "analysis",
                    "reasoning",
                    "use",
                    "summarize",
                    "concise",
                    "scatter",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "complex",
                    "without",
                    "steps",
                    "question",
                    "mathematical",
                    "answer",
                    "equations",
                    "reasoning",
                    "dataset",
                    "not",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "final",
                    "provided",
                    "each",
                    "reasoning",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "response",
                    "text",
                    "each",
                    "from",
                    "words",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "use",
                    "output",
                    "after",
                    "all",
                    "dataset",
                    "segment",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "within",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "apply",
                    "speechfriendly",
                    "its",
                    "when",
                    "reasoning",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "words",
                    "reasoning",
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight a fundamental trade-off within the two-stage paradigm: optimizing for reasoning capability tends to degrade speech-suitability, and vice versa.</p>\n\n",
                "matched_terms": [
                    "within",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "both",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speechfriendly",
                    "all",
                    "data",
                    "both",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "all",
                    "each",
                    "both",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "before",
                    "both",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "process",
                    "before",
                    "segments",
                    "reasoning",
                    "segment",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "its",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "from",
                    "both",
                    "reasoning",
                    "accuracy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "its",
                    "use",
                    "from",
                    "reasoning",
                    "adding",
                    "not",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "content",
                    "without",
                    "from",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "information",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "from",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "use",
                    "all",
                    "data",
                    "from",
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "reasoning",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "process",
                    "solve",
                    "from",
                    "reasoning",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "essential",
                    "process",
                    "all",
                    "from",
                    "reasoning",
                    "must",
                    "summary",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must follow the same logical progression as the reasoning process.</p>\n\n",
                "matched_terms": [
                    "same",
                    "logical",
                    "progression",
                    "process",
                    "follow",
                    "reasoning",
                    "must",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must not repeat information provided in the question.</p>\n\n",
                "matched_terms": [
                    "question",
                    "provided",
                    "must",
                    "summary",
                    "not",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must be speech-friendly and free of complex sentences or hard-to-read words.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "complex",
                    "sentences",
                    "words",
                    "must",
                    "summary",
                    "hardtoread"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "its",
                    "corresponding",
                    "process",
                    "use",
                    "segments",
                    "after",
                    "them",
                    "each",
                    "reasoning",
                    "summary",
                    "segment",
                    "immediately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "prompts",
                    "follow",
                    "provided",
                    "both",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "within",
                    "all",
                    "speechfriendly",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "use",
                    "final",
                    "response",
                    "all",
                    "reasoning",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "format",
                    "matching",
                    "context",
                    "clear",
                    "answer",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we use gpt-4.1-mini-2025-04-11 to automatically assess answer correctness. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F7\" title=\"Figure 7 &#8227; B.4 LLM-Based Answer Verification &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the verifier is prompted with the question, a model-generated response, and the corresponding ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "use",
                    "response",
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "details",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each output is evaluated along four key dimensions. We provide annotators with the following definitions for each factor, which offer additional guidance beyond the brief descriptions in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "key",
                    "output",
                    "each",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Naturalness:</span> measures whether the response sounds like something a real person would say in a conversation. This is NOT a measure of acoustic quality&#8212;focus on the wording and phrasing, not the voice.</p>\n\n",
                "matched_terms": [
                    "phrasing",
                    "not",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness:</span> measures whether the response gets to the point without including unnecessary or excessive information. Focus on whether the response is brief and relevant, or if it feels too long or contains details that aren&#8217;t needed.</p>\n\n",
                "matched_terms": [
                    "without",
                    "response",
                    "relevant",
                    "unnecessary",
                    "details",
                    "long",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Understandability:</span> measures how clearly the response communicates its meaning. Focus on whether you can easily grasp what the response is trying to say, without getting lost or confused by the way the information is presented.</p>\n\n",
                "matched_terms": [
                    "its",
                    "without",
                    "response",
                    "how",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "your",
                    "response",
                    "all",
                    "general",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "data",
                    "each",
                    "from",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Carefully read the question and listen to the speech-based response before rating.</p>\n\n",
                "matched_terms": [
                    "question",
                    "before",
                    "read",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each criterion, provide a brief explanation to justify the assigned score.</p>\n\n",
                "matched_terms": [
                    "each",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "all",
                    "use",
                    "receive",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "each",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "formatting",
                    "complex",
                    "provide",
                    "maintain",
                    "steps",
                    "when",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "provide",
                    "use",
                    "all",
                    "each",
                    "both",
                    "from",
                    "reasoning",
                    "dataset"
                ]
            }
        ]
    },
    "A3.T6": {
        "caption": "Table 6: \nDetailed speech-suitability scores across all three datasets, comparing various approaches: chain-of-thought (CoT), speech-friendly prompting (SFP), finetuning (SFF), Qwen2.5-Omni-7B, and our proposed methods, TVS (Seq) and TVS (ReVerT). Results show consistent trends across all datasets. As we move from 2WikiMultiHopQA (denoted as 2MHQA) to GSM8K and then SciBench, the tasks increasingly demand stronger reasoning capabilities. Correspondingly, the length of test-time reasoning grows and overall speech-suitability decreases.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WC(&#8595;)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FRE(&#8593;)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DD(&#8595;)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NV(&#8595;)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Models</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2MHQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GSM8K</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">SciBench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2MHQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GSM8K</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">SciBench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2MHQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GSM8K</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">SciBench</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2MHQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">GSM8K</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">SciBench</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Qwen2.5-Omni-7B</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">49.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">101.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">138.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">90.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">72.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.51</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.24</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.95</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.004</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.78</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">30.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Qwen2.5-7B-Instruct</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CoT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">149.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">194.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">436.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">51.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">61.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.51</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.42</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.18</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.737</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">9.079</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">327.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SFP</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">53.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">101.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">326.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">59.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">68.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.44</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.33</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.22</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.449</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.887</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">122.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SFF</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">40.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">48.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">80.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">70.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">75.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.13</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.011</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.034</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">46.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">90.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">71.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.80</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.49</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.021</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.026</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.7</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">30.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">78.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.79</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.20</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.075</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.043</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">3.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Llama-3.1-8B-Instruct</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CoT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">145.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">153.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">296.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">58.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">61.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.85</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.08</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">14.604</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">67.109</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">384.7</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SFP</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">27.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">87.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">212.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">61.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">85.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">68.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.70</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.45</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.69</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.155</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">11.043</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">243.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SFF</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">84.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">71.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">75.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.48</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.28</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.29</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.015</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.035</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">12.5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">45.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">42.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">79.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">70.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">72.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.62</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.52</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.019</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.005</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">41.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">44.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">76.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">71.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">72.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.55</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.21</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.43</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.173</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">1.9</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"13\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">gpt-4o-mini-2024-07-18</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CoT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">104.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">175.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">291.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">53.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">67.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">63.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.85</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.55</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">11.463</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.687</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">352.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SFP</th>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">20.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">73.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">213.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">58.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">82.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">57.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.79</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.14</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.011</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.215</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">54.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">43.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">79.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">69.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">72.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.63</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.27</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.52</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.014</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.008</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">6.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">32.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">45.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">74.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">70.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">88.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">73.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.63</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">4.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.52</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.041</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.019</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.3</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "demand",
            "models",
            "speechfriendly",
            "cot",
            "scibench",
            "length",
            "overall",
            "dd↓",
            "various",
            "stronger",
            "chainofthought",
            "our",
            "correspondingly",
            "move",
            "revert",
            "trends",
            "2wikimultihopqa",
            "decreases",
            "speechsuitability",
            "show",
            "from",
            "tasks",
            "testtime",
            "methods",
            "gsm8k",
            "sfp",
            "capabilities",
            "wc↓",
            "fre↑",
            "scores",
            "across",
            "comparing",
            "nv↓",
            "finetuning",
            "2mhqa",
            "results",
            "reasoning",
            "tvs",
            "gpt4omini20240718",
            "datasets",
            "then",
            "proposed",
            "denoted",
            "three",
            "prompting",
            "qwen25omni7b",
            "seq",
            "llama318binstruct",
            "all",
            "sff",
            "consistent",
            "increasingly",
            "qwen257binstruct",
            "detailed",
            "grows",
            "approaches"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "speechfriendly",
                    "revert",
                    "increasingly",
                    "show",
                    "from",
                    "results",
                    "reasoning",
                    "capabilities",
                    "approaches",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "increasingly",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "then",
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "prompting",
                    "testtime",
                    "methods",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "speechfriendly",
                    "revert",
                    "reasoning",
                    "capabilities",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which enhances the speech-friendliness of generated utterances while preserving the problem-solving capabilities of the underlying reasoning model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cot",
                    "prompting",
                    "tasks",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "reasoning",
                    "capabilities",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "then",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "then",
                    "speechfriendly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "denoted",
                    "speechfriendly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "speechfriendly",
                    "proposed",
                    "three",
                    "revert",
                    "all",
                    "reasoning",
                    "detailed",
                    "datasets",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "then",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "tvs",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "cot",
                    "prompting",
                    "qwen25omni7b",
                    "sff",
                    "reasoning",
                    "sfp",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "qwen25omni7b",
                    "seq",
                    "llama318binstruct",
                    "all",
                    "qwen257binstruct",
                    "gpt4omini20240718"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "three",
                    "2wikimultihopqa",
                    "gsm8k",
                    "reasoning",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "from",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "all",
                    "revert",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "across",
                    "three",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "speechsuitability",
                    "results",
                    "reasoning",
                    "capabilities",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "scibench",
                    "prompting",
                    "2wikimultihopqa",
                    "qwen25omni7b",
                    "overall",
                    "speechsuitability",
                    "gsm8k",
                    "reasoning",
                    "chainofthought",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "2wikimultihopqa",
                    "reasoning",
                    "capabilities",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight a fundamental trade-off within the two-stage paradigm: optimizing for reasoning capability tends to degrade speech-suitability, and vice versa.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechsuitability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "scores",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "three",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "llama318binstruct",
                    "all",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "overall",
                    "all",
                    "speechsuitability",
                    "show",
                    "results",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "capabilities",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "revert",
                    "seq",
                    "reasoning",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scibench",
                    "three",
                    "revert",
                    "speechsuitability",
                    "from",
                    "results",
                    "gsm8k",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "across",
                    "revert",
                    "show",
                    "speechsuitability",
                    "from",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "testtime",
                    "from",
                    "results",
                    "methods",
                    "reasoning",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "overall",
                    "from",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "consistent",
                    "all",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "2wikimultihopqa",
                    "from",
                    "gsm8k",
                    "detailed",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "all",
                    "from",
                    "2wikimultihopqa",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "prompting",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "all",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because enforcing all constraints simultaneously in a single instruction yields suboptimal results, we adopt a progressive approach, providing the language model with one constraint at a time.</p>\n\n",
                "matched_terms": [
                    "all",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "proposed",
                    "cot",
                    "prompting",
                    "revert",
                    "seq",
                    "qwen25omni7b",
                    "methods",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "seq",
                    "llama318binstruct",
                    "all",
                    "sff",
                    "qwen257binstruct",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "all",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scibench",
                    "three",
                    "2wikimultihopqa",
                    "all",
                    "qwen257binstruct",
                    "from",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "three",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "across",
                    "scibench",
                    "2wikimultihopqa",
                    "all",
                    "gsm8k",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speechsuitability",
                    "results",
                    "detailed",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "revert",
                    "2wikimultihopqa",
                    "all",
                    "qwen257binstruct",
                    "from",
                    "results",
                    "gsm8k",
                    "reasoning",
                    "approaches",
                    "various",
                    "our"
                ]
            }
        ]
    },
    "A3.T7": {
        "caption": "Table 7: Inter-criteria correlations for human evaluation metrics using Spearman’s rank correlation coefficient (SRCC). The table presents pairwise correlations between Naturalness (Natu.), Conciseness (Conc.), Understandability (Unde.), and Overall Quality (Over.) scores.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">SRCC</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Natu.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Conc.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Unde.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Over.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Natu.</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.757</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.550</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.878</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Conc.</th>\n<td class=\"ltx_td ltx_align_right\">0.757</td>\n<td class=\"ltx_td ltx_align_right\">1.000</td>\n<td class=\"ltx_td ltx_align_right\">0.594</td>\n<td class=\"ltx_td ltx_align_right\">0.761</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Unde.</th>\n<td class=\"ltx_td ltx_align_right\">0.550</td>\n<td class=\"ltx_td ltx_align_right\">0.594</td>\n<td class=\"ltx_td ltx_align_right\">1.000</td>\n<td class=\"ltx_td ltx_align_right\">0.700</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Over.</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.878</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.761</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.700</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">1.000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "naturalness",
            "evaluation",
            "coefficient",
            "correlations",
            "unde",
            "understandability",
            "pairwise",
            "conciseness",
            "between",
            "scores",
            "over",
            "presents",
            "spearman’s",
            "metrics",
            "intercriteria",
            "natu",
            "conc",
            "correlation",
            "rank",
            "human",
            "srcc",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "conciseness",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics",
                    "conciseness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "understandability",
                    "overall",
                    "naturalness",
                    "evaluation",
                    "conciseness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "evaluation",
                    "human",
                    "conciseness",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "understandability",
                    "rank",
                    "overall",
                    "naturalness",
                    "evaluation",
                    "conciseness",
                    "human",
                    "quality",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "presents",
                    "human",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "naturalness",
                    "conciseness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate each evaluation criterion on a 1&#8211;5 Likert scale, where 1 represents the lowest and 5 the highest quality.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "presents"
                ]
            }
        ]
    },
    "A4.T8": {
        "caption": "Table 8: Sample generation results for various methods on GSM8K. Our models, TVS (Seq) and TVS (ReVerT), consistently produce accurate answers with logically sound, step-by-step reasoning, while maintaining high speech-suitableness. In contrast, speech-friendly baseline methods frequently generate answers that are not only incorrect but also logically flawed.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sample text responses for various methods on GSM8K</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Question</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Each class in a school has 20 students. There are 3 classes. The school as a whole is 50% boys and 50% girls. The first class has 15 girls. The second class has 12 girls. How many boys are in the third class?</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Answer</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">17</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">To determine the number of boys in the third class, we need to follow these steps:\n<br class=\"ltx_break\"/>1. <span class=\"ltx_text ltx_font_bold\">Calculate the total number of students in the school:\n<br class=\"ltx_break\"/></span>- Each class has 20 students.\n<br class=\"ltx_break\"/>- There are 3 classes.</span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex1\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Total number of students}=20\\times 3=60\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex1.m1\" intent=\":literal\"><semantics><mrow><mtext>Total number of students</mtext><mo>=</mo><mrow><mn>20</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow><mo>=</mo><mn>60</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Total number of students}=20\\times 3=60</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_p\">2. <span class=\"ltx_text ltx_font_bold\">Determine the number of boys and girls in the school:\n<br class=\"ltx_break\"/></span>- The school is 50% boys and 50% girls.</span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex2\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Number of boys}=\\frac{50}{100}\\times 60=30\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex2.m1\" intent=\":literal\"><semantics><mrow><mtext>Number of boys</mtext><mo>=</mo><mrow><mfrac><mn>50</mn><mn>100</mn></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>60</mn></mrow><mo>=</mo><mn>30</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Number of boys}=\\frac{50}{100}\\times 60=30</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex3\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Number of girls}=\\frac{50}{100}\\times 60=30\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex3.m1\" intent=\":literal\"><semantics><mrow><mtext>Number of girls</mtext><mo>=</mo><mrow><mfrac><mn>50</mn><mn>100</mn></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>60</mn></mrow><mo>=</mo><mn>30</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Number of girls}=\\frac{50}{100}\\times 60=30</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_p\">3. <span class=\"ltx_text ltx_font_bold\">Find the number of girls in the first and second classes:\n<br class=\"ltx_break\"/></span>- The first class has 15 girls.\n<br class=\"ltx_break\"/>- The second class has 12 girls.\n<br class=\"ltx_break\"/>4. <span class=\"ltx_text ltx_font_bold\">Calculate the total number of girls in the first two classes:</span></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex4\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Total number of girls in first and second classes}=15+12=27\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex4.m1\" intent=\":literal\"><semantics><mrow><mtext>Total number of girls in first and second classes</mtext><mo>=</mo><mrow><mn>15</mn><mo>+</mo><mn>12</mn></mrow><mo>=</mo><mn>27</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Total number of girls in first and second classes}=15+12=27</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_p\">5. <span class=\"ltx_text ltx_font_bold\">Determine the number of girls in the third class:\n<br class=\"ltx_break\"/></span>- Total number of girls in the school is 30.</span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex5\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Number of girls in the third class}=30-27=3\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex5.m1\" intent=\":literal\"><semantics><mrow><mtext>Number of girls in the third class</mtext><mo>=</mo><mrow><mn>30</mn><mo>&#8722;</mo><mn>27</mn></mrow><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Number of girls in the third class}=30-27=3</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_p\">6. <span class=\"ltx_text ltx_font_bold\">Calculate the number of boys in the third class:\n<br class=\"ltx_break\"/></span>- Each class has 20 students.\n<br class=\"ltx_break\"/>- The third class has 3 girls.</span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex6\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Number of boys in the third class}=20-3=17\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex6.m1\" intent=\":literal\"><semantics><mrow><mtext>Number of boys in the third class</mtext><mo>=</mo><mrow><mn>20</mn><mo>&#8722;</mo><mn>3</mn></mrow><mo>=</mo><mn>17</mn></mrow><annotation encoding=\"application/x-tex\">\\text{Number of boys in the third class}=20-3=17</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<span class=\"ltx_p\">Thus, the number of boys in the third class is <math alttext=\"\\boxed{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{17}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T8.m1\" intent=\":literal\"><semantics><menclose notation=\"box\"><mtext class=\"ltx_mathvariant_bold\">17</mtext></menclose><annotation encoding=\"application/x-tex\">\\boxed{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{17}}}</annotation></semantics></math>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">In each class, there are 20 students. Since the school is 50% boys and 50% girls, each class also has 10 boys and 10 girls. The first class has 15 girls, which means it has 5 boys (since it&#8217;s supposed to have 10 girls, but there are 15, so 20 - 15 = 5 boys). The second class has 12 girls, which means it has 8 boys (since it&#8217;s supposed to have 10 girls, but there are 12, so 20 - 12 = 8 boys). So far, we have:\n<br class=\"ltx_break\"/>- First class: 5 boys &#8195;- Second class: 8 boys\n<br class=\"ltx_break\"/>That&#8217;s a total of 13 boys in the first two classes. Since each class is supposed to have 10 boys, the third class must have 7 boys (because 10 - 3 = 7, where 3 is the number of boys already counted in the first two classes). So, there are <span class=\"ltx_text ltx_font_bold\">7</span> boys in the third class.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Finetuning</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The first class has 15 girls, so it has 5 boys. The second class has 12 girls, which means it has 8 boys. Together, these two classes have 23 boys. Since each class has 20 students, there are 60 boys in total. That means the third class must have <span class=\"ltx_text ltx_font_bold\">37</span> boys.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The school has 30 boys and 30 girls. The first class has 15 girls. The second class has 12 girls. Together, they have 27 girls. That means the third class has 3 girls. Since each class has 20 students, the third class has 17 boys. So, there are <span class=\"ltx_text ltx_font_bold\">17</span> boys in the third class.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">There are 60 students in total. The school is half boys and half girls. That means there are 30 boys. The first class has 15 girls. The second class has 12 girls. Together, they have 27 girls. If we take 27 away from 30, we get 3 girls in the third class. Since each class has 20 students, the third class has <span class=\"ltx_text ltx_font_bold\">17</span> boys.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "speechfriendly",
            "supposed",
            "high",
            "get",
            "each",
            "must",
            "various",
            "also",
            "consistently",
            "from",
            "determine",
            "first",
            "how",
            "girls",
            "responses",
            "frequently",
            "results",
            "reasoning",
            "tvs",
            "only",
            "where",
            "incorrect",
            "seq",
            "steps",
            "because",
            "maintaining",
            "they",
            "class20317",
            "answers",
            "together",
            "follow",
            "class30273",
            "whole",
            "far",
            "not",
            "our",
            "classes",
            "revert",
            "generate",
            "generation",
            "students20×360texttotal",
            "gsm8k",
            "take",
            "text",
            "logically",
            "boys",
            "17boxedcolorrgb000definecolornamedpgfstrokecolorrgb000textbf17",
            "total",
            "prompting",
            "since",
            "class30−273textnumber",
            "there",
            "calculate",
            "stepbystep",
            "models",
            "find",
            "which",
            "thus",
            "away",
            "flawed",
            "classes151227texttotal",
            "methods",
            "girlsfrac50100times",
            "finetuning",
            "boys50100×6030textnumber",
            "school",
            "accurate",
            "girls50100×6030textnumber",
            "number",
            "question",
            "answer",
            "baseline",
            "classes151227",
            "while",
            "second",
            "have",
            "need",
            "has",
            "chainofthought",
            "two",
            "class",
            "third",
            "sound",
            "that’s",
            "it’s",
            "already",
            "students",
            "boysfrac50100times",
            "speechsuitableness",
            "contrast",
            "class20−317textnumber",
            "produce",
            "half",
            "many",
            "means",
            "students20times",
            "counted"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "produce",
                    "also",
                    "revert",
                    "from",
                    "results",
                    "reasoning",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "produce",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "produce",
                    "responses",
                    "prompting",
                    "which",
                    "methods",
                    "reasoning",
                    "not",
                    "first",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "revert",
                    "which",
                    "accurate",
                    "reasoning",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which enhances the speech-friendliness of generated utterances while preserving the problem-solving capabilities of the underlying reasoning model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "while",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "question",
                    "speechfriendly",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "prompting",
                    "have",
                    "reasoning",
                    "has",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "produce",
                    "which",
                    "each",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "generation",
                    "reasoning",
                    "text",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "second",
                    "which",
                    "since",
                    "first",
                    "text",
                    "how",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "which",
                    "reasoning",
                    "only",
                    "two",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "while",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "revert",
                    "which",
                    "generation",
                    "each",
                    "reasoning",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "since",
                    "reasoning",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "since",
                    "steps",
                    "each",
                    "must"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "total",
                    "number",
                    "each",
                    "reasoning",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "revert",
                    "steps",
                    "generate",
                    "each",
                    "because",
                    "reasoning",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "revert",
                    "since",
                    "each",
                    "reasoning",
                    "only",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "total",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "which",
                    "seq",
                    "reasoning",
                    "tvs",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "speechfriendly",
                    "produce",
                    "finetuning",
                    "responses",
                    "prompting",
                    "generate",
                    "reasoning",
                    "baseline",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "seq",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "while",
                    "steps",
                    "question",
                    "answer",
                    "gsm8k",
                    "reasoning",
                    "not",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "from",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "each",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "also",
                    "text",
                    "each",
                    "from",
                    "not",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "produce",
                    "revert",
                    "since",
                    "generate",
                    "gsm8k",
                    "has",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "responses",
                    "each",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "each",
                    "results",
                    "reasoning",
                    "baseline",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "also",
                    "prompting",
                    "two",
                    "gsm8k",
                    "reasoning",
                    "only",
                    "where",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "responses",
                    "high",
                    "contrast",
                    "reasoning",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "while",
                    "two",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "also",
                    "revert",
                    "seq",
                    "text",
                    "where",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "seq",
                    "high",
                    "consistently",
                    "each",
                    "results",
                    "gsm8k",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "which",
                    "seq",
                    "reasoning",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "first",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "while",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "from",
                    "results",
                    "gsm8k",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "revert",
                    "which",
                    "from",
                    "reasoning",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "while",
                    "where",
                    "second",
                    "third",
                    "from",
                    "results",
                    "methods",
                    "reasoning",
                    "has",
                    "not",
                    "first",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "already",
                    "generation",
                    "from",
                    "because",
                    "reasoning",
                    "thus",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "gsm8k",
                    "first",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "each",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "prompting",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "generate",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "must",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must follow the same logical progression as the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "must",
                    "follow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must not repeat information provided in the question.</p>\n\n",
                "matched_terms": [
                    "question",
                    "must",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must be speech-friendly and free of complex sentences or hard-to-read words.</p>\n\n",
                "matched_terms": [
                    "must",
                    "speechfriendly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because enforcing all constraints simultaneously in a single instruction yields suboptimal results, we adopt a progressive approach, providing the language model with one constraint at a time.</p>\n\n",
                "matched_terms": [
                    "because",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "prompting",
                    "revert",
                    "seq",
                    "follow",
                    "methods",
                    "reasoning",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "seq",
                    "steps",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "generate",
                    "generation",
                    "determine",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "means",
                    "responses",
                    "which",
                    "answer",
                    "not",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we use gpt-4.1-mini-2025-04-11 to automatically assess answer correctness. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F7\" title=\"Figure 7 &#8227; B.4 LLM-Based Answer Verification &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the verifier is prompted with the question, a model-generated response, and the corresponding ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "each",
                    "from",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each output is evaluated along four key dimensions. We provide annotators with the following definitions for each factor, which offer additional guidance beyond the brief descriptions in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality:</span> measures your general impression of the response, taking into account all aspects such as clarity, naturalness, and conciseness. Focus on how well the response works as a whole.</p>\n\n",
                "matched_terms": [
                    "whole",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate each evaluation criterion on a 1&#8211;5 Likert scale, where 1 represents the lowest and 5 the highest quality.</p>\n\n",
                "matched_terms": [
                    "each",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "each",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "responses",
                    "prompting",
                    "steps",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "from",
                    "responses"
                ]
            }
        ]
    },
    "A4.T9": {
        "caption": "Table 9: Sample generation results for various methods on 2WikiMultiHopQA. Compared to baselines, both TVS (Seq) and TVS (ReVerT) deliver more accurate reasoning and maintain higher speech-suitableness in their responses. While baseline methods frequently produce incorrect answers or include irrelevant information, our models consistently provide factually correct and well-structured explanations that are both clear and suitable for spoken delivery. This highlights the effectiveness of our approach in balancing reasoning capability with speech-oriented generation quality.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sample text responses for various methods on 2WikiMultiHopQA</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Question</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Which country John Of Bohemia&#8217;s mother is from?</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Answer</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Germany</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">To determine which country John of Bohemia&#8217;s mother is from, we need to follow these steps:\n<br class=\"ltx_break\"/>1. <span class=\"ltx_text ltx_font_bold\">Identify John of Bohemia</span>: John of Bohemia was a Bohemian nobleman and king who reigned as King of Bohemia from 1347 to 1349.\n<br class=\"ltx_break\"/>2. <span class=\"ltx_text ltx_font_bold\">Research his family background</span>: John of Bohemia was born in Prague, Bohemia (now part of the Czech Republic) in 1319. His father was Wenceslaus II, Holy Roman Emperor, and his mother was Elisabeth of Bavaria.\n<br class=\"ltx_break\"/>3. <span class=\"ltx_text ltx_font_bold\">Determine Elisabeth of Bavaria&#8217;s origin</span>: Elisabeth of Bavaria was born in 1294 in Landshut, Bavaria, which is now part of Germany. She was a member of the House of Wittelsbach.\n<br class=\"ltx_break\"/>Given this information, we can conclude that John of Bohemia&#8217;s mother, Elisabeth of Bavaria, is from Bavaria, which is a region in modern-day Germany.\n<br class=\"ltx_break\"/>Therefore, the country John of Bohemia&#8217;s mother is from is <span class=\"ltx_text ltx_font_bold\">Germany</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">John of Bohemia&#8217;s mother was from France. Specifically, she was from the region of <span class=\"ltx_text ltx_font_bold\">Champagne</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Finetuning</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">John of Bohemia was a king of Bohemia. His mother was Margaret of Durazzo. She was the daughter of Stephen Du&#353;an, who ruled Serbia. So, John of Bohemia&#8217;s mother was from <span class=\"ltx_text ltx_font_bold\">Serbia</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">John of Bohemia was a Bohemian nobleman and king. His mother was Elisabeth of Bavaria. She was born in Landshut, Bavaria. That area is now part of Germany. So, John of Bohemia&#8217;s mother is from <span class=\"ltx_text ltx_font_bold\">Germany</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">John of Bohemia was a Bohemian nobleman and king. He was born in Prague and had a Bavarian mother named Elisabeth. Elisabeth was born in Landshut, Bavaria. So, John of Bohemia&#8217;s mother is from <span class=\"ltx_text ltx_font_bold\">Germany</span>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "speechfriendly",
            "durazzo",
            "their",
            "include",
            "region",
            "ruled",
            "spoken",
            "nobleman",
            "various",
            "who",
            "information",
            "mother",
            "consistently",
            "from",
            "determine",
            "republic",
            "responses",
            "maintain",
            "higher",
            "frequently",
            "both",
            "results",
            "reasoning",
            "tvs",
            "incorrect",
            "elisabeth",
            "given",
            "seq",
            "steps",
            "daughter",
            "explanations",
            "speechoriented",
            "reigned",
            "answers",
            "king",
            "holy",
            "therefore",
            "follow",
            "conclude",
            "bavaria",
            "our",
            "father",
            "effectiveness",
            "provide",
            "correct",
            "roman",
            "france",
            "revert",
            "background",
            "2wikimultihopqa",
            "generation",
            "country",
            "modernday",
            "highlights",
            "wittelsbach",
            "text",
            "identify",
            "clear",
            "bavaria’s",
            "stephen",
            "bavarian",
            "prompting",
            "house",
            "approach",
            "margaret",
            "wellstructured",
            "models",
            "which",
            "baselines",
            "serbia",
            "dušan",
            "delivery",
            "emperor",
            "methods",
            "balancing",
            "area",
            "member",
            "finetuning",
            "suitable",
            "accurate",
            "irrelevant",
            "bohemia",
            "origin",
            "named",
            "deliver",
            "question",
            "more",
            "answer",
            "family",
            "baseline",
            "bohemia’s",
            "czech",
            "now",
            "born",
            "factually",
            "while",
            "capability",
            "research",
            "need",
            "wenceslaus",
            "prague",
            "chainofthought",
            "germany",
            "she",
            "bohemian",
            "john",
            "his",
            "specifically",
            "had",
            "landshut",
            "speechsuitableness",
            "produce",
            "compared",
            "part",
            "champagne",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
            "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "produce",
                    "their",
                    "reasoning",
                    "revert",
                    "delivery",
                    "from",
                    "results",
                    "spoken",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "produce",
                    "suitable",
                    "their",
                    "reasoning",
                    "spoken",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "produce",
                    "responses",
                    "suitable",
                    "reasoning",
                    "prompting",
                    "which",
                    "approach",
                    "methods",
                    "spoken",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "revert",
                    "which",
                    "accurate",
                    "compared",
                    "approach",
                    "both",
                    "reasoning",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which enhances the speech-friendliness of generated utterances while preserving the problem-solving capabilities of the underlying reasoning model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "while",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "question",
                    "speechfriendly",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "reasoning",
                    "prompting",
                    "research",
                    "spoken",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "produce",
                    "which",
                    "approach",
                    "spoken",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "compared",
                    "more",
                    "generation",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "text",
                    "which",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "which",
                    "approach",
                    "both",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "from",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "revert",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "more",
                    "revert",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "revert",
                    "which",
                    "generation",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "speechoriented",
                    "revert",
                    "compared",
                    "more",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Speak</span> stage, we convert the utterances to speech using a TTS model. Specifically, we employ a TTS model that supports both streaming input and output, allowing the system to process streaming outputs from the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage and play the generated speech with minimal delay for the user.</p>\n\n",
                "matched_terms": [
                    "specifically",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "their",
                    "steps",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "revert",
                    "steps",
                    "more",
                    "reasoning",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "revert",
                    "which",
                    "seq",
                    "reasoning",
                    "tvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "produce",
                    "finetuning",
                    "responses",
                    "reasoning",
                    "include",
                    "prompting",
                    "baselines",
                    "spoken",
                    "baseline",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "responses",
                    "revert",
                    "seq",
                    "specifically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "while",
                    "2wikimultihopqa",
                    "steps",
                    "question",
                    "answer",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "from",
                    "capability",
                    "revert",
                    "2wikimultihopqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "suitable",
                    "delivery",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "produce",
                    "revert",
                    "specifically",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "quality",
                    "from",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "results",
                    "baseline",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "responses",
                    "therefore",
                    "2wikimultihopqa",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight a fundamental trade-off within the two-stage paradigm: optimizing for reasoning capability tends to degrade speech-suitability, and vice versa.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "capability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "while",
                    "baselines",
                    "both",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "both",
                    "text",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "2wikimultihopqa",
                    "seq",
                    "baselines",
                    "consistently",
                    "both",
                    "results",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "their",
                    "reasoning",
                    "revert",
                    "which",
                    "seq",
                    "specifically",
                    "both",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "seq",
                    "revert",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "therefore",
                    "revert",
                    "while",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert",
                    "more",
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "reasoning",
                    "revert",
                    "which",
                    "compared",
                    "approach",
                    "capability",
                    "research",
                    "from",
                    "both",
                    "spoken",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "while",
                    "from",
                    "results",
                    "methods",
                    "reasoning",
                    "explanations",
                    "chainofthought",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "suitable",
                    "their",
                    "reasoning",
                    "more",
                    "generation",
                    "from",
                    "spoken",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We thank the reviewers for the valuable feedback. This work was supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) under the following projects: No. RS-2025-25442338 (AI Star Fellowship Support Program), No. RS-2022-II220156 (Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation), No. RS-2025-02263841 (Development of a Real-time Multimodal Framework for Comprehensive Deepfake Detection Incorporating Common Sense Error Analysis), and IITP-2025-RS-2024-00437633 (IITP&#8211;ITRC). It was also supported by R&amp;BD Program (CD200024) through the Seoul Business Agency(SBA)\nfunded by The Seoul Metropolitan Government. This research was also conducted as part of the Sovereign AI Foundation Model Project(Data Track), organized by the Ministry of Science and ICT(MSIT) and supported by the National Information Society Agency(NIA), S.Korea (2025-AI Data-wi43). Gunhee Kim is the corresponding author.</p>\n\n",
                "matched_terms": [
                    "their",
                    "part",
                    "research",
                    "information",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "their",
                    "research",
                    "both",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "2wikimultihopqa",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "2wikimultihopqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "prompting",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must follow the same logical progression as the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "follow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must not repeat information provided in the question.</p>\n\n",
                "matched_terms": [
                    "question",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because enforcing all constraints simultaneously in a single instruction yields suboptimal results, we adopt a progressive approach, providing the language model with one constraint at a time.</p>\n\n",
                "matched_terms": [
                    "approach",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "prompting",
                    "revert",
                    "seq",
                    "follow",
                    "both",
                    "methods",
                    "reasoning",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "seq",
                    "steps",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "generation",
                    "revert",
                    "determine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "clear",
                    "answer",
                    "which",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we use gpt-4.1-mini-2025-04-11 to automatically assess answer correctness. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F7\" title=\"Figure 7 &#8227; B.4 LLM-Based Answer Verification &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the verifier is prompted with the question, a model-generated response, and the corresponding ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "question",
                    "therefore",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide comprehensive details regarding our human evaluation protocol.</p>\n\n",
                "matched_terms": [
                    "our",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "2wikimultihopqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each output is evaluated along four key dimensions. We provide annotators with the following definitions for each factor, which offer additional guidance beyond the brief descriptions in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "which",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "from",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "provide",
                    "responses",
                    "their",
                    "2wikimultihopqa",
                    "research",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "provide",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "responses",
                    "provide",
                    "prompting",
                    "maintain",
                    "steps",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "from",
                    "responses"
                ]
            }
        ]
    },
    "A4.T10": {
        "caption": "Table 10: Step-by-step reasoning output from the Chain-of-Thought baseline on SciBench. This example illustrates a correct and complete logical progression, resulting in the correct numerical answer.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sample text responses for various methods on SciBench (1)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Question</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Let <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m2\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> be independent with distributions <math alttext=\"N(5,16)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>5</mn><mo>,</mo><mn>16</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(5,16)</annotation></semantics></math> and <math alttext=\"N(6,9)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>6</mn><mo>,</mo><mn>9</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(6,9)</annotation></semantics></math>, respectively. Evaluate <math alttext=\"P(X&gt;Y)=\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">P(X&gt;Y)=</annotation></semantics></math> <math alttext=\"P(X-Y&gt;0)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X-Y&gt;0)</annotation></semantics></math>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Answer</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.4207</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">Chain-of-Thought</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">To evaluate <math alttext=\"P(X&gt;Y)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X&gt;Y)</annotation></semantics></math>, we need to find the probability that <math alttext=\"X-Y&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">X-Y&gt;0</annotation></semantics></math>. Given that <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m9\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m10\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are independent normal random variables, we can determine the distribution of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m11\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>.\n<br class=\"ltx_break\"/>### Step 1: Determine the Distribution of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m12\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>\n<br class=\"ltx_break\"/>Since <math alttext=\"X\\sim N(5,16)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m13\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8764;</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>5</mn><mo>,</mo><mn>16</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">X\\sim N(5,16)</annotation></semantics></math> and <math alttext=\"Y\\sim N(6,9)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m14\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>&#8764;</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>6</mn><mo>,</mo><mn>9</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Y\\sim N(6,9)</annotation></semantics></math>:\n<br class=\"ltx_break\"/>- The mean of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m15\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is <math alttext=\"\\mu_{X}=5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m16\" intent=\":literal\"><semantics><mrow><msub><mi>&#956;</mi><mi>X</mi></msub><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\mu_{X}=5</annotation></semantics></math>.\n<br class=\"ltx_break\"/>- The variance of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m17\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is <math alttext=\"\\sigma_{X}^{2}=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m18\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#963;</mi><mi>X</mi><mn>2</mn></msubsup><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma_{X}^{2}=16</annotation></semantics></math>, so the standard deviation is <math alttext=\"\\sigma_{X}=4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m19\" intent=\":literal\"><semantics><mrow><msub><mi>&#963;</mi><mi>X</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma_{X}=4</annotation></semantics></math>.\n<br class=\"ltx_break\"/>- The mean of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m20\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> is <math alttext=\"\\mu_{Y}=6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m21\" intent=\":literal\"><semantics><mrow><msub><mi>&#956;</mi><mi>Y</mi></msub><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\mu_{Y}=6</annotation></semantics></math>.\n<br class=\"ltx_break\"/>- The variance of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m22\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> is <math alttext=\"\\sigma_{Y}^{2}=9\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m23\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#963;</mi><mi>Y</mi><mn>2</mn></msubsup><mo>=</mo><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma_{Y}^{2}=9</annotation></semantics></math>, so the standard deviation is <math alttext=\"\\sigma_{Y}=3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m24\" intent=\":literal\"><semantics><mrow><msub><mi>&#963;</mi><mi>Y</mi></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sigma_{Y}=3</annotation></semantics></math>.\n<br class=\"ltx_break\"/>For the difference <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m25\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>:\n<br class=\"ltx_break\"/>- The mean of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m26\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex7\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\mu_{X-Y}=\\mu_{X}-\\mu_{Y}=5-6=-1\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex7.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#956;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub><mo>=</mo><mrow><msub><mi>&#956;</mi><mi>X</mi></msub><mo>&#8722;</mo><msub><mi>&#956;</mi><mi>Y</mi></msub></mrow><mo>=</mo><mrow><mn>5</mn><mo>&#8722;</mo><mn>6</mn></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\mu_{X-Y}=\\mu_{X}-\\mu_{Y}=5-6=-1\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">- The variance of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m27\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex8\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\sigma_{X-Y}^{2}=\\sigma_{X}^{2}+\\sigma_{Y}^{2}=16+9=25\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex8.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#963;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mrow><msubsup><mi>&#963;</mi><mi>X</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>&#963;</mi><mi>Y</mi><mn>2</mn></msubsup></mrow><mo>=</mo><mrow><mn>16</mn><mo>+</mo><mn>9</mn></mrow><mo>=</mo><mn>25</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\sigma_{X-Y}^{2}=\\sigma_{X}^{2}+\\sigma_{Y}^{2}=16+9=25\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">Therefore, the standard deviation of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m28\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex9\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\sigma_{X-Y}=\\sqrt{25}=5\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex9.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#963;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub><mo>=</mo><msqrt><mn>25</mn></msqrt><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\sigma_{X-Y}=\\sqrt{25}=5\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">Thus, <math alttext=\"X-Y\\sim N(-1,25)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m29\" intent=\":literal\"><semantics><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&#8764;</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>25</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">X-Y\\sim N(-1,25)</annotation></semantics></math>.\n<br class=\"ltx_break\"/>### Step 2: Standardize <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m30\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>\n<br class=\"ltx_break\"/>We need to find <math alttext=\"P(X-Y&gt;0)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m31\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X-Y&gt;0)</annotation></semantics></math>. To do this, we standardize <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m32\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex10\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;Z=\\frac{(X-Y)-\\mu_{X-Y}}{\\sigma_{X-Y}}=\\frac{(X-Y)-(-1)}{5}=\\frac{X-Y+1}{5}\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex10.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>&#8722;</mo><msub><mi>&#956;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub></mrow><msub><mi>&#963;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub></mfrac><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>&#8722;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mn>5</mn></mfrac><mo>=</mo><mfrac><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>+</mo><mn>1</mn></mrow><mn>5</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\newline\nZ=\\frac{(X-Y)-\\mu_{X-Y}}{\\sigma_{X-Y}}=\\frac{(X-Y)-(-1)}{5}=\\frac{X-Y+1}{5}\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">We want to find:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex11\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;P(X-Y&gt;0)=P\\left(\\frac{X-Y+1}{5}&gt;\\frac{0+1}{5}\\right)=P\\left(Z&gt;\\frac{1}{5}\\right)\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex11.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>+</mo><mn>1</mn></mrow><mn>5</mn></mfrac><mo>&gt;</mo><mfrac><mrow><mn>0</mn><mo>+</mo><mn>1</mn></mrow><mn>5</mn></mfrac></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mfrac><mn>1</mn><mn>5</mn></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\newline\nP(X-Y&gt;0)=P\\left(\\frac{X-Y+1}{5}&gt;\\frac{0+1}{5}\\right)=P\\left(Z&gt;\\frac{1}{5}\\right)\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">### Step 3: Use the Standard Normal Distribution\n<br class=\"ltx_break\"/>The variable <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m33\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> follows a standard normal distribution <math alttext=\"N(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m34\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(0,1)</annotation></semantics></math>. We need to find <math alttext=\"P(Z&gt;0.2)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m35\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z&gt;0.2)</annotation></semantics></math>.\n<br class=\"ltx_break\"/>Using standard normal distribution tables or a calculator, we find:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex12\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;P(Z&gt;0.2)=1-P(Z\\leq 0.2)\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex12.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&#8804;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\newline\nP(Z&gt;0.2)=1-P(Z\\leq 0.2)\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">From standard normal tables, <math alttext=\"P(Z\\leq 0.2)\\approx 0.5793\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m36\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&#8804;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mn>0.5793</mn></mrow><annotation encoding=\"application/x-tex\">P(Z\\leq 0.2)\\approx 0.5793</annotation></semantics></math>.\n<br class=\"ltx_break\"/>Therefore:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex13\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;P(Z&gt;0.2)=1-0.5793=0.4207\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex13.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mn>0.5793</mn></mrow><mo>=</mo><mn>0.4207</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\nP(Z&gt;0.2)=1-0.5793=0.4207\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">### Final Answer\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex14\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;P(X&gt;Y)=P(X-Y&gt;0)=\\boxed{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{0.4207}}}\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex14.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><menclose notation=\"box\"><mtext class=\"ltx_mathvariant_bold\">0.4207</mtext></menclose></mrow><annotation encoding=\"application/x-tex\">\\newline\nP(X&gt;Y)=P(X-Y&gt;0)=\\boxed{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{0.4207}}}\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "p​z021−p​z≤02newline",
            "x∼n​516xsim",
            "various",
            "complete",
            "probability",
            "from",
            "μx5mux5",
            "determine",
            "x−yxy",
            "distribution",
            "resulting",
            "calculator",
            "responses",
            "let",
            "normal",
            "sigmaxy2sigmax2sigmay216925newline",
            "p​z≤02≈05793pzleq",
            "μx−yμx−μy5−6−1newline",
            "reasoning",
            "use",
            "x−y0xy0",
            "given",
            "n​01n01",
            "n​516n516",
            "follows",
            "σx216sigmax216",
            "logical",
            "μy6muy6",
            "scibench",
            "therefore",
            "final",
            "example",
            "p​z021−0579304207newline",
            "σy29sigmay29",
            "deviation",
            "correct",
            "difference",
            "σx−y255newline",
            "n​69n69",
            "sigmaxysqrt255newline",
            "text",
            "mean",
            "illustrates",
            "p​xypxy",
            "respectively",
            "standard",
            "since",
            "zx−y−μx−yσx−yx−y−−15x−y15newline",
            "stepbystep",
            "want",
            "σy3sigmay3",
            "p​x−y0pxy0",
            "find",
            "standardize",
            "02newline",
            "output",
            "pz021pzleq",
            "thus",
            "02approx",
            "step",
            "numerical",
            "p​z02pz02",
            "methods",
            "pxy0pleftfracxy15frac015rightpleftzfrac15rightnewline",
            "tables",
            "p​xyp​x−y004207newline",
            "n69",
            "x−y∼n​−125xysim",
            "p​x−y0p​x−y15015p​z15newline",
            "variables",
            "variance",
            "evaluate",
            "question",
            "answer",
            "baseline",
            "muxymuxmuy561newline",
            "progression",
            "n516",
            "need",
            "distributions",
            "pxypxy0boxedcolorrgb000definecolornamedpgfstrokecolorrgb000textbf04207newline",
            "chainofthought",
            "σx4sigmax4",
            "independent",
            "variable",
            "pz0210579304207newline",
            "zfracxymuxysigmaxyfracxy15fracxy15newline",
            "y∼n​69ysim",
            "n125",
            "random",
            "σx−y2σx2σy216925newline"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "from",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "output",
                    "example",
                    "illustrates",
                    "methods",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "output",
                    "example",
                    "since",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "resulting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, we leverage the problem-solving abilities of a reasoning LLM.\nUpon receiving a user query, the LLM solves the query using chain-of-thought reasoning. The reasoning output is then streamed to the subsequent stage.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "complete",
                    "output",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "output",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Speak</span> stage, we convert the utterances to speech using a TTS model. Specifically, we employ a TTS model that supports both streaming input and output, allowing the system to process streaming outputs from the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage and play the generated speech with minimal delay for the user.</p>\n\n",
                "matched_terms": [
                    "output",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "example",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "step",
                    "standard",
                    "output",
                    "use",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "standard",
                    "follows",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from",
                    "let"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th token in the response sequence <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>, and <math alttext=\"p_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">p_{\\theta}</annotation></semantics></math> is the model&#8217;s output probability.</p>\n\n",
                "matched_terms": [
                    "output",
                    "probability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "step",
                    "responses",
                    "standard",
                    "output",
                    "use",
                    "reasoning",
                    "baseline",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "use",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "scibench",
                    "question",
                    "answer",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "evaluate",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "evaluate",
                    "final",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluate",
                    "from",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "use",
                    "output",
                    "since",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "responses",
                    "example",
                    "independent",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "baseline",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "therefore",
                    "scibench",
                    "example",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "therefore",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "complete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "complete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "illustrates",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "text",
                    "from",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "use",
                    "from",
                    "methods",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "thus",
                    "from",
                    "independent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "use",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "stepbystep",
                    "step",
                    "standard",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from",
                    "step",
                    "resulting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must follow the same logical progression as the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "logical",
                    "progression"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "use",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "baseline",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "use",
                    "final",
                    "probability",
                    "determine",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we use gpt-4.1-mini-2025-04-11 to automatically assess answer correctness. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F7\" title=\"Figure 7 &#8227; B.4 LLM-Based Answer Verification &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the verifier is prompted with the question, a model-generated response, and the corresponding ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "question",
                    "therefore",
                    "answer",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "random",
                    "evaluate",
                    "scibench",
                    "use",
                    "example",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "from",
                    "independent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "use",
                    "example",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "scibench",
                    "standard",
                    "reasoning",
                    "chainofthought"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "from",
                    "responses"
                ]
            }
        ]
    },
    "A4.T11": {
        "caption": "Table 11: Sample generation results from various methods on SciBench. Our models, TVS (Seq) and TVS (ReVerT), consistently produce accurate answers with logically sound and precise numerical reasoning, while maintaining clarity and suitability for spoken delivery. In contrast, baseline methods frequently exhibit logical errors or numerical calculation mistakes. Interestingly, even when prompted for concise and speech-friendly responses, the models tend to generate still structured and verbose outputs on SciBench, due to the inherent complexity of scientific questions.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sample text responses for various methods on SciBench (2)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Question</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Let <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m2\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> be independent with distributions <math alttext=\"N(5,16)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>5</mn><mo>,</mo><mn>16</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(5,16)</annotation></semantics></math> and <math alttext=\"N(6,9)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>6</mn><mo>,</mo><mn>9</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(6,9)</annotation></semantics></math>, respectively. Evaluate <math alttext=\"P(X&gt;Y)=\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">P(X&gt;Y)=</annotation></semantics></math> <math alttext=\"P(X-Y&gt;0)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X-Y&gt;0)</annotation></semantics></math>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Answer</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0.4207</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Prompting</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">To find <math alttext=\"P(X&gt;Y)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X&gt;Y)</annotation></semantics></math>, we need to look at the distribution of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m8\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>. Since <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m9\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m10\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are independent, the difference <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m11\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> will also follow a normal distribution.\n<br class=\"ltx_break\"/>First, let&#8217;s determine the mean and variance of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m12\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math>:\n<br class=\"ltx_break\"/>- The mean of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m13\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is 5, and the mean of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m14\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> is 6. So, the mean of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m15\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex15\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\mu_{X-Y}=\\mu_{X}-\\mu_{Y}=5-6=-1\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex15.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#956;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub><mo>=</mo><mrow><msub><mi>&#956;</mi><mi>X</mi></msub><mo>&#8722;</mo><msub><mi>&#956;</mi><mi>Y</mi></msub></mrow><mo>=</mo><mrow><mn>5</mn><mo>&#8722;</mo><mn>6</mn></mrow><mo>=</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\mu_{X-Y}=\\mu_{X}-\\mu_{Y}=5-6=-1\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">- The variance of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m16\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is 16, and the variance of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m17\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> is 9. Since <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m18\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m19\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are independent, the variance of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m20\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex16\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\sigma^{2}_{X-Y}=\\sigma^{2}_{X}+\\sigma^{2}_{Y}=16+9=25\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex16.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#963;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mrow><msubsup><mi>&#963;</mi><mi>X</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>&#963;</mi><mi>Y</mi><mn>2</mn></msubsup></mrow><mo>=</mo><mrow><mn>16</mn><mo>+</mo><mn>9</mn></mrow><mo>=</mo><mn>25</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\sigma^{2}_{X-Y}=\\sigma^{2}_{X}+\\sigma^{2}_{Y}=16+9=25\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">Therefore, the standard deviation of <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m21\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> is:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex17\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;\\sigma_{X-Y}=\\sqrt{25}=5\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex17.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#963;</mi><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow></msub><mo>=</mo><msqrt><mn>25</mn></msqrt><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\n\\sigma_{X-Y}=\\sqrt{25}=5\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">So, <math alttext=\"X-Y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m22\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X-Y</annotation></semantics></math> follows a normal distribution <math alttext=\"N(-1,25)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m23\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>25</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N(-1,25)</annotation></semantics></math>.\n<br class=\"ltx_break\"/>Now, we want to find <math alttext=\"P(X-Y&gt;0)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m24\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(X-Y&gt;0)</annotation></semantics></math>. This is equivalent to finding the probability that a normally distributed random variable with mean <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m25\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> and standard deviation <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m26\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> is greater than <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m27\" intent=\":literal\"><mn>0</mn></math>.\n<br class=\"ltx_break\"/>We can standardize this by converting it to a standard normal distribution:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex18\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;Z=\\frac{(X-Y)-(-1)}{5}=\\frac{X-Y+1}{5}\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex18.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>&#8722;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mn>5</mn></mfrac><mo>=</mo><mfrac><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>+</mo><mn>1</mn></mrow><mn>5</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\newline\nZ=\\frac{(X-Y)-(-1)}{5}=\\frac{X-Y+1}{5}\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">We need to find <math alttext=\"P(Z&gt;\\frac{0+1}{5})=P(Z&gt;0.2)\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m28\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mfrac><mrow><mn>0</mn><mo>+</mo><mn>1</mn></mrow><mn>5</mn></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z&gt;\\frac{0+1}{5})=P(Z&gt;0.2)</annotation></semantics></math>.\n<br class=\"ltx_break\"/>Using standard normal tables or a calculator, we find that:\n<br class=\"ltx_break\"/></span>\n<span class=\"ltx_equation ltx_eqn_table\" id=\"A4.Ex19\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\newline&#10;P(Z&gt;0.2)\\approx 0.4207\\newline&#10;\" class=\"ltx_Math\" display=\"block\" id=\"A4.Ex19.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo>&gt;</mo><mn>0.2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mn>0.4207</mn></mrow><annotation encoding=\"application/x-tex\">\\newline\nP(Z&gt;0.2)\\approx 0.4207\\newline\n</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_p\">Therefore, <math alttext=\"P(X&gt;Y)=P(X-Y&gt;0)\\approx{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{0.4207}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m29\" intent=\":literal\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>&gt;</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>X</mi><mo>&#8722;</mo><mi>Y</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mtext class=\"ltx_mathvariant_bold\">0.4207</mtext></mrow><annotation encoding=\"application/x-tex\">P(X&gt;Y)=P(X-Y&gt;0)\\approx{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\textbf{0.4207}}</annotation></semantics></math>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Speech-Friendly Finetuning</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The difference between X and Y, X minus Y, follows a normal distribution. Its mean is 5 minus 6, which is negative 1. Its variance is the sum of the variances of X and Y, so it&#8217;s 25. To find P(X greater than Y), we look at P(X minus Y greater than 0). This is the same as finding the probability that a standard normal variable is greater than 1 divided by 5. That value is about 0.3085. So, P(X greater than Y) is approximately <span class=\"ltx_text ltx_font_bold\">0.3085</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The difference between X and Y is X minus Y. The mean of X minus Y is negative one. The variance is 25, so the standard deviation is five. To find P(X minus Y &gt; 0), we standardize X minus Y. This gives us Z equals (X minus Y plus one) divided by five. We want to find P(Z &gt; 0.2). Since Z is a standard normal variable, we use the standard normal distribution. From the table, P(Z &gt; 0.2) is about 0.4207. So, P(X &gt; Y) is <span class=\"ltx_text ltx_font_bold\">0.4207</span>.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">The mean of X minus Y is 5 minus 6, which is -1. The variance is the sum of the variances, 16 plus 9, which equals 25. The standard deviation is the square root of 25, which is 5. To find P(X minus Y is greater than 0), we standardize X minus Y. This gives us a standard normal distribution with mean 0 and standard deviation 5. We want to find the probability that this standard normal is greater than 0.2. Using the standard normal distribution, we find that P(X minus Y is greater than 0) is about <span class=\"ltx_text ltx_font_bold\">0.42</span>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "speechfriendly",
            "structured",
            "zfracxy15fracxy15newline",
            "sigma2xysigma2xsigma2y16925newline",
            "spoken",
            "questions",
            "various",
            "equivalent",
            "p​z02≈04207newline",
            "−11",
            "pz02approx",
            "also",
            "n​−125n125",
            "probability",
            "consistently",
            "from",
            "concise",
            "x−yxy",
            "determine",
            "distribution",
            "first",
            "clarity",
            "calculator",
            "verbose",
            "responses",
            "let",
            "normal",
            "tend",
            "frequently",
            "μx−yμx−μy5−6−1newline",
            "results",
            "reasoning",
            "tvs",
            "errors",
            "negative",
            "use",
            "seq",
            "inherent",
            "n​516n516",
            "follows",
            "maintaining",
            "precise",
            "logical",
            "answers",
            "scibench",
            "therefore",
            "even",
            "still",
            "follow",
            "converting",
            "gives",
            "interestingly",
            "zx−y−−15x−y15newline",
            "our",
            "its",
            "difference",
            "deviation",
            "σx−y255newline",
            "suitability",
            "look",
            "revert",
            "generate",
            "generation",
            "n​69n69",
            "sigmaxysqrt255newline",
            "about",
            "text",
            "variances",
            "mean",
            "approximately",
            "p​xypxy",
            "logically",
            "respectively",
            "will",
            "standard",
            "prompting",
            "since",
            "models",
            "want",
            "standardize",
            "p​x−y0pxy0",
            "find",
            "normally",
            "which",
            "divided",
            "when",
            "numerical",
            "prompted",
            "delivery",
            "methods",
            "exhibit",
            "tables",
            "between",
            "value",
            "root",
            "finetuning",
            "accurate",
            "sum",
            "distributed",
            "variance",
            "same",
            "outputs",
            "evaluate",
            "scientific",
            "five",
            "question",
            "answer",
            "mistakes",
            "baseline",
            "now",
            "muxymuxmuy561newline",
            "while",
            "need",
            "calculation",
            "finding",
            "plus",
            "distributions",
            "let’s",
            "minus",
            "independent",
            "variable",
            "sound",
            "it’s",
            "equals",
            "greater",
            "due",
            "p​z015p​z02pzfrac015pz02",
            "square",
            "than",
            "complexity",
            "one",
            "contrast",
            "random",
            "produce",
            "04207newline",
            "p​xyp​x−y0≈04207pxypxy0approxcolorrgb000definecolornamedpgfstrokecolorrgb000textbf04207",
            "σx−y2σx2σy216925newline"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To provide a deeper insight into the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we present representative qualitative examples from each evaluation dataset. All examples use Qwen2.5-7B-Instruct as the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model. Specifically, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T8\" title=\"Table 8 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents results on GSM8K, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> on 2WikiMultiHopQA, and Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T10\" title=\"Table 10 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T11\" title=\"Table 11 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> on SciBench. These examples highlight the strengths and limitations of various approaches in terms of both reasoning capability and speech-friendliness. Our framework demonstrates balanced performance, producing outputs that are logically sound and well-suited for verbal communication.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Think-Verbalize-Speak</span>, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is <span class=\"ltx_text ltx_font_italic\">verbalizing</span>, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://yhytoto12.github.io/TVS-ReVerT\" title=\"\">https://yhytoto12.github.io/TVS-ReVerT</a></p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "outputs",
                    "while",
                    "due",
                    "produce",
                    "also",
                    "reasoning",
                    "revert",
                    "delivery",
                    "from",
                    "results",
                    "between",
                    "spoken",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans inherently distinguish between their internal thoughts and their external expressions, effortlessly reformulating their thought processes into formats suitable for verbal communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib14\" title=\"\">1993</a>; Indefrey and Levelt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib11\" title=\"\">2004</a>; Sahin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib18\" title=\"\">2009</a>)</cite>. Current spoken dialogue systems, despite rapid advances, lack mechanisms that emulate this fundamental human capacity. This limitation becomes increasingly significant as reasoning models that produce extensive chain-of-thought to address complex problems gain popularity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "produce",
                    "reasoning",
                    "between",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current spoken dialogue systems typically employ a two-stage framework, herein referred to as the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>; Dongre et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib4\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>; Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib5\" title=\"\">2025</a>)</cite>. In this approach, the system first constructs the content of the speech (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>), and then generates the corresponding spoken output (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). However, large language models (LLMs), which are commonly used in the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, combined with test-time computing methods such as chain-of-thought reasoning, often yield responses that are not suitable for spoken dialogue. While some studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>; Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> address this issue by guiding the model to produce speech-friendly outputs through fine-tuning or prompting, enforcing a speech-friendly thought formats may substantially deteriorate the reasoning performance. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates an example of this issue.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "finetuning",
                    "produce",
                    "responses",
                    "outputs",
                    "reasoning",
                    "prompting",
                    "which",
                    "methods",
                    "spoken",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which introduces an intermediate verbalization stage that translates raw model reasoning into speech-friendly, comprehensible utterances. Through this verbalization process, our system produces natural, concise speech output without sacrificing problem-solving capabilities. To mitigate the latency in the naive two-stage sequential implementation, we present the <span class=\"ltx_text ltx_font_bold\">RE</span>asoning to <span class=\"ltx_text ltx_font_bold\">VER</span>bal <span class=\"ltx_text ltx_font_bold\">T</span>ext (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) model, which utilizes efficient, incremental verbalization and achieves up to 66% reduction in response time compared to the sequential approach. Extensive automatic and human evaluations confirm that our method generates speech output that is both natural and accurate, with minimal loss in reasoning capabilities and robust performance across different reasoning models and verbalize model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "revert",
                    "which",
                    "accurate",
                    "concise",
                    "reasoning",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our key contributions to the field are as follows:</p>\n\n",
                "matched_terms": [
                    "follows",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which enhances the speech-friendliness of generated utterances while preserving the problem-solving capabilities of the underlying reasoning model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "while",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalization model that significantly reduces system latency by performing verbalization in parallel with the underlying reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop the solve-summarize-scatter data pipeline that transforms existing question answering (QA) datasets into <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> training datasets by generating reasoning sequences with incremental, speech-friendly summaries. We publicly release the dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "question",
                    "speechfriendly",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs have achieved significant progress through model and dataset scaling, these advancements alone remain insufficient for addressing complex tasks such as arithmetic and commonsense reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>; Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib21\" title=\"\">c</a>)</cite>. The introduction of chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib22\" title=\"\">2022</a>)</cite> has unlocked enhanced reasoning abilities in LLMs.\nSubsequent research has developed specialized reasoning models that incorporate non-linear reasoning processes, such as reflection and backtracking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib15\" title=\"\">2024</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib8\" title=\"\">2025</a>)</cite>. However, these enhanced reasoning processes are lengthy and verbose, making them difficult for users to stay engaged during spoken interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "verbose",
                    "reasoning",
                    "prompting",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems are typically categorized as cascaded or end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib12\" title=\"\">2024</a>)</cite>. Cascaded systems employ a pipeline architecture comprising automatic speech recognition (ASR), a dialogue model, and a text-to-speech (TTS) component, using text as the intermediate representation. This modular approach allows for the integration of state-of-the-art components at each stage. However, LLM-based dialogue models within these systems often produce outputs optimized for reading, such as bullet points, sentence fragments, or formatted equations, rather than for spoken communication, which can undermine the naturalness of speech-based interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "produce",
                    "outputs",
                    "than",
                    "which",
                    "spoken",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems eliminate the dependency on intermediate text, thereby preserving paralinguistic cues and facilitating more natural speech generation. Recent work includes fully textless models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib13\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib27\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib3\" title=\"\">2024</a>)</cite>, text-speech interleaved architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib26\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib20\" title=\"\">2024b</a>)</cite>, and parallel decoding approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib23\" title=\"\">2024</a>; Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib6\" title=\"\">2025</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>. While end-to-end systems are more effective at generating speech-friendly outputs, they typically exhibit weaker reasoning capabilities compared to conventional LLMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "while",
                    "outputs",
                    "generation",
                    "exhibit",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on speech-suitable text can be divided into two main categories. The first is normalization, which converts non-standard text into standard, pronounceable forms. For example, MathReader&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hyeon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib10\" title=\"\">2025</a>)</cite> translates LaTeX mathematical expressions into English, which is crucial for LLM-based spoken dialogue systems since LLMs often output LaTeX equations when solving arithmetic problems.\nThe second category considers how content should be verbalized for effective spoken communication, based on the fundamental differences between textual and audio media. For instance, unlike text, audio requires listeners to engage with content sequentially, without the ability to selectively skip or return to sections. Building on this observation, <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite> introduces the concept of \"speechworthiness,\" referring to properties that make text well-suited for verbal communication, including clarity, utterance length, and information density.</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "standard",
                    "which",
                    "since",
                    "divided",
                    "when",
                    "between",
                    "spoken",
                    "text",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>, modifies the traditional cascaded system by generating response content in two stages: a reasoning stage that ensures response accuracy (<span class=\"ltx_text ltx_font_smallcaps\">Think</span>) and a translation stage that converts the reasoning output into a verbal response (<span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span>). The system subsequently converts the resulting response to speech (<span class=\"ltx_text ltx_font_smallcaps\">Speak</span>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of our approach with <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as the verbalizer. We employ an off-the-shelf reasoning LLM and a streaming TTS model, both of which remain frozen; only the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model undergoes training under our framework.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "which",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage, the system receives the streaming reasoning output from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage and translates it into speech-friendly utterances. A naive approach would be the sequential approach, where the system completes the reasoning stage and then generates the speech-friendly translations based on the complete output. However, the sequential approach introduces significant latency.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this issue, we propose <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>, a latency-efficient verbalizer. As described in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#alg1\" title=\"Algorithm 1 &#8227; 3.2 Verbalize &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates asynchronously with the reasoning LLM from the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> stage, incrementally generating speech-friendly utterances based on partial reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "speechfriendly",
                    "revert",
                    "from",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model operates in two distinct modes: <span class=\"ltx_text ltx_font_italic\">thinking mode</span> (<math alttext=\"{\\mathcal{S}}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{T}</annotation></semantics></math>) and <span class=\"ltx_text ltx_font_italic\">verbalizing mode</span> (<math alttext=\"{\\mathcal{S}}_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{S}}_{V}</annotation></semantics></math>). In thinking mode, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> receives and processes the outputs of the reasoning model. While the reasoning LLM emits output token by token, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> processes these outputs in segments, defined by a predetermined set of delimiters. This chunk-based processing enables more efficient computation through hardware parallelism.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "outputs",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After processing each segment, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> determines whether to initiate verbalization via single token generation. If the next token is <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math>, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> continues processing additional reasoning segments. If the next token is <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math>, the model transitions to verbalizing mode, where <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> translates the accumulated reasoning segments into speech-friendly output tokens. The model continues generating verbalized text until it produces the <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> token, at which point it forwards the generated text to the subsequent stage, returns to thinking mode, and resumes processing reasoning segments.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S2.F2\" title=\"Figure 2 &#8227; Reasoning in LLMs &#8227; 2 Related Work &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the state diagram of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "revert",
                    "which",
                    "generation",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> functions as an incremental, asynchronous, speech-oriented summarizer of the reasoning output. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs no reasoning itself, it can be implemented with a more compact model compared to the reasoning LLM.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the <span class=\"ltx_text ltx_font_smallcaps\">Speak</span> stage, we convert the utterances to speech using a TTS model. Specifically, we employ a TTS model that supports both streaming input and output, allowing the system to process streaming outputs from the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage and play the generated speech with minimal delay for the user.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the reasoning LLM and streaming TTS models remain frozen, we describe only the training procedure for the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model. We below discuss the training data format, the dataset construction pipeline, and the training objective.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "revert",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each training example comprises a user query <math alttext=\"{\\mathcal{Q}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Q}}</annotation></semantics></math> and the corresponding response <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math>. Since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> performs incremental summarization of reasoning steps, the training data must be structured such that summaries are interleaved with their respective reasoning segments. Formally, <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> is represented as</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "since",
                    "structured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> is the segments of the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th reasoning step, and <math alttext=\"{\\mathcal{V}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{V}}_{k}</annotation></semantics></math> is the verbalized text, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens, as a speech-friendly summary of <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math>.\nSometimes, <math alttext=\"{\\mathcal{R}}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}</annotation></semantics></math> consists of multiple reasoning segments, denoted as <math alttext=\"{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mi>k</mi><msub><mi>m</mi><mi>k</mi></msub></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathcal{R}}_{k}=[{\\mathcal{R}}_{k}^{1},\\dots,{\\mathcal{R}}_{k}^{m_{k}}]</annotation></semantics></math>, where each segment is separated by delimiters <math alttext=\"{\\mathcal{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{D}}</annotation></semantics></math> (<em class=\"ltx_emph ltx_font_italic\">i.e.</em>, newline), and <math alttext=\"m_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">m_{k}</annotation></semantics></math> indicates the total number of segments.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "speechfriendly",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because no publicly available datasets conform to the required format, we propose a simple LLM-based pipeline to generate a dataset in our desired format with a standard QA dataset as input. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Speak &#8227; 3 Think-Verbalize-Speak &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents an overview of the proposed pipeline. The pipeline consists of three steps: <span class=\"ltx_text ltx_font_italic\">solve</span>, <span class=\"ltx_text ltx_font_italic\">summarize</span>, and <span class=\"ltx_text ltx_font_italic\">scatter</span>. In the solve step, the reasoning LLM solves the user query using a standard chain-of-thought reasoning. In the summarize step, we generate a speech-friendly summary for the generated reasoning output. In the scatter step, we scatter the summaries across the reasoning process such that each summary appears immediately after its associated reasoning step, enclosed by <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> and <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math> tokens. We use the output of the scatter step as the training data for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>.\nFor all three steps, we employ gpt-4.1-mini-2025-04-11 as the processing model. More detailed procedures and prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A1\" title=\"Appendix A Dataset &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "its",
                    "standard",
                    "use",
                    "revert",
                    "generate",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training procedure for <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> closely follows standard LLM finetuning. We begin by initializing <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> with a pretrained LLM and finetune it using cross-entropy loss on the next-token prediction task, applied selectively to the training data described above. Importantly, since <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is not required to perform the reasoning process itself, we compute the loss only within the verbalization segments of each sequence. For positions outside these verbalization segments, the model is trained to predict a special <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token, signaling that it is still in the thinking mode.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "standard",
                    "revert",
                    "still",
                    "since",
                    "follows",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, let <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math> denote the set of token positions within verbalization segments, the set of token positions spanning from <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> to <math alttext=\"\\langle\\texttt{eov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">eov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{eov}\\rangle</annotation></semantics></math>, inclusive. Conversely, let <math alttext=\"\\mathcal{I}_{\\textsc{Think}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Think</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Think}}</annotation></semantics></math> represent the set of token positions outside <math alttext=\"\\mathcal{I}_{\\textsc{Verbal}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mtext class=\"ltx_font_smallcaps\">Verbal</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{I}_{\\textsc{Verbal}}</annotation></semantics></math>, corresponding to the tokens used for LLM reasoning. Then, the total loss is</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from",
                    "let"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of our <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and the verbalizer model across multiple experimental settings.\nFull details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.SS2\" title=\"B.2 Training &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate two versions of <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span>: (1) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">Seq</span>)</span>, which performs reasoning followed by verbalization sequentially; and (2) <span class=\"ltx_text ltx_font_bold\">TVS (<span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>)</span>, in which the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model incrementally verbalizes the reasoning outputs.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "evaluate",
                    "revert",
                    "which",
                    "seq",
                    "reasoning",
                    "tvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, we include several baselines based on the <span class=\"ltx_text ltx_font_smallcaps\">Think-Speak</span> framework:\n(1) Chain-of-Thought (<span class=\"ltx_text ltx_font_bold\">CoT</span>) employs a standard zero-shot chain-of-thought prompting technique to elicit step-by-step reasoning;\n(2) Speech-Friendly Prompting (<span class=\"ltx_text ltx_font_bold\">SFP</span>) applies prompting strategies to encourage the model to generate concise, speech-appropriate outputs, following the guidelines established by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>; and\n(3) Speech-Friendly Finetuning (<span class=\"ltx_text ltx_font_bold\">SFF</span>) uses a finetuned model to directly produce speech-friendly responses. For finetuning, we use the same dataset as our model, but replace the output of the scatter step with that of the summarize step.\nAdditionally, we include Qwen2.5-Omni-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib24\" title=\"\">2025</a>)</cite>, an end-to-end spoken dialogue system finetuned to produce speech-friendly outputs, as a baseline for comparative analysis.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speechfriendly",
                    "outputs",
                    "finetuning",
                    "produce",
                    "responses",
                    "standard",
                    "reasoning",
                    "prompting",
                    "use",
                    "generate",
                    "concise",
                    "spoken",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, we experiment with multiple LLMs, specifically Qwen2.5-7B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite>, Llama-3.1-8B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib7\" title=\"\">2024</a>)</cite>, and gpt-4o-mini-2024-07-18&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib17\" title=\"\">2024</a>)</cite>. For <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we use Qwen2.5-3B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib25\" title=\"\">2024</a>)</cite> as the base model and fine-tune it. For all models except Qwen2.5-Omni-7B, we employ gpt-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib16\" title=\"\">2025</a>)</cite> as the speak model to convert textual responses into speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "responses",
                    "use",
                    "revert",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the following three datasets for our evaluation setup:\n(1) GSM8K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib2\" title=\"\">2021</a>)</cite> focuses on arithmetic reasoning, based on grade-school level math problems. The solutions are generally straightforward and linear, involving simple, easy-to-follow steps without complex mathematical elements;\n(2) 2WikiMultiHopQA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib9\" title=\"\">2020</a>)</cite> requires multi-hop retrieval of Wikipedia documents to answer a question. While the dataset is not primarily designed to assess complex reasoning, multi-hop QA in a closed-book setting demands step-by-step reasoning abilities; and\n(3) SciBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib19\" title=\"\">2024a</a>)</cite> assesses college-level scientific problem-solving abilities. The solutions are often involve complex equations, formulas, and other components that are not easily communicated verbally.</p>\n\n",
                "matched_terms": [
                    "while",
                    "scibench",
                    "scientific",
                    "question",
                    "answer",
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct the training set of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> as a subset of examples from the GSM8K and 2WikiMultiHopQA training sets.\nSciBench remains unseen during training and serves to evaluate the model&#8217;s out-of-domain generalization capability.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "evaluate",
                    "revert",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the reasoning capabilities of dialogue systems. Each system generates responses to the provided questions, and we assess the correctness of the final outputs using an LLM-as-a-judge framework.\nWe report the accuracy for this evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "outputs",
                    "responses",
                    "reasoning",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate whether the responses from each system are suitable for verbal delivery. We adopt the four metrics also used by <cite class=\"ltx_cite ltx_citemacro_citet\">Cho et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#bib.bib1\" title=\"\">2024</a>)</cite>: (1) Word count (WC) measures the overall conciseness of the response and is computed using simple whitespace delimitation; (2) Flesch Reading Ease (FRE) score assesses text readability based on the number of syllables per word and words per sentence. Although not directly related to speech, the FRE score is correlated with listenability; (3) Dependency depth (DD) is the maximum depth of the response dependency tree computed by Spacy dependency parser<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://spacy.io/api/dependencyparser\" title=\"\">https://spacy.io/api/dependencyparser</a></span></span></span>. DD helps assess the sentence complexity; (4) Nonvocalizable character count (NV) evaluates the appropriateness of the response for verbal delivery by identifying the presence of nonvocalizable content.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "responses",
                    "also",
                    "delivery",
                    "complexity",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure the time-to-response of the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework and evaluate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> in latency reduction. Since we use a streaming TTS model, we focus on the time required to generate the first spoken output, specifically <math alttext=\"\\mathbf{T}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{1}</annotation></semantics></math>, the time taken for the system to enter the verbalizing mode after receiving the user&#8217;s query, and <math alttext=\"\\mathbf{T}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{2}</annotation></semantics></math>, the additional time required to produce the first verbalized segment after verbalization has started. We report latencies at the 50th percentile with Qwen2.5-3B-Instruct as the verbalizer. All experiments are conducted on the GSM8K dataset using the PyTorch <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library with bfloat16 precision on an NVIDIA A6000 GPU.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "produce",
                    "use",
                    "revert",
                    "since",
                    "generate",
                    "spoken",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation where Amazon Mechanical Turk annotators rate system responses using a 5-point Likert scale across four criteria: naturalness, conciseness, understandability, and overall quality. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T1\" title=\"Table 1 &#8227; Automatic Speech-Friendliness Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides the definitions of each criterion. We randomly sample 60 examples, 20 from each dataset, and collect annotations from three independent raters per example. Unlike previous evaluations that rely on textual assessment, this evaluation is <span class=\"ltx_text ltx_font_italic\">speech-based</span>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "independent",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the results of the automatic evaluations for the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> model and the baseline systems. In most cases, the chain-of-thought strategy achieves the highest reasoning benchmark accuracies within each <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model category, but demonstrates the lowest performance in speech-suitability evaluations.\nThis indicates that the chain-of-thought strategy exhibits highly polarized performance with respect to reasoning capabilities and speech-friendliness.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "baseline",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we apply two most widely used solutions to these issues: prompting and finetuning. While the speech-friendly prompting yields only a minimal decrease in reasoning accuracies, it ignores the instructions when faced with challenging questions and resorts to chain-of-thought reasoning, thereby harming its speech-suitability scores. An example in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A4.T9\" title=\"Table 9 &#8227; D.2 Qualitative Results &#8227; Appendix D Additional Analysis &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and human evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Does speech-friendliness compromise models&#8217; reasoning capabilities? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveal similar issues. Despite receiving the highest overall scores on 2WikiMultiHopQA, its scores, especially the conciseness score, drop significantly on GSM8K and SciBench. Qwen2.5-Omni-7B also exhibits a similar trend, where its speech-friendliness diminishes with rising problem difficulty.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "while",
                    "its",
                    "finetuning",
                    "also",
                    "therefore",
                    "scibench",
                    "prompting",
                    "when",
                    "reasoning",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the speech-friendly finetuning system receives high speech-friendliness scores but low reasoning benchmark scores. In other words, it yields highly intelligible responses but not intelligent ones. Notably, the system achieves the highest scores on the 2WikiMultiHopQA dataset. We attribute this to the model acquiring additional knowledge during training, as the dataset does not strictly separate train set and development set knowledge bases. Therefore, the high score is likely unrelated to the system&#8217;s reasoning capabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "responses",
                    "therefore",
                    "contrast",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, by design, should mirror the accuracy scores of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s chain-of-thought strategy, we observe a slight decrease in accuracy on the SciBench dataset. We attribute this to two possible factors: (1) out-of-domain characteristics and (2) inherent task difficulty. However, even with the drop in accuracy, both versions of our framework vastly outperform other baselines.</p>\n\n",
                "matched_terms": [
                    "while",
                    "scibench",
                    "even",
                    "inherent",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe an anomalous result on the 2WikiMultiHopQA dataset for the Llama-3.1-8B-Instruct <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model, where both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> variants outperform the chain-of-thought strategy. We attribute this to the same factor identified in the speech-friendly finetuning strategy issue, as all three systems share the same target text in the training data.</p>\n\n",
                "matched_terms": [
                    "same",
                    "speechfriendly",
                    "finetuning",
                    "also",
                    "revert",
                    "seq",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech-suitability measures, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> outperform all other baselines in automatic evaluation. In human evaluation, we analyze the results for each dataset. On 2WikiMultiHopQA, all systems achieve high scores. In GSM8K and SciBench, all systems show a performance drop in conciseness and understandability. Regardless, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> consistently rank as the top two models in terms of naturalness, conciseness, and overall quality criteria.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scibench",
                    "revert",
                    "seq",
                    "consistently",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, the introduction of the <span class=\"ltx_text ltx_font_smallcaps\">Verbalize</span> stage in our framework enables exceptional speech-friendliness with minimal compromise in the reasoning capabilities of the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As stated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.SS2\" title=\"5.2 How does the explicit verbalization stage affect performance? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, both <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> perform well across different datasets, with minimal differences in their effectiveness as verbalizers. The primary distinction between the two models lies in their latency. Specifically, <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> waits for the reasoning process to complete before verbalizations, which requires approximately 8.08 seconds, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S4.T2\" title=\"Table 2 &#8227; ReVerT Latency Evaluation &#8227; 4.3 Evaluation Procedure and Measures &#8227; 4 Experiments &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). Such latency is unsuitable for real-time spoken conversation settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reasoning",
                    "revert",
                    "which",
                    "seq",
                    "approximately",
                    "between",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> incrementally processes verbalizable segments before the reasoning process is complete, receiving the first segment in an average of 2.72 seconds, a 66% reduction in latency compared to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span>. In voice-interface conversations, this latency can be effectively masked by brief filler phrases such as &#8220;Let me think,&#8221; making it acceptable for real-time applications.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "revert",
                    "first",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> achieves performance comparable to <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> while significantly reducing latency, suggesting that <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> is preferable for most real-time applications.</p>\n\n",
                "matched_terms": [
                    "therefore",
                    "revert",
                    "while",
                    "seq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss the effect the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model size has on its performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#S5.T4\" title=\"Table 4 &#8227; 5.3 When should I use ReVerT over Seq? &#8227; 5 Results and Discussion &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the performance of three differently sized <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models: Qwen2.5-0.5B-Instruct, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct. The results indicate that the performance loss from decreasing model size is more pronounced on SciBench than on GSM8K, likely reflecting the greater task difficulty of the former dataset. Notably, the speech-suitability scores remain stable despite reductions in model size.</p>\n\n",
                "matched_terms": [
                    "models",
                    "greater",
                    "its",
                    "than",
                    "scibench",
                    "revert",
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, although model size affects <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>&#8217;s performance, the degradation is not substantial. This suggests that smaller <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> models remain a viable option in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "revert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address a critical gap between text readability and speech-suitability in spoken dialogue system responses. We present the <span class=\"ltx_text ltx_font_smallcaps\">Think-Verbalize-Speak</span> framework, which decouples reasoning from verbalization to achieve both reasoning accuracy and speech-friendliness. Extensive automatic and human evaluations show that our framework enhances speech-suitability with minimal compromise of reasoning capability across benchmarks. Additionally, we introduce the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model for incremental verbalization, which reduces latency compared to the sequential approach. Extending the framework to multi-turn or full-duplex interactions presents a promising avenue for future research.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "reasoning",
                    "revert",
                    "which",
                    "from",
                    "between",
                    "spoken",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our framework shows promising results, it has several limitations. First, it focuses on single-turn conversational settings and does not support multi-turn or full-duplex interactions, where reasoning and verbalization may occur in parallel with multiple user interactions. Extending the framework to handle such interactive scenarios remains an important direction for future work. Second, the current verbalization model does not allow control over the level of explanation detail. Adding support for adjustable granularity, ranging from brief summaries to step-by-step explanations, could improve adaptability to different user needs. Third, our work focuses on chain-of-thought reasoning, but extending it to other test-time computation methods with intermediate traces, such as multi-step retrieval or tool use, could broaden its applicability.</p>\n\n",
                "matched_terms": [
                    "its",
                    "while",
                    "use",
                    "from",
                    "results",
                    "methods",
                    "reasoning",
                    "first",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces no additional epistemic or safety risks beyond those already present in the underlying reasoning model. This is because the verbalization model is designed solely to rephrase the outputs of a frozen, pretrained reasoning LLM into speech-friendly language without altering their content or logic. It performs no independent reasoning, decision-making, or content generation beyond linguistic reformulation. Consequently, factual inaccuracies, biases, or harmful outputs originate entirely from the reasoning model. The verbalization stage merely translates those outputs into a form more suitable for spoken communication. Thus, the overall risk profile of the system is bounded by that of the underlying reasoning model, and our model introduces no novel vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "outputs",
                    "reasoning",
                    "independent",
                    "generation",
                    "from",
                    "spoken",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models, datasets, and other artifacts used in this work are released under licenses that permit research use. Our usage of these resources is consistent with both the terms of their licenses and the intended purposes specified by their creators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "use",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides a detailed procedure for generating our training dataset. First, we sample a set of raw question-answer pairs from the GSM8K and 2WikiMultiHopQA.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "from",
                    "first",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the GSM8K training set, we use all 7,473 examples. From the 2WikiMultiHopQA dataset, we sample 1,000 examples from each of the 4 data types: inference, comparison, bridge_comparison, and compositional.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "use",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we simply induce step-by-step reasoning process, using standard zero-shot chain-of-thought prompting.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "standard",
                    "prompting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we generate a summary of the reasoning process from <span class=\"ltx_text ltx_font_italic\">solve</span>. We impose the following constraints on the resulting summary:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "generate",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must contain all essential information from the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The summary must follow the same logical progression as the reasoning process.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "same",
                    "follow",
                    "logical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because enforcing all constraints simultaneously in a single instruction yields suboptimal results, we adopt a progressive approach, providing the language model with one constraint at a time.</p>\n\n",
                "matched_terms": [
                    "one",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this step, we distribute the summary throughout the reasoning process, placing each summary segment immediately after its corresponding reasoning segment. To encourage fine-grained control over the placement of summary segments, we manually label 16 samples and use them as few-shot examples.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "use",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the specific prompts used in our experiments, including those for baseline methods and our proposed verbalizer.\nFor the CoT reasoning experiments, we adopt the system prompt illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In the case of speech-friendly prompting and finetuning, we follow the instruction template shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F5\" title=\"Figure 5 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nOur proposed verbalizer (both the <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span>) uses the prompt presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F6\" title=\"Figure 6 &#8227; B.1 Prompts &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments involving Qwen2.5-Omni-7B, we employ the default system prompt provided by the model.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "finetuning",
                    "prompting",
                    "revert",
                    "seq",
                    "follow",
                    "methods",
                    "reasoning",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We finetune our verbalization model for <span class=\"ltx_text ltx_font_smallcaps\">Seq</span> and <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model using Qwen2.5-3B-Instruct with full-parameter optimization. All models are trained for one epoch with 4 A6000GPU, totaling 1.3k steps (within 1 hour) with a batch size of 8. For optimization, we employ the AdamW optimizer with a learning rate of <math alttext=\"2\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-5}</annotation></semantics></math>, a cosine learning rate scheduler, and a warmup ratio of 0.1. The optimizer parameters are set to <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, with a weight decay of 0.1.\nFor speech-friendly finetuning (SFF), we finetune Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct using LoRA with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math> and <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>. and other training configurations are kept identical to those described above.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechfriendly",
                    "finetuning",
                    "revert",
                    "seq",
                    "one",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling with a temperature of 0.1 and a nucleus probability <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math> for all response generation for inference. For the <span class=\"ltx_text ltx_font_smallcaps\">ReVerT</span> model, we employ greedy decoding for next-token prediction to determine whether to initiate verbalization (i.e., generate the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> or <math alttext=\"\\langle\\texttt{con}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">con</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{con}\\rangle</annotation></semantics></math> token). Upon receiving the final reasoning segment, the verbalizer is manually appended with the <math alttext=\"\\langle\\texttt{bov}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext class=\"ltx_mathvariant_monospace\">bov</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\texttt{bov}\\rangle</annotation></semantics></math> token rather than relying on sampling.</p>\n\n",
                "matched_terms": [
                    "than",
                    "use",
                    "revert",
                    "probability",
                    "generate",
                    "generation",
                    "determine",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize an LLM-based answer verification method to overcome the limitations of rule-based evaluation. In the context of speech-friendliness, responses should be clear, natural, and easily understandable, which means they may not always conform to a specific format or template. Such characteristics render exact matching and rule-based answer extraction unreliable.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "which",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, we use gpt-4.1-mini-2025-04-11 to automatically assess answer correctness. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A2.F7\" title=\"Figure 7 &#8227; B.4 LLM-Based Answer Verification &#8227; Appendix B Experimental Setup &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the verifier is prompted with the question, a model-generated response, and the corresponding ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "therefore",
                    "use",
                    "prompted",
                    "question",
                    "answer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate 60 examples, with a random sample of 20 from the three target datasets: GSM8K, 2WikiMultiHopQA, and SciBench. Each example is evaluated independently by three annotators.\nWe use the Qwen2.5-7B-Instruct model as the thinking LLM across all evaluated systems.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "random",
                    "evaluate",
                    "scibench",
                    "use",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit the annotators via Amazon Mechanical Turk (MTurk). For each data point, we collect ratings from three independent workers to mitigate subjectivity. We provide the annotators with the following instructions:</p>\n\n",
                "matched_terms": [
                    "from",
                    "independent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compensation rates are set at $0.5 per example for GSM8K and 2WikiMultiHopQA, and $0.7 per example for SciBench, reflecting the varying complexity and required annotation effort across datasets.\nBased on the compensation rate per example and average completion time, all participants receive above minimum wage compensation. Participants provide informed consent for the use of their annotations for research purposes.\nExplanations are manually reviewed to filter out low-effort or inconsistent responses.</p>\n\n",
                "matched_terms": [
                    "scibench",
                    "use",
                    "complexity",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present additional analyses of our experimental results to complement the main findings discussed in the paper. We provide qualitative examples for each dataset and method, along with detailed dataset-wise results of speech-suitability scores and human evaluation results.</p>\n\n",
                "matched_terms": [
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide speech-suitability scores for individual datasets in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T6\" title=\"Table 6 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that speech-friendly prompting falls short in speech-suitability scores, especially on the SciBench benchmark. We attribute these performance gaps to the <span class=\"ltx_text ltx_font_smallcaps\">Think</span> model&#8217;s inability to maintain speech-friendly formatting when addressing complex technical questions that require multiple reasoning steps and computations. In such cases, the model reverts to standard chain-of-thought responses despite explicit instructions for speech adaptation. This finding suggests that prompt-based approaches alone prove insufficient for speech adaptation in highly technical domains.</p>\n\n",
                "matched_terms": [
                    "speechfriendly",
                    "responses",
                    "scibench",
                    "standard",
                    "prompting",
                    "finding",
                    "when",
                    "reasoning",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we present inter-criteria correlation statistics from the human evaluation scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16028v1#A3.T7\" title=\"Table 7 &#8227; Annotation Procedure. &#8227; C.2 Evaluation Criteria. &#8227; Appendix C Human Evaluation &#8227; Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. Notably, Understandability exhibit significantly lower correlation with other criteria, suggesting that comprehensible responses may introduce verbosity and compromise speech naturalness through excessive detail and repetition.</p>\n\n",
                "matched_terms": [
                    "from",
                    "responses",
                    "exhibit"
                ]
            }
        ]
    }
}