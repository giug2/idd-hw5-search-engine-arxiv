{
    "S3.T2.fig1": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 1: WolBanking77 dataset statistics",
        "body": "WOLOF\nFRENCH\n\n\nMin Word Count\n2\n2\n\n\nMax Word Count\n81\n83\n\n\nMean Word Count\n12.22\n12.47\n\n\nMedian Word Count\n10\n10",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">WOLOF</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\">FRENCH</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Min Word Count</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">Max Word Count</td>\n<td class=\"ltx_td ltx_align_right\">81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">Mean Word Count</td>\n<td class=\"ltx_td ltx_align_right\">12.22</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">12.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">Median Word Count</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">10</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\">10</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "word",
            "wolof",
            "french",
            "median",
            "wolbanking77",
            "statistics",
            "min",
            "count",
            "max",
            "mean",
            "dataset"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class=\"ltx_cite ltx_citemacro_citep\">[Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class=\"ltx_cite ltx_citemacro_citep\">[Mart&#237;nez and Short, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib51\" title=\"\">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class=\"ltx_cite ltx_citemacro_citep\">[Anthony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib8\" title=\"\">2024</a>, Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models based on neural networks has emerged in recent years in the field of ID <cite class=\"ltx_cite ltx_citemacro_citep\">[Gerz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>, Krishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib45\" title=\"\">2021</a>, Si et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib72\" title=\"\">2023b</a>]</cite>. However, very few datasets in African languages <cite class=\"ltx_cite ltx_citemacro_citet\">Alexis et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib5\" title=\"\">2022</a>], Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>], Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>], Mwongela et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib57\" title=\"\">2023</a>], Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> have been explored. Even less in the field of e-banking for Sub-Saharan languages. Most existing datasets are in English <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite>, as their development requires significant financial and human resources. In this paper, we present an intent classification dataset in Wolof, an African and Sub-Saharan language, based on the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> which originally consists of 13,083 customer service queries labeled with 77 intents. In addition, our work also draws on the MINDS-14 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite>, which contains 14 intents derived from a commercial e-banking system and includes an audio component.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With regard to the Wolof language, datasets have been published in the literature, particularly in the field of NLP and Automatic Speech Recognition (ASR). For example, some datasets include Wolof texts such as afriqa dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ogundepo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib60\" title=\"\">2023</a>]</cite> which deals with the question answering (QA) task, masakhaner versions 1 and 2 <cite class=\"ltx_cite ltx_citemacro_citet\">Adelani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib2\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib3\" title=\"\">2022b</a>]</cite> which targets the Name Entity Recognition (NER) task, UD_Wolof-WTB <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/UniversalDependencies/UD_Wolof-WTB\" title=\"\">https://github.com/UniversalDependencies/UD_Wolof-WTB</a></span></span></span> or even MasakhaPOS <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/masakhane-io/lacuna_pos_ner\" title=\"\">https://github.com/masakhane-io/lacuna_pos_ner</a></span></span></span> which addresses the part-of-speech tagging (POS) task. In addition, several datasets for the machine translation (MT) task such as OPUS, <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> FLORES 200 <cite class=\"ltx_cite ltx_citemacro_citet\">team et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib74\" title=\"\">2022</a>]</cite>, NTREX-128 <cite class=\"ltx_cite ltx_citemacro_citet\">Federmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib28\" title=\"\">2022</a>]</cite> and MAFAND-MT <cite class=\"ltx_cite ltx_citemacro_citep\">[Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib1\" title=\"\">2022a</a>]</cite>. In the field of ASR, several datasets containing Wolof have also been published, including ALFFA <cite class=\"ltx_cite ltx_citemacro_citet\">Gauthier et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib31\" title=\"\">2016</a>]</cite>, fleurs <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite> and more recently KALLAAMA <cite class=\"ltx_cite ltx_citemacro_citet\">Gauthier et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib32\" title=\"\">2024</a>]</cite>. The cited datasets cover fairly general domains, such as news and religion. However, to date, there is only one text dataset dedicated to the banking sector named INJONGO <cite class=\"ltx_cite ltx_citemacro_citet\">Yu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib83\" title=\"\">2025</a>]</cite> (with other domains such as home, kitchen and dining, travel and utility). This dataset includes slot-filling and intent classification tasks for 16 African languages including Wolof.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, 9,791 customer service queries from the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> was translated to French and Wolof by a team of linguistic experts from the Centre de Linguistique Appliqu&#233;e de Dakar (CLAD). Additionally, this work introduces another dataset based on MINDS-14 <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite>, in which each query is paired with audio recordings from multiple speakers with diverse accents and ages (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS3\" title=\"8.3 Speakers age distribution &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.3</span></a>). The dataset includes 10 intents represented with both text and audio examples. The open source tool Lig-Aikuma <cite class=\"ltx_cite ltx_citemacro_citet\">Blachon et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib12\" title=\"\">2016</a>]</cite> was used to produce the voice recordings. Our contributions include:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An audio dataset with 263 sentences covering 10 intents in the banking and transport domains, including diverse voices and accents, making it the first of its kind in the Wolof language at the time of writing.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A text dataset with 9,791 sentences translated from English to French and Wolof covering 77 intents in banking domain.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A dataset documentation (datasheets) for WolBanking77.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text dataset contains a total of 9,791 sentences and 77 intents from the English train set of Banking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite>, manually translated to French and Wolof thanks to a team of linguistic experts from the Centre de Linguistique Appliqu&#233;e de Dakar (CLAD).\nThe Wolof version was translated and localized according to the local context, for instance, \"<span class=\"ltx_text ltx_font_italic\">ATM</span>\" and \"<span class=\"ltx_text ltx_font_italic\">app</span>\" translated as \"<span class=\"ltx_text ltx_font_italic\">GAB</span>\" and \"<span class=\"ltx_text ltx_font_italic\">aplikaasiyo<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>N</span>\", see the example in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F1\" title=\"Figure 1 &#8227; 3.1 Textual data &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Duplicated sentences translated in Wolof was removed for the ID task to avoid many-to-one translations from English to Wolof. Two versions of the dataset are reported on table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.T2\" title=\"Table 2 &#8227; 3.1 Textual data &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first version is comprised of 5k samples which is a sub-sample of the second version of 9,791 samples. Train set is 80% of both versions and test set represents 20%. Note that intents are unbalanced, the most represented intent has a frequency of 200 while the least represented one has a frequency of 24.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some statistics were collected on the data translated into French and Wolof. Statistics on table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.T2\" title=\"Table 2 &#8227; 3.1 Textual data &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show a slightly higher number of words per query for French (83) compared to Wolof (81).</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "statistics",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio corpus based on MINDS-14 <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite> consists of 186 queries and 77 responses, initially translated from French into English by a team of linguistic experts from the Centre de Linguistique Appliqu&#233;e de Dakar (CLAD). The dataset also includes a phonetic transcription of the Wolof text to facilitate the correct pronunciation of sentences by different speakers. Additional intents was added to the initial MINDS-14 dataset, namely: <span class=\"ltx_text ltx_font_italic\">OPEN_ACCOUNT</span>, for information related to opening a bank account, <span class=\"ltx_text ltx_font_italic\">BUS_RESERVATION</span>, for booking a transport bus (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS5\" title=\"8.5 Transportation data &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.5</span></a>), <span class=\"ltx_text ltx_font_italic\">TECHNICAL_VISIT</span> for scheduling a vehicle inspection appointment. <span class=\"ltx_text ltx_font_italic\">TRANSFER_MONEY</span> for the intent to transfer money and <span class=\"ltx_text ltx_font_italic\">AMOUNT</span> to specify the amount to be transferred. See table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.SS2\" title=\"3.2 Audio recordings and transcriptions &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> for the complete list of intents in the audio dataset.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">mDeBERTa-v3-base-mnli-xnli</span>: Based on DeBERTaV3-base <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib38\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib37\" title=\"\">2021</a>]</cite>, this model was published by <cite class=\"ltx_cite ltx_citemacro_citet\">Moritz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib56\" title=\"\">2022</a>]</cite> and pre-trained on the CC100 multilingual dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib20\" title=\"\">2020</a>], Wenzek et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib80\" title=\"\">2020</a>]</cite> with 100 different languages including Wolof. This model can perform Natural Language Inference (NLI) on 100 languages.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriTeVa V2 Base</span>: A multilingual T5 model released by <cite class=\"ltx_cite ltx_citemacro_citet\">Oladipo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib61\" title=\"\">2023</a>]</cite> and pre-trained on WURA dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Oladipo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib61\" title=\"\">2023</a>]</cite> covering 16 African Languages not including Wolof.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Canary Flash</span>: Canary Flash <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> has been pre-trained on several languages (English, German, French, Spanish) and on various tasks such as ASR and translation. Canary Flash is based on Canary <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> that is an encoder-decoder model whose encoder is based on the FastConformer model <cite class=\"ltx_cite ltx_citemacro_citet\">Rekesh et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib70\" title=\"\">2023</a>]</cite> and the decoder on the Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite>. Canary has the distinctive feature of concatenating tokenizers <cite class=\"ltx_cite ltx_citemacro_citet\">Dhawan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib25\" title=\"\">2023</a>]</cite> from different languages using SentencePiece.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/google/sentencepiece\" title=\"\">Google Sentencepiece Tokenizer</a></span></span></span> In this work, English, Spanish, French and Wolof languages are concatenated with Canary&#8217;s Tokenizer. These tokens are then transformed into token embedding before being fed into the Transformer decoder. In addition to the tokens of each language, Canary uses 1,152 special tokens representation. At the time of writing, Canary has three variants: canary-1b, canary-1b-flash, and canary-180m-flash. Canary-1b-flash version is chosen for the experiments because of its multilingual support. The canary-1b-flash model was trained on 85K hours of speech data, including 31K hours of public data (FLEURS <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite>, CoVOST v2 <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib79\" title=\"\">2021b</a>]</cite>) and the remainder on private data. The Mozilla CommonVoice 12 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite> was used as validation data for each language.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Access to financial services or public transportation services can be facilitated for illiterate people by providing them with voice interfaces in their language of communication. In this paper, an Intent Detection dataset is presented in Wolof language in two modalities which are voice and text. Additionally, dataset description and benchmarks are presented. WolBanking77 is published and shared freely under CC BY 4.0 license along with the code and datasheets <cite class=\"ltx_cite ltx_citemacro_citet\">Gebru et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib33\" title=\"\">2021</a>]</cite> with the aim of inspiring low budget research into low-resource languages.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Is the Wolof language as used in Mauritania and Gambia substantially distinct from the Senegalese variety, such that the demonstrated performance on the audio dataset may not generalise to these Wolof-speaking populations?</span>\n</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">According to statistics (from Agence Nationale de la Statistique et de la D&#233;mographie <cite class=\"ltx_cite ltx_citemacro_citet\">ANSD [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib6\" title=\"\">2013</a>]</cite>), the literacy rate is higher among the younger populations of 10-14 years old and 15-19 years old, with 58.1% and 64.1% respectively. However, the literacy rate decreases among older populations over 70 years of age (between 13% and 20%). In view of these figures, we intend to take the representation of the older population in future versions of the dataset.</p>\n\n",
                "matched_terms": [
                    "statistics",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the word &#8220;illiterate&#8221; in the sense of not being able to understand the official language, which is French. &#8220; In Senegal, <cite class=\"ltx_cite ltx_citemacro_citet\">ANSD [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib7\" title=\"\">2021</a>]</cite> reports an overall illiteracy rate of 48,2%, reaching 62,7% in rural area. Literacy rate relates to the official language of a country. In Senegal, the official language is French but is seldom spoken by the population in their daily lives. Senegalese people primarily use their native languages or Wolof, as a vehicular language, to communicate.&#8221; <cite class=\"ltx_cite ltx_citemacro_citep\">[Gauthier et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib32\" title=\"\">2024</a>]</cite></p>\n\n",
                "matched_terms": [
                    "word",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main goal for creating this dataset is to design a speech chatbot for Wolof, in order to equip illiterate people from Senegal with an AI tool allowing them to access banking services, as well as buying a transit pass from public transportation, etc.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To protect the anonymity of participants, personal data such as speaker names, locations and androidIDs have been removed from the dataset. We notified each participant that the collected audio will be used as a public dataset for research purposes. Each participant read and signed a consent form in which they declared their free and informed consent to participate in the study as part of the data collection described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS11\" title=\"8.11 Consent form &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.11</span></a>. A total amount of $5000 was paid to the linguists for the translation and phonetic transcription of all sentences in Wolof and French.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "wolof",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To construct sentences in Wolof, there are several processes such as 8 class markers for words in singular form and 2 class markers for words in plural form. These markers are positioned in front of or right after the word such as: <span class=\"ltx_text ltx_font_italic\">nit ki</span> (to designate a person), <span class=\"ltx_text ltx_font_italic\">nit &#241;i</span> (to designate several persons). In addition, we also find other construction processes such as consonantal alternation, pre-nasalization and suffixation. For some derivations, there is the simultaneous presence of several of these processes. For example, pre-nasalization can be accompanied by suffixation <cite class=\"ltx_cite ltx_citemacro_citep\">[Ndione, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib58\" title=\"\">2013</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "wolof"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset was created with the aim of propelling research on Intent Detection (ID) task in Wolof which is a language spoken in West Africa in order to allow illiterate people to be able to interact with digital systems through the voice channel. We introduce a text dataset and audio dataset including utterances accompanied by their transcription with their corresponding intent in order to be able to simulate a dialogue between the user and the system.</p>\n\n",
                "matched_terms": [
                    "wolof",
                    "dataset"
                ]
            }
        ]
    },
    "S3.T2.fig2": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 2: WolBanking77 dataset split",
        "body": "SPLIT\n5k sample\nall\n\n\nTRAIN SET\n4,000\n7,832\n\n\nTEST SET\n1,000\n1,959",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SPLIT</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">5k sample</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\">all</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\">TRAIN SET</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">4,000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\">7,832</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">TEST SET</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">1,000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\">1,959</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "train",
            "all",
            "set",
            "sample",
            "wolbanking77",
            "split",
            "test",
            "dataset"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class=\"ltx_cite ltx_citemacro_citep\">[Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class=\"ltx_cite ltx_citemacro_citep\">[Mart&#237;nez and Short, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib51\" title=\"\">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class=\"ltx_cite ltx_citemacro_citep\">[Anthony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib8\" title=\"\">2024</a>, Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A dataset documentation (datasheets) for WolBanking77.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text dataset contains a total of 9,791 sentences and 77 intents from the English train set of Banking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite>, manually translated to French and Wolof thanks to a team of linguistic experts from the Centre de Linguistique Appliqu&#233;e de Dakar (CLAD).\nThe Wolof version was translated and localized according to the local context, for instance, \"<span class=\"ltx_text ltx_font_italic\">ATM</span>\" and \"<span class=\"ltx_text ltx_font_italic\">app</span>\" translated as \"<span class=\"ltx_text ltx_font_italic\">GAB</span>\" and \"<span class=\"ltx_text ltx_font_italic\">aplikaasiyo<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>N</span>\", see the example in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F1\" title=\"Figure 1 &#8227; 3.1 Textual data &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "train",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Duplicated sentences translated in Wolof was removed for the ID task to avoid many-to-one translations from English to Wolof. Two versions of the dataset are reported on table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.T2\" title=\"Table 2 &#8227; 3.1 Textual data &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first version is comprised of 5k samples which is a sub-sample of the second version of 9,791 samples. Train set is 80% of both versions and test set represents 20%. Note that intents are unbalanced, the most represented intent has a frequency of 200 while the least represented one has a frequency of 24.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "set",
                    "train",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To anonymize participant names, the system generates a user_id to replace the actual names (further details on the ethical collection and processing of personal data are provided in Appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS11\" title=\"8.11 Consent form &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.11</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S10\" title=\"10 Ethics Statement &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Next, a text file containing the sentences to be pronounced is selected. During recording, sentences were presented one at a time, with the option to cancel if a mispronunciation occurred at any stage. In terms of gender diversity, a total of 31 recording sessions were conducted, including 14 males, 14 females and 3 unspecified. The text and audio data were split with 80% allocated for train set and 20% for test set. Prior to data splitting, preprocessing steps were applied, including the removal of punctuation marks and numbers, and conversion of text to lowercase. Corrupted audio files were also removed from the dataset.</p>\n\n",
                "matched_terms": [
                    "train",
                    "set",
                    "split",
                    "test",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Access to financial services or public transportation services can be facilitated for illiterate people by providing them with voice interfaces in their language of communication. In this paper, an Intent Detection dataset is presented in Wolof language in two modalities which are voice and text. Additionally, dataset description and benchmarks are presented. WolBanking77 is published and shared freely under CC BY 4.0 license along with the code and datasheets <cite class=\"ltx_cite ltx_citemacro_citet\">Gebru et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib33\" title=\"\">2021</a>]</cite> with the aim of inspiring low budget research into low-resource languages.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text intent dataset and the speech intent dataset have been independently created. However, it could be possible to associate some speech intents with text intents such as BALANCE, CASH_DEPOSIT, FREEZE and TRANSFER_MONEY. It is indeed possible that a sample can belong to several intents, however this possibility has not been considered in this work.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To protect the anonymity of participants, personal data such as speaker names, locations and androidIDs have been removed from the dataset. We notified each participant that the collected audio will be used as a public dataset for research purposes. Each participant read and signed a consent form in which they declared their free and informed consent to participate in the study as part of the data collection described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS11\" title=\"8.11 Consent form &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.11</span></a>. A total amount of $5000 was paid to the linguists for the translation and phonetic transcription of all sentences in Wolof and French.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).</span>\n</p>\n\n",
                "matched_terms": [
                    "sample",
                    "set",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset contain all possible instances.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are 80% of the data for training and 20% for validation. The intent categories were considered during the split by stratifying the data according to the intent target. This separation guarantees us to have all the intents both in the training set and in the test set.</p>\n\n",
                "matched_terms": [
                    "split",
                    "set",
                    "test",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</span>\n</p>\n\n",
                "matched_terms": [
                    "sample",
                    "set",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Wolbanking77 is not sampled from a larger set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">No. The dataset does not contain all the raw data and metadata.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset can be used for anything related to intent classification as well as to train speech and text models.</p>\n\n",
                "matched_terms": [
                    "train",
                    "dataset"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 4: Precision, recall and f1-score scores for Small Language Models on WolBanking77. Best models are highlighted in lightgray.",
        "body": "5k samples\n\n\nAll samples\n\n\n\nModel\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\n\n\n\n\nLlama-3.2-1B-Instruct\n0.74\n0.71\n0.72\n0.52\n0.45\n0.46\n\n\n\n\\rowcolorlightgray Llama-3.2-3B-Instruct\n0.76\n0.75\n0.75\n0.56\n0.55\n0.55",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">5k samples</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">All samples</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Llama-3.2-1B-Instruct</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightgray Llama-3.2-3B-Instruct</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.55</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "samples",
            "language",
            "highlighted",
            "llama323binstruct",
            "lightgray",
            "f1score",
            "small",
            "all",
            "rowcolorlightgray",
            "best",
            "models",
            "wolbanking77",
            "model",
            "scores",
            "precision",
            "llama321binstruct",
            "recall"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class=\"ltx_cite ltx_citemacro_citep\">[Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class=\"ltx_cite ltx_citemacro_citep\">[Mart&#237;nez and Short, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib51\" title=\"\">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class=\"ltx_cite ltx_citemacro_citep\">[Anthony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib8\" title=\"\">2024</a>, Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models based on neural networks has emerged in recent years in the field of ID <cite class=\"ltx_cite ltx_citemacro_citep\">[Gerz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>, Krishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib45\" title=\"\">2021</a>, Si et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib72\" title=\"\">2023b</a>]</cite>. However, very few datasets in African languages <cite class=\"ltx_cite ltx_citemacro_citet\">Alexis et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib5\" title=\"\">2022</a>], Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>], Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>], Mwongela et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib57\" title=\"\">2023</a>], Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> have been explored. Even less in the field of e-banking for Sub-Saharan languages. Most existing datasets are in English <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite>, as their development requires significant financial and human resources. In this paper, we present an intent classification dataset in Wolof, an African and Sub-Saharan language, based on the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> which originally consists of 13,083 customer service queries labeled with 77 intents. In addition, our work also draws on the MINDS-14 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite>, which contains 14 intents derived from a commercial e-banking system and includes an audio component.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Jarrar et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib42\" title=\"\">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class=\"ltx_cite ltx_citemacro_citet\">Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class=\"ltx_cite ltx_citemacro_citet\">Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset was collected with the participation of students from Cheikh Anta Diop University in Dakar (UCAD), specifically from the Faculty of Letters and Human Sciences. Each participant recorded their voice using the elicitation mode of the Lig-Aikuma software <cite class=\"ltx_cite ltx_citemacro_citep\">[Blachon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib12\" title=\"\">2016</a>]</cite>. A total of 186 utterances were recorded by participants in a controlled environment. The elicitation mode of the Lig-Aikuma software was installed on an Android tablet. At the start of each session, information about the participant are requested, including native language, region of origin (see figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F3\" title=\"Figure 3 &#8227; 3.2 Audio recordings and transcriptions &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), gender, year of birth and name. This information is subsequently stored as metadata on the tablet&#8217;s memory card. Note that, the scores presented in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F3\" title=\"Figure 3 &#8227; 3.2 Audio recordings and transcriptions &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> are not representative of the population of each region. We tried to cover various ethnic groups in the country in order to have diverse accents and dialects as described in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS2\" title=\"8.2 Linguistic variation in the Wolof language across Senegal &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.2</span></a>. Additional details on demographic data and distribution can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS1\" title=\"8.1 WolBanking77 supplementary details &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "llama323binstruct",
                    "small",
                    "models",
                    "wolbanking77",
                    "precision",
                    "llama321binstruct",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BERT base (uncased)</span>: A transformers model pre-trained on English introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Devlin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib24\" title=\"\">2019</a>]</cite>, using a Masked Language Modeling (MLM) objective.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Afro-xlmr-large</span>: Based on XLM-R-large <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib20\" title=\"\">2020</a>]</cite> using MLM objective, this model was published by <cite class=\"ltx_cite ltx_citemacro_citet\">Alabi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib4\" title=\"\">2022</a>]</cite> and pre-trained on 17 African languages but not Wolof, while still demonstrating in the original paper good scores for NER task on Wolof.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfroLM_active_learning</span>: Based on XLM-RoBERTa (XLM-R) using MLM objective <cite class=\"ltx_cite ltx_citemacro_citet\">Ogueji et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib59\" title=\"\">2021</a>]</cite>, this model was published by <cite class=\"ltx_cite ltx_citemacro_citep\">[Dossou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib27\" title=\"\">2022</a>]</cite>. AfroLM_active_learning is a Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages including Wolof.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">mDeBERTa-v3-base-mnli-xnli</span>: Based on DeBERTaV3-base <cite class=\"ltx_cite ltx_citemacro_citet\">He et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib38\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib37\" title=\"\">2021</a>]</cite>, this model was published by <cite class=\"ltx_cite ltx_citemacro_citet\">Moritz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib56\" title=\"\">2022</a>]</cite> and pre-trained on the CC100 multilingual dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib20\" title=\"\">2020</a>], Wenzek et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib80\" title=\"\">2020</a>]</cite> with 100 different languages including Wolof. This model can perform Natural Language Inference (NLI) on 100 languages.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "small",
                    "all",
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art ASR are selected as baseline models, pre-trained on multiple languages with the possibility of fine-tuning them for a specific language and domain. A description of these models, with additional details, is provided below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Canary Flash</span>: Canary Flash <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> has been pre-trained on several languages (English, German, French, Spanish) and on various tasks such as ASR and translation. Canary Flash is based on Canary <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> that is an encoder-decoder model whose encoder is based on the FastConformer model <cite class=\"ltx_cite ltx_citemacro_citet\">Rekesh et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib70\" title=\"\">2023</a>]</cite> and the decoder on the Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite>. Canary has the distinctive feature of concatenating tokenizers <cite class=\"ltx_cite ltx_citemacro_citet\">Dhawan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib25\" title=\"\">2023</a>]</cite> from different languages using SentencePiece.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/google/sentencepiece\" title=\"\">Google Sentencepiece Tokenizer</a></span></span></span> In this work, English, Spanish, French and Wolof languages are concatenated with Canary&#8217;s Tokenizer. These tokens are then transformed into token embedding before being fed into the Transformer decoder. In addition to the tokens of each language, Canary uses 1,152 special tokens representation. At the time of writing, Canary has three variants: canary-1b, canary-1b-flash, and canary-180m-flash. Canary-1b-flash version is chosen for the experiments because of its multilingual support. The canary-1b-flash model was trained on 85K hours of speech data, including 31K hours of public data (FLEURS <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite>, CoVOST v2 <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib79\" title=\"\">2021b</a>]</cite>) and the remainder on private data. The Mozilla CommonVoice 12 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite> was used as validation data for each language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "small",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models presented in this article are based on <span class=\"ltx_text ltx_font_italic\">Pytorch</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib65\" title=\"\">2019</a>]</cite>. All experiments were conducted on Runpod <span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_href\" href=\"https://www.runpod.io/\" title=\"\">https://www.runpod.io/</a></span></span></span> and Kaggle.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://www.kaggle.com/\" title=\"\">https://www.kaggle.com/</a></span></span></span> P100 GPU (16GB VRAM) was used to finetune BERT-Base and mDeBERTa-v3-base. RTX 4090 GPU for Afro-xlmr-large, AfroLM_active_learning, AfriteVa V2 and Llama-3.2 models. For Zero-shot and Few-shot classification, RTX 2000 Ada GPU (16 GB VRAM) was used. Few-shot classification was performed using <span class=\"ltx_text ltx_font_italic\">SetFit huggingface</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Tunstall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib76\" title=\"\">2022</a>]</cite>. <span class=\"ltx_text ltx_font_italic\">2-shots</span> refers to 2 samples per intent while <span class=\"ltx_text ltx_font_italic\">8-shots</span> represents 8 samples per intent. All pre-trained text models are hosted on <span class=\"ltx_text ltx_font_italic\">huggingface platform</span>, they were fine-tuned using <span class=\"ltx_text ltx_font_italic\">transformers</span> library <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> except for Llama-3.2 which was fine-tuned using <span class=\"ltx_text ltx_font_italic\">torchtune</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[torchtune maintainers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib75\" title=\"\">2024</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "all",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "small",
                    "models",
                    "wolbanking77",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Access to financial services or public transportation services can be facilitated for illiterate people by providing them with voice interfaces in their language of communication. In this paper, an Intent Detection dataset is presented in Wolof language in two modalities which are voice and text. Additionally, dataset description and benchmarks are presented. WolBanking77 is published and shared freely under CC BY 4.0 license along with the code and datasheets <cite class=\"ltx_cite ltx_citemacro_citet\">Gebru et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib33\" title=\"\">2021</a>]</cite> with the aim of inspiring low budget research into low-resource languages.</p>\n\n",
                "matched_terms": [
                    "language",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Wolof language used in Mauritania and Gambia is not substantially different. However, the differences are to be seen rather in linguistic evolutions and in particular with the borrowings from foreign languages present, in particular Arabic and English. These differences may pose a problem for the generalization of models trained on the audio data that do not yet cover these variants of Wolof. This is also present in the accent of the Wolof locutors which highly differs under the effect of the official language (English or Arabic). This has a strong impact on Wolof words pronunciation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The school dropout rate is high in Senegal and this has resulted in a significant number of people who went to school and left without acquiring basic skills in the official language (French). As a result, in our situation, basic skills in French are lacking for more than half of the Senegalese population. One of the solutions has been to resort to literacy in national languages with satisfactory results with the experiments in Pulaar carried out by NGO TOSTAN and the examples of functional literacy carried out by UNESCO. Despite this, a large segment of the population still remains in a situation where basic linguistic skills, either in the official language or in one of the national languages, are acquired very little or not at all.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the models listed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> for zero-shot classification. We use the pre-trained Masked Language Modeling (MLM) models (i.e. encoder models) on the datasets mentioned in this section.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>\n\n",
                "matched_terms": [
                    "models",
                    "f1score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained LASER3 sentence encoder, which vectorizes each sentence and all possible combinations of classification models (MPL and CNN) are compared in the results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>. Note that all classification models were trained from scratch. For all the evaluations, punctuation are removed and each sentence was converted to lowercase.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "best",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can note some limitations in our dataset, such as class imbalance in intents, which may cause the model to favor majority classes at the expense of minority ones. During the ASR evaluation, we noticed spelling errors in rare words, which could be corrected using a language model.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Wolof is a member of the West Atlantic sub-branch of the Niger-Congo language family. A small minority from various other ethnic groups have adopted Wolof as their first language, while more than half of non-native speakers use Wolof as their second or third language. Together, these three groups of Wolof speakers represent about 90% of the population, making Senegal one of the most linguistically unified nations in West Africa <cite class=\"ltx_cite ltx_citemacro_citep\">[Mbodj, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib53\" title=\"\">2014</a>]</cite>. Wolof is also spoken by the majority of the population in Gambia <cite class=\"ltx_cite ltx_citemacro_citep\">[Ndione, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib58\" title=\"\">2013</a>]</cite>. Genetically related languages are Pulaar and Serere, which are all languages of the Niger-Congo phylum, and of the Atlantic branch <cite class=\"ltx_cite ltx_citemacro_citet\">Ndione [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib58\" title=\"\">2013</a>]</cite> see figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S11.F7\" title=\"Figure 7 &#8227; 11 Wolof language &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "small",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Python programming language has been used to clean the data. All source codes are available on <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">Github</a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 5: Word Error Rates (WER) with 4 hours of speech. Training time is reported in minutes.",
        "body": "Model\nParameters\nTraining time\nSteps\nWER\n\n\nPhi-4-multimodal-instruct\n5.6B\n32\n1000\n3.1%\n\n\nDistil-whisper-large-v3.5\n756M\n44\n1000\n4.63%\n\n\n\n\\rowcolorlightgray Canary-1b-flash\n\n1B\n20\n1000\n0.59%",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Parameters</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Training time</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Steps</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\">WER</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Phi-4-multimodal-instruct</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">5.6B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">32</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">1000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\">3.1%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Distil-whisper-large-v3.5</td>\n<td class=\"ltx_td ltx_align_left\">756M</td>\n<td class=\"ltx_td ltx_align_left\">44</td>\n<td class=\"ltx_td ltx_align_left\">1000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\">4.63%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightgray <span class=\"ltx_text ltx_font_bold\">Canary-1b-flash</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">1B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">20</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">1000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.59%</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "756m",
            "minutes",
            "parameters",
            "wer",
            "speech",
            "reported",
            "time",
            "error",
            "phi4multimodalinstruct",
            "distilwhisperlargev35",
            "56b",
            "rates",
            "rowcolorlightgray",
            "canary1bflash",
            "training",
            "hours",
            "word",
            "steps",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "rates",
                    "error",
                    "speech",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Jarrar et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib42\" title=\"\">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class=\"ltx_cite ltx_citemacro_citet\">Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class=\"ltx_cite ltx_citemacro_citet\">Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results obtained with state-of-the-art models in various tasks such as Automatic Speech Recognition (ASR) and Intent Detection (ID). We also provide training and evaluation code to support the reproducibility of experimental benchmarks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F2\" title=\"Figure 2 &#8227; 3.2 Audio recordings and transcriptions &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows top 5 most frequent words in the datastet after excluding stopwords. The dataset contains 272 unique words and 3,204 audio clips. The audio clips are in WAV format, single channel, with a sampling rate of 16 kHz. Sentences have an average duration of 4,815 ms. The intents <span class=\"ltx_text ltx_font_italic\">&#8217;CASH_DEPOSIT&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;FREEZE&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;LATEST_TRANSACTIONS&#8217;</span> contain the longest queries, whereas <span class=\"ltx_text ltx_font_italic\">&#8217;AMOUNT&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;BUS_RESERVATION&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;OPEN_ACCOUNT&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8217;TRANSFER_MONEY&#8217;</span> generally have shorter query durations. The total duration of the dataset is approximately 4 hours and 17 minutes.\n<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:179.4pt;\">\n<span class=\"ltx_table ltx_align_center\" id=\"S3.T3\">\n<span class=\"ltx_caption\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>List of intents.</span>\n</span>\n<span class=\"ltx_para\" id=\"S3.SS2.p2.p1\">\n<span class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">intent</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">domain</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">BALANCE</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">e-banking</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">CASH_DEPOSIT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">FREEZE</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">LATEST_TRANSACTIONS</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAY_BILL</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">OPEN_ACCOUNT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">TRANSFER_MONEY</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">AMOUNT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">BUS_RESERVATION</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">transport</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">TECHNICAL_VISIT</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_border_bb\"/></span>\n</span>\n</span>\n</span></span>\n<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:155.2pt;\">\n<span class=\"ltx_para\" id=\"S3.SS2.p2.p2\"><span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_centering\" height=\"128.04\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 232.39 128.04\" width=\"232.39\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,128.04) matrix(1 0 0 -1 0 0) translate(0.19,0) translate(0,32.6) matrix(0.7 0.0 0.0 0.7 -0.19 -32.6)\"><g class=\"ltx_nestedsvg\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1 0 0 1 0 0) translate(47.62,0) translate(0,57.79)\"><clippath id=\"pgfcp1\"><path d=\"M -47.35 -208.07 L 284.09 -208.07 L 284.09 320.22 L -47.35 320.22\"/></clippath><g clip-path=\"url(#pgfcp1)\"><g color=\"#808080\" fill=\"#808080\" stroke=\"#808080\" stroke-width=\"0.2pt\" style=\"--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;\"><path d=\"M 0 -14.17 L 0 -8.26 M 59.19 -14.17 L 59.19 -8.26 M 118.37 -14.17 L 118.37 -8.26 M 177.56 -14.17 L 177.56 -8.26 M 236.74 -14.17 L 236.74 -8.26\" style=\"fill:none\"/></g><g/></g><path d=\"M -47.35 -11.22 L 281.32 -11.22\" style=\"fill:none\"/><g transform=\"matrix(1.0 0.0 0.0 1.0 281.32 -11.22)\"><path d=\"M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21\" style=\"stroke:none\"/></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 -15.07 -33.99)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :1.95em;--fo_height:0.61em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"21.47\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">kont</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 39.64 -38.47)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :2.49em;--fo_height:0.61em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"27.49\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">xaalis</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 84.11 -51.85)\"><foreignobject height=\"8.72\" overflow=\"visible\" style=\"--fo_width :4.27em;--fo_height:0.61em;--fo_depth :0.18em;\" transform=\"matrix(1 0 0 -1 0 6.78)\" width=\"47.07\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">b&#235;ggoona</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 150.04 -45.11)\"><foreignobject height=\"8.72\" overflow=\"visible\" style=\"--fo_width :3.46em;--fo_height:0.61em;--fo_depth :0.18em;\" transform=\"matrix(1 0 0 -1 0 6.78)\" width=\"38.13\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">j&#235;flante</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 221.3 -32.56)\"><foreignobject height=\"4.17\" overflow=\"visible\" style=\"--fo_width :1.85em;--fo_height:0.38em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 4.17)\" width=\"20.42\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">xam</span></span></span></foreignobject></g><clippath id=\"pgfcp2\"><path d=\"M -47.35 -11.22 L 284.09 -11.22 L 284.09 123.37 L -47.35 123.37 Z\"/></clippath><g clip-path=\"url(#pgfcp2)\"><g color=\"#0000FF\" fill=\"#B3B3FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;\"><path d=\"M -11.07 -11.22 h 22.14 v 123.37 h -22.14 Z M 48.12 -11.22 h 22.14 v 40.73 h -22.14 Z M 107.3 -11.22 h 22.14 v 31.88 h -22.14 Z M 166.49 -11.22 h 22.14 v 14.17 h -22.14 Z M 225.67 -11.22 h 22.14 v 11.22 h -22.14 Z\"/></g><g/></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -5.52 115.66)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"65\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m1\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">65</mn><annotation encoding=\"application/x-tex\">65</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 53.67 33.02)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"37\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">37</mn><annotation encoding=\"application/x-tex\">37</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 112.86 24.17)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 172.04 6.46)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 231.23 3.51)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math></span></span></foreignobject></g></g></g></svg></span>\n</span>\n<span class=\"ltx_figure ltx_align_center\" id=\"S3.F2\">\n<span class=\"ltx_caption\" style=\"font-size:70%;\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Top 5 most frequent words: kont (account), xaalis (money), b&#235;ggoona (I wanted to), j&#235;flante (operation), xam (know).</span>\n</span></span>\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "hours",
                    "minutes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To anonymize participant names, the system generates a user_id to replace the actual names (further details on the ethical collection and processing of personal data are provided in Appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS11\" title=\"8.11 Consent form &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.11</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S10\" title=\"10 Ethics Statement &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Next, a text file containing the sentences to be pronounced is selected. During recording, sentences were presented one at a time, with the option to cancel if a mispronunciation occurred at any stage. In terms of gender diversity, a total of 31 recording sessions were conducted, including 14 males, 14 females and 3 unspecified. The text and audio data were split with 80% allocated for train set and 20% for test set. Prior to data splitting, preprocessing steps were applied, including the removal of punctuation marks and numbers, and conversion of text to lowercase. Corrupted audio files were also removed from the dataset.</p>\n\n",
                "matched_terms": [
                    "time",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "error",
                    "wer",
                    "reported",
                    "phi4multimodalinstruct",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intent Detection (ID) is an NLP task that involves classifying a sentence to identify its underlying intent. Several contributions have been made in the field. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite> published MINDS-14, the first training and evaluation dataset for the ID task using spoken data. It includes 14 intents derived from e-banking domain, with spoken examples in 14 different languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Si et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib71\" title=\"\">2023a</a>]</cite> introduced SpokenWOZ, a large-scale speech-text dataset for spoken Task-Oriented Dialogue in 8 domains and 249 hours of audio. <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> published BANKING77, a dataset comprising 13,083 examples across 77 intents in banking domain.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Canary Flash</span>: Canary Flash <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> has been pre-trained on several languages (English, German, French, Spanish) and on various tasks such as ASR and translation. Canary Flash is based on Canary <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> that is an encoder-decoder model whose encoder is based on the FastConformer model <cite class=\"ltx_cite ltx_citemacro_citet\">Rekesh et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib70\" title=\"\">2023</a>]</cite> and the decoder on the Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite>. Canary has the distinctive feature of concatenating tokenizers <cite class=\"ltx_cite ltx_citemacro_citet\">Dhawan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib25\" title=\"\">2023</a>]</cite> from different languages using SentencePiece.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/google/sentencepiece\" title=\"\">Google Sentencepiece Tokenizer</a></span></span></span> In this work, English, Spanish, French and Wolof languages are concatenated with Canary&#8217;s Tokenizer. These tokens are then transformed into token embedding before being fed into the Transformer decoder. In addition to the tokens of each language, Canary uses 1,152 special tokens representation. At the time of writing, Canary has three variants: canary-1b, canary-1b-flash, and canary-180m-flash. Canary-1b-flash version is chosen for the experiments because of its multilingual support. The canary-1b-flash model was trained on 85K hours of speech data, including 31K hours of public data (FLEURS <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite>, CoVOST v2 <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib79\" title=\"\">2021b</a>]</cite>) and the remainder on private data. The Mozilla CommonVoice 12 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite> was used as validation data for each language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "canary1bflash",
                    "time",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "phi4multimodalinstruct",
                    "model",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "distilwhisperlargev35",
                    "model",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "phi4multimodalinstruct",
                    "distilwhisperlargev35",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T8\" title=\"Table 8 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the hyperparameters used to train Phi-4-multimodal-instruct, Distil-whisper-large-v3.5 and Canary-1b-flash. Hyperparameters has been chosen by following HuggingFace <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> and NVIDIA Nemo <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> documentations. AdamW has been used as a default optimizer.</p>\n\n",
                "matched_terms": [
                    "phi4multimodalinstruct",
                    "distilwhisperlargev35",
                    "canary1bflash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "time",
                    "model",
                    "training",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parameters",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the audio version, there are 263 instances covering 10 intents in total. 4 hours and 17 minutes of audios from spoken sentences. For the text version, there are 9,791 instances covering 77 intents in total.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "minutes"
                ]
            }
        ]
    },
    "S8.T6": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 6: Age distribution of the speakers",
        "body": "Stat\nAge\n\n\n\n\nmean\n26\n\n\nstd\n4\n\n\nmin\n22\n\n\n25%\n23\n\n\n50%\n25\n\n\n75%\n27\n\n\nmax\n36",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Stat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Age</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">mean</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">std</th>\n<td class=\"ltx_td ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">min</th>\n<td class=\"ltx_td ltx_align_center\">22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">25%</th>\n<td class=\"ltx_td ltx_align_center\">23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">50%</th>\n<td class=\"ltx_td ltx_align_center\">25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">75%</th>\n<td class=\"ltx_td ltx_align_center\">27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">max</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speakers",
            "age",
            "stat",
            "min",
            "distribution",
            "max",
            "std",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The age distribution of the participants are presented in table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T6\" title=\"Table 6 &#8227; 8.3 Speakers age distribution &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">It would also be useful to know the intersection of age and literacy in the population. If older speakers, for example, were proportionally less literate in the language, this might argue for strengthening the representation of older voices in the dataset, given that this development is aimed at improving accessibility through voice-interactive technology.</span>\n</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "age"
                ]
            }
        ]
    },
    "S8.T7": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 7: Hyperparameters for NLP models. mDeBERTa-v3* and AfritevaV2 have same hyperparameters.",
        "body": "Model\nbert-base-uncased\nAfroXLMR\nmDeBERTa-v3*\nAfroLM\nLlama3.2\n\n\nLearning Rate\n2e-05\n2e-05\n2e-05\n2e-05\n3e-4\n\n\nTrain Batch Size\n32\n4\n8\n16\n4\n\n\nEval Batch Size\n32\n8\n8\n8\n4\n\n\nWarmup ratio\n0.1\n0.1\n0.1\n0.1\n-\n\n\n# epochs\n20\n20\n20\n20\n20",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">bert-base-uncased</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AfroXLMR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">mDeBERTa-v3*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AfroLM</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">Llama3.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2e-05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2e-05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2e-05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2e-05</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">3e-4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Train Batch Size</td>\n<td class=\"ltx_td ltx_align_center\">32</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Eval Batch Size</td>\n<td class=\"ltx_td ltx_align_center\">32</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Warmup ratio</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"># epochs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">20</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "train",
            "warmup",
            "bertbaseuncased",
            "afritevav2",
            "rate",
            "3e4",
            "hyperparameters",
            "llama32",
            "same",
            "batch",
            "2e05",
            "learning",
            "mdebertav3",
            "afroxlmr",
            "eval",
            "afrolm",
            "ratio",
            "models",
            "size",
            "nlp",
            "model",
            "epochs",
            "have"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">NLP models hyperparameters are reported on table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T7\" title=\"Table 7 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, AdamW_torch_fused is the default optimizer for NLP models except for Llama3.2 that is AdamW.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "have",
                    "rate",
                    "nlp",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, to build such a virtual assistant, it is necessary to understand human requests in order to provide appropriate responses. In Natural Language Processing (NLP), the field that deals with the understanding of human language, is Natural Language Understanding (NLU). In our case, we focus on both text and speech given that our objective is to work with African languages rooted in oral traditions. Before doing an NLU task, the first step is to understand the information contained in speech using Spoken Language Understanding (SLU). According to the World Bank <cite class=\"ltx_cite ltx_citemacro_citet\">Bank [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib11\" title=\"\">2022</a>]</cite>, the rate of adult illiterate (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS4\" title=\"8.4 Illiteracy in Senegal &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.4</span></a>) population in Senegal is 42%, which means that to facilitate access to a virtual assistant for these populations, it is essential to offer voice processing solutions. To determine the intent of a user request, <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite> frame the problem as an Intent Detection (ID) classification task, performed either directly from text or from audio transcriptions.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "nlp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models based on neural networks has emerged in recent years in the field of ID <cite class=\"ltx_cite ltx_citemacro_citep\">[Gerz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>, Krishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib45\" title=\"\">2021</a>, Si et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib72\" title=\"\">2023b</a>]</cite>. However, very few datasets in African languages <cite class=\"ltx_cite ltx_citemacro_citet\">Alexis et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib5\" title=\"\">2022</a>], Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>], Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>], Mwongela et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib57\" title=\"\">2023</a>], Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> have been explored. Even less in the field of e-banking for Sub-Saharan languages. Most existing datasets are in English <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite>, as their development requires significant financial and human resources. In this paper, we present an intent classification dataset in Wolof, an African and Sub-Saharan language, based on the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> which originally consists of 13,083 customer service queries labeled with 77 intents. In addition, our work also draws on the MINDS-14 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite>, which contains 14 intents derived from a commercial e-banking system and includes an audio component.</p>\n\n",
                "matched_terms": [
                    "models",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With regard to the Wolof language, datasets have been published in the literature, particularly in the field of NLP and Automatic Speech Recognition (ASR). For example, some datasets include Wolof texts such as afriqa dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ogundepo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib60\" title=\"\">2023</a>]</cite> which deals with the question answering (QA) task, masakhaner versions 1 and 2 <cite class=\"ltx_cite ltx_citemacro_citet\">Adelani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib2\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib3\" title=\"\">2022b</a>]</cite> which targets the Name Entity Recognition (NER) task, UD_Wolof-WTB <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/UniversalDependencies/UD_Wolof-WTB\" title=\"\">https://github.com/UniversalDependencies/UD_Wolof-WTB</a></span></span></span> or even MasakhaPOS <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/masakhane-io/lacuna_pos_ner\" title=\"\">https://github.com/masakhane-io/lacuna_pos_ner</a></span></span></span> which addresses the part-of-speech tagging (POS) task. In addition, several datasets for the machine translation (MT) task such as OPUS, <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> FLORES 200 <cite class=\"ltx_cite ltx_citemacro_citet\">team et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib74\" title=\"\">2022</a>]</cite>, NTREX-128 <cite class=\"ltx_cite ltx_citemacro_citet\">Federmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib28\" title=\"\">2022</a>]</cite> and MAFAND-MT <cite class=\"ltx_cite ltx_citemacro_citep\">[Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib1\" title=\"\">2022a</a>]</cite>. In the field of ASR, several datasets containing Wolof have also been published, including ALFFA <cite class=\"ltx_cite ltx_citemacro_citet\">Gauthier et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib31\" title=\"\">2016</a>]</cite>, fleurs <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite> and more recently KALLAAMA <cite class=\"ltx_cite ltx_citemacro_citet\">Gauthier et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib32\" title=\"\">2024</a>]</cite>. The cited datasets cover fairly general domains, such as news and religion. However, to date, there is only one text dataset dedicated to the banking sector named INJONGO <cite class=\"ltx_cite ltx_citemacro_citet\">Yu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib83\" title=\"\">2025</a>]</cite> (with other domains such as home, kitchen and dining, travel and utility). This dataset includes slot-filling and intent classification tasks for 16 African languages including Wolof.</p>\n\n",
                "matched_terms": [
                    "nlp",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Jarrar et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib42\" title=\"\">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class=\"ltx_cite ltx_citemacro_citet\">Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class=\"ltx_cite ltx_citemacro_citet\">Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S3.F2\" title=\"Figure 2 &#8227; 3.2 Audio recordings and transcriptions &#8227; 3 Data collection &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows top 5 most frequent words in the datastet after excluding stopwords. The dataset contains 272 unique words and 3,204 audio clips. The audio clips are in WAV format, single channel, with a sampling rate of 16 kHz. Sentences have an average duration of 4,815 ms. The intents <span class=\"ltx_text ltx_font_italic\">&#8217;CASH_DEPOSIT&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;FREEZE&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;LATEST_TRANSACTIONS&#8217;</span> contain the longest queries, whereas <span class=\"ltx_text ltx_font_italic\">&#8217;AMOUNT&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;BUS_RESERVATION&#8217;</span>, <span class=\"ltx_text ltx_font_italic\">&#8217;OPEN_ACCOUNT&#8217;</span> and <span class=\"ltx_text ltx_font_italic\">&#8217;TRANSFER_MONEY&#8217;</span> generally have shorter query durations. The total duration of the dataset is approximately 4 hours and 17 minutes.\n<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:179.4pt;\">\n<span class=\"ltx_table ltx_align_center\" id=\"S3.T3\">\n<span class=\"ltx_caption\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>List of intents.</span>\n</span>\n<span class=\"ltx_para\" id=\"S3.SS2.p2.p1\">\n<span class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">intent</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">domain</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">BALANCE</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">e-banking</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">CASH_DEPOSIT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">FREEZE</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">LATEST_TRANSACTIONS</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAY_BILL</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">OPEN_ACCOUNT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">TRANSFER_MONEY</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">AMOUNT</span></span>\n<span class=\"ltx_td ltx_nopad_r\"/></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">BUS_RESERVATION</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">transport</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">TECHNICAL_VISIT</span></span>\n<span class=\"ltx_td ltx_nopad_r ltx_border_bb\"/></span>\n</span>\n</span>\n</span></span>\n<span class=\"ltx_inline-logical-block ltx_minipage ltx_align_top\" style=\"width:155.2pt;\">\n<span class=\"ltx_para\" id=\"S3.SS2.p2.p2\"><span class=\"ltx_inline-block\"><svg class=\"ltx_picture ltx_centering\" height=\"128.04\" id=\"S3.SS2.p2.pic1\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 232.39 128.04\" width=\"232.39\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,128.04) matrix(1 0 0 -1 0 0) translate(0.19,0) translate(0,32.6) matrix(0.7 0.0 0.0 0.7 -0.19 -32.6)\"><g class=\"ltx_nestedsvg\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1 0 0 1 0 0) translate(47.62,0) translate(0,57.79)\"><clippath id=\"pgfcp1\"><path d=\"M -47.35 -208.07 L 284.09 -208.07 L 284.09 320.22 L -47.35 320.22\"/></clippath><g clip-path=\"url(#pgfcp1)\"><g color=\"#808080\" fill=\"#808080\" stroke=\"#808080\" stroke-width=\"0.2pt\" style=\"--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;\"><path d=\"M 0 -14.17 L 0 -8.26 M 59.19 -14.17 L 59.19 -8.26 M 118.37 -14.17 L 118.37 -8.26 M 177.56 -14.17 L 177.56 -8.26 M 236.74 -14.17 L 236.74 -8.26\" style=\"fill:none\"/></g><g/></g><path d=\"M -47.35 -11.22 L 281.32 -11.22\" style=\"fill:none\"/><g transform=\"matrix(1.0 0.0 0.0 1.0 281.32 -11.22)\"><path d=\"M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21\" style=\"stroke:none\"/></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 -15.07 -33.99)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :1.95em;--fo_height:0.61em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"21.47\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">kont</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 39.64 -38.47)\"><foreignobject height=\"6.73\" overflow=\"visible\" style=\"--fo_width :2.49em;--fo_height:0.61em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.73)\" width=\"27.49\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">xaalis</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 84.11 -51.85)\"><foreignobject height=\"8.72\" overflow=\"visible\" style=\"--fo_width :4.27em;--fo_height:0.61em;--fo_depth :0.18em;\" transform=\"matrix(1 0 0 -1 0 6.78)\" width=\"47.07\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">b&#235;ggoona</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 150.04 -45.11)\"><foreignobject height=\"8.72\" overflow=\"visible\" style=\"--fo_width :3.46em;--fo_height:0.61em;--fo_depth :0.18em;\" transform=\"matrix(1 0 0 -1 0 6.78)\" width=\"38.13\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">j&#235;flante</span></span></span></foreignobject></g><g fill=\"#000000\" stroke=\"#000000\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(0.7071 0.7071 -0.7071 0.7071 221.3 -32.56)\"><foreignobject height=\"4.17\" overflow=\"visible\" style=\"--fo_width :1.85em;--fo_height:0.38em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 4.17)\" width=\"20.42\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:70%;\">xam</span></span></span></foreignobject></g><clippath id=\"pgfcp2\"><path d=\"M -47.35 -11.22 L 284.09 -11.22 L 284.09 123.37 L -47.35 123.37 Z\"/></clippath><g clip-path=\"url(#pgfcp2)\"><g color=\"#0000FF\" fill=\"#B3B3FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;\"><path d=\"M -11.07 -11.22 h 22.14 v 123.37 h -22.14 Z M 48.12 -11.22 h 22.14 v 40.73 h -22.14 Z M 107.3 -11.22 h 22.14 v 31.88 h -22.14 Z M 166.49 -11.22 h 22.14 v 14.17 h -22.14 Z M 225.67 -11.22 h 22.14 v 11.22 h -22.14 Z\"/></g><g/></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 -5.52 115.66)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"65\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m1\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">65</mn><annotation encoding=\"application/x-tex\">65</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 53.67 33.02)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"37\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">37</mn><annotation encoding=\"application/x-tex\">37</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 112.86 24.17)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 172.04 6.46)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math></span></span></foreignobject></g><g color=\"#0000FF\" fill=\"#0000FF\" stroke=\"#0000FF\" style=\"--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;\" transform=\"matrix(1.0 0.0 0.0 1.0 231.23 3.51)\"><foreignobject height=\"6.24\" overflow=\"visible\" style=\"--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 6.24)\" width=\"11.03\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.pic1.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.700em\">27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math></span></span></foreignobject></g></g></g></svg></span>\n</span>\n<span class=\"ltx_figure ltx_align_center\" id=\"S3.F2\">\n<span class=\"ltx_caption\" style=\"font-size:70%;\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Top 5 most frequent words: kont (account), xaalis (money), b&#235;ggoona (I wanted to), j&#235;flante (operation), xam (know).</span>\n</span></span>\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "rate",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intent Detection (ID) is an NLP task that involves classifying a sentence to identify its underlying intent. Several contributions have been made in the field. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite> published MINDS-14, the first training and evaluation dataset for the ID task using spoken data. It includes 14 intents derived from e-banking domain, with spoken examples in 14 different languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Si et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib71\" title=\"\">2023a</a>]</cite> introduced SpokenWOZ, a large-scale speech-text dataset for spoken Task-Oriented Dialogue in 8 domains and 249 hours of audio. <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> published BANKING77, a dataset comprising 13,083 examples across 77 intents in banking domain.</p>\n\n",
                "matched_terms": [
                    "nlp",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "same",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "nlp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "have",
                    "learning",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models presented in this article are based on <span class=\"ltx_text ltx_font_italic\">Pytorch</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib65\" title=\"\">2019</a>]</cite>. All experiments were conducted on Runpod <span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_href\" href=\"https://www.runpod.io/\" title=\"\">https://www.runpod.io/</a></span></span></span> and Kaggle.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://www.kaggle.com/\" title=\"\">https://www.kaggle.com/</a></span></span></span> P100 GPU (16GB VRAM) was used to finetune BERT-Base and mDeBERTa-v3-base. RTX 4090 GPU for Afro-xlmr-large, AfroLM_active_learning, AfriteVa V2 and Llama-3.2 models. For Zero-shot and Few-shot classification, RTX 2000 Ada GPU (16 GB VRAM) was used. Few-shot classification was performed using <span class=\"ltx_text ltx_font_italic\">SetFit huggingface</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Tunstall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib76\" title=\"\">2022</a>]</cite>. <span class=\"ltx_text ltx_font_italic\">2-shots</span> refers to 2 samples per intent while <span class=\"ltx_text ltx_font_italic\">8-shots</span> represents 8 samples per intent. All pre-trained text models are hosted on <span class=\"ltx_text ltx_font_italic\">huggingface platform</span>, they were fine-tuned using <span class=\"ltx_text ltx_font_italic\">transformers</span> library <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> except for Llama-3.2 which was fine-tuned using <span class=\"ltx_text ltx_font_italic\">torchtune</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[torchtune maintainers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib75\" title=\"\">2024</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "llama32"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "epochs",
                    "llama32"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "afrolm",
                    "models",
                    "model",
                    "afroxlmr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "afrolm",
                    "models",
                    "afroxlmr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Literacy, considering the various languages present, but also the formal and informal systems, is progressing in Senegal even if it remains below 60% for adults according to statistics from UNESCO or the World Bank <cite class=\"ltx_cite ltx_citemacro_citep\">[macrotrends, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib50\" title=\"\">2022</a>]</cite>. According to these same sources, we note that among young people aged 15 to 24, the rate is higher and is around 78%, a sign of this progression. It is clear that there is a disparity on two levels, firstly, at the gender level, in fact adult women are far more victims of illiteracy than men of the same age group and also, urban areas suffer less than rural areas (UNESCO) <cite class=\"ltx_cite ltx_citemacro_citep\">[countryeconomy, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib23\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T8\" title=\"Table 8 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the hyperparameters used to train Phi-4-multimodal-instruct, Distil-whisper-large-v3.5 and Canary-1b-flash. Hyperparameters has been chosen by following HuggingFace <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> and NVIDIA Nemo <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> documentations. AdamW has been used as a default optimizer.</p>\n\n",
                "matched_terms": [
                    "train",
                    "hyperparameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prompt format was used to train the Llama3.2 model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "train",
                    "llama32"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "same",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "rate",
                    "models",
                    "size",
                    "hyperparameters",
                    "batch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset contain regions of origin showing that different voices and accents have been recorded to cover as more ethnic as possible and to make models robust to different accents.</p>\n\n",
                "matched_terms": [
                    "models",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset can be used for anything related to intent classification as well as to train speech and text models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "train"
                ]
            }
        ]
    },
    "S8.T8": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 8: Hyperparameters for ASR models.",
        "body": "Model\nCanary-1b-flash\nDistil-whisper-large-v3.5\nPhi-4\n\n\nLearning Rate\n3e-4\n1e-5\n1e-4\n\n\nTrain Batch Size\n-\n8\n16\n\n\nEval Batch Size\n8\n8\n-\n\n\nGradient Accumulation Steps\n-\n1\n1\n\n\nLr Scheduler Warmup Steps\n2500\n500\n-\n\n\nOptimizer\nAdamW\nAdamW\nAdamW_torch\n\n\n# steps\n1000\n1000\n1000",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Canary-1b-flash</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Distil-whisper-large-v3.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">Phi-4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3e-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1e-5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Train Batch Size</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Eval Batch Size</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Gradient Accumulation Steps</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Lr Scheduler Warmup Steps</td>\n<td class=\"ltx_td ltx_align_center\">2500</td>\n<td class=\"ltx_td ltx_align_center\">500</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Optimizer</td>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AdamW_torch</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"># steps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">1000</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "train",
            "warmup",
            "optimizer",
            "scheduler",
            "rate",
            "asr",
            "3e4",
            "hyperparameters",
            "adamwtorch",
            "batch",
            "distilwhisperlargev35",
            "accumulation",
            "learning",
            "gradient",
            "1e5",
            "canary1bflash",
            "eval",
            "adamw",
            "models",
            "size",
            "steps",
            "phi4",
            "1e4",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T8\" title=\"Table 8 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the hyperparameters used to train Phi-4-multimodal-instruct, Distil-whisper-large-v3.5 and Canary-1b-flash. Hyperparameters has been chosen by following HuggingFace <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> and NVIDIA Nemo <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> documentations. AdamW has been used as a default optimizer.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "asr",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Jarrar et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib42\" title=\"\">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class=\"ltx_cite ltx_citemacro_citet\">Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class=\"ltx_cite ltx_citemacro_citet\">Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results obtained with state-of-the-art models in various tasks such as Automatic Speech Recognition (ASR) and Intent Detection (ID). We also provide training and evaluation code to support the reproducibility of experimental benchmarks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To anonymize participant names, the system generates a user_id to replace the actual names (further details on the ethical collection and processing of personal data are provided in Appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS11\" title=\"8.11 Consent form &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.11</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S10\" title=\"10 Ethics Statement &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Next, a text file containing the sentences to be pronounced is selected. During recording, sentences were presented one at a time, with the option to cancel if a mispronunciation occurred at any stage. In terms of gender diversity, a total of 31 recording sessions were conducted, including 14 males, 14 females and 3 unspecified. The text and audio data were split with 80% allocated for train set and 20% for test set. Prior to data splitting, preprocessing steps were applied, including the removal of punctuation marks and numbers, and conversion of text to lowercase. Corrupted audio files were also removed from the dataset.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "asr",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">State-of-the-art ASR are selected as baseline models, pre-trained on multiple languages with the possibility of fine-tuning them for a specific language and domain. A description of these models, with additional details, is provided below:</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Canary Flash</span>: Canary Flash <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> has been pre-trained on several languages (English, German, French, Spanish) and on various tasks such as ASR and translation. Canary Flash is based on Canary <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> that is an encoder-decoder model whose encoder is based on the FastConformer model <cite class=\"ltx_cite ltx_citemacro_citet\">Rekesh et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib70\" title=\"\">2023</a>]</cite> and the decoder on the Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite>. Canary has the distinctive feature of concatenating tokenizers <cite class=\"ltx_cite ltx_citemacro_citet\">Dhawan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib25\" title=\"\">2023</a>]</cite> from different languages using SentencePiece.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/google/sentencepiece\" title=\"\">Google Sentencepiece Tokenizer</a></span></span></span> In this work, English, Spanish, French and Wolof languages are concatenated with Canary&#8217;s Tokenizer. These tokens are then transformed into token embedding before being fed into the Transformer decoder. In addition to the tokens of each language, Canary uses 1,152 special tokens representation. At the time of writing, Canary has three variants: canary-1b, canary-1b-flash, and canary-180m-flash. Canary-1b-flash version is chosen for the experiments because of its multilingual support. The canary-1b-flash model was trained on 85K hours of speech data, including 31K hours of public data (FLEURS <cite class=\"ltx_cite ltx_citemacro_citet\">Conneau et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib21\" title=\"\">2023</a>]</cite>, CoVOST v2 <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib79\" title=\"\">2021b</a>]</cite>) and the remainder on private data. The Mozilla CommonVoice 12 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite> was used as validation data for each language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "canary1bflash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "distilwhisperlargev35",
                    "model",
                    "learning",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "distilwhisperlargev35",
                    "asr",
                    "models",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "steps",
                    "canary1bflash",
                    "distilwhisperlargev35"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">NLP models hyperparameters are reported on table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T7\" title=\"Table 7 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, AdamW_torch_fused is the default optimizer for NLP models except for Llama3.2 that is AdamW.</p>\n\n",
                "matched_terms": [
                    "models",
                    "adamw",
                    "optimizer",
                    "hyperparameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prompt format was used to train the Llama3.2 model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "rate",
                    "models",
                    "size",
                    "hyperparameters",
                    "batch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can note some limitations in our dataset, such as class imbalance in intents, which may cause the model to favor majority classes at the expense of minority ones. During the ASR evaluation, we noticed spelling errors in rare words, which could be corrected using a language model.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset can be used for anything related to intent classification as well as to train speech and text models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "train"
                ]
            }
        ]
    },
    "S8.T9": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 9: Best hyperparameters for LASER3+CNN.",
        "body": "Model\nLASER3+CNN\n\n\nLearning Rate\n0.013364046097018405\n\n\nLast size of non classification layer\n128\n\n\nBatch Size\n2",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">LASER3+CNN</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Learning Rate</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.013364046097018405</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Last size of non classification layer</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Batch Size</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">2</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "layer",
            "learning",
            "best",
            "rate",
            "size",
            "hyperparameters",
            "last",
            "laser3cnn",
            "batch",
            "classification",
            "non",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, to build such a virtual assistant, it is necessary to understand human requests in order to provide appropriate responses. In Natural Language Processing (NLP), the field that deals with the understanding of human language, is Natural Language Understanding (NLU). In our case, we focus on both text and speech given that our objective is to work with African languages rooted in oral traditions. Before doing an NLU task, the first step is to understand the information contained in speech using Spoken Language Understanding (SLU). According to the World Bank <cite class=\"ltx_cite ltx_citemacro_citet\">Bank [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib11\" title=\"\">2022</a>]</cite>, the rate of adult illiterate (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS4\" title=\"8.4 Illiteracy in Senegal &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.4</span></a>) population in Senegal is 42%, which means that to facilitate access to a virtual assistant for these populations, it is essential to offer voice processing solutions. To determine the intent of a user request, <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite> frame the problem as an Intent Detection (ID) classification task, performed either directly from text or from audio transcriptions.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several datasets, both monolingual and multilingual, for NLU conversational question-answering system, have been published in the last decade. For instance, The Chatbot Corpus dataset composed of questions and answers and also The StackExchange Corpus based on the StackExchange platform both available on GitHub <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/sebischair/NLU-Evaluation-Corpora\" title=\"\">https://github.com/sebischair/NLU-Evaluation-Corpora</a></span></span></span> and evaluated by <cite class=\"ltx_cite ltx_citemacro_citep\">[Braun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib13\" title=\"\">2017</a>]</cite>. More recently, MINDS-14 <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite> a multilingual dataset in the field of e-banking has been made publicly available. When considering multilingual intent classification benchmarking, we can mention for instance the MultiATIS++ dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Xu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib82\" title=\"\">2020</a>]</cite>, which pertains to the aviation field, and has been expanded upon the original English ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> to six additional languages. More recently, the XTREME-S benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Alexis et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib5\" title=\"\">2022</a>]</cite> aims at evaluating several tasks such as speech recognition, translation, classification and retrieval. XTREME-S brings together several datasets covering 102 various languages, including 20 languages of Sub-Saharan Africa.</p>\n\n",
                "matched_terms": [
                    "last",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class=\"ltx_cite ltx_citemacro_citet\">Jarrar et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib42\" title=\"\">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class=\"ltx_cite ltx_citemacro_citet\">Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Price [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib67\" title=\"\">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class=\"ltx_cite ltx_citemacro_citet\">Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "model",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "model",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "model",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            }
        ]
    },
    "S8.T10": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 10: ML baselines models precision, recall and f1-score results on WolBanking77. Best models are highlighted in lightgray.",
        "body": "5k samples\n\n\nAll samples\n\n\n\nModel\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\n\n\nBoW+KNN\n0.48\n0.39\n0.39\n0.37\n0.31\n0.31\n\n\nBoW+SVM\n0.70\n0.69\n0.68\n0.49\n0.48\n0.48\n\n\n\n\\rowcolorlightgray BoW+LR\n0.70\n0.69\n0.68\n0.54\n0.53\n0.53\n\n\nBoW+NB\n0.54\n0.51\n0.50\n0.28\n0.26\n0.26",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">5k samples</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">All samples</span></th>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F1</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BoW+KNN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BoW+SVM</th>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.49</td>\n<td class=\"ltx_td ltx_align_center\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">0.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightgray BoW+LR</th>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">0.53</td>\n<td class=\"ltx_td ltx_align_center\">0.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">BoW+NB</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.26</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "highlighted",
            "samples",
            "bowlr",
            "lightgray",
            "f1score",
            "rowcolorlightgray",
            "all",
            "bowknn",
            "best",
            "models",
            "bownb",
            "bowsvm",
            "results",
            "wolbanking77",
            "model",
            "precision",
            "recall"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "models",
                    "wolbanking77",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class=\"ltx_cite ltx_citemacro_citep\">[Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class=\"ltx_cite ltx_citemacro_citep\">[Mart&#237;nez and Short, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib51\" title=\"\">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class=\"ltx_cite ltx_citemacro_citep\">[Anthony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib8\" title=\"\">2024</a>, Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results obtained with state-of-the-art models in various tasks such as Automatic Speech Recognition (ASR) and Intent Detection (ID). We also provide training and evaluation code to support the reproducibility of experimental benchmarks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "models",
                    "results",
                    "wolbanking77",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "all",
                    "models",
                    "results",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "model",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models presented in this article are based on <span class=\"ltx_text ltx_font_italic\">Pytorch</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib65\" title=\"\">2019</a>]</cite>. All experiments were conducted on Runpod <span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_href\" href=\"https://www.runpod.io/\" title=\"\">https://www.runpod.io/</a></span></span></span> and Kaggle.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://www.kaggle.com/\" title=\"\">https://www.kaggle.com/</a></span></span></span> P100 GPU (16GB VRAM) was used to finetune BERT-Base and mDeBERTa-v3-base. RTX 4090 GPU for Afro-xlmr-large, AfroLM_active_learning, AfriteVa V2 and Llama-3.2 models. For Zero-shot and Few-shot classification, RTX 2000 Ada GPU (16 GB VRAM) was used. Few-shot classification was performed using <span class=\"ltx_text ltx_font_italic\">SetFit huggingface</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Tunstall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib76\" title=\"\">2022</a>]</cite>. <span class=\"ltx_text ltx_font_italic\">2-shots</span> refers to 2 samples per intent while <span class=\"ltx_text ltx_font_italic\">8-shots</span> represents 8 samples per intent. All pre-trained text models are hosted on <span class=\"ltx_text ltx_font_italic\">huggingface platform</span>, they were fine-tuned using <span class=\"ltx_text ltx_font_italic\">transformers</span> library <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> except for Llama-3.2 which was fine-tuned using <span class=\"ltx_text ltx_font_italic\">torchtune</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[torchtune maintainers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib75\" title=\"\">2024</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "all",
                    "models",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The school dropout rate is high in Senegal and this has resulted in a significant number of people who went to school and left without acquiring basic skills in the official language (French). As a result, in our situation, basic skills in French are lacking for more than half of the Senegalese population. One of the solutions has been to resort to literacy in national languages with satisfactory results with the experiments in Pulaar carried out by NGO TOSTAN and the examples of functional literacy carried out by UNESCO. Despite this, a large segment of the population still remains in a situation where basic linguistic skills, either in the official language or in one of the national languages, are acquired very little or not at all.</p>\n\n",
                "matched_terms": [
                    "results",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "models",
                    "f1score",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained LASER3 sentence encoder, which vectorizes each sentence and all possible combinations of classification models (MPL and CNN) are compared in the results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>. Note that all classification models were trained from scratch. For all the evaluations, punctuation are removed and each sentence was converted to lowercase.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "best",
                    "models",
                    "results",
                    "model"
                ]
            }
        ]
    },
    "S8.T11": {
        "source_file": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
        "caption": "Table 11: LASER3 with classification models precision, recall and f1-score results on WolBanking77. LASER3+CNN (tuning) denotes LASER3+CNN with CNN hyperparameter tuning. Best models are highlighted in lightgray.",
        "body": "5k samples\n\n\nAll samples\n\n\n\nModel\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\n\n\n\n\n\n\\rowcolorlightgray LASER3+MLP\n0.56\n0.56\n0.55\n0.43\n0.42\n0.42\n\n\nLASER3+CNN\n0.01\n0.05\n0.01\n0.01\n0.05\n0.01\n\n\nLASER3+CNN (tuning)\n0.53\n0.50\n0.49\n0.33\n0.32\n0.32",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">5k samples</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">All samples</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>lightgray LASER3+MLP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LASER3+CNN</th>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">LASER3+CNN (tuning)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.32</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "f1score",
            "tuning",
            "classification",
            "lightgray",
            "samples",
            "recall",
            "denotes",
            "cnn",
            "best",
            "results",
            "laser3",
            "laser3cnn",
            "laser3mlp",
            "rowcolorlightgray",
            "wolbanking77",
            "precision",
            "highlighted",
            "hyperparameter",
            "all",
            "models",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset&#8217;s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/abdoukarim/wolbanking77\" title=\"\">wolbanking77</a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class=\"ltx_cite ltx_citemacro_citep\">[Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class=\"ltx_cite ltx_citemacro_citep\">[Mart&#237;nez and Short, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib51\" title=\"\">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class=\"ltx_cite ltx_citemacro_citep\">[Anthony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib8\" title=\"\">2024</a>, Ouattara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib63\" title=\"\">2025</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models based on neural networks has emerged in recent years in the field of ID <cite class=\"ltx_cite ltx_citemacro_citep\">[Gerz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>, Krishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib45\" title=\"\">2021</a>, Si et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib72\" title=\"\">2023b</a>]</cite>. However, very few datasets in African languages <cite class=\"ltx_cite ltx_citemacro_citet\">Alexis et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib5\" title=\"\">2022</a>], Mastel et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib52\" title=\"\">2023</a>], Moghe et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib55\" title=\"\">2023</a>], Mwongela et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib57\" title=\"\">2023</a>], Kasule et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib44\" title=\"\">2024</a>]</cite> have been explored. Even less in the field of e-banking for Sub-Saharan languages. Most existing datasets are in English <cite class=\"ltx_cite ltx_citemacro_citet\">Coucke et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib22\" title=\"\">2018</a>]</cite>, as their development requires significant financial and human resources. In this paper, we present an intent classification dataset in Wolof, an African and Sub-Saharan language, based on the Banking77 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Casanueva et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib15\" title=\"\">2020</a>]</cite> which originally consists of 13,083 customer service queries labeled with 77 intents. In addition, our work also draws on the MINDS-14 dataset <cite class=\"ltx_cite ltx_citemacro_citet\">Gerz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib34\" title=\"\">2021</a>]</cite>, which contains 14 intents derived from a commercial e-banking system and includes an audio component.</p>\n\n",
                "matched_terms": [
                    "models",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results obtained with state-of-the-art models in various tasks such as Automatic Speech Recognition (ASR) and Intent Detection (ID). We also provide training and evaluation code to support the reproducibility of experimental benchmarks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class=\"ltx_cite ltx_citemacro_citet\">Chan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib16\" title=\"\">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class=\"ltx_cite ltx_citemacro_citet\">Graves et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib35\" title=\"\">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.\nMore recently, the NVIDIA team <cite class=\"ltx_cite ltx_citemacro_citet\">&#379;elasko et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib84\" title=\"\">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class=\"ltx_cite ltx_citemacro_citet\">Puvvada et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib68\" title=\"\">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "model",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class=\"ltx_cite ltx_citemacro_citet\">Microsoft et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib54\" title=\"\">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface&#8217;s OpenASR leaderboard <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\" title=\"\">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS9\" title=\"8.9 Reference scores from classic Machine Learning Techniques &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS10\" title=\"8.10 Pretrained Sentence Encoder &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class=\"ltx_text ltx_font_bold\">Llama-3.2-3B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class=\"ltx_text ltx_font_bold\">Llama-3.2-1B-Instruct</span> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class=\"ltx_text ltx_font_italic\">F1</span>, <span class=\"ltx_text ltx_font_italic\">precision</span> and <span class=\"ltx_text ltx_font_italic\">recall</span> metrics are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "cnn",
                    "models",
                    "results",
                    "wolbanking77",
                    "classification",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS6\" title=\"8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "hyperparameter",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next sections present and detail the pre-trained models used in the ID task. Experiments for zero and few shot classification are also discussed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS7\" title=\"8.7 Zero-shot classification &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.7</span></a>), following pre-trained models are considered:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "wolbanking77",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F1-score metrics for Zero-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "f1score",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS4\" title=\"5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "all",
                    "models",
                    "results",
                    "wolbanking77",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class=\"ltx_cite ltx_citemacro_citep\">[Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib41\" title=\"\">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openai/tiktoken\" title=\"\">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class=\"ltx_cite ltx_citemacro_citet\">Vaswani et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib77\" title=\"\">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class=\"ltx_cite ltx_citemacro_citep\">[Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib26\" title=\"\">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class=\"ltx_cite ltx_citemacro_citep\">[Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib36\" title=\"\">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>\n\n",
                "matched_terms": [
                    "cnn",
                    "model",
                    "tuning",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib69\" title=\"\">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class=\"ltx_cite ltx_citemacro_citet\">Gandhi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib30\" title=\"\">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Ardila et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib9\" title=\"\">2020</a>]</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Panayotov et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib64\" title=\"\">2015</a>]</cite> , VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib78\" title=\"\">2021a</a>]</cite> , TED-LIUM <cite class=\"ltx_cite ltx_citemacro_citet\">Hernandez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib40\" title=\"\">2018</a>]</cite>, People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_citet\">Galvez et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib29\" title=\"\">2021</a>]</cite>, GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib17\" title=\"\">2021</a>]</cite>, AMI <cite class=\"ltx_cite ltx_citemacro_citet\">Carletta et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib14\" title=\"\">2006</a>]</cite>, and Yodas <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib48\" title=\"\">2023</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models presented in this article are based on <span class=\"ltx_text ltx_font_italic\">Pytorch</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Paszke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib65\" title=\"\">2019</a>]</cite>. All experiments were conducted on Runpod <span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_href\" href=\"https://www.runpod.io/\" title=\"\">https://www.runpod.io/</a></span></span></span> and Kaggle.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://www.kaggle.com/\" title=\"\">https://www.kaggle.com/</a></span></span></span> P100 GPU (16GB VRAM) was used to finetune BERT-Base and mDeBERTa-v3-base. RTX 4090 GPU for Afro-xlmr-large, AfroLM_active_learning, AfriteVa V2 and Llama-3.2 models. For Zero-shot and Few-shot classification, RTX 2000 Ada GPU (16 GB VRAM) was used. Few-shot classification was performed using <span class=\"ltx_text ltx_font_italic\">SetFit huggingface</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[Tunstall et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib76\" title=\"\">2022</a>]</cite>. <span class=\"ltx_text ltx_font_italic\">2-shots</span> refers to 2 samples per intent while <span class=\"ltx_text ltx_font_italic\">8-shots</span> represents 8 samples per intent. All pre-trained text models are hosted on <span class=\"ltx_text ltx_font_italic\">huggingface platform</span>, they were fine-tuned using <span class=\"ltx_text ltx_font_italic\">transformers</span> library <cite class=\"ltx_cite ltx_citemacro_citet\">Wolf et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib81\" title=\"\">2020</a>]</cite> except for Llama-3.2 which was fine-tuned using <span class=\"ltx_text ltx_font_italic\">torchtune</span> library <cite class=\"ltx_cite ltx_citemacro_citep\">[torchtune maintainers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib75\" title=\"\">2024</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "models",
                    "classification",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.SS8\" title=\"8.8 Text prompt for Llama3.2 &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>\n\n",
                "matched_terms": [
                    "cnn",
                    "models",
                    "wolbanking77",
                    "classification",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class=\"ltx_text ltx_font_italic\">NVIDIA NEMO</span> framework <cite class=\"ltx_cite ltx_citemacro_citet\">Kuchaiev et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib46\" title=\"\">2019</a>]</cite> and <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class=\"ltx_text ltx_font_italic\">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F4\" title=\"Figure 4 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "all",
                    "models",
                    "results",
                    "classification",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.F5\" title=\"Figure 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:slm</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.T5\" title=\"Table 5 &#8227; 5.4 Results and Discussions &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "wolbanking77",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The school dropout rate is high in Senegal and this has resulted in a significant number of people who went to school and left without acquiring basic skills in the official language (French). As a result, in our situation, basic skills in French are lacking for more than half of the Senegalese population. One of the solutions has been to resort to literacy in national languages with satisfactory results with the experiments in Pulaar carried out by NGO TOSTAN and the examples of functional literacy carried out by UNESCO. Despite this, a large segment of the population still remains in a situation where basic linguistic skills, either in the official language or in one of the national languages, are acquired very little or not at all.</p>\n\n",
                "matched_terms": [
                    "results",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the models listed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1\" title=\"5.1.1 Zero-shot classification with WolBanking77 &#8227; 5.1 Intent detection models &#8227; 5 Dataset evaluation &amp; Experiments &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a> for zero-shot classification. We use the pre-trained Masked Language Modeling (MLM) models (i.e. encoder models) on the datasets mentioned in this section.</p>\n\n",
                "matched_terms": [
                    "models",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>\n\n",
                "matched_terms": [
                    "f1score",
                    "cnn",
                    "models",
                    "results",
                    "laser3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASER3:</span> <cite class=\"ltx_cite ltx_citemacro_citet\">Heffernan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib39\" title=\"\">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://www.masakhane.io/\" title=\"\">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP&#8217;22 workshop.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span><a class=\"ltx_ref ltx_href\" href=\"https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html\" title=\"\">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span><a class=\"ltx_ref ltx_href\" href=\"https://opus.nlpl.eu/\" title=\"\">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class=\"ltx_text ltx_font_bold\">LASER2</span>, while the student model is referred to as <span class=\"ltx_text ltx_font_bold\">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class=\"ltx_cite ltx_citemacro_citet\">Artetxe and Schwenk [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib10\" title=\"\">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>\n\n",
                "matched_terms": [
                    "results",
                    "laser3",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained LASER3 sentence encoder, which vectorizes each sentence and all possible combinations of classification models (MPL and CNN) are compared in the results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>. Note that all classification models were trained from scratch. For all the evaluations, punctuation are removed and each sentence was converted to lowercase.</p>\n\n",
                "matched_terms": [
                    "all",
                    "cnn",
                    "models",
                    "results",
                    "laser3",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class=\"ltx_cite ltx_citemacro_citep\">[Liaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#bib.bib49\" title=\"\">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19271v3#S8.T9\" title=\"Table 9 &#8227; 8.6 Hyperparameters &#8227; 8 Technical Appendices and Supplementary Material &#8227; WolBanking77: Wolof Banking Speech Intent Classification Dataset\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. This leads to the improved results reported in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:tab:laser_sent_enc</span>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "f1score",
                    "cnn",
                    "best",
                    "models",
                    "tuning",
                    "results",
                    "laser3",
                    "classification",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset can be used for anything related to intent classification as well as to train speech and text models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "classification"
                ]
            }
        ]
    }
}