{
    "S4.T1": {
        "source_file": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "caption": "Table 1: Objective Evaluation Metrics for Comparison with Baseline Codecs. S-T represents SpeechTokenizer for simplicity.",
        "body": "Tokenizer\nCB\nNq\nFR\nBR (bps)\nPESQ ↑\\uparrow\n\nSTOI ↑\\uparrow\n\nSTFT ↓\\downarrow\n\nMel ↓\\downarrow\n\nSIM ↑\\uparrow\n\n\n\nEncodec\n1024\n8\n75Hz\n6k\n2.76\n0.94\n0.11\n2.13\n0.89\n\n\nDAC-8\n1024\n8\n75Hz\n6k\n3.46\n0.95\n0.06\n2.02\n0.96\n\n\nS-T\n1024\n8\n50Hz\n4k\n2.66\n0.92\n0.59\n7.07\n0.84\n\n\nEncodec-2\n1024\n2\n75Hz\n1.5k\n1.56\n0.94\n0.23\n4.45\n0.90\n\n\nDAC-2\n1024\n2\n75Hz\n1.5k\n1.51\n0.83\n0.12\n3.36\n0.49\n\n\nBigCodec\n8192\n1\n80Hz\n1.04k\n2.68\n0.93\n-\n-\n0.84\n\n\nXcodec\n1024\n2\n50Hz\n1k\n2.33\n0.87\n-\n-\n0.72\n\n\nS-T\n1024\n2\n50Hz\n1k\n1.25\n0.77\n0.68\n8.02\n0.36\n\n\nMimi\n2048\n8\n12.5Hz\n1.1k\n2.24\n0.90\n-\n-\n0.73\n\n\nMBCodec\n2048\n8\n25Hz\n2.2k\n2.98\n0.94\n0.17\n3.62\n0.87\n\n\nS3Codec\n4096\n8\n12.5Hz\n1.2k\n2.85\n0.94\n0.12\n4.01\n0.89",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Tokenizer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">CB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Nq</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">FR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">BR (bps)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">PESQ <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">STOI <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">STFT <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Mel <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Encodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">75Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">6k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">DAC-8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">75Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">6k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">3.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">0.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">S-T</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">50Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">4k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">7.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Encodec-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">75Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.5k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">4.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">DAC-2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">75Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.5k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">BigCodec</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8192</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">80Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.04k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Xcodec</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">50Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">S-T</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1024</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">50Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Mimi</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2048</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">12.5Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.1k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">MBCodec</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2048</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">25Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.2k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.87</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D4D4D4;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">S3Codec</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">4096</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">12.5Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">1.2k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">2.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">0.89</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechtokenizer",
            "xcodec",
            "stoi",
            "↓downarrow",
            "sim",
            "104k",
            "mbcodec",
            "125hz",
            "objective",
            "stft",
            "s3codec",
            "simplicity",
            "12k",
            "75hz",
            "metrics",
            "codecs",
            "11k",
            "baseline",
            "represents",
            "encodec",
            "bigcodec",
            "22k",
            "tokenizer",
            "dac2",
            "mel",
            "bps",
            "evaluation",
            "pesq",
            "dac8",
            "↑uparrow",
            "50hz",
            "encodec2",
            "80hz",
            "15k",
            "mimi",
            "25hz",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span>\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, S3Codec achieves SOTA-comparable performance with a very low frame rate in most evaluation dimensions. S3codec achieves higher SIM scores than MBcodec, Mimi, and SpeechTokenizer with the same codebooks. In terms of the restoration and perception indictors PESQ and STOI, S3codec is comparable to the high bitrates Encodec and DAC-8. At the evaluation dimension of STFT and Mel indicators, S3Codec also performs well among low-bitrate codecs. These results provide preliminary evidence of the model&#8217;s effectiveness in reconstructing speech. As for the semantic evaluation, results in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> demonstrate the superiority of S3Codec.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite the remarkable progress in LLM-based zero-shot TTS, several fundamental challenges persist. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens, a task handled by a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib23\" title=\"\">2022</a>; Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib7\" title=\"\">2023</a>)</cite>. Semantic tokens, typically derived from discretized self-supervised learning (SSL) models, are considered to exhibit high alignment with text while leading to poor reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib16\" title=\"\">2025</a>)</cite>. In contrast, acoustic tokens often derived from speech codecs trained through residual vector quantization GAN (RVQ-GAN), are recognized for capturing the details of the audio waveform, enabling high-quality synthesis, but lack explicit semantic grounding, forcing the LLM to learn the complex mapping from text to raw acoustic properties from scratch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. We assume that a better audio tokenizer should contain rich semantic information to facilitate an easy understanding of audio content, thus reducing the language model&#8217;s burden in interpreting tokens, and contains acoustic information for speech reconstruction. For better linguistic understanding and acoustic reconstruction, inspired by Mimi codec and SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite>, we propose S3Codec, a split residual vector quantization speech codec with semantic distillation. However, rather than using SSL models, we adopt a pretrained state-of-the-art ASR model for semantic distillation, which we assume brings more explicit linguistic features.</p>\n\n",
                "matched_terms": [
                    "speechtokenizer",
                    "tokenizer",
                    "s3codec",
                    "codecs",
                    "mimi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Tokenization.</span> The success of autoregressive language models has spurred progress in speech LLMs, where speech tokenizers are essential for converting continuous signals into discrete tokens. Speech tokenizers are typically categorized as acoustic or semantic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib36\" title=\"\">2025a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib42\" title=\"\">2025</a>)</cite>. Acoustic tokens, optimized for signal reconstruction, capture detailed acoustic features beneficial for generation, but perform poorly on understanding tasks like ASR. Previous semantic tokenizers can be trained in two ways: (1) applying clustering or VQ to the representations of self-supervised learning models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite>. (2) applying a VQ layer to the intermediate layer of ASR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>)</cite>. These semantic tokenizers typically use a single codebook, have a simple architecture, are rich in linguistic information, and are well-suited for LLMs. However, finer-grained acoustic details such as pitch and prosody, are lost, resulting in poor performance on generation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#321;ajszczak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib25\" title=\"\">2024</a>; Betker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib2\" title=\"\">2023</a>)</cite>. An alternative for audio tokenization is to use multi-codebook residual vector quantization (RVQ). In RVQ, an audio frame is represented by a sum of vectors from several quantizers, allowing for high-fidelity reconstruction over a range of bitrates by capturing details that single-codebook models often miss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib8\" title=\"\">2022</a>; Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib45\" title=\"\">2021</a>)</cite>. To align residual speech codec tokens with large text models, recent efforts have explored modeling both semantic and acoustic features simultaneously. SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite> enhances the RVQGAN paradigm with semantic distillation to guide the first layer of RVQ to align with a teacher SSL model. X-codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>)</cite> proposes an X-shaped structure where each layer of RVQ contains semantic and acoustic information. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> argues that distilling semantic information into the first level of a single RVQ will trade the auido quality restoration performance of the residual codebooks. Similar to Mimi, we propose S3Codec: a Split RVQ Speech Tokenizer with Semantic Distillation. Unlike Mimi, we adopt DAC architecture with pretrained Whisper for semantic distillation. This approach allows S3Codec to have good acoustic restoration ability and stronger linguistic information.</p>\n\n",
                "matched_terms": [
                    "speechtokenizer",
                    "xcodec",
                    "tokenizer",
                    "s3codec",
                    "mimi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the architecture. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "speechtokenizer",
                    "mimi",
                    "represents",
                    "s3codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119810;</mi><mi>t</mi><mn>0</mn></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> represents the first encoded embeddings for the frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}</annotation></semantics></math> represents the semantic embeddings obtained from the Whisper Encoder, <math alttext=\"\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}</annotation></semantics></math> represents the projection operation that maps whisper latent embedding to the space of audio embedding, <math alttext=\"L_{\\mathcal{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathcal{S}}</annotation></semantics></math> represents the length of semantic frames, and <math alttext=\"\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}</annotation></semantics></math> represents the projected whisper embedding for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. The details of the overall training objective are listed in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A3\" title=\"Appendix C Model architecture and training recipe &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a dataset <math alttext=\"\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>&#119857;</mi><mo>,</mo><mi>&#119858;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}</annotation></semantics></math>, where <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> is an audio sample and <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> is the corresponding text transcription. We use a pre-trained neural codec model to encode each audio sample into discrete codes, denoted as <math alttext=\"\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mtext>S3Codec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math>, where <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the downsampled utterance length. <math alttext=\"\\mathbf{A}_{t}\\in\\mathbb{R}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119808;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{t}\\in\\mathbb{R}^{K}</annotation></semantics></math> represents the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codes for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and <math alttext=\"\\mathcal{A}_{t}^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{k}</annotation></semantics></math> represents the code for the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th codebook of frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Mathematically, given the text prompt <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the speech prompt <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math>, our target is to train a neural language model to generate the discrete code matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> with the optimization objective of maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The semantic transformer functions as a thinker responsible for processing and understanding the text and the audio modality, and generating high-level representations. Mathematically, let <math alttext=\"\\mathcal{T}\\in\\mathbb{R}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>M</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}\\in\\mathbb{R}^{M}</annotation></semantics></math> represent the tokenized textual prompt, <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math> represent the corresponding speech, and <math alttext=\"\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119808;</mi><mi>i</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}</annotation></semantics></math> represent the speech codes in the i-th codebook, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represents the length of the encoded text token and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of the encoded speech token. Given tokenized text prompt and encoded prompt audio codes, the semantic transformer learns the linguistic features of the text <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the discrete acoustic representation of the prompt audio <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math> and outputs a latent feature <math alttext=\"\\mathbf{H}^{ctx}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}</annotation></semantics></math> as a guide for the generation of subsequent speech tokens. The optimization objective of the semantic transformer is maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Next Token Embedding Prediction.</span> In order to inject RVQ speech representation into LLM, we sum the codebook dimensions of the multi-codebook parallel sequence. Aggregation brings rich linguistic and acoustic content to the semantic transformer; however, the speech representation of each time step is no longer a quantitative representation. To solve this problem, we propose direct embedding prediction. Instead of predicting discrete token IDs and computing cross-entropy loss, we directly predict the next embedding vector in the continuous semantic space and optimize using Mean Squared Error Loss between predicted and target embeddings. Specifically, our model learns to predict the next semantic embedding as <math alttext=\"\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119815;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119815;</mi><mn>1</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119815;</mi><mn>2</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})</annotation></semantics></math>, where <math alttext=\"\\mathbf{H}^{ctx}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t}</annotation></semantics></math> represents the continuous semantic embedding at position <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To be more task-specific, we denote <math alttext=\"\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>&#8784;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119827;</mi><mo>&#8853;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})</annotation></semantics></math>, where <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>&#119827;</mi><annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation></semantics></math> represents the text modality, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the audio modality, and <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> represents the concatenate operation. We split the high-level representation and focus on speech modality; thus the optimization objective can be formulated as follows:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span> To assess the reconstruction performance of S3Codec, we employ several state-of-the-art neural codecs as baselines, including Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib8\" title=\"\">2022</a>)</cite>, DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, QDAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>, SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite>, BigCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib40\" title=\"\">2024</a>)</cite>, Xcodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>)</cite> and MBCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib46\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechtokenizer",
                    "xcodec",
                    "s3codec",
                    "codecs",
                    "mbcodec",
                    "encodec",
                    "bigcodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> To evaluate the performance of S3Codec, we employ several metrics, including SIM, STFT Distance, Mel Distance, short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite>. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. More detailed evaluation set up is listed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "s3codec",
                    "stft",
                    "mel",
                    "stoi",
                    "metrics",
                    "evaluation",
                    "pesq",
                    "sim",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We adopt the word error rate (WER) and speaker similarity (SIM) metrics for objective evaluation. For WER, we employ Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite> and Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib15\" title=\"\">2023</a>)</cite> as the automatic speech recognition engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task to obtain speaker embeddings used to calculate the cosine similarity of speech samples of each test utterance against reference clips. For naturalness, we use SpeechMOS MOS prediction model to calculate UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib33\" title=\"\">2022</a>)</cite> scores for evaluation.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "evaluation",
                    "objective",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span> To evaluate CaT-TTS&#8217;s zero-shot TTS capatility, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Quantization and Reconstruction Analysis &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As can be seen, CaT-TTS demonstrates a significant superiority in intelligibility for zero-shot TTS scenarios. With WER <math alttext=\"1.56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>1.56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.56\\%</annotation></semantics></math>, <math alttext=\"2.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>2.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.35\\%</annotation></semantics></math> and <math alttext=\"9.75\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>9.75</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.75\\%</annotation></semantics></math> in test-zh, test-en and test-hard, respectively, CaT-TTS achieves best or best comparable performance among these baselines, especially in pure AR based models. In terms of speaking similarity, like the other Pure AR based models, Cat-TTS&#8217;s performance is inferior to NAR-involved models, especially pure NAR models. The reason is that NAR models like F5-TTS generate based on more explicit acoustic features like Mel-Spectrogram, and AR+NAR models typically construct acoustic information with acoustic guidance like speaker similarity vector in the NAR stage. Although with higher indicator performance, we think it may degrade diversity and cause more storage and processing cost during training. To do a further comprehensive comparison on zero-shot TTS performance, we compared recent prominent AR-based two-stage TTS models including VALL-E, CosyVoice, QTTS and reproduced Llama-CosyVoice as baseline models, and testify the synthesis capability in a more real scenarios. The evaluation datasets including PGC-Hard and PGC-Poly, which contain more complex real-life sentences and polyphonetic characters, respectively. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that CaT-TTS has SOTA comparable in-context learning ability. With WER 9.75<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, 7.03<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 13.97<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> in Seed-TTS test-zh-hard, PGC-Hard and PGC-Poly, respectively. Q-TTS and VALL-E are Transformer-based TTS systems powered by codec, which is similar to CaT-TTS. As can be seen, CaT-TTS achieves better performance. Although without additional acoustic information supplement through flow-matching, CaT-TTS has comparable or superior performance in terms of UTMOS and WER, demonstrating the context-learning ability of our system.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "baseline",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality UnderStanding.</span> To demonstrate the effectiveness and superiority of the modality understanding loss. We trained two models in sub-dataset with the same architecture but with small model size, and one of them is trained without semantic guidance. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the comparison results. With the loss of semantic guidance removed, this leads to performance decreases, especially with the WER increasing from <math alttext=\"3.31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3.31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.31\\%</annotation></semantics></math> to <math alttext=\"3.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.97\\%</annotation></semantics></math> in SeedTTS-test, <math alttext=\"9.74\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mn>9.74</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.74\\%</annotation></semantics></math> to <math alttext=\"11.83\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>11.83</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.83\\%</annotation></semantics></math> in PGC-Hard and <math alttext=\"16.57\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mn>16.57</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">16.57\\%</annotation></semantics></math> to <math alttext=\"18.34\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mn>18.34</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">18.34\\%</annotation></semantics></math> in PGC-Poly, and the speech quality indicators SIM and UTMOS have also been reduced. During model training, semantic loss forces the semantic transformer to enhance its understanding of text and semantic modalities, thus improving the linguistic understanding ability of CaT-TTS. These results underscore the pivotal role of semantic loss in ensuring accurate semantic information learning, which is essential for maintaining high-fidelity generation of acoustic transformer.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span> To train the model, we employ a GAN-based objective with a combination of two discriminators: a multi-period discriminator [18] with periods of [2, 3, 5, 7, 11], and a complex multi-scale STFT discriminator. The STFT discriminator operates on three resolutions with window lengths [2048, 1024, 512] and a hop length of 1/4 the window size, using frequency band splits of [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. The total loss function is a weighted sum of a GAN loss, feature matching loss, a codebook loss, and a multi-resolution reconstruction loss. The reconstruction loss is computed as the L1 distance between the log-mel spectrograms of the original and reconstructed audio over seven different resolutions. These resolutions use window lengths of [32, 64, 128, 256, 512, 1024, 2048] with a corresponding number of mel bands [5, 10, 20, 40, 80, 160, 320], respectively.</p>\n\n",
                "matched_terms": [
                    "stft",
                    "mel",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained with a multi-task objective that jointly optimizes for reconstruction fidelity and semantic alignment. The primary task is reconstruction, which is guided by a GAN-based objective comprising a reconstruction term, a discriminative loss, and an RVQ commitment loss. This is complemented by a semantic distillation task, which introduces an additional loss term to ensure the model&#8217;s representations are linguistically meaningful. In the following, <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> represents an speech signal and <math alttext=\"\\hat{\\mathbf{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{x}}</annotation></semantics></math> denotes the reconstructed signal.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, S3Codec consists of an autoencoder and Residual Vector Quantizer. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. It projects a 16kHz waveform into 1280-dimensional embeddings sampled at 50Hz, while S3Codec projects a 24kHz waveform into 4096-dimensional at 12.5 Hz. During training, we thus downsample the waveforms and project them to the same dimension as targets for distillation. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel. Their outputs will be summed up; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "speechtokenizer",
                    "s3codec",
                    "represents",
                    "mimi",
                    "50hz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Setup.</span> To evaluate the preservation of acoustic information, we employ several metrics. Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a pre-trained speaker verification model. STFT and Mel represent the spectrogram distance between original and reconstructed speech. We also use short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> to measure speech intelligibility and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite> to assess audio quality. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. To demonstrate the semantic alignment, we trained small CaT-TTS models powered by S3Codec and DAC, respectively.</p>\n\n",
                "matched_terms": [
                    "stft",
                    "s3codec",
                    "mel",
                    "stoi",
                    "metrics",
                    "evaluation",
                    "pesq",
                    "sim",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the capability of semantic preservation of S3Codec, we trained CaT-TTS small powered with S3Codec and DAC, respectively. We use WER as the evaluation metric, representing the speech intelligibility of the generated results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.T5\" title=\"Table 5 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the evaluation results on Seed-TTS test zh easy, PGC-Hard and PGC-Poly. Compared to S3Codec-based system, DAC-based model&#8217;s performance on speech intelligibility has decreased. The reason lies that DAC dose not contain structured linguistic features as S3Codec, which makes the LM model harder to understand, leading to worse performance than S3Codec.</p>\n\n",
                "matched_terms": [
                    "s3codec",
                    "evaluation"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "caption": "Table 2: Objective evaluation in the SeedTTS test datasets.",
        "body": "Model\ntest-zh\ntest-en\ntest-hard\n\n\n\nWER(%)(\\%) ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nWER(%)(\\%) ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nWER(%)(\\%) ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\nNAR-involved Models\n\n\nMaskGCT\n2.27\n0.774\n2.62\n0.714\n10.27\n0.748\n\n\nE2 TTS (32 NFE)\n1.97\n0.730\n2.19\n0.710\n-\n-\n\n\nF5-TTS (32 NFE)\n1.56\n0.741\n1.83\n0.647\n8.67\n0.713\n\n\nSeed-TTS\n1.12\n0.796\n2.25\n0.762\n7.59\n0.776\n\n\nFireRedTTS\n1.51\n0.635\n3.82\n0.460\n17.45\n0.621\n\n\nCosyVoice\n3.63\n0.723\n4.29\n0.609\n11.75\n0.709\n\n\nCosyVoice 2\n1.45\n0.748\n2.57\n0.652\n6.83\n0.724\n\n\nCosyVoice 3-0.5B\n1.16\n0.780\n2.02\n0.718\n6.08\n0.758\n\n\nPure AR based Models\n\n\nQTTS\n1.66\n0.648\n3.17\n0.652\n14.45\n0.641\n\n\nSpark-TTS\n1.20\n0.672\n1.98\n0.584\n-\n-\n\n\nLlasa-1B-250k\n1.89\n0.668\n3.22\n0.572\n12.13\n0.638\n\n\nLlasa-3B-250k\n1.60\n0.675\n3.14\n0.579\n13.37\n0.652\n\n\nLlasa-8B-250k\n1.59\n0.684\n2.97\n0.574\n11.09\n0.660\n\n\nCaT-TTS\n1.56\n0.678\n2.35\n0.668\n9.75\n0.674",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-hard</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)</annotation></semantics></math></span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)</annotation></semantics></math></span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)</annotation></semantics></math></span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\"><span class=\"ltx_text ltx_font_bold\">NAR-involved Models</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MaskGCT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.774</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.714</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.748</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">E2 TTS (32 NFE)</th>\n<td class=\"ltx_td ltx_align_center\">1.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.730</td>\n<td class=\"ltx_td ltx_align_center\">2.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.710</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">F5-TTS (32 NFE)</th>\n<td class=\"ltx_td ltx_align_center\">1.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.741</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.647</td>\n<td class=\"ltx_td ltx_align_center\">8.67</td>\n<td class=\"ltx_td ltx_align_center\">0.713</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Seed-TTS</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.796</span></td>\n<td class=\"ltx_td ltx_align_center\">2.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.762</span></td>\n<td class=\"ltx_td ltx_align_center\">7.59</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.776</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FireRedTTS</th>\n<td class=\"ltx_td ltx_align_center\">1.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.635</td>\n<td class=\"ltx_td ltx_align_center\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.460</td>\n<td class=\"ltx_td ltx_align_center\">17.45</td>\n<td class=\"ltx_td ltx_align_center\">0.621</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center\">3.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.723</td>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.609</td>\n<td class=\"ltx_td ltx_align_center\">11.75</td>\n<td class=\"ltx_td ltx_align_center\">0.709</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice 2</th>\n<td class=\"ltx_td ltx_align_center\">1.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.748</td>\n<td class=\"ltx_td ltx_align_center\">2.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.652</td>\n<td class=\"ltx_td ltx_align_center\">6.83</td>\n<td class=\"ltx_td ltx_align_center\">0.724</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice 3-0.5B</th>\n<td class=\"ltx_td ltx_align_center\">1.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.780</td>\n<td class=\"ltx_td ltx_align_center\">2.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.718</td>\n<td class=\"ltx_td ltx_align_center\">6.08</td>\n<td class=\"ltx_td ltx_align_center\">0.758</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"7\"><span class=\"ltx_text ltx_font_bold\">Pure AR based Models</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">QTTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.648</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.652</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">14.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.641</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Spark-TTS</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.672</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.584</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Llasa-1B-250k</th>\n<td class=\"ltx_td ltx_align_center\">1.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.668</td>\n<td class=\"ltx_td ltx_align_center\">3.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.572</td>\n<td class=\"ltx_td ltx_align_center\">12.13</td>\n<td class=\"ltx_td ltx_align_center\">0.638</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Llasa-3B-250k</th>\n<td class=\"ltx_td ltx_align_center\">1.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.675</td>\n<td class=\"ltx_td ltx_align_center\">3.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.579</td>\n<td class=\"ltx_td ltx_align_center\">13.37</td>\n<td class=\"ltx_td ltx_align_center\">0.652</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Llasa-8B-250k</th>\n<td class=\"ltx_td ltx_align_center\">1.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.684</span></td>\n<td class=\"ltx_td ltx_align_center\">2.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.574</td>\n<td class=\"ltx_td ltx_align_center\">11.09</td>\n<td class=\"ltx_td ltx_align_center\">0.660</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D4D4D4;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">CaT-TTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">1.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">0.678</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">2.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">0.668</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">9.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">0.674</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cosyvoice",
            "seedtts",
            "↓downarrow",
            "sim",
            "f5tts",
            "testzh",
            "datasets",
            "nfe",
            "objective",
            "305b",
            "test",
            "narinvolved",
            "pure",
            "tts",
            "wer",
            "cattts",
            "model",
            "qtts",
            "llasa8b250k",
            "evaluation",
            "maskgct",
            "↑uparrow",
            "testen",
            "llasa3b250k",
            "sparktts",
            "models",
            "testhard",
            "llasa1b250k",
            "fireredtts",
            "based"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span> To evaluate CaT-TTS&#8217;s zero-shot TTS capatility, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Quantization and Reconstruction Analysis &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As can be seen, CaT-TTS demonstrates a significant superiority in intelligibility for zero-shot TTS scenarios. With WER <math alttext=\"1.56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>1.56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.56\\%</annotation></semantics></math>, <math alttext=\"2.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>2.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.35\\%</annotation></semantics></math> and <math alttext=\"9.75\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>9.75</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.75\\%</annotation></semantics></math> in test-zh, test-en and test-hard, respectively, CaT-TTS achieves best or best comparable performance among these baselines, especially in pure AR based models. In terms of speaking similarity, like the other Pure AR based models, Cat-TTS&#8217;s performance is inferior to NAR-involved models, especially pure NAR models. The reason is that NAR models like F5-TTS generate based on more explicit acoustic features like Mel-Spectrogram, and AR+NAR models typically construct acoustic information with acoustic guidance like speaker similarity vector in the NAR stage. Although with higher indicator performance, we think it may degrade diversity and cause more storage and processing cost during training. To do a further comprehensive comparison on zero-shot TTS performance, we compared recent prominent AR-based two-stage TTS models including VALL-E, CosyVoice, QTTS and reproduced Llama-CosyVoice as baseline models, and testify the synthesis capability in a more real scenarios. The evaluation datasets including PGC-Hard and PGC-Poly, which contain more complex real-life sentences and polyphonetic characters, respectively. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that CaT-TTS has SOTA comparable in-context learning ability. With WER 9.75<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, 7.03<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 13.97<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> in Seed-TTS test-zh-hard, PGC-Hard and PGC-Poly, respectively. Q-TTS and VALL-E are Transformer-based TTS systems powered by codec, which is similar to CaT-TTS. As can be seen, CaT-TTS achieves better performance. Although without additional acoustic information supplement through flow-matching, CaT-TTS has comparable or superior performance in terms of UTMOS and WER, demonstrating the context-learning ability of our system.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an &#8220;Understand-then-Generate&#8221; dual-Transformer architecture that decouples comprehension from rendering. An initial &#8220;Understanding&#8221; Transformer models the cross-modal relationship between text and the audio&#8217;s semantic tokens to form a high-level utterance plan. A subsequent &#8220;Generation&#8221; Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. Extensive experiments demonstrate that the synergy of our principled architecture and semantically-aware codec allows CaT-TTS to achieve new state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in generation robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "tts",
                    "datasets",
                    "cattts",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM) based autoregressive (AR) models have achieved state-of-the-art quality in zero-shot Text-to-Speech (TTS) with discrete audio representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>. With a few seconds of audio prompt, current TTS models are able to synthesize speech for any given text and mimic the speaker of the audio prompt. Contrary to NAR models &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib26\" title=\"\">2023</a>)</cite>, the sequential nature of AR models, where each acoustic token is conditioned on all its predecessors, naturally captures the long-range temporal dependencies essential for rendering intricate intonation, rhythm, and emotional nuance. This sequential process synergizes perfectly with the in-context learning (ICL) capabilities of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>)</cite>, providing a powerful mechanism for propagating the fine-grained acoustic characteristics of a voice prompt throughout a newly synthesized utterance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the remarkable progress in LLM-based zero-shot TTS, several fundamental challenges persist. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens, a task handled by a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib23\" title=\"\">2022</a>; Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib7\" title=\"\">2023</a>)</cite>. Semantic tokens, typically derived from discretized self-supervised learning (SSL) models, are considered to exhibit high alignment with text while leading to poor reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib16\" title=\"\">2025</a>)</cite>. In contrast, acoustic tokens often derived from speech codecs trained through residual vector quantization GAN (RVQ-GAN), are recognized for capturing the details of the audio waveform, enabling high-quality synthesis, but lack explicit semantic grounding, forcing the LLM to learn the complex mapping from text to raw acoustic properties from scratch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. We assume that a better audio tokenizer should contain rich semantic information to facilitate an easy understanding of audio content, thus reducing the language model&#8217;s burden in interpreting tokens, and contains acoustic information for speech reconstruction. For better linguistic understanding and acoustic reconstruction, inspired by Mimi codec and SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite>, we propose S3Codec, a split residual vector quantization speech codec with semantic distillation. However, rather than using SSL models, we adopt a pretrained state-of-the-art ASR model for semantic distillation, which we assume brings more explicit linguistic features.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that speech synthesis is fundamentally an information-increasing process, where a thorough understanding of the source conditions is a prerequisite for accurate and effective generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib6\" title=\"\">2023</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib41\" title=\"\">2025</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib39\" title=\"\">2024</a>)</cite>. To embody this principle, we propose CaT-TTS, a novel &#8221;Comprehend-and-Talk&#8221; text-to-speech framework, realized through a dual-transformer architecture that explicitly decouples contextual comprehension from acoustic rendering. Our first module, the Semantic Transformer, operates autoregressively on the semantic level. Its sole purpose is to model the rich interplay between the input text and the core semantic content of the voice prompt, building a holistic high-level representation, a latent &#8220;plan&#8221; for the entire utterance. Following this, our second module, the Acoustic Transformer, takes this contextual plan as its foundation and executes the synthesis. It generates the detailed acoustic tokens autoregressively. This design allows the model first to understand &#8220;what&#8221; and &#8220;how&#8221;, and then generates the &#8220;sound&#8221;, which dramatically reduces the modeling burden at each step, leading to more coherent and expressive output.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our architectural design provides a more stable foundation, the challenge of long sequence lengths in speech remains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib48\" title=\"\">2023b</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib26\" title=\"\">2023</a>)</cite>. Even with our proposed high compression ratio codec, which significantly shortens the acoustic token sequences, the risk of error accumulation persists in any AR system. To overcome this challenge, inspired by Classifier-Free Guidance (CFG) in diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho &amp; Salimans, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib20\" title=\"\">2022</a>)</cite> and Parallel Scaling Laws&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib3\" title=\"\">2025</a>)</cite>, we introduce Masked Audio Parallel Inference (MAPI). It constructs parallel computing streams with different masked audio tokens and aggregates these streams adaptively with learnable weights. This technique acts as a corrective mechanism, steering the model back on track when it begins to &#8220;hallucinate&#8221; and ensuring robust output.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, we propose a novel zero-shot TTS system CaT-TTS powered by S3Codec. S3Codec encompasses acoustic and semantic information with low bit rates. Based on S3Codec, Cat-TTS embodies an understand and then generate rules via a dual language modeling strategy. To mitigate the error accumulation problem in audio language models, we introduce Masked Audio Parallel Inference strategy, which is beneficial for more robust token generation. Extensive experiments have shown that CaT-TTS has achieved a comparable or superior quality to existing models in terms of speech quality, similarity, and intelligibility.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "models",
                    "tts",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Tokenization.</span> The success of autoregressive language models has spurred progress in speech LLMs, where speech tokenizers are essential for converting continuous signals into discrete tokens. Speech tokenizers are typically categorized as acoustic or semantic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib36\" title=\"\">2025a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib42\" title=\"\">2025</a>)</cite>. Acoustic tokens, optimized for signal reconstruction, capture detailed acoustic features beneficial for generation, but perform poorly on understanding tasks like ASR. Previous semantic tokenizers can be trained in two ways: (1) applying clustering or VQ to the representations of self-supervised learning models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite>. (2) applying a VQ layer to the intermediate layer of ASR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>)</cite>. These semantic tokenizers typically use a single codebook, have a simple architecture, are rich in linguistic information, and are well-suited for LLMs. However, finer-grained acoustic details such as pitch and prosody, are lost, resulting in poor performance on generation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#321;ajszczak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib25\" title=\"\">2024</a>; Betker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib2\" title=\"\">2023</a>)</cite>. An alternative for audio tokenization is to use multi-codebook residual vector quantization (RVQ). In RVQ, an audio frame is represented by a sum of vectors from several quantizers, allowing for high-fidelity reconstruction over a range of bitrates by capturing details that single-codebook models often miss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib8\" title=\"\">2022</a>; Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib45\" title=\"\">2021</a>)</cite>. To align residual speech codec tokens with large text models, recent efforts have explored modeling both semantic and acoustic features simultaneously. SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite> enhances the RVQGAN paradigm with semantic distillation to guide the first layer of RVQ to align with a teacher SSL model. X-codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>)</cite> proposes an X-shaped structure where each layer of RVQ contains semantic and acoustic information. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> argues that distilling semantic information into the first level of a single RVQ will trade the auido quality restoration performance of the residual codebooks. Similar to Mimi, we propose S3Codec: a Split RVQ Speech Tokenizer with Semantic Distillation. Unlike Mimi, we adopt DAC architecture with pretrained Whisper for semantic distillation. This approach allows S3Codec to have good acoustic restoration ability and stronger linguistic information.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-based Zero-Shot TTS.</span> Inspired by the success of LLM, several recent works adopt language models to model text-to-speech (TTS) tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib4\" title=\"\">2024a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib29\" title=\"\">2024</a>)</cite>. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning to enable zero-shot TTS. VALL-E pioneered treating TTS as a conditional language modeling problem by converting waveforms into neural codec tokens. Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>)</cite> integrates multiple AR models to support multispeaker TTS with minimal supervision. Many systems use a single discrete codebook to quantize semantic features&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>)</cite>. Although simple, this bottleneck loses fine acoustic detail&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Recent TTS systems have often combined an AR language model with additional components&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, such as diffusion, to generate more natural, controllable speech when trained on large datasets. While these methods can produce high-quality results, most of them neglect the interactive understanding of speech and text modalities, instead requiring continuous and fine-grained acoustic features for supplementation. Storing and processing such large-scale features is prohibitive, hindering training on hundreds of billions of tokens. In contrast, our approach utilizes a dual-autoregressive structure powered by a split RVQ discretization technique, with the first semantic transformer for modality understanding and the second acoustic transformer for acoustic information generation based on the context guide produced by the semantic transformer. This understand-then-generate paradigm fits the natural flow of speech, takes advantage of the context learning of LLMs, and avoids the need for additional acoustic features for supplementary reconstruction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "tts",
                    "datasets",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the architecture. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a dataset <math alttext=\"\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>&#119857;</mi><mo>,</mo><mi>&#119858;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}</annotation></semantics></math>, where <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> is an audio sample and <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> is the corresponding text transcription. We use a pre-trained neural codec model to encode each audio sample into discrete codes, denoted as <math alttext=\"\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mtext>S3Codec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math>, where <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the downsampled utterance length. <math alttext=\"\\mathbf{A}_{t}\\in\\mathbb{R}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119808;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{t}\\in\\mathbb{R}^{K}</annotation></semantics></math> represents the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codes for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and <math alttext=\"\\mathcal{A}_{t}^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{k}</annotation></semantics></math> represents the code for the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th codebook of frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Mathematically, given the text prompt <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the speech prompt <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math>, our target is to train a neural language model to generate the discrete code matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> with the optimization objective of maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build such a model, we propose a dual auto-regressive Transformer modeling framework. The dual auto-regressive (AR) Transformer models the residual vector quantization (RVQ) output as a two-level autoregressive process, operating first along the temporal axis and subsequently across codebooks. The core intuition behind this design is to preserve both the causal nature of speech generation and the hierarchical refinement characteristic of RVQ. We denote the first transformer as the semantic transformer, following the causal nature of speech generation and context learning, while the second transformer is the acoustic transformer, modeling the acoustic feature in a coarse-to-fine manner.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Next Token Embedding Prediction.</span> In order to inject RVQ speech representation into LLM, we sum the codebook dimensions of the multi-codebook parallel sequence. Aggregation brings rich linguistic and acoustic content to the semantic transformer; however, the speech representation of each time step is no longer a quantitative representation. To solve this problem, we propose direct embedding prediction. Instead of predicting discrete token IDs and computing cross-entropy loss, we directly predict the next embedding vector in the continuous semantic space and optimize using Mean Squared Error Loss between predicted and target embeddings. Specifically, our model learns to predict the next semantic embedding as <math alttext=\"\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119815;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119815;</mi><mn>1</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119815;</mi><mn>2</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})</annotation></semantics></math>, where <math alttext=\"\\mathbf{H}^{ctx}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t}</annotation></semantics></math> represents the continuous semantic embedding at position <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To be more task-specific, we denote <math alttext=\"\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>&#8784;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119827;</mi><mo>&#8853;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})</annotation></semantics></math>, where <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>&#119827;</mi><annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation></semantics></math> represents the text modality, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the audio modality, and <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> represents the concatenate operation. We split the high-level representation and focus on speech modality; thus the optimization objective can be formulated as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The purpose of the acoustic transformer is to reconstruct discrete speech representations from coarse-grained to fine-grained based on the learned preceding text and speech modal information. The optimization objective of the acoustic transformer is maximizing the following distribution:</p>\n\n",
                "matched_terms": [
                    "based",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> To evaluate the performance of S3Codec, we employ several metrics, including SIM, STFT Distance, Mel Distance, short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite>. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. More detailed evaluation set up is listed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "objective",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span>\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, S3Codec achieves SOTA-comparable performance with a very low frame rate in most evaluation dimensions. S3codec achieves higher SIM scores than MBcodec, Mimi, and SpeechTokenizer with the same codebooks. In terms of the restoration and perception indictors PESQ and STOI, S3codec is comparable to the high bitrates Encodec and DAC-8. At the evaluation dimension of STFT and Mel indicators, S3Codec also performs well among low-bitrate codecs. These results provide preliminary evidence of the model&#8217;s effectiveness in reconstructing speech. As for the semantic evaluation, results in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> demonstrate the superiority of S3Codec.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span> To train the CaT-TTS models, we have amassed a considerable dataset comprising multiple languages. The dataset contains about 200k hours labeled speech, with about <math alttext=\"85\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>85</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">85\\%</annotation></semantics></math> Chinese data and <math alttext=\"15\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">15\\%</annotation></semantics></math> English data. We evaluate our zero-shot TTS models with five benchmarks: (1) Seed-TTS test-en, a test set introduced in Seed-TTS of sample extracted from English public corpora, includes 1,000 samples from the Common Voice dataset. (2) SeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese public corpora, includes 2,020 samples from the DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib18\" title=\"\">2021</a>)</cite> dataset. (3) Seed-TTS test-hard, includes 400 samples that consist of complex Chinese sentences. (4) PGC-Hard, includes 1500 Chinese samples, containing Professionally-Generated Content. (5) PGC-Poly, includes 1500 Chinese samples, containing polyphonic characters. The PGC testset is specially designed to test model generalization on difficult, out-of-domain voices.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "models",
                    "tts",
                    "seedtts",
                    "testhard",
                    "testzh",
                    "datasets",
                    "cattts",
                    "testen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We adopt the word error rate (WER) and speaker similarity (SIM) metrics for objective evaluation. For WER, we employ Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite> and Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib15\" title=\"\">2023</a>)</cite> as the automatic speech recognition engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task to obtain speaker embeddings used to calculate the cosine similarity of speech samples of each test utterance against reference clips. For naturalness, we use SpeechMOS MOS prediction model to calculate UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib33\" title=\"\">2022</a>)</cite> scores for evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "evaluation",
                    "sim",
                    "wer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span> We compare our models with state-of-the-art zero-shot TTS systems, including Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>, FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib17\" title=\"\">2024</a>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib38\" title=\"\">2024</a>)</cite>, E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib14\" title=\"\">2024</a>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">2024b</a>)</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>)</cite> and QTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Details of each model can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS2\" title=\"F.2 Baseline Details &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>. In particular, we also compare the performance of SOTA two-stage models, including VALL-E, CosyVoice, CosyVoice 2, QTTS and self-implement AR (Llama)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib13\" title=\"\">2024</a>)</cite> + flow-matching models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib28\" title=\"\">2022</a>)</cite>, where L-CosyVoice50 means Llama backbone with 50 Hz semantic codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> and L-CosyVoice25 means with 25 Hz.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qtts",
                    "cosyvoice",
                    "models",
                    "fireredtts",
                    "tts",
                    "seedtts",
                    "f5tts",
                    "maskgct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> We train CaT-TTS on 8 NVIDIA H20 96GB GPUs. The parallel stream is set to 4. For more details about the model architecture, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS1\" title=\"F.1 CaT-TTS architecture &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. We optimize the model with the AdamW optimizer with a learning rate of 1e-5 and 20K warm-up steps.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality UnderStanding.</span> To demonstrate the effectiveness and superiority of the modality understanding loss. We trained two models in sub-dataset with the same architecture but with small model size, and one of them is trained without semantic guidance. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the comparison results. With the loss of semantic guidance removed, this leads to performance decreases, especially with the WER increasing from <math alttext=\"3.31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3.31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.31\\%</annotation></semantics></math> to <math alttext=\"3.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.97\\%</annotation></semantics></math> in SeedTTS-test, <math alttext=\"9.74\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mn>9.74</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.74\\%</annotation></semantics></math> to <math alttext=\"11.83\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>11.83</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.83\\%</annotation></semantics></math> in PGC-Hard and <math alttext=\"16.57\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mn>16.57</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">16.57\\%</annotation></semantics></math> to <math alttext=\"18.34\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mn>18.34</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">18.34\\%</annotation></semantics></math> in PGC-Poly, and the speech quality indicators SIM and UTMOS have also been reduced. During model training, semantic loss forces the semantic transformer to enhance its understanding of text and semantic modalities, thus improving the linguistic understanding ability of CaT-TTS. These results underscore the pivotal role of semantic loss in ensuring accurate semantic information learning, which is essential for maintaining high-fidelity generation of acoustic transformer.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "sim",
                    "wer",
                    "cattts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Audio Parallel Inference.</span>\nTo evaluate the effectiveness of masked parallel inference, we trained CaT-TTS-small in the subset of the collected dataset. We set different parallel streams and evaluated the performance in the PGC-Hard, PGC-Poly, and SeedTTS test-zh-easy dataset. Results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> show the average performance analysis of MAPI parallel streams. The left subfigure shows the speech intelligibility improvement that MAPI brings. The middle subfigure shows that as the number of parallel streams increases, the acoustic performance SIM score and the UTMOS score show an upward trend. To demonstrate the robustness of MAPI, each sample in these datasets will be evaluated 10 times. As can be seen in the right subfigure in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of each inference is more stable in terms of the WER indicator. Due to the parallel computing capability of GPU, MAPI almost does not bring additional time consumption, but as the number of parallel streams increases, the utilization of GPU resources also increases. It is necessary to select the most appropriate number of parallel streams according to the requirements of the actual scenario.</p>\n\n",
                "matched_terms": [
                    "seedtts",
                    "datasets",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced CaT-TTS, a novel Text-to-Speech system designed to address key challenges in representation and generation. At its core is S3Codec, a split RVQ codec that resolves the trade-off between reconstruction fidelity and semantic interpretability by injecting linguistic features via ASR-based distillation. Building on this semantically aware representation, we proposed a principled &#8220;Understand-then-Generate&#8221; paradigm, realized through a dual-Transformer architecture that decouples contextual comprehension from acoustic rendering. To complement this, we developed Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that enhances generation stability by dynamically mitigating local decoding errors. Extensive experiments demonstrate that the synergy between our architecture and codec allows CaT-TTS to achieve state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span> To train the model, we employ a GAN-based objective with a combination of two discriminators: a multi-period discriminator [18] with periods of [2, 3, 5, 7, 11], and a complex multi-scale STFT discriminator. The STFT discriminator operates on three resolutions with window lengths [2048, 1024, 512] and a hop length of 1/4 the window size, using frequency band splits of [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. The total loss function is a weighted sum of a GAN loss, feature matching loss, a codebook loss, and a multi-resolution reconstruction loss. The reconstruction loss is computed as the L1 distance between the log-mel spectrograms of the original and reconstructed audio over seven different resolutions. These resolutions use window lengths of [32, 64, 128, 256, 512, 1024, 2048] with a corresponding number of mel bands [5, 10, 20, 40, 80, 160, 320], respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained with a multi-task objective that jointly optimizes for reconstruction fidelity and semantic alignment. The primary task is reconstruction, which is guided by a GAN-based objective comprising a reconstruction term, a discriminative loss, and an RVQ commitment loss. This is complemented by a semantic distillation task, which introduces an additional loss term to ensure the model&#8217;s representations are linguistically meaningful. In the following, <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> represents an speech signal and <math alttext=\"\\hat{\\mathbf{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{x}}</annotation></semantics></math> denotes the reconstructed signal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, S3Codec consists of an autoencoder and Residual Vector Quantizer. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. It projects a 16kHz waveform into 1280-dimensional embeddings sampled at 50Hz, while S3Codec projects a 24kHz waveform into 4096-dimensional at 12.5 Hz. During training, we thus downsample the waveforms and project them to the same dimension as targets for distillation. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel. Their outputs will be summed up; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Setup.</span> To evaluate the preservation of acoustic information, we employ several metrics. Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a pre-trained speaker verification model. STFT and Mel represent the spectrogram distance between original and reconstructed speech. We also use short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> to measure speech intelligibility and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite> to assess audio quality. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. To demonstrate the semantic alignment, we trained small CaT-TTS models powered by S3Codec and DAC, respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "evaluation",
                    "sim",
                    "cattts",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the capability of semantic preservation of S3Codec, we trained CaT-TTS small powered with S3Codec and DAC, respectively. We use WER as the evaluation metric, representing the speech intelligibility of the generated results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.T5\" title=\"Table 5 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the evaluation results on Seed-TTS test zh easy, PGC-Hard and PGC-Poly. Compared to S3Codec-based system, DAC-based model&#8217;s performance on speech intelligibility has decreased. The reason lies that DAC dose not contain structured linguistic features as S3Codec, which makes the LM model harder to understand, leading to worse performance than S3Codec.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "seedtts",
                    "evaluation",
                    "wer",
                    "cattts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the CaT-TTS small, the semantic transformer is 8 decoder-only transformer layers, with 1024 model dimension, and the acoustic transformer is 4 decoder-only transformer layers, with 512 model dimension.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>)</cite>:</span> AR + NAR TTS system. The first AR model predicts the first codebook, and the second transformer predict the remaining codebooks.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>:</span> Hybrid TTS system. A two-stage model that employs an AR LM for semantic token prediction and flow matching for acoustic feature generation.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib17\" title=\"\">2024</a>)</cite>:</span> Hybrid TTS system. A two-stage model similar to Seed-TTS, using an AR LM for semantic tokens and flow matching for acoustic features.</p>\n\n",
                "matched_terms": [
                    "fireredtts",
                    "tts",
                    "model",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib38\" title=\"\">2024</a>)</cite>:</span> NAR TTS system. A NAR model that applies masking-based generative strategies for speech synthesis.</p>\n\n",
                "matched_terms": [
                    "maskgct",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib14\" title=\"\">2024</a>)</cite>:</span> NAR TTS system. A flow matching-based model that\npredicts Mel spectrograms as acoustic features.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>)</cite>:</span> NAR TTS system. A flow matching-based model that\npredicts Mel spectrograms as acoustic features.</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice series&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib12\" title=\"\">2025</a>)</cite>:</span> Hybrid TTS system. AR for semantic prediction and flow-matching for acoustic feature generation.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>)</cite>:</span> Single codebook Neural Audio Codec based Pure language TTS model. Powered by BiCodec and Qwen LLM.</p>\n\n",
                "matched_terms": [
                    "sparktts",
                    "model",
                    "pure",
                    "tts",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">QTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>:</span> Pure Codec based language audio model. A two-stage AR+AR model. RVQ-based two stage speech synthesis modeling.</p>\n\n",
                "matched_terms": [
                    "pure",
                    "model",
                    "qtts",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Llasa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>)</cite>:</span> A single-stream codecbased TTS model that uses a single AR language model for direct single-stream code prediction.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "caption": "Table 3: Objective evaluation on hard mandarin test. † represents the self-implemented model. −- means the average evaluation results across three sets.",
        "body": "Model\nModel Size\nWER(%)↓(\\%)\\downarrow\nSIM↑\\uparrow\nUTMOS↑\\uparrow\n\n\nSeed-Hard\nPGC-Hard\nPGC-Poly\n-\n-\n\n\nCosyVoice\n0.3B\n11.75\n7.86\n16.22\n0.709\n3.01\n\n\nCosyVoice 2\n0.5B\n6.83\n6.11\n14.25\n0.713\n3.02\n\n\nL-CosyVoice50†\n\n0.2B\n9.52\n8.15\n18.71\n0.691\n2.92\n\n\nL-CosyVoice25†\n\n0.5B\n7.46\n6.83\n13.84\n0.706\n2.99\n\n\nQ-TTS\n0.2B\n14.45\n7.89\n14.37\n0.654\n3.03\n\n\nVALL-E†\n\n0.2B\n13.12\n9.68\n15.71\n0.631\n3.05\n\n\nCaT-TTS\n0.4B\n9.75\n7.03\n13.97\n0.672\n3.13",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model Size</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"(\\%)\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">&#8595;</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Seed-Hard</td>\n<td class=\"ltx_td ltx_align_center\">PGC-Hard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">PGC-Poly</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.3B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.709</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice 2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.5B</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">14.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.713</span></td>\n<td class=\"ltx_td ltx_align_center\">3.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">L-CosyVoice50<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.2B</th>\n<td class=\"ltx_td ltx_align_center\">9.52</td>\n<td class=\"ltx_td ltx_align_center\">8.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.691</td>\n<td class=\"ltx_td ltx_align_center\">2.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">L-CosyVoice25<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.5B</th>\n<td class=\"ltx_td ltx_align_center\">7.46</td>\n<td class=\"ltx_td ltx_align_center\">6.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">13.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.706</td>\n<td class=\"ltx_td ltx_align_center\">2.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Q-TTS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.2B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">14.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.654</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VALL-E<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.2B</th>\n<td class=\"ltx_td ltx_align_center\">13.12</td>\n<td class=\"ltx_td ltx_align_center\">9.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">15.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.631</td>\n<td class=\"ltx_td ltx_align_center\">3.05</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D4D4D4;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">CaT-TTS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">0.4B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">9.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">7.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#D4D4D4;\">13.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D4D4D4;\">0.672</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D4D4D4;\">3.13</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "03b",
            "sets",
            "size",
            "cosyvoice",
            "selfimplemented",
            "means",
            "objective",
            "three",
            "pgchard",
            "utmos↑uparrow",
            "test",
            "across",
            "mandarin",
            "pgcpoly",
            "represents",
            "cattts",
            "results",
            "model",
            "qtts",
            "lcosyvoice50†",
            "evaluation",
            "sim↑uparrow",
            "02b",
            "average",
            "wer↓downarrow",
            "hard",
            "seedhard",
            "lcosyvoice25†",
            "valle†",
            "04b",
            "05b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span> To evaluate CaT-TTS&#8217;s zero-shot TTS capatility, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Quantization and Reconstruction Analysis &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As can be seen, CaT-TTS demonstrates a significant superiority in intelligibility for zero-shot TTS scenarios. With WER <math alttext=\"1.56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>1.56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.56\\%</annotation></semantics></math>, <math alttext=\"2.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>2.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.35\\%</annotation></semantics></math> and <math alttext=\"9.75\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>9.75</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.75\\%</annotation></semantics></math> in test-zh, test-en and test-hard, respectively, CaT-TTS achieves best or best comparable performance among these baselines, especially in pure AR based models. In terms of speaking similarity, like the other Pure AR based models, Cat-TTS&#8217;s performance is inferior to NAR-involved models, especially pure NAR models. The reason is that NAR models like F5-TTS generate based on more explicit acoustic features like Mel-Spectrogram, and AR+NAR models typically construct acoustic information with acoustic guidance like speaker similarity vector in the NAR stage. Although with higher indicator performance, we think it may degrade diversity and cause more storage and processing cost during training. To do a further comprehensive comparison on zero-shot TTS performance, we compared recent prominent AR-based two-stage TTS models including VALL-E, CosyVoice, QTTS and reproduced Llama-CosyVoice as baseline models, and testify the synthesis capability in a more real scenarios. The evaluation datasets including PGC-Hard and PGC-Poly, which contain more complex real-life sentences and polyphonetic characters, respectively. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that CaT-TTS has SOTA comparable in-context learning ability. With WER 9.75<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, 7.03<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 13.97<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> in Seed-TTS test-zh-hard, PGC-Hard and PGC-Poly, respectively. Q-TTS and VALL-E are Transformer-based TTS systems powered by codec, which is similar to CaT-TTS. As can be seen, CaT-TTS achieves better performance. Although without additional acoustic information supplement through flow-matching, CaT-TTS has comparable or superior performance in terms of UTMOS and WER, demonstrating the context-learning ability of our system.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an &#8220;Understand-then-Generate&#8221; dual-Transformer architecture that decouples comprehension from rendering. An initial &#8220;Understanding&#8221; Transformer models the cross-modal relationship between text and the audio&#8217;s semantic tokens to form a high-level utterance plan. A subsequent &#8220;Generation&#8221; Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. Extensive experiments demonstrate that the synergy of our principled architecture and semantically-aware codec allows CaT-TTS to achieve new state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in generation robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that speech synthesis is fundamentally an information-increasing process, where a thorough understanding of the source conditions is a prerequisite for accurate and effective generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib6\" title=\"\">2023</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib41\" title=\"\">2025</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib39\" title=\"\">2024</a>)</cite>. To embody this principle, we propose CaT-TTS, a novel &#8221;Comprehend-and-Talk&#8221; text-to-speech framework, realized through a dual-transformer architecture that explicitly decouples contextual comprehension from acoustic rendering. Our first module, the Semantic Transformer, operates autoregressively on the semantic level. Its sole purpose is to model the rich interplay between the input text and the core semantic content of the voice prompt, building a holistic high-level representation, a latent &#8220;plan&#8221; for the entire utterance. Following this, our second module, the Acoustic Transformer, takes this contextual plan as its foundation and executes the synthesis. It generates the detailed acoustic tokens autoregressively. This design allows the model first to understand &#8220;what&#8221; and &#8220;how&#8221;, and then generates the &#8220;sound&#8221;, which dramatically reduces the modeling burden at each step, leading to more coherent and expressive output.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-based Zero-Shot TTS.</span> Inspired by the success of LLM, several recent works adopt language models to model text-to-speech (TTS) tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib4\" title=\"\">2024a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib29\" title=\"\">2024</a>)</cite>. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning to enable zero-shot TTS. VALL-E pioneered treating TTS as a conditional language modeling problem by converting waveforms into neural codec tokens. Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>)</cite> integrates multiple AR models to support multispeaker TTS with minimal supervision. Many systems use a single discrete codebook to quantize semantic features&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>)</cite>. Although simple, this bottleneck loses fine acoustic detail&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Recent TTS systems have often combined an AR language model with additional components&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, such as diffusion, to generate more natural, controllable speech when trained on large datasets. While these methods can produce high-quality results, most of them neglect the interactive understanding of speech and text modalities, instead requiring continuous and fine-grained acoustic features for supplementation. Storing and processing such large-scale features is prohibitive, hindering training on hundreds of billions of tokens. In contrast, our approach utilizes a dual-autoregressive structure powered by a split RVQ discretization technique, with the first semantic transformer for modality understanding and the second acoustic transformer for acoustic information generation based on the context guide produced by the semantic transformer. This understand-then-generate paradigm fits the natural flow of speech, takes advantage of the context learning of LLMs, and avoids the need for additional acoustic features for supplementary reconstruction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the architecture. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "model",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119810;</mi><mi>t</mi><mn>0</mn></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> represents the first encoded embeddings for the frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}</annotation></semantics></math> represents the semantic embeddings obtained from the Whisper Encoder, <math alttext=\"\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}</annotation></semantics></math> represents the projection operation that maps whisper latent embedding to the space of audio embedding, <math alttext=\"L_{\\mathcal{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathcal{S}}</annotation></semantics></math> represents the length of semantic frames, and <math alttext=\"\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}</annotation></semantics></math> represents the projected whisper embedding for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. The details of the overall training objective are listed in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A3\" title=\"Appendix C Model architecture and training recipe &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a dataset <math alttext=\"\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>&#119857;</mi><mo>,</mo><mi>&#119858;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}</annotation></semantics></math>, where <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> is an audio sample and <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> is the corresponding text transcription. We use a pre-trained neural codec model to encode each audio sample into discrete codes, denoted as <math alttext=\"\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mtext>S3Codec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math>, where <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the downsampled utterance length. <math alttext=\"\\mathbf{A}_{t}\\in\\mathbb{R}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119808;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{t}\\in\\mathbb{R}^{K}</annotation></semantics></math> represents the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codes for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and <math alttext=\"\\mathcal{A}_{t}^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{k}</annotation></semantics></math> represents the code for the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th codebook of frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Mathematically, given the text prompt <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the speech prompt <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math>, our target is to train a neural language model to generate the discrete code matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> with the optimization objective of maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build such a model, we propose a dual auto-regressive Transformer modeling framework. The dual auto-regressive (AR) Transformer models the residual vector quantization (RVQ) output as a two-level autoregressive process, operating first along the temporal axis and subsequently across codebooks. The core intuition behind this design is to preserve both the causal nature of speech generation and the hierarchical refinement characteristic of RVQ. We denote the first transformer as the semantic transformer, following the causal nature of speech generation and context learning, while the second transformer is the acoustic transformer, modeling the acoustic feature in a coarse-to-fine manner.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The semantic transformer functions as a thinker responsible for processing and understanding the text and the audio modality, and generating high-level representations. Mathematically, let <math alttext=\"\\mathcal{T}\\in\\mathbb{R}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>M</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}\\in\\mathbb{R}^{M}</annotation></semantics></math> represent the tokenized textual prompt, <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math> represent the corresponding speech, and <math alttext=\"\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119808;</mi><mi>i</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}</annotation></semantics></math> represent the speech codes in the i-th codebook, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represents the length of the encoded text token and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of the encoded speech token. Given tokenized text prompt and encoded prompt audio codes, the semantic transformer learns the linguistic features of the text <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the discrete acoustic representation of the prompt audio <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math> and outputs a latent feature <math alttext=\"\\mathbf{H}^{ctx}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}</annotation></semantics></math> as a guide for the generation of subsequent speech tokens. The optimization objective of the semantic transformer is maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Token Sequence Modeling.</span> To be able to inject discrete speech representations into LLM, some research proposes to use a single codebook codec to make the speech modality well adapted in the way of text tokens. CaT-TTS fits the RVQ paradigm and, specifically, the multiple codebook information at each time step will be aggregated as the speech representation of the current time step. Thus, at each time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the audio representation can be formulated as <math alttext=\"\\mathbf{S}_{t}=\\sum_{i=0}^{K-1}\\mathcal{A}_{t}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mi>t</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow></msubsup><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>i</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{t}=\\sum_{i=0}^{K-1}\\mathcal{A}_{t}^{i}</annotation></semantics></math>, where <math alttext=\"\\mathcal{A}_{t}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m11\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{i}</annotation></semantics></math> represents the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m12\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th encoded representation for frame t.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Next Token Embedding Prediction.</span> In order to inject RVQ speech representation into LLM, we sum the codebook dimensions of the multi-codebook parallel sequence. Aggregation brings rich linguistic and acoustic content to the semantic transformer; however, the speech representation of each time step is no longer a quantitative representation. To solve this problem, we propose direct embedding prediction. Instead of predicting discrete token IDs and computing cross-entropy loss, we directly predict the next embedding vector in the continuous semantic space and optimize using Mean Squared Error Loss between predicted and target embeddings. Specifically, our model learns to predict the next semantic embedding as <math alttext=\"\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119815;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119815;</mi><mn>1</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119815;</mi><mn>2</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})</annotation></semantics></math>, where <math alttext=\"\\mathbf{H}^{ctx}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t}</annotation></semantics></math> represents the continuous semantic embedding at position <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To be more task-specific, we denote <math alttext=\"\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>&#8784;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119827;</mi><mo>&#8853;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})</annotation></semantics></math>, where <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>&#119827;</mi><annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation></semantics></math> represents the text modality, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the audio modality, and <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> represents the concatenate operation. We split the high-level representation and focus on speech modality; thus the optimization objective can be formulated as follows:</p>\n\n",
                "matched_terms": [
                    "represents",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> To evaluate the performance of S3Codec, we employ several metrics, including SIM, STFT Distance, Mel Distance, short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite>. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. More detailed evaluation set up is listed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span>\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, S3Codec achieves SOTA-comparable performance with a very low frame rate in most evaluation dimensions. S3codec achieves higher SIM scores than MBcodec, Mimi, and SpeechTokenizer with the same codebooks. In terms of the restoration and perception indictors PESQ and STOI, S3codec is comparable to the high bitrates Encodec and DAC-8. At the evaluation dimension of STFT and Mel indicators, S3Codec also performs well among low-bitrate codecs. These results provide preliminary evidence of the model&#8217;s effectiveness in reconstructing speech. As for the semantic evaluation, results in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> demonstrate the superiority of S3Codec.</p>\n\n",
                "matched_terms": [
                    "results",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span> To train the CaT-TTS models, we have amassed a considerable dataset comprising multiple languages. The dataset contains about 200k hours labeled speech, with about <math alttext=\"85\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>85</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">85\\%</annotation></semantics></math> Chinese data and <math alttext=\"15\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">15\\%</annotation></semantics></math> English data. We evaluate our zero-shot TTS models with five benchmarks: (1) Seed-TTS test-en, a test set introduced in Seed-TTS of sample extracted from English public corpora, includes 1,000 samples from the Common Voice dataset. (2) SeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese public corpora, includes 2,020 samples from the DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib18\" title=\"\">2021</a>)</cite> dataset. (3) Seed-TTS test-hard, includes 400 samples that consist of complex Chinese sentences. (4) PGC-Hard, includes 1500 Chinese samples, containing Professionally-Generated Content. (5) PGC-Poly, includes 1500 Chinese samples, containing polyphonic characters. The PGC testset is specially designed to test model generalization on difficult, out-of-domain voices.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "test",
                    "pgcpoly",
                    "cattts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We adopt the word error rate (WER) and speaker similarity (SIM) metrics for objective evaluation. For WER, we employ Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite> and Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib15\" title=\"\">2023</a>)</cite> as the automatic speech recognition engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task to obtain speaker embeddings used to calculate the cosine similarity of speech samples of each test utterance against reference clips. For naturalness, we use SpeechMOS MOS prediction model to calculate UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib33\" title=\"\">2022</a>)</cite> scores for evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "mandarin",
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span> We compare our models with state-of-the-art zero-shot TTS systems, including Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>, FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib17\" title=\"\">2024</a>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib38\" title=\"\">2024</a>)</cite>, E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib14\" title=\"\">2024</a>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">2024b</a>)</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>)</cite> and QTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Details of each model can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS2\" title=\"F.2 Baseline Details &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>. In particular, we also compare the performance of SOTA two-stage models, including VALL-E, CosyVoice, CosyVoice 2, QTTS and self-implement AR (Llama)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib13\" title=\"\">2024</a>)</cite> + flow-matching models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib28\" title=\"\">2022</a>)</cite>, where L-CosyVoice50 means Llama backbone with 50 Hz semantic codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> and L-CosyVoice25 means with 25 Hz.</p>\n\n",
                "matched_terms": [
                    "model",
                    "means",
                    "qtts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> We train CaT-TTS on 8 NVIDIA H20 96GB GPUs. The parallel stream is set to 4. For more details about the model architecture, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS1\" title=\"F.1 CaT-TTS architecture &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. We optimize the model with the AdamW optimizer with a learning rate of 1e-5 and 20K warm-up steps.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality UnderStanding.</span> To demonstrate the effectiveness and superiority of the modality understanding loss. We trained two models in sub-dataset with the same architecture but with small model size, and one of them is trained without semantic guidance. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the comparison results. With the loss of semantic guidance removed, this leads to performance decreases, especially with the WER increasing from <math alttext=\"3.31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3.31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.31\\%</annotation></semantics></math> to <math alttext=\"3.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.97\\%</annotation></semantics></math> in SeedTTS-test, <math alttext=\"9.74\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mn>9.74</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.74\\%</annotation></semantics></math> to <math alttext=\"11.83\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>11.83</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.83\\%</annotation></semantics></math> in PGC-Hard and <math alttext=\"16.57\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mn>16.57</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">16.57\\%</annotation></semantics></math> to <math alttext=\"18.34\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mn>18.34</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">18.34\\%</annotation></semantics></math> in PGC-Poly, and the speech quality indicators SIM and UTMOS have also been reduced. During model training, semantic loss forces the semantic transformer to enhance its understanding of text and semantic modalities, thus improving the linguistic understanding ability of CaT-TTS. These results underscore the pivotal role of semantic loss in ensuring accurate semantic information learning, which is essential for maintaining high-fidelity generation of acoustic transformer.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "size",
                    "pgcpoly",
                    "cattts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Audio Parallel Inference.</span>\nTo evaluate the effectiveness of masked parallel inference, we trained CaT-TTS-small in the subset of the collected dataset. We set different parallel streams and evaluated the performance in the PGC-Hard, PGC-Poly, and SeedTTS test-zh-easy dataset. Results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> show the average performance analysis of MAPI parallel streams. The left subfigure shows the speech intelligibility improvement that MAPI brings. The middle subfigure shows that as the number of parallel streams increases, the acoustic performance SIM score and the UTMOS score show an upward trend. To demonstrate the robustness of MAPI, each sample in these datasets will be evaluated 10 times. As can be seen in the right subfigure in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of each inference is more stable in terms of the WER indicator. Due to the parallel computing capability of GPU, MAPI almost does not bring additional time consumption, but as the number of parallel streams increases, the utilization of GPU resources also increases. It is necessary to select the most appropriate number of parallel streams according to the requirements of the actual scenario.</p>\n\n",
                "matched_terms": [
                    "average",
                    "pgchard",
                    "pgcpoly",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span> To train the model, we employ a GAN-based objective with a combination of two discriminators: a multi-period discriminator [18] with periods of [2, 3, 5, 7, 11], and a complex multi-scale STFT discriminator. The STFT discriminator operates on three resolutions with window lengths [2048, 1024, 512] and a hop length of 1/4 the window size, using frequency band splits of [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. The total loss function is a weighted sum of a GAN loss, feature matching loss, a codebook loss, and a multi-resolution reconstruction loss. The reconstruction loss is computed as the L1 distance between the log-mel spectrograms of the original and reconstructed audio over seven different resolutions. These resolutions use window lengths of [32, 64, 128, 256, 512, 1024, 2048] with a corresponding number of mel bands [5, 10, 20, 40, 80, 160, 320], respectively.</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "size",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained with a multi-task objective that jointly optimizes for reconstruction fidelity and semantic alignment. The primary task is reconstruction, which is guided by a GAN-based objective comprising a reconstruction term, a discriminative loss, and an RVQ commitment loss. This is complemented by a semantic distillation task, which introduces an additional loss term to ensure the model&#8217;s representations are linguistically meaningful. In the following, <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> represents an speech signal and <math alttext=\"\\hat{\\mathbf{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{x}}</annotation></semantics></math> denotes the reconstructed signal.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, S3Codec consists of an autoencoder and Residual Vector Quantizer. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. It projects a 16kHz waveform into 1280-dimensional embeddings sampled at 50Hz, while S3Codec projects a 24kHz waveform into 4096-dimensional at 12.5 Hz. During training, we thus downsample the waveforms and project them to the same dimension as targets for distillation. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel. Their outputs will be summed up; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "model",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Configuration. </span> All audio samples are 24kHz. The codec has 8 codebooks, each with 4096 entries. For optimization, we use AdamW optimizer with moving average coefficients <math alttext=\"\\beta_{1}=0.8\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.8</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.8</annotation></semantics></math> and <math alttext=\"\\beta_{2}=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.99</annotation></semantics></math>. The model converges within approximately 900k training steps using a batch size of 128.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Setup.</span> To evaluate the preservation of acoustic information, we employ several metrics. Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a pre-trained speaker verification model. STFT and Mel represent the spectrogram distance between original and reconstructed speech. We also use short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> to measure speech intelligibility and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite> to assess audio quality. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. To demonstrate the semantic alignment, we trained small CaT-TTS models powered by S3Codec and DAC, respectively.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model",
                    "objective",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the capability of semantic preservation of S3Codec, we trained CaT-TTS small powered with S3Codec and DAC, respectively. We use WER as the evaluation metric, representing the speech intelligibility of the generated results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.T5\" title=\"Table 5 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the evaluation results on Seed-TTS test zh easy, PGC-Hard and PGC-Poly. Compared to S3Codec-based system, DAC-based model&#8217;s performance on speech intelligibility has decreased. The reason lies that DAC dose not contain structured linguistic features as S3Codec, which makes the LM model harder to understand, leading to worse performance than S3Codec.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "test",
                    "pgcpoly",
                    "evaluation",
                    "cattts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the CaT-TTS small, the semantic transformer is 8 decoder-only transformer layers, with 1024 model dimension, and the acoustic transformer is 4 decoder-only transformer layers, with 512 model dimension.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">QTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>:</span> Pure Codec based language audio model. A two-stage AR+AR model. RVQ-based two stage speech synthesis modeling.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qtts"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "caption": "Table 4: Objective Evaluation. Comparison between models trained with and without semantic guidance.",
        "body": "Model\nWER(%)↓(\\%)\\downarrow\nSIM↑\\uparrow\n\nUTMOS ↑\\uparrow\n\n\n\nSeedTTS-test\nPGC-Hard\nPGC-Poly\n-\n-\n\n\nCaT-TTS w/o\n3.97\n11.83\n18.34\n0.649\n2.64\n\n\nCaT-TTS\n3.31\n9.74\n16.57\n0.658\n2.78",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"(\\%)\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">&#8595;</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_font_bold\">UTMOS</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">SeedTTS-test</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">PGC-Hard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">PGC-Poly</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">CaT-TTS w/o</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">11.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">18.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.649</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">CaT-TTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">9.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">16.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.658</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.78</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pgchard",
            "model",
            "models",
            "semantic",
            "seedttstest",
            "evaluation",
            "pgcpoly",
            "without",
            "sim↑uparrow",
            "utmos",
            "between",
            "wer↓downarrow",
            "cattts",
            "trained",
            "↑uparrow",
            "guidance",
            "comparison",
            "objective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality UnderStanding.</span> To demonstrate the effectiveness and superiority of the modality understanding loss. We trained two models in sub-dataset with the same architecture but with small model size, and one of them is trained without semantic guidance. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the comparison results. With the loss of semantic guidance removed, this leads to performance decreases, especially with the WER increasing from <math alttext=\"3.31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3.31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.31\\%</annotation></semantics></math> to <math alttext=\"3.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.97\\%</annotation></semantics></math> in SeedTTS-test, <math alttext=\"9.74\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mn>9.74</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.74\\%</annotation></semantics></math> to <math alttext=\"11.83\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>11.83</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.83\\%</annotation></semantics></math> in PGC-Hard and <math alttext=\"16.57\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mn>16.57</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">16.57\\%</annotation></semantics></math> to <math alttext=\"18.34\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mn>18.34</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">18.34\\%</annotation></semantics></math> in PGC-Poly, and the speech quality indicators SIM and UTMOS have also been reduced. During model training, semantic loss forces the semantic transformer to enhance its understanding of text and semantic modalities, thus improving the linguistic understanding ability of CaT-TTS. These results underscore the pivotal role of semantic loss in ensuring accurate semantic information learning, which is essential for maintaining high-fidelity generation of acoustic transformer.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an &#8220;Understand-then-Generate&#8221; dual-Transformer architecture that decouples comprehension from rendering. An initial &#8220;Understanding&#8221; Transformer models the cross-modal relationship between text and the audio&#8217;s semantic tokens to form a high-level utterance plan. A subsequent &#8220;Generation&#8221; Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. Extensive experiments demonstrate that the synergy of our principled architecture and semantically-aware codec allows CaT-TTS to achieve new state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in generation robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "between",
                    "cattts",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM) based autoregressive (AR) models have achieved state-of-the-art quality in zero-shot Text-to-Speech (TTS) with discrete audio representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>. With a few seconds of audio prompt, current TTS models are able to synthesize speech for any given text and mimic the speaker of the audio prompt. Contrary to NAR models &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib26\" title=\"\">2023</a>)</cite>, the sequential nature of AR models, where each acoustic token is conditioned on all its predecessors, naturally captures the long-range temporal dependencies essential for rendering intricate intonation, rhythm, and emotional nuance. This sequential process synergizes perfectly with the in-context learning (ICL) capabilities of LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>)</cite>, providing a powerful mechanism for propagating the fine-grained acoustic characteristics of a voice prompt throughout a newly synthesized utterance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the remarkable progress in LLM-based zero-shot TTS, several fundamental challenges persist. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens, a task handled by a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib23\" title=\"\">2022</a>; Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib7\" title=\"\">2023</a>)</cite>. Semantic tokens, typically derived from discretized self-supervised learning (SSL) models, are considered to exhibit high alignment with text while leading to poor reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib16\" title=\"\">2025</a>)</cite>. In contrast, acoustic tokens often derived from speech codecs trained through residual vector quantization GAN (RVQ-GAN), are recognized for capturing the details of the audio waveform, enabling high-quality synthesis, but lack explicit semantic grounding, forcing the LLM to learn the complex mapping from text to raw acoustic properties from scratch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. We assume that a better audio tokenizer should contain rich semantic information to facilitate an easy understanding of audio content, thus reducing the language model&#8217;s burden in interpreting tokens, and contains acoustic information for speech reconstruction. For better linguistic understanding and acoustic reconstruction, inspired by Mimi codec and SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite>, we propose S3Codec, a split residual vector quantization speech codec with semantic distillation. However, rather than using SSL models, we adopt a pretrained state-of-the-art ASR model for semantic distillation, which we assume brings more explicit linguistic features.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "trained",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that speech synthesis is fundamentally an information-increasing process, where a thorough understanding of the source conditions is a prerequisite for accurate and effective generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib6\" title=\"\">2023</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib41\" title=\"\">2025</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib39\" title=\"\">2024</a>)</cite>. To embody this principle, we propose CaT-TTS, a novel &#8221;Comprehend-and-Talk&#8221; text-to-speech framework, realized through a dual-transformer architecture that explicitly decouples contextual comprehension from acoustic rendering. Our first module, the Semantic Transformer, operates autoregressively on the semantic level. Its sole purpose is to model the rich interplay between the input text and the core semantic content of the voice prompt, building a holistic high-level representation, a latent &#8220;plan&#8221; for the entire utterance. Following this, our second module, the Acoustic Transformer, takes this contextual plan as its foundation and executes the synthesis. It generates the detailed acoustic tokens autoregressively. This design allows the model first to understand &#8220;what&#8221; and &#8220;how&#8221;, and then generates the &#8220;sound&#8221;, which dramatically reduces the modeling burden at each step, leading to more coherent and expressive output.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "between",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our architectural design provides a more stable foundation, the challenge of long sequence lengths in speech remains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib48\" title=\"\">2023b</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib26\" title=\"\">2023</a>)</cite>. Even with our proposed high compression ratio codec, which significantly shortens the acoustic token sequences, the risk of error accumulation persists in any AR system. To overcome this challenge, inspired by Classifier-Free Guidance (CFG) in diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho &amp; Salimans, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib20\" title=\"\">2022</a>)</cite> and Parallel Scaling Laws&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib3\" title=\"\">2025</a>)</cite>, we introduce Masked Audio Parallel Inference (MAPI). It constructs parallel computing streams with different masked audio tokens and aggregates these streams adaptively with learnable weights. This technique acts as a corrective mechanism, steering the model back on track when it begins to &#8220;hallucinate&#8221; and ensuring robust output.</p>\n\n",
                "matched_terms": [
                    "models",
                    "guidance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, we propose a novel zero-shot TTS system CaT-TTS powered by S3Codec. S3Codec encompasses acoustic and semantic information with low bit rates. Based on S3Codec, Cat-TTS embodies an understand and then generate rules via a dual language modeling strategy. To mitigate the error accumulation problem in audio language models, we introduce Masked Audio Parallel Inference strategy, which is beneficial for more robust token generation. Extensive experiments have shown that CaT-TTS has achieved a comparable or superior quality to existing models in terms of speech quality, similarity, and intelligibility.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "models",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Tokenization.</span> The success of autoregressive language models has spurred progress in speech LLMs, where speech tokenizers are essential for converting continuous signals into discrete tokens. Speech tokenizers are typically categorized as acoustic or semantic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib36\" title=\"\">2025a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib42\" title=\"\">2025</a>)</cite>. Acoustic tokens, optimized for signal reconstruction, capture detailed acoustic features beneficial for generation, but perform poorly on understanding tasks like ASR. Previous semantic tokenizers can be trained in two ways: (1) applying clustering or VQ to the representations of self-supervised learning models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite>. (2) applying a VQ layer to the intermediate layer of ASR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">b</a>)</cite>. These semantic tokenizers typically use a single codebook, have a simple architecture, are rich in linguistic information, and are well-suited for LLMs. However, finer-grained acoustic details such as pitch and prosody, are lost, resulting in poor performance on generation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#321;ajszczak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib25\" title=\"\">2024</a>; Betker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib2\" title=\"\">2023</a>)</cite>. An alternative for audio tokenization is to use multi-codebook residual vector quantization (RVQ). In RVQ, an audio frame is represented by a sum of vectors from several quantizers, allowing for high-fidelity reconstruction over a range of bitrates by capturing details that single-codebook models often miss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib8\" title=\"\">2022</a>; Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib45\" title=\"\">2021</a>)</cite>. To align residual speech codec tokens with large text models, recent efforts have explored modeling both semantic and acoustic features simultaneously. SpeechTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib47\" title=\"\">2023a</a>)</cite> enhances the RVQGAN paradigm with semantic distillation to guide the first layer of RVQ to align with a teacher SSL model. X-codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib43\" title=\"\">2025a</a>)</cite> proposes an X-shaped structure where each layer of RVQ contains semantic and acoustic information. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> argues that distilling semantic information into the first level of a single RVQ will trade the auido quality restoration performance of the residual codebooks. Similar to Mimi, we propose S3Codec: a Split RVQ Speech Tokenizer with Semantic Distillation. Unlike Mimi, we adopt DAC architecture with pretrained Whisper for semantic distillation. This approach allows S3Codec to have good acoustic restoration ability and stronger linguistic information.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "trained",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-based Zero-Shot TTS.</span> Inspired by the success of LLM, several recent works adopt language models to model text-to-speech (TTS) tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib4\" title=\"\">2024a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib29\" title=\"\">2024</a>)</cite>. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning to enable zero-shot TTS. VALL-E pioneered treating TTS as a conditional language modeling problem by converting waveforms into neural codec tokens. Spear-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib22\" title=\"\">2023</a>)</cite> integrates multiple AR models to support multispeaker TTS with minimal supervision. Many systems use a single discrete codebook to quantize semantic features&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib37\" title=\"\">2025b</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib44\" title=\"\">2025b</a>)</cite>. Although simple, this bottleneck loses fine acoustic detail&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Recent TTS systems have often combined an AR language model with additional components&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, such as diffusion, to generate more natural, controllable speech when trained on large datasets. While these methods can produce high-quality results, most of them neglect the interactive understanding of speech and text modalities, instead requiring continuous and fine-grained acoustic features for supplementation. Storing and processing such large-scale features is prohibitive, hindering training on hundreds of billions of tokens. In contrast, our approach utilizes a dual-autoregressive structure powered by a split RVQ discretization technique, with the first semantic transformer for modality understanding and the second acoustic transformer for acoustic information generation based on the context guide produced by the semantic transformer. This understand-then-generate paradigm fits the natural flow of speech, takes advantage of the context learning of LLMs, and avoids the need for additional acoustic features for supplementary reconstruction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "trained",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the architecture. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">S3Codec is trained with the combination of reconstruction, semantic distillation and adversarial losses. Reconstruction and adversarial losses can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5\" title=\"Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>. For semantic distillation task, we calculated the cosine distance between the output of the first quantizer and the transformed Whisper embeddings, which is denoted as <math alttext=\"\\cos(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\cos(\\cdot)</annotation></semantics></math>, to perform distillation. Formally, the distillation loss is defined as follows:</p>\n\n",
                "matched_terms": [
                    "trained",
                    "between",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119810;</mi><mi>t</mi><mn>0</mn></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}^{0}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> represents the first encoded embeddings for the frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, <math alttext=\"\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}^{\\mathcal{S}}\\in\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}</annotation></semantics></math> represents the semantic embeddings obtained from the Whisper Encoder, <math alttext=\"\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub></mrow></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\cdot):\\mathbb{R}^{L_{\\mathcal{S}}\\times D_{\\mathcal{S}}}\\to\\mathbb{R}^{L\\times D}</annotation></semantics></math> represents the projection operation that maps whisper latent embedding to the space of audio embedding, <math alttext=\"L_{\\mathcal{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>L</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathcal{S}}</annotation></semantics></math> represents the length of semantic frames, and <math alttext=\"\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mtext>Proj</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119812;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\textrm{Proj}(\\mathbf{E}^{\\mathcal{S}})_{t}</annotation></semantics></math> represents the projected whisper embedding for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. The details of the overall training objective are listed in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A3\" title=\"Appendix C Model architecture and training recipe &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a dataset <math alttext=\"\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>&#119857;</mi><mo>,</mo><mi>&#119858;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}</annotation></semantics></math>, where <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> is an audio sample and <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> is the corresponding text transcription. We use a pre-trained neural codec model to encode each audio sample into discrete codes, denoted as <math alttext=\"\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mtext>S3Codec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math>, where <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the downsampled utterance length. <math alttext=\"\\mathbf{A}_{t}\\in\\mathbb{R}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119808;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{t}\\in\\mathbb{R}^{K}</annotation></semantics></math> represents the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codes for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and <math alttext=\"\\mathcal{A}_{t}^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{k}</annotation></semantics></math> represents the code for the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th codebook of frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Mathematically, given the text prompt <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the speech prompt <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math>, our target is to train a neural language model to generate the discrete code matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> with the optimization objective of maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build such a model, we propose a dual auto-regressive Transformer modeling framework. The dual auto-regressive (AR) Transformer models the residual vector quantization (RVQ) output as a two-level autoregressive process, operating first along the temporal axis and subsequently across codebooks. The core intuition behind this design is to preserve both the causal nature of speech generation and the hierarchical refinement characteristic of RVQ. We denote the first transformer as the semantic transformer, following the causal nature of speech generation and context learning, while the second transformer is the acoustic transformer, modeling the acoustic feature in a coarse-to-fine manner.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The semantic transformer functions as a thinker responsible for processing and understanding the text and the audio modality, and generating high-level representations. Mathematically, let <math alttext=\"\\mathcal{T}\\in\\mathbb{R}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>M</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}\\in\\mathbb{R}^{M}</annotation></semantics></math> represent the tokenized textual prompt, <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math> represent the corresponding speech, and <math alttext=\"\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119808;</mi><mi>i</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}^{i}\\in\\mathbb{R}^{L},i=\\{0,\\cdots,K-1\\}</annotation></semantics></math> represent the speech codes in the i-th codebook, where <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represents the length of the encoded text token and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of the encoded speech token. Given tokenized text prompt and encoded prompt audio codes, the semantic transformer learns the linguistic features of the text <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the discrete acoustic representation of the prompt audio <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m7\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math> and outputs a latent feature <math alttext=\"\\mathbf{H}^{ctx}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m8\" intent=\":literal\"><semantics><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}</annotation></semantics></math> as a guide for the generation of subsequent speech tokens. The optimization objective of the semantic transformer is maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Next Token Embedding Prediction.</span> In order to inject RVQ speech representation into LLM, we sum the codebook dimensions of the multi-codebook parallel sequence. Aggregation brings rich linguistic and acoustic content to the semantic transformer; however, the speech representation of each time step is no longer a quantitative representation. To solve this problem, we propose direct embedding prediction. Instead of predicting discrete token IDs and computing cross-entropy loss, we directly predict the next embedding vector in the continuous semantic space and optimize using Mean Squared Error Loss between predicted and target embeddings. Specifically, our model learns to predict the next semantic embedding as <math alttext=\"\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119815;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119815;</mi><mn>1</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119815;</mi><mn>2</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})</annotation></semantics></math>, where <math alttext=\"\\mathbf{H}^{ctx}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t}</annotation></semantics></math> represents the continuous semantic embedding at position <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To be more task-specific, we denote <math alttext=\"\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>&#8784;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119827;</mi><mo>&#8853;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})</annotation></semantics></math>, where <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>&#119827;</mi><annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation></semantics></math> represents the text modality, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the audio modality, and <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> represents the concatenate operation. We split the high-level representation and focus on speech modality; thus the optimization objective can be formulated as follows:</p>\n\n",
                "matched_terms": [
                    "between",
                    "model",
                    "semantic",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The combination of the semantic transformer and the acoustic transformer can guide the generation of target audio through the understanding of text and speech modalities, which conforms to the objective laws of human speech production. Finally, the overall optimization objective Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.E2\" title=\"In 3.2.1 Problem Formulation &#8227; 3.2 Dual Language Modeling of Audio Tokens &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can be detailed as:</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the uncertainty of each token prediction, especially in speech generation, errors accumulate, which reduces the expressiveness of the generated speech. To address this challenge, inspired by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib3\" title=\"\">2025</a>)</cite>, we introduce Masked Audio Parallel Scaling in the semantic generation module. Specifically, for each prompt token sequence, we duplicate it <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> times and apply a masking strategy to speech tokens separately with a certain probability, resulting in a total of <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> token sequences. The model then produces <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> output sequences, and these <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> candidates are weighted and summed with a learnable weight to produce the final output sequence. Formally, in our speech generation task, the discrete text token embeddings and audio embeddings will be concatenated, resulting in the input embeddings, denoted as <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{L_{in}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{L_{in}\\times D}</annotation></semantics></math>. Specifically, we denote our trained semantic transformer <math alttext=\"\\theta_{\\mathcal{S}}:\\mathbb{R}^{L_{in}\\times D}\\rightarrow\\mathbb{R}^{L_{in}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>L</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta_{\\mathcal{S}}:\\mathbb{R}^{L_{in}\\times D}\\rightarrow\\mathbb{R}^{L_{in}\\times D}</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is the parameter, <math alttext=\"L_{in}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{in}</annotation></semantics></math> is the length of input text and audio embeddings and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the model dimension, the final output is formulated in the following form:</p>\n\n",
                "matched_terms": [
                    "trained",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> To evaluate the performance of S3Codec, we employ several metrics, including SIM, STFT Distance, Mel Distance, short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite>. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. More detailed evaluation set up is listed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span>\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, S3Codec achieves SOTA-comparable performance with a very low frame rate in most evaluation dimensions. S3codec achieves higher SIM scores than MBcodec, Mimi, and SpeechTokenizer with the same codebooks. In terms of the restoration and perception indictors PESQ and STOI, S3codec is comparable to the high bitrates Encodec and DAC-8. At the evaluation dimension of STFT and Mel indicators, S3Codec also performs well among low-bitrate codecs. These results provide preliminary evidence of the model&#8217;s effectiveness in reconstructing speech. As for the semantic evaluation, results in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> demonstrate the superiority of S3Codec.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span> To train the CaT-TTS models, we have amassed a considerable dataset comprising multiple languages. The dataset contains about 200k hours labeled speech, with about <math alttext=\"85\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>85</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">85\\%</annotation></semantics></math> Chinese data and <math alttext=\"15\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">15\\%</annotation></semantics></math> English data. We evaluate our zero-shot TTS models with five benchmarks: (1) Seed-TTS test-en, a test set introduced in Seed-TTS of sample extracted from English public corpora, includes 1,000 samples from the Common Voice dataset. (2) SeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese public corpora, includes 2,020 samples from the DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib18\" title=\"\">2021</a>)</cite> dataset. (3) Seed-TTS test-hard, includes 400 samples that consist of complex Chinese sentences. (4) PGC-Hard, includes 1500 Chinese samples, containing Professionally-Generated Content. (5) PGC-Poly, includes 1500 Chinese samples, containing polyphonic characters. The PGC testset is specially designed to test model generalization on difficult, out-of-domain voices.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "models",
                    "pgcpoly",
                    "cattts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We adopt the word error rate (WER) and speaker similarity (SIM) metrics for objective evaluation. For WER, we employ Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite> and Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib15\" title=\"\">2023</a>)</cite> as the automatic speech recognition engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task to obtain speaker embeddings used to calculate the cosine similarity of speech samples of each test utterance against reference clips. For naturalness, we use SpeechMOS MOS prediction model to calculate UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib33\" title=\"\">2022</a>)</cite> scores for evaluation.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "model",
                    "objective",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span> We compare our models with state-of-the-art zero-shot TTS systems, including Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>, FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib17\" title=\"\">2024</a>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib38\" title=\"\">2024</a>)</cite>, E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib14\" title=\"\">2024</a>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib5\" title=\"\">2024b</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib10\" title=\"\">2024a</a>)</cite>, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib11\" title=\"\">2024b</a>)</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib35\" title=\"\">2023</a>)</cite> and QTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib19\" title=\"\">2025</a>)</cite>. Details of each model can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS2\" title=\"F.2 Baseline Details &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>. In particular, we also compare the performance of SOTA two-stage models, including VALL-E, CosyVoice, CosyVoice 2, QTTS and self-implement AR (Llama)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib13\" title=\"\">2024</a>)</cite> + flow-matching models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib28\" title=\"\">2022</a>)</cite>, where L-CosyVoice50 means Llama backbone with 50 Hz semantic codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> and L-CosyVoice25 means with 25 Hz.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> We train CaT-TTS on 8 NVIDIA H20 96GB GPUs. The parallel stream is set to 4. For more details about the model architecture, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS1\" title=\"F.1 CaT-TTS architecture &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. We optimize the model with the AdamW optimizer with a learning rate of 1e-5 and 20K warm-up steps.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span> To evaluate CaT-TTS&#8217;s zero-shot TTS capatility, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Quantization and Reconstruction Analysis &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As can be seen, CaT-TTS demonstrates a significant superiority in intelligibility for zero-shot TTS scenarios. With WER <math alttext=\"1.56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>1.56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.56\\%</annotation></semantics></math>, <math alttext=\"2.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>2.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.35\\%</annotation></semantics></math> and <math alttext=\"9.75\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>9.75</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.75\\%</annotation></semantics></math> in test-zh, test-en and test-hard, respectively, CaT-TTS achieves best or best comparable performance among these baselines, especially in pure AR based models. In terms of speaking similarity, like the other Pure AR based models, Cat-TTS&#8217;s performance is inferior to NAR-involved models, especially pure NAR models. The reason is that NAR models like F5-TTS generate based on more explicit acoustic features like Mel-Spectrogram, and AR+NAR models typically construct acoustic information with acoustic guidance like speaker similarity vector in the NAR stage. Although with higher indicator performance, we think it may degrade diversity and cause more storage and processing cost during training. To do a further comprehensive comparison on zero-shot TTS performance, we compared recent prominent AR-based two-stage TTS models including VALL-E, CosyVoice, QTTS and reproduced Llama-CosyVoice as baseline models, and testify the synthesis capability in a more real scenarios. The evaluation datasets including PGC-Hard and PGC-Poly, which contain more complex real-life sentences and polyphonetic characters, respectively. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that CaT-TTS has SOTA comparable in-context learning ability. With WER 9.75<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, 7.03<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 13.97<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> in Seed-TTS test-zh-hard, PGC-Hard and PGC-Poly, respectively. Q-TTS and VALL-E are Transformer-based TTS systems powered by codec, which is similar to CaT-TTS. As can be seen, CaT-TTS achieves better performance. Although without additional acoustic information supplement through flow-matching, CaT-TTS has comparable or superior performance in terms of UTMOS and WER, demonstrating the context-learning ability of our system.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "models",
                    "evaluation",
                    "pgcpoly",
                    "without",
                    "utmos",
                    "cattts",
                    "guidance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Audio Parallel Inference.</span>\nTo evaluate the effectiveness of masked parallel inference, we trained CaT-TTS-small in the subset of the collected dataset. We set different parallel streams and evaluated the performance in the PGC-Hard, PGC-Poly, and SeedTTS test-zh-easy dataset. Results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> show the average performance analysis of MAPI parallel streams. The left subfigure shows the speech intelligibility improvement that MAPI brings. The middle subfigure shows that as the number of parallel streams increases, the acoustic performance SIM score and the UTMOS score show an upward trend. To demonstrate the robustness of MAPI, each sample in these datasets will be evaluated 10 times. As can be seen in the right subfigure in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of each inference is more stable in terms of the WER indicator. Due to the parallel computing capability of GPU, MAPI almost does not bring additional time consumption, but as the number of parallel streams increases, the utilization of GPU resources also increases. It is necessary to select the most appropriate number of parallel streams according to the requirements of the actual scenario.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "utmos",
                    "pgchard",
                    "pgcpoly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced CaT-TTS, a novel Text-to-Speech system designed to address key challenges in representation and generation. At its core is S3Codec, a split RVQ codec that resolves the trade-off between reconstruction fidelity and semantic interpretability by injecting linguistic features via ASR-based distillation. Building on this semantically aware representation, we proposed a principled &#8220;Understand-then-Generate&#8221; paradigm, realized through a dual-Transformer architecture that decouples contextual comprehension from acoustic rendering. To complement this, we developed Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that enhances generation stability by dynamically mitigating local decoding errors. Extensive experiments demonstrate that the synergy between our architecture and codec allows CaT-TTS to achieve state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "between",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span> To train the model, we employ a GAN-based objective with a combination of two discriminators: a multi-period discriminator [18] with periods of [2, 3, 5, 7, 11], and a complex multi-scale STFT discriminator. The STFT discriminator operates on three resolutions with window lengths [2048, 1024, 512] and a hop length of 1/4 the window size, using frequency band splits of [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. The total loss function is a weighted sum of a GAN loss, feature matching loss, a codebook loss, and a multi-resolution reconstruction loss. The reconstruction loss is computed as the L1 distance between the log-mel spectrograms of the original and reconstructed audio over seven different resolutions. These resolutions use window lengths of [32, 64, 128, 256, 512, 1024, 2048] with a corresponding number of mel bands [5, 10, 20, 40, 80, 160, 320], respectively.</p>\n\n",
                "matched_terms": [
                    "between",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained with a multi-task objective that jointly optimizes for reconstruction fidelity and semantic alignment. The primary task is reconstruction, which is guided by a GAN-based objective comprising a reconstruction term, a discriminative loss, and an RVQ commitment loss. This is complemented by a semantic distillation task, which introduces an additional loss term to ensure the model&#8217;s representations are linguistically meaningful. In the following, <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> represents an speech signal and <math alttext=\"\\hat{\\mathbf{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{x}}</annotation></semantics></math> denotes the reconstructed signal.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "model",
                    "semantic",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where the mean is computed over all dimensions and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of layers in discriminators.\n<span class=\"ltx_text ltx_font_bold\">RVQ Commitment Loss</span> We add a commitment loss <math alttext=\"\\mathcal{L}_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{w}</annotation></semantics></math> between the pre-quantized value, and\nits quantized value, without gradient computed for the quantized value. The commitment loss is defined as : <math alttext=\"\\mathcal{L}_{w}=\\sum_{i=1}^{N_{q}}||\\mathbf{z}_{i}-\\mathbf{z}_{q_{i}}||\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>w</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo rspace=\"0em\">&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>q</mi></msub></msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><msub><mi>&#119859;</mi><mi>i</mi></msub><mo>&#8722;</mo><msub><mi>&#119859;</mi><msub><mi>q</mi><mi>i</mi></msub></msub></mrow><mo stretchy=\"false\">&#8214;</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{w}=\\sum_{i=1}^{N_{q}}||\\mathbf{z}_{i}-\\mathbf{z}_{q_{i}}||</annotation></semantics></math>, where <math alttext=\"\\mathbf{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i}</annotation></semantics></math> and <math alttext=\"\\mathbf{z}_{q_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m5\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><msub><mi>q</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{q_{i}}</annotation></semantics></math> denote current residual and nearest entry in the corresponding codebook respectively.</p>\n\n",
                "matched_terms": [
                    "between",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To discretize waveforms into audio tokens, we introduce S3Codec, a neural audio codec that operates as an autoencoder with a discrete bottleneck. As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 S3Codec: Split RVQ with Semantic Distillation for Speech Tokenizer &#8227; 3 Method &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows, S3Codec consists of an autoencoder and Residual Vector Quantizer. Based on the DAC architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib24\" title=\"\">2023</a>)</cite>, the encoder projects a single-channel waveform <math alttext=\"\\mathbf{x}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\mathbb{R}^{T}</annotation></semantics></math> to a latent representation <math alttext=\"\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>=</mo><mrow><mtext>enc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119857;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}=\\textrm{enc}(\\mathbf{x})\\in\\mathbb{R}^{L\\times D}</annotation></semantics></math> by cascading residual convolutional blocks that interleave dilated and strided convolutions along with Snake nonlinearities and weight normalizaton, and Quantizer quantize the latent representation to disrete representations <math alttext=\"\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents the length of encoded tokens, <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m6\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the dimension of codebook. Similarly to SpeechTokenizer and Mimi, we distill semantic information into the first level of RVQ. However, instead of using SSL models like HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib21\" title=\"\">2021</a>)</cite> as a semantic teacher, we adopt Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite>, a state-of-the-art model for automatic speech recognition and speech translation whose hidden representation contains rich explicit linguistic features. It projects a 16kHz waveform into 1280-dimensional embeddings sampled at 50Hz, while S3Codec projects a 24kHz waveform into 4096-dimensional at 12.5 Hz. During training, we thus downsample the waveforms and project them to the same dimension as targets for distillation. Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib9\" title=\"\">2024</a>)</cite> found that, while distillation significantly improves the phonetic discriminability of the first quantizer, it also negatively affects the audio quality. To address the issue, we split the RVQ layers in a way similar to Mimi. Rather than a single RVQ with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m7\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> levels, we distill the semantic information into a plain VQ and apply an RVQ with <math alttext=\"K-1\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K-1</annotation></semantics></math> levels in parallel. Their outputs will be summed up; thus the constraint of acoustic information being conserved in the residual of the semantic quantizer is removed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Setup.</span> To evaluate the preservation of acoustic information, we employ several metrics. Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a pre-trained speaker verification model. STFT and Mel represent the spectrogram distance between original and reconstructed speech. We also use short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> to measure speech intelligibility and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite> to assess audio quality. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. To demonstrate the semantic alignment, we trained small CaT-TTS models powered by S3Codec and DAC, respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective",
                    "models",
                    "evaluation",
                    "between",
                    "cattts",
                    "trained",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the capability of semantic preservation of S3Codec, we trained CaT-TTS small powered with S3Codec and DAC, respectively. We use WER as the evaluation metric, representing the speech intelligibility of the generated results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.T5\" title=\"Table 5 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the evaluation results on Seed-TTS test zh easy, PGC-Hard and PGC-Poly. Compared to S3Codec-based system, DAC-based model&#8217;s performance on speech intelligibility has decreased. The reason lies that DAC dose not contain structured linguistic features as S3Codec, which makes the LM model harder to understand, leading to worse performance than S3Codec.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "pgcpoly",
                    "evaluation",
                    "cattts",
                    "trained",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the CaT-TTS small, the semantic transformer is 8 decoder-only transformer layers, with 1024 model dimension, and the acoustic transformer is 4 decoder-only transformer layers, with 512 model dimension.</p>\n\n",
                "matched_terms": [
                    "cattts",
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib1\" title=\"\">2024</a>)</cite>:</span> Hybrid TTS system. A two-stage model that employs an AR LM for semantic token prediction and flow matching for acoustic feature generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib17\" title=\"\">2024</a>)</cite>:</span> Hybrid TTS system. A two-stage model similar to Seed-TTS, using an AR LM for semantic tokens and flow matching for acoustic features.</p>\n\n",
                "matched_terms": [
                    "model",
                    "semantic"
                ]
            }
        ]
    },
    "A5.T5": {
        "source_file": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "caption": "Table 5: Objective Word Error Rate evaluation.",
        "body": "Model\nSeedTTS-test\nPGC-Hard\nPGC-Poly\n\n\n\n\nDAC-Based\n4.21\n12.83\n19.27\n\n\nS3Codec-Based\n3.30\n9.75\n16.53",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">SeedTTS-test</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">PGC-Hard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">PGC-Poly</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">DAC-Based</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">4.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">12.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">19.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">S3Codec-Based</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">9.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">16.53</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pgchard",
            "model",
            "dacbased",
            "seedttstest",
            "s3codecbased",
            "rate",
            "evaluation",
            "pgcpoly",
            "word",
            "error",
            "objective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To demonstrate the capability of semantic preservation of S3Codec, we trained CaT-TTS small powered with S3Codec and DAC, respectively. We use WER as the evaluation metric, representing the speech intelligibility of the generated results. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.T5\" title=\"Table 5 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the evaluation results on Seed-TTS test zh easy, PGC-Hard and PGC-Poly. Compared to S3Codec-based system, DAC-based model&#8217;s performance on speech intelligibility has decreased. The reason lies that DAC dose not contain structured linguistic features as S3Codec, which makes the LM model harder to understand, leading to worse performance than S3Codec.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an &#8220;Understand-then-Generate&#8221; dual-Transformer architecture that decouples comprehension from rendering. An initial &#8220;Understanding&#8221; Transformer models the cross-modal relationship between text and the audio&#8217;s semantic tokens to form a high-level utterance plan. A subsequent &#8220;Generation&#8221; Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors. Extensive experiments demonstrate that the synergy of our principled architecture and semantically-aware codec allows CaT-TTS to achieve new state-of-the-art performance in zero-shot voice cloning, with MAPI providing a measurable boost in generation robustness on benchmark datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our architectural design provides a more stable foundation, the challenge of long sequence lengths in speech remains&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib48\" title=\"\">2023b</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib26\" title=\"\">2023</a>)</cite>. Even with our proposed high compression ratio codec, which significantly shortens the acoustic token sequences, the risk of error accumulation persists in any AR system. To overcome this challenge, inspired by Classifier-Free Guidance (CFG) in diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho &amp; Salimans, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib20\" title=\"\">2022</a>)</cite> and Parallel Scaling Laws&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib3\" title=\"\">2025</a>)</cite>, we introduce Masked Audio Parallel Inference (MAPI). It constructs parallel computing streams with different masked audio tokens and aggregates these streams adaptively with learnable weights. This technique acts as a corrective mechanism, steering the model back on track when it begins to &#8220;hallucinate&#8221; and ensuring robust output.</p>\n\n",
                "matched_terms": [
                    "model",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a dataset <math alttext=\"\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>&#119857;</mi><mo>,</mo><mi>&#119858;</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{\\mathbf{x},\\mathbf{y}\\}</annotation></semantics></math>, where <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> is an audio sample and <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> is the corresponding text transcription. We use a pre-trained neural codec model to encode each audio sample into discrete codes, denoted as <math alttext=\"\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mtext>S3Codec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119858;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\textrm{S3Codec}(\\mathbf{y})=\\mathbf{A}\\in\\mathbb{R}^{K\\times L}</annotation></semantics></math>, where <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> represents the number of codebooks, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the downsampled utterance length. <math alttext=\"\\mathbf{A}_{t}\\in\\mathbb{R}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119808;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}_{t}\\in\\mathbb{R}^{K}</annotation></semantics></math> represents the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codes for frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and <math alttext=\"\\mathcal{A}_{t}^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>t</mi><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{A}_{t}^{k}</annotation></semantics></math> represents the code for the <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-th codebook of frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Mathematically, given the text prompt <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> and the speech prompt <math alttext=\"\\tilde{\\mathbf{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{A}}</annotation></semantics></math>, our target is to train a neural language model to generate the discrete code matrix <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m15\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> with the optimization objective of maximizing the distribution:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Next Token Embedding Prediction.</span> In order to inject RVQ speech representation into LLM, we sum the codebook dimensions of the multi-codebook parallel sequence. Aggregation brings rich linguistic and acoustic content to the semantic transformer; however, the speech representation of each time step is no longer a quantitative representation. To solve this problem, we propose direct embedding prediction. Instead of predicting discrete token IDs and computing cross-entropy loss, we directly predict the next embedding vector in the continuous semantic space and optimize using Mean Squared Error Loss between predicted and target embeddings. Specifically, our model learns to predict the next semantic embedding as <math alttext=\"\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119815;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>&#952;</mi><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119815;</mi><mn>1</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119815;</mi><mn>2</mn><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t+1}=\\theta_{\\mathcal{S}}(\\mathbf{H}^{ctx}_{1},\\mathbf{H}^{ctx}_{2},...,\\mathbf{H}^{ctx}_{t})</annotation></semantics></math>, where <math alttext=\"\\mathbf{H}^{ctx}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>t</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}_{t}</annotation></semantics></math> represents the continuous semantic embedding at position <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To be more task-specific, we denote <math alttext=\"\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119815;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msup><mo>&#8784;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119827;</mi><mo>&#8853;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}^{ctx}\\doteq(\\mathbf{T}\\oplus\\mathbf{S})</annotation></semantics></math>, where <math alttext=\"\\mathbf{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>&#119827;</mi><annotation encoding=\"application/x-tex\">\\mathbf{T}</annotation></semantics></math> represents the text modality, <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> represents the audio modality, and <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> represents the concatenate operation. We split the high-level representation and focus on speech modality; thus the optimization objective can be formulated as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "error",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> To evaluate the performance of S3Codec, we employ several metrics, including SIM, STFT Distance, Mel Distance, short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite>. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. More detailed evaluation set up is listed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span>\nAs shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, S3Codec achieves SOTA-comparable performance with a very low frame rate in most evaluation dimensions. S3codec achieves higher SIM scores than MBcodec, Mimi, and SpeechTokenizer with the same codebooks. In terms of the restoration and perception indictors PESQ and STOI, S3codec is comparable to the high bitrates Encodec and DAC-8. At the evaluation dimension of STFT and Mel indicators, S3Codec also performs well among low-bitrate codecs. These results provide preliminary evidence of the model&#8217;s effectiveness in reconstructing speech. As for the semantic evaluation, results in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.SS2\" title=\"E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a> demonstrate the superiority of S3Codec.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span> To train the CaT-TTS models, we have amassed a considerable dataset comprising multiple languages. The dataset contains about 200k hours labeled speech, with about <math alttext=\"85\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>85</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">85\\%</annotation></semantics></math> Chinese data and <math alttext=\"15\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">15\\%</annotation></semantics></math> English data. We evaluate our zero-shot TTS models with five benchmarks: (1) Seed-TTS test-en, a test set introduced in Seed-TTS of sample extracted from English public corpora, includes 1,000 samples from the Common Voice dataset. (2) SeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese public corpora, includes 2,020 samples from the DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib18\" title=\"\">2021</a>)</cite> dataset. (3) Seed-TTS test-hard, includes 400 samples that consist of complex Chinese sentences. (4) PGC-Hard, includes 1500 Chinese samples, containing Professionally-Generated Content. (5) PGC-Poly, includes 1500 Chinese samples, containing polyphonic characters. The PGC testset is specially designed to test model generalization on difficult, out-of-domain voices.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "model",
                    "pgcpoly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We adopt the word error rate (WER) and speaker similarity (SIM) metrics for objective evaluation. For WER, we employ Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib31\" title=\"\">2023</a>)</cite> and Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib15\" title=\"\">2023</a>)</cite> as the automatic speech recognition engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task to obtain speaker embeddings used to calculate the cosine similarity of speech samples of each test utterance against reference clips. For naturalness, we use SpeechMOS MOS prediction model to calculate UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib33\" title=\"\">2022</a>)</cite> scores for evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rate",
                    "evaluation",
                    "word",
                    "error",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> We train CaT-TTS on 8 NVIDIA H20 96GB GPUs. The parallel stream is set to 4. For more details about the model architecture, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A6.SS1\" title=\"F.1 CaT-TTS architecture &#8227; Appendix F Implementations of CaT-TTS and Baseline Details &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. We optimize the model with the AdamW optimizer with a learning rate of 1e-5 and 20K warm-up steps.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Results.</span> To evaluate CaT-TTS&#8217;s zero-shot TTS capatility, we assess its performance on Seed-TTS-eval and compare it with existing zero-shot TTS models. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Quantization and Reconstruction Analysis &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As can be seen, CaT-TTS demonstrates a significant superiority in intelligibility for zero-shot TTS scenarios. With WER <math alttext=\"1.56\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mn>1.56</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1.56\\%</annotation></semantics></math>, <math alttext=\"2.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mn>2.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.35\\%</annotation></semantics></math> and <math alttext=\"9.75\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><mrow><mn>9.75</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.75\\%</annotation></semantics></math> in test-zh, test-en and test-hard, respectively, CaT-TTS achieves best or best comparable performance among these baselines, especially in pure AR based models. In terms of speaking similarity, like the other Pure AR based models, Cat-TTS&#8217;s performance is inferior to NAR-involved models, especially pure NAR models. The reason is that NAR models like F5-TTS generate based on more explicit acoustic features like Mel-Spectrogram, and AR+NAR models typically construct acoustic information with acoustic guidance like speaker similarity vector in the NAR stage. Although with higher indicator performance, we think it may degrade diversity and cause more storage and processing cost during training. To do a further comprehensive comparison on zero-shot TTS performance, we compared recent prominent AR-based two-stage TTS models including VALL-E, CosyVoice, QTTS and reproduced Llama-CosyVoice as baseline models, and testify the synthesis capability in a more real scenarios. The evaluation datasets including PGC-Hard and PGC-Poly, which contain more complex real-life sentences and polyphonetic characters, respectively. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that CaT-TTS has SOTA comparable in-context learning ability. With WER 9.75<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, 7.03<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 13.97<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> in Seed-TTS test-zh-hard, PGC-Hard and PGC-Poly, respectively. Q-TTS and VALL-E are Transformer-based TTS systems powered by codec, which is similar to CaT-TTS. As can be seen, CaT-TTS achieves better performance. Although without additional acoustic information supplement through flow-matching, CaT-TTS has comparable or superior performance in terms of UTMOS and WER, demonstrating the context-learning ability of our system.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "evaluation",
                    "pgcpoly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Modality UnderStanding.</span> To demonstrate the effectiveness and superiority of the modality understanding loss. We trained two models in sub-dataset with the same architecture but with small model size, and one of them is trained without semantic guidance. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Zero-shot TTS Performance &#8227; 4 Experiments &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the comparison results. With the loss of semantic guidance removed, this leads to performance decreases, especially with the WER increasing from <math alttext=\"3.31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3.31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.31\\%</annotation></semantics></math> to <math alttext=\"3.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.97\\%</annotation></semantics></math> in SeedTTS-test, <math alttext=\"9.74\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mn>9.74</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.74\\%</annotation></semantics></math> to <math alttext=\"11.83\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>11.83</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.83\\%</annotation></semantics></math> in PGC-Hard and <math alttext=\"16.57\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mn>16.57</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">16.57\\%</annotation></semantics></math> to <math alttext=\"18.34\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mn>18.34</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">18.34\\%</annotation></semantics></math> in PGC-Poly, and the speech quality indicators SIM and UTMOS have also been reduced. During model training, semantic loss forces the semantic transformer to enhance its understanding of text and semantic modalities, thus improving the linguistic understanding ability of CaT-TTS. These results underscore the pivotal role of semantic loss in ensuring accurate semantic information learning, which is essential for maintaining high-fidelity generation of acoustic transformer.</p>\n\n",
                "matched_terms": [
                    "seedttstest",
                    "pgchard",
                    "model",
                    "pgcpoly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Audio Parallel Inference.</span>\nTo evaluate the effectiveness of masked parallel inference, we trained CaT-TTS-small in the subset of the collected dataset. We set different parallel streams and evaluated the performance in the PGC-Hard, PGC-Poly, and SeedTTS test-zh-easy dataset. Results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> show the average performance analysis of MAPI parallel streams. The left subfigure shows the speech intelligibility improvement that MAPI brings. The middle subfigure shows that as the number of parallel streams increases, the acoustic performance SIM score and the UTMOS score show an upward trend. To demonstrate the robustness of MAPI, each sample in these datasets will be evaluated 10 times. As can be seen in the right subfigure in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#A5.F4\" title=\"Figure 4 &#8227; E.2 Semantic preservation of S3Codec &#8227; Appendix E Semantic superiority of S3Codec &#8227; Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of each inference is more stable in terms of the WER indicator. Due to the parallel computing capability of GPU, MAPI almost does not bring additional time consumption, but as the number of parallel streams increases, the utilization of GPU resources also increases. It is necessary to select the most appropriate number of parallel streams according to the requirements of the actual scenario.</p>\n\n",
                "matched_terms": [
                    "pgchard",
                    "pgcpoly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span> To train the model, we employ a GAN-based objective with a combination of two discriminators: a multi-period discriminator [18] with periods of [2, 3, 5, 7, 11], and a complex multi-scale STFT discriminator. The STFT discriminator operates on three resolutions with window lengths [2048, 1024, 512] and a hop length of 1/4 the window size, using frequency band splits of [0.0, 0.1, 0.25, 0.5, 0.75, 1.0]. The total loss function is a weighted sum of a GAN loss, feature matching loss, a codebook loss, and a multi-resolution reconstruction loss. The reconstruction loss is computed as the L1 distance between the log-mel spectrograms of the original and reconstructed audio over seven different resolutions. These resolutions use window lengths of [32, 64, 128, 256, 512, 1024, 2048] with a corresponding number of mel bands [5, 10, 20, 40, 80, 160, 320], respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model is trained with a multi-task objective that jointly optimizes for reconstruction fidelity and semantic alignment. The primary task is reconstruction, which is guided by a GAN-based objective comprising a reconstruction term, a discriminative loss, and an RVQ commitment loss. This is complemented by a semantic distillation task, which introduces an additional loss term to ensure the model&#8217;s representations are linguistically meaningful. In the following, <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math> represents an speech signal and <math alttext=\"\\hat{\\mathbf{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{x}}</annotation></semantics></math> denotes the reconstructed signal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Setup.</span> To evaluate the preservation of acoustic information, we employ several metrics. Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a pre-trained speaker verification model. STFT and Mel represent the spectrogram distance between original and reconstructed speech. We also use short-time objective intelligibility (STOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib34\" title=\"\">2010</a>)</cite> to measure speech intelligibility and perceptual evaluation of speech quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib32\" title=\"\">2001</a>)</cite> to assess audio quality. All evaluations were conducted on the LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22062v1#bib.bib30\" title=\"\">2015</a>)</cite> test-clean subset. To demonstrate the semantic alignment, we trained small CaT-TTS models powered by S3Codec and DAC, respectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective",
                    "evaluation"
                ]
            }
        ]
    }
}