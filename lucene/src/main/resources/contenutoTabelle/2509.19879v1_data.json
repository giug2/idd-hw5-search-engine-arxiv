{
    "S2.T1": {
        "caption": "TABLE I: Composition of speakers in COPAS dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Category</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Male</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Female</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Healthy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Dysarthria</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hearing disorder</td>\n<td class=\"ltx_td ltx_align_center\">9</td>\n<td class=\"ltx_td ltx_align_center\">17</td>\n<td class=\"ltx_td ltx_align_center\">26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Laryngectomy</td>\n<td class=\"ltx_td ltx_align_center\">15</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Voice disorder</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">5</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Glossectomy</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">122</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "healthy",
            "hearing",
            "copas",
            "dysarthria",
            "total",
            "disorder",
            "voice",
            "category",
            "speakers",
            "female",
            "composition",
            "dataset",
            "male",
            "glossectomy",
            "laryngectomy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We test the potential of our proposed PLFs for pathological speech analysis by following the training setup described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS1\" title=\"II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-A</span></a> with the CoGeN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib14\" title=\"\">14</a>]</cite> dataset, consisting of speech samples from 174 healthy Dutch speakers. The <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> parameter of equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> is set to 4 for all experiments. Subsequently, we extract the PLFs introduced in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS2\" title=\"II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a> from 122 Dutch speakers from the COPAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib15\" title=\"\">15</a>]</cite> dataset, consisting of 26 normal control speakers and 96 speakers with various speech disorders. See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.T1\" title=\"TABLE I &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> for the COPAS dataset composition. The recorded speech samples consists of the speakers reading the phonetically diverse text 'Papa en Marloes' commonly used in Dutch speech therapy. In addition, an intelligibility score based on the outcome of the Dutch Intelligibility Assessment&#160;(DIA) test&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib16\" title=\"\">16</a>]</cite> is also available.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech pathologies include a wide array of disorders that profoundly influence the ability to communicate. The originating causes are various and include surgical interventions in the vocal tract (e.g. laryngectomy, glossectomy), reduced motor speech capabilities (e.g. dysarthria) or as a developmental side-effect following a hearing disorder.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "disorder",
                    "hearing",
                    "glossectomy",
                    "laryngectomy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This creates a total of <math alttext=\"21\\times 7=147\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>21</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow><mo>=</mo><mn>147</mn></mrow><annotation encoding=\"application/x-tex\">21\\times 7=147</annotation></semantics></math> features in our text-independent PLF histogram. We expect that patients with a speech disorder will not be be able to use the full range of the PLFs, thus showing deviating outcomes from the healthy population. These deviations can subsequently be detected by downstream models for pathological speech analysis. For example, in speakers exhibiting hypernasality, the bin means of the nasal descriptor in a sufficiently long speech utterance are expected to be heavily skewed towards the H-bins.</p>\n\n",
                "matched_terms": [
                    "disorder",
                    "total",
                    "healthy",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the predictive capabilities of our proposed PLFs for pathological speech analysis, we train both an intelligibility prediction and speech pathology classification model using the PLFs from the COPAS dataset. In addition, we report results using other commonly used features in speech pathology analysis. We employ the <span class=\"ltx_text ltx_font_italic\">emobase</span> feature set of the openSMILE library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib17\" title=\"\">17</a>]</cite>, consisting of acoustic properties and corresponding statistical derivatives of the audio signal. While being reasonably interpretable, raw acoustic properties are only of limited relevance for pathological speech analysis. Subsequently, we employ speaker embeddings from the publicly available state-of-the-art ECAPA2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib18\" title=\"\">18</a>]</cite> model. Speaker embeddings are known to capture a wide variety of speaker characterizing components but its individual features are not interpretable. Finally, we use the WER of a state-of-the-art ASR system based on the publicly available XSL-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib19\" title=\"\">19</a>]</cite> architecture, fine-tuned on the Dutch subset of the Mozilla Common Voice dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib20\" title=\"\">20</a>]</cite>. ASR-based metrics provides the possibility of a linguistic analysis of the transcribed text, although in most cases this will be limited to a word-level analysis.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "dataset",
                    "copas"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight in how individual PLFs relate towards speech intelligibility, we also look at the Pearson correlation coefficient&#160;(PCC) of the mean frame-level PLF values of each speech sample in the COPAS dataset, together with the best performing bin feature of the corresponding PLF histogram. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T4\" title=\"TABLE IV &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, with the correlation of the best predictive PLFs depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F4\" title=\"Figure 4 &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, subdivided by speech pathology. Moderate to strong positive correlations (<math alttext=\"r\\geq 0.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8805;</mo><mn>0.4</mn></mrow><annotation encoding=\"application/x-tex\">r\\geq 0.4</annotation></semantics></math>) with intelligibility could be found for the coronal and alveolar PLFs. Both rely on the ability of tongue displacement towards the upper teeth and roof of the mouth, possibly indicating the impact of tongue mobility on speech intelligibility. A moderately strong negative correlation (<math alttext=\"r=-0.44\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.44</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.44</annotation></semantics></math>) was present for the dorsal PLF. Dorsal sounds rely on the back of the tongue which requires less precise movement than coronal phonemes and could be used as as compensation for the disability to produce the latter. In addition, nasality was moderately negatively (<math alttext=\"r=-0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.34</annotation></semantics></math>) associated with intelligibility which corroborates results from clinical research, where increased nasality as determined by perceptual ratings of SLTs and objective measurements using nasometers was associated with reduced intelligibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib21\" title=\"\">21</a>]</cite>. In addition, we observe that the histogram-based features always show higher PCCs than their corresponding mean, indicating the importance of our proposed histogram binning strategy. While a direct comparison between our phonological features and the corresponding measurements by SLTs is missing at the moment, the current results shows the viability and additional value provided by our proposed PLFs for automatic and objective pathological speech analysis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "copas"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: Pathology classification (ACC) and intelligibility prediction (RMSE) performance of features on COPAS dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Features</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">T-I</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Interpretability</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ACC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">RMSE</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Training mean/mode</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">N.A.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.39</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">12.88</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">openSMILE features</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">medium</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ECAPA2 embedding</th>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">low</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center\">9.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">XSL-R WER</th>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">medium</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">8.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PLF PER (<span class=\"ltx_text ltx_font_italic\">ours</span>)</th>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">medium</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">8.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">PLF histogram (<span class=\"ltx_text ltx_font_italic\">ours</span>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">high</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">8.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "embedding",
            "wer",
            "high",
            "copas",
            "low",
            "prediction",
            "training",
            "performance",
            "medium",
            "meanmode",
            "ecapa2",
            "plf",
            "classification",
            "opensmile",
            "interpretability",
            "ours",
            "xslr",
            "features",
            "histogram",
            "acc",
            "pathology",
            "rmse",
            "dataset",
            "intelligibility"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3.T2\" title=\"TABLE II &#8227; III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes the results for pathology classification and intelligibility prediction of our proposed PLFs and other state-of-the-art features, together with a corresponding interpretability rating. For reference, the results of a weak baseline model are also given, which simply predicts the mean intelligibility score and majority pathology class of the training partitions. We observe that the baseline text-independent (T-I) features generally perform slightly worse on intelligibility prediction in comparison to our text-dependent PLF PER and XSL-R WER systems, a result often reported in other research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib5\" title=\"\">5</a>]</cite>. However, the pathology classification accuracy of respectively 59% and 43% for the PLF PER and XSL-R WER systems highlight the limited applicability of ASR transcription-based metrics for broader pathological speech analysis, given the singular nature of those features. In contrast, our proposed text-independent PLF histogram outperforms all features on intelligibility prediction and is only surpassed by the ECAPA2 embeddings on the pathology classification task. However, the PLF histograms offer superior interpretability in comparison to the latent features of the ECAPA2 embeddings, making them an overall more compelling option for pathological speech analysis.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Paralinguistic properties of speech are essential in analyzing and choosing optimal treatment options for patients with speech disorders. However, automatic modeling of these characteristics is difficult due to the lack of labeled speech datasets describing paralinguistic properties, especially at the frame-level. In this paper, we propose a weakly supervised training method which exploits the known acoustic properties of phonemes by training an ASR model with an interpretable frame-level phonological feature bottleneck layer. Subsequently, we assess the viability of these phonological features in speech pathology analysis by developing corresponding models for intelligibility prediction and speech pathology classification. Models using our proposed phonological features perform similar to other state-of-the-art acoustic features on both tasks with a classification accuracy of 75% and a 8.43 RMSE on speech intelligibility prediction. In contrast to others, our phonological features are text-independent and highly interpretable, providing potentially useful insights for speech therapists.</p>\n\n",
                "matched_terms": [
                    "features",
                    "prediction",
                    "classification",
                    "training",
                    "pathology",
                    "rmse",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of pathological speech and the subsequent treatment options are traditionally determined by a subjective assessment of speech- and language-therapists&#160;(SLTs). However, perceptual evaluation of speech by SLTs is error-prone and resource intensive, contributing towards the interest of automatic and objective speech evaluation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib1\" title=\"\">1</a>]</cite>. In this regard, methods relying on automatic speech recognition (ASR) models have proven effective, as objective speech intelligibility metrics based on ASR transcription accuracy have shown to correlate strongly with the perceptual evaluation of speech therapists&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib4\" title=\"\">4</a>]</cite>. However, some major disadvantages are still present in ASR-based systems: the method requires a reference text and interpretability of the results is constrained to a linguistic analysis of the transcription. In addition, ASR-based metrics have only shown satisfactory results on the intelligibility regression task, covering only a small area of speech pathology analysis. Some promising results have been established by using latent representations of speech from deep neural networks (DNNs) for both pathology classification and intelligibility prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib6\" title=\"\">6</a>]</cite>. For example, speaker embeddings derived from the popular x-vector&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib7\" title=\"\">7</a>]</cite> or ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib8\" title=\"\">8</a>]</cite> models are commonly used as a text-independent alternatives for ASR-based metrics. However, models employing speaker embeddings have limited interpretability, greatly reducing their usefulness in a clinical context.</p>\n\n",
                "matched_terms": [
                    "prediction",
                    "classification",
                    "pathology",
                    "interpretability",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate these issues, we propose a weakly supervised training method to extract frame-level phonological features (PLFs) from speech which combines the high predictive capacity of modern DNN-based features with high interpretability, essential for proper pathological speech analysis. Initially, the extraction of frame-level phonetic properties was introduced to improve the performance of downstream phoneme recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib10\" title=\"\">10</a>]</cite>. Some initial attempts have been made to use these phonetic properties for subsequent usage in pathological speech analysis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib12\" title=\"\">12</a>]</cite> but their application was limited to intelligibility prediction and lacks a comparison with current state-of-the-art ASR- and DNN-based features. To address these shortcomings, we establish the feasibility of our proposed PLFs for both intelligibility prediction and speech pathology classification and provide a comparison with current state-of-the-art features commonly used in the literature. In addition, we compare the behavior of individual PLFs with current clinical research to asses their potential as an objective paralinguistic measurement.</p>\n\n",
                "matched_terms": [
                    "features",
                    "prediction",
                    "high",
                    "classification",
                    "training",
                    "pathology",
                    "interpretability",
                    "intelligibility",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rest of the paper is organized as follows: section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2\" title=\"II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes our proposed weakly supervised PLF training setup and utterance-level feature extraction methods. Subsequently, section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3\" title=\"III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> introduces our experimental setup for intelligibility prediction and pathology classification using the proposed PLFs. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4\" title=\"IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> continues with the corresponding results and analysis and is followed by the concluding remarks in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S5\" title=\"V Conclusion &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>.</p>\n\n",
                "matched_terms": [
                    "plf",
                    "prediction",
                    "classification",
                    "training",
                    "pathology",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we will describe the two major stages in our proposed PLF framework. The first stage is training the frame-level PLFs with the guidance of the phonetic transcription of the training dataset and is depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The second stage converts the frame-level PLFs to an utterance-level representation for usage in downstream modeling tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "plf",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed<span class=\"ltx_note ltx_role_footnotetext\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">&#8224;</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">&#8224;</sup><span class=\"ltx_note_type\">footnotetext: </span>Supported by Research Foundation Flanders (FWO) grant S004923N and EU Horizon 2020 programme TAPAS under Marie Curie grant 766287.</span></span></span> PLF training framework consists of a front-end feature extractor, which transforms the input audio to latent acoustic frame-level embeddings, followed by a PLF back-end, projecting the acoustic embeddings to <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> phonological features which are subsequently used to classify <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> phonemes. The input features for the front-end module consist of 24-dimensional Mel-spectrograms extracted from the input audio with a 32 ms window length and a 10 ms frame shift. Subsequently, SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib13\" title=\"\">13</a>]</cite> is applied to prevent overfitting. The front-end feature extractor is composed of stacked 2D-convolutions followed by a fully-connected layer, transforming the frame-level features to 512-dimensional acoustic embeddings. More details of the front-end module are depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "features",
                    "plf",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the PLF-backend is guided by three paths with distinct losses to model robust phonological features. The first path projects each acoustic embedding to the PLF vector <math alttext=\"\\boldsymbol{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119959;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{v}</annotation></semantics></math> <math alttext=\"\\in\\mathbb{R}^{F\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\in\\mathbb{R}^{F\\times 1}</annotation></semantics></math>, representing the logits of the phonological features. Subsequently, the PLF logits are converted to phone posterior probabilities by using the PLF-to-phone conversion matrix <math alttext=\"\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119924;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}</annotation></semantics></math> as depicted in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The values in the conversion matrix represents the expected response of our 21 defined PLFs for each phone and can range from <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> (not active) to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (active), with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><mn>0</mn></math> representing irrelevancy. Subsequently, we calculate the posterior probability <math alttext=\"P(f\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(f\\mid p)</annotation></semantics></math> of a PLF <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> having the expected value for each phoneme <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as follows:</p>\n\n",
                "matched_terms": [
                    "features",
                    "embedding",
                    "plf",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All PLFs are updated independently during training except the PLFs related to horizontal (front, central or back) and vertical (high, mid or low) vowel position (indicated by the striped regions in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For these two grouped PLFs, we take a weighted sum of the posterior probabilities of their corresponding attributes as calculated in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E1\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, with the weight being determined by their matching <math alttext=\"M_{p,f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>M</mi><mrow><mi>p</mi><mo>,</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{p,f}</annotation></semantics></math> value. This allows us to define a more granular vowel position needed to model all phonemes correctly. Additionally, it also enables the ability to model positional vowel transitions within a single phoneme, as is present in e.g. diphthongs. Subsequently, we estimate the posterior log probability <math alttext=\"log(P(p\\mid\\boldsymbol{v}))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>&#8739;</mo><mi>&#119959;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">log(P(p\\mid\\boldsymbol{v}))</annotation></semantics></math> of phone <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as:</p>\n\n",
                "matched_terms": [
                    "high",
                    "low",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, a third path simply calculates the cross-entropy from the predicted phone logits derived directly from the acoustic embedding. This is reminiscent of a traditional ASR training setup and mainly serves to ensure a robust acoustic embedding.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our training setup of the PLFs includes phone predictions, it is possible to calculate the phone error rate&#160;(PER) if a ground-truth phonetic transcription is available. This measurement is analogues to metrics based on the word error rate&#160;(WER) of current ASR models, which are often employed in pathological speech analysis for intelligibility prediction. We also include the subcomponents of the PER (insertion, deletion and substitution rate) to allow for a more expressive feature. The phone posterior are derived from the PLF-to-phone conversion in path 1 depicted Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "prediction",
                    "intelligibility",
                    "training",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the requirement of a phonetic transcription poses a limit on the applicability of the PLF phone error rate feature, we also introduce a text-independent PLF histogram feature. For each row corresponding to a PLF in <math alttext=\"\\boldsymbol{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119933;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{V}</annotation></semantics></math>, we calculate a 20-bin normalized histogram. The mean value of the three lowest (L0 - L2) and highest bins (H2 - H0) are used as our PLF histogram features, with the addition of a middle bin (M) represented by the remaining mass:</p>\n\n",
                "matched_terms": [
                    "histogram",
                    "features",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This creates a total of <math alttext=\"21\\times 7=147\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>21</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow><mo>=</mo><mn>147</mn></mrow><annotation encoding=\"application/x-tex\">21\\times 7=147</annotation></semantics></math> features in our text-independent PLF histogram. We expect that patients with a speech disorder will not be be able to use the full range of the PLFs, thus showing deviating outcomes from the healthy population. These deviations can subsequently be detected by downstream models for pathological speech analysis. For example, in speakers exhibiting hypernasality, the bin means of the nasal descriptor in a sufficiently long speech utterance are expected to be heavily skewed towards the H-bins.</p>\n\n",
                "matched_terms": [
                    "histogram",
                    "features",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test the potential of our proposed PLFs for pathological speech analysis by following the training setup described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS1\" title=\"II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-A</span></a> with the CoGeN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib14\" title=\"\">14</a>]</cite> dataset, consisting of speech samples from 174 healthy Dutch speakers. The <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> parameter of equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> is set to 4 for all experiments. Subsequently, we extract the PLFs introduced in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS2\" title=\"II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a> from 122 Dutch speakers from the COPAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib15\" title=\"\">15</a>]</cite> dataset, consisting of 26 normal control speakers and 96 speakers with various speech disorders. See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.T1\" title=\"TABLE I &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> for the COPAS dataset composition. The recorded speech samples consists of the speakers reading the phonetically diverse text 'Papa en Marloes' commonly used in Dutch speech therapy. In addition, an intelligibility score based on the outcome of the Dutch Intelligibility Assessment&#160;(DIA) test&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib16\" title=\"\">16</a>]</cite> is also available.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "intelligibility",
                    "training",
                    "copas"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the predictive capabilities of our proposed PLFs for pathological speech analysis, we train both an intelligibility prediction and speech pathology classification model using the PLFs from the COPAS dataset. In addition, we report results using other commonly used features in speech pathology analysis. We employ the <span class=\"ltx_text ltx_font_italic\">emobase</span> feature set of the openSMILE library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib17\" title=\"\">17</a>]</cite>, consisting of acoustic properties and corresponding statistical derivatives of the audio signal. While being reasonably interpretable, raw acoustic properties are only of limited relevance for pathological speech analysis. Subsequently, we employ speaker embeddings from the publicly available state-of-the-art ECAPA2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib18\" title=\"\">18</a>]</cite> model. Speaker embeddings are known to capture a wide variety of speaker characterizing components but its individual features are not interpretable. Finally, we use the WER of a state-of-the-art ASR system based on the publicly available XSL-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib19\" title=\"\">19</a>]</cite> architecture, fine-tuned on the Dutch subset of the Mozilla Common Voice dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib20\" title=\"\">20</a>]</cite>. ASR-based metrics provides the possibility of a linguistic analysis of the transcribed text, although in most cases this will be limited to a word-level analysis.</p>\n\n",
                "matched_terms": [
                    "xslr",
                    "features",
                    "copas",
                    "wer",
                    "ecapa2",
                    "prediction",
                    "classification",
                    "pathology",
                    "dataset",
                    "opensmile",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the small size of the dataset, we employ a five-fold cross-validation setup. 20% of the training part of each fold is used as a validation set to tune the hyperparameters of the predictive models using grid search. We treat the model type as an additional hyperparameter to ensure a fair comparison across all input features. The model types included are: linear/logistic regression, support vector machines, decision trees and multi-layer perceptrons. Once the optimal set of hyperparameters is determined, the model is retrained using the complete training partition of the fold and evaluated on the test partition. The mean accuracy on the test partitions is reported for pathology classification with the root mean square error (RMSE) given for intelligibility prediction, with scores ranging from 0 to 100.</p>\n\n",
                "matched_terms": [
                    "features",
                    "prediction",
                    "classification",
                    "training",
                    "pathology",
                    "rmse",
                    "dataset",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the impact of individual components of our PLF training setup, we perform an ablation study with the results given in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T3\" title=\"TABLE III &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. When we disable training of the learnable scalar matrix <math alttext=\"\\boldsymbol{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mi>&#119930;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{S}</annotation></semantics></math>, the intelligibility RMSE degrades with 6.4% relative on average. This indicates that our system benefits from deviating from the strict mapping of the fixed conversion matrix <math alttext=\"\\boldsymbol{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{M}</annotation></semantics></math>. Similarly, the 3.8% RMSE relative degradation observed when we disable the direct phone classification path hints towards the benefits of a robust acoustic embedding to guide the learning of the PLFs.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "plf",
                    "classification",
                    "training",
                    "rmse",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight in how individual PLFs relate towards speech intelligibility, we also look at the Pearson correlation coefficient&#160;(PCC) of the mean frame-level PLF values of each speech sample in the COPAS dataset, together with the best performing bin feature of the corresponding PLF histogram. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T4\" title=\"TABLE IV &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, with the correlation of the best predictive PLFs depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F4\" title=\"Figure 4 &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, subdivided by speech pathology. Moderate to strong positive correlations (<math alttext=\"r\\geq 0.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8805;</mo><mn>0.4</mn></mrow><annotation encoding=\"application/x-tex\">r\\geq 0.4</annotation></semantics></math>) with intelligibility could be found for the coronal and alveolar PLFs. Both rely on the ability of tongue displacement towards the upper teeth and roof of the mouth, possibly indicating the impact of tongue mobility on speech intelligibility. A moderately strong negative correlation (<math alttext=\"r=-0.44\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.44</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.44</annotation></semantics></math>) was present for the dorsal PLF. Dorsal sounds rely on the back of the tongue which requires less precise movement than coronal phonemes and could be used as as compensation for the disability to produce the latter. In addition, nasality was moderately negatively (<math alttext=\"r=-0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.34</annotation></semantics></math>) associated with intelligibility which corroborates results from clinical research, where increased nasality as determined by perceptual ratings of SLTs and objective measurements using nasometers was associated with reduced intelligibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib21\" title=\"\">21</a>]</cite>. In addition, we observe that the histogram-based features always show higher PCCs than their corresponding mean, indicating the importance of our proposed histogram binning strategy. While a direct comparison between our phonological features and the corresponding measurements by SLTs is missing at the moment, the current results shows the viability and additional value provided by our proposed PLFs for automatic and objective pathological speech analysis.</p>\n\n",
                "matched_terms": [
                    "features",
                    "copas",
                    "plf",
                    "histogram",
                    "pathology",
                    "dataset",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented weakly supervised phonological features for interpretable and objective pathological speech analysis. Experiments showed that the modeling capabilities of the proposed PLFs are competitive with other state-of-the-art features while being text-independent and providing superior interpretability. In addition, our initial analysis shows promising behavior of our PLFs in comparison to the existing clinical literature regarding the relationship between intelligibility and speech characteristics.</p>\n\n",
                "matched_terms": [
                    "intelligibility",
                    "interpretability",
                    "features"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Ablation experiments of proposed PLF training setup.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Experiment</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">PLF PER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">PLF histogram</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">RMSE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">RMSE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">No learnable conversion matrix</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">No direct phone classification</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">9.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8.58</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "phone",
            "proposed",
            "learnable",
            "setup",
            "plf",
            "iii",
            "ablation",
            "histogram",
            "experiment",
            "acc",
            "conversion",
            "matrix",
            "training",
            "rmse",
            "classification",
            "experiments",
            "baseline",
            "direct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To quantify the impact of individual components of our PLF training setup, we perform an ablation study with the results given in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T3\" title=\"TABLE III &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. When we disable training of the learnable scalar matrix <math alttext=\"\\boldsymbol{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mi>&#119930;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{S}</annotation></semantics></math>, the intelligibility RMSE degrades with 6.4% relative on average. This indicates that our system benefits from deviating from the strict mapping of the fixed conversion matrix <math alttext=\"\\boldsymbol{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{M}</annotation></semantics></math>. Similarly, the 3.8% RMSE relative degradation observed when we disable the direct phone classification path hints towards the benefits of a robust acoustic embedding to guide the learning of the PLFs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Paralinguistic properties of speech are essential in analyzing and choosing optimal treatment options for patients with speech disorders. However, automatic modeling of these characteristics is difficult due to the lack of labeled speech datasets describing paralinguistic properties, especially at the frame-level. In this paper, we propose a weakly supervised training method which exploits the known acoustic properties of phonemes by training an ASR model with an interpretable frame-level phonological feature bottleneck layer. Subsequently, we assess the viability of these phonological features in speech pathology analysis by developing corresponding models for intelligibility prediction and speech pathology classification. Models using our proposed phonological features perform similar to other state-of-the-art acoustic features on both tasks with a classification accuracy of 75% and a 8.43 RMSE on speech intelligibility prediction. In contrast to others, our phonological features are text-independent and highly interpretable, providing potentially useful insights for speech therapists.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "proposed",
                    "rmse",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate these issues, we propose a weakly supervised training method to extract frame-level phonological features (PLFs) from speech which combines the high predictive capacity of modern DNN-based features with high interpretability, essential for proper pathological speech analysis. Initially, the extraction of frame-level phonetic properties was introduced to improve the performance of downstream phoneme recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib10\" title=\"\">10</a>]</cite>. Some initial attempts have been made to use these phonetic properties for subsequent usage in pathological speech analysis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib12\" title=\"\">12</a>]</cite> but their application was limited to intelligibility prediction and lacks a comparison with current state-of-the-art ASR- and DNN-based features. To address these shortcomings, we establish the feasibility of our proposed PLFs for both intelligibility prediction and speech pathology classification and provide a comparison with current state-of-the-art features commonly used in the literature. In addition, we compare the behavior of individual PLFs with current clinical research to asses their potential as an objective paralinguistic measurement.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "proposed",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rest of the paper is organized as follows: section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2\" title=\"II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes our proposed weakly supervised PLF training setup and utterance-level feature extraction methods. Subsequently, section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3\" title=\"III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> introduces our experimental setup for intelligibility prediction and pathology classification using the proposed PLFs. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4\" title=\"IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> continues with the corresponding results and analysis and is followed by the concluding remarks in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S5\" title=\"V Conclusion &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "setup",
                    "plf",
                    "iii",
                    "classification",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we will describe the two major stages in our proposed PLF framework. The first stage is training the frame-level PLFs with the guidance of the phonetic transcription of the training dataset and is depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The second stage converts the frame-level PLFs to an utterance-level representation for usage in downstream modeling tasks.</p>\n\n",
                "matched_terms": [
                    "plf",
                    "proposed",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed<span class=\"ltx_note ltx_role_footnotetext\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">&#8224;</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">&#8224;</sup><span class=\"ltx_note_type\">footnotetext: </span>Supported by Research Foundation Flanders (FWO) grant S004923N and EU Horizon 2020 programme TAPAS under Marie Curie grant 766287.</span></span></span> PLF training framework consists of a front-end feature extractor, which transforms the input audio to latent acoustic frame-level embeddings, followed by a PLF back-end, projecting the acoustic embeddings to <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> phonological features which are subsequently used to classify <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> phonemes. The input features for the front-end module consist of 24-dimensional Mel-spectrograms extracted from the input audio with a 32 ms window length and a 10 ms frame shift. Subsequently, SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib13\" title=\"\">13</a>]</cite> is applied to prevent overfitting. The front-end feature extractor is composed of stacked 2D-convolutions followed by a fully-connected layer, transforming the frame-level features to 512-dimensional acoustic embeddings. More details of the front-end module are depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "plf",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the PLF-backend is guided by three paths with distinct losses to model robust phonological features. The first path projects each acoustic embedding to the PLF vector <math alttext=\"\\boldsymbol{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119959;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{v}</annotation></semantics></math> <math alttext=\"\\in\\mathbb{R}^{F\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\in\\mathbb{R}^{F\\times 1}</annotation></semantics></math>, representing the logits of the phonological features. Subsequently, the PLF logits are converted to phone posterior probabilities by using the PLF-to-phone conversion matrix <math alttext=\"\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119924;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}</annotation></semantics></math> as depicted in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The values in the conversion matrix represents the expected response of our 21 defined PLFs for each phone and can range from <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> (not active) to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (active), with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><mn>0</mn></math> representing irrelevancy. Subsequently, we calculate the posterior probability <math alttext=\"P(f\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(f\\mid p)</annotation></semantics></math> of a PLF <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> having the expected value for each phoneme <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as follows:</p>\n\n",
                "matched_terms": [
                    "phone",
                    "plf",
                    "matrix",
                    "conversion",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All PLFs are updated independently during training except the PLFs related to horizontal (front, central or back) and vertical (high, mid or low) vowel position (indicated by the striped regions in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For these two grouped PLFs, we take a weighted sum of the posterior probabilities of their corresponding attributes as calculated in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E1\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, with the weight being determined by their matching <math alttext=\"M_{p,f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>M</mi><mrow><mi>p</mi><mo>,</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{p,f}</annotation></semantics></math> value. This allows us to define a more granular vowel position needed to model all phonemes correctly. Additionally, it also enables the ability to model positional vowel transitions within a single phoneme, as is present in e.g. diphthongs. Subsequently, we estimate the posterior log probability <math alttext=\"log(P(p\\mid\\boldsymbol{v}))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>&#8739;</mo><mi>&#119959;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">log(P(p\\mid\\boldsymbol{v}))</annotation></semantics></math> of phone <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as:</p>\n\n",
                "matched_terms": [
                    "phone",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The PLF-to-phone conversion matrix imposes a strict and manually determined mapping between PLFs and phonemes. Therefore, we allow in a second path small deviations from <math alttext=\"\\boldsymbol{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{M}</annotation></semantics></math> by employing a trainable scaling matrix <math alttext=\"\\boldsymbol{S}~\\in~\\mathbb{R}^{P\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#119930;</mi><mo lspace=\"0.608em\" rspace=\"0.608em\">&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{S}~\\in~\\mathbb{R}^{P\\times F}</annotation></semantics></math> with <math alttext=\"\\forall s_{pf}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0.167em\">&#8704;</mo><msub><mi>s</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\forall s_{pf}&gt;0</annotation></semantics></math>. Subsequently, the phone posteriors are calculated in equations&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E1\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> using <math alttext=\"\\boldsymbol{M}~\\cdot~\\boldsymbol{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>&#119924;</mi><mo lspace=\"0.552em\" rspace=\"0.552em\">&#8901;</mo><mi>&#119930;</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{M}~\\cdot~\\boldsymbol{S}</annotation></semantics></math> as the conversion matrix. By only allowing positive scalars, the expected presence of a PLF for a certain phoneme according to <math alttext=\"\\boldsymbol{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{M}</annotation></semantics></math> is preserved while permitting the weight to change. Additionally, a trainable offset and scaling factor is employed on the final log phone posteriors to compensate for the lack of prior phoneme probabilities and an incorrect feature independency assumption, respectively.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "plf",
                    "matrix",
                    "conversion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, a third path simply calculates the cross-entropy from the predicted phone logits derived directly from the acoustic embedding. This is reminiscent of a traditional ASR training setup and mainly serves to ensure a robust acoustic embedding.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "setup",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our training setup of the PLFs includes phone predictions, it is possible to calculate the phone error rate&#160;(PER) if a ground-truth phonetic transcription is available. This measurement is analogues to metrics based on the word error rate&#160;(WER) of current ASR models, which are often employed in pathological speech analysis for intelligibility prediction. We also include the subcomponents of the PER (insertion, deletion and substitution rate) to allow for a more expressive feature. The phone posterior are derived from the PLF-to-phone conversion in path 1 depicted Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "phone",
                    "setup",
                    "conversion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the requirement of a phonetic transcription poses a limit on the applicability of the PLF phone error rate feature, we also introduce a text-independent PLF histogram feature. For each row corresponding to a PLF in <math alttext=\"\\boldsymbol{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119933;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{V}</annotation></semantics></math>, we calculate a 20-bin normalized histogram. The mean value of the three lowest (L0 - L2) and highest bins (H2 - H0) are used as our PLF histogram features, with the addition of a middle bin (M) represented by the remaining mass:</p>\n\n",
                "matched_terms": [
                    "histogram",
                    "phone",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This creates a total of <math alttext=\"21\\times 7=147\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>21</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow><mo>=</mo><mn>147</mn></mrow><annotation encoding=\"application/x-tex\">21\\times 7=147</annotation></semantics></math> features in our text-independent PLF histogram. We expect that patients with a speech disorder will not be be able to use the full range of the PLFs, thus showing deviating outcomes from the healthy population. These deviations can subsequently be detected by downstream models for pathological speech analysis. For example, in speakers exhibiting hypernasality, the bin means of the nasal descriptor in a sufficiently long speech utterance are expected to be heavily skewed towards the H-bins.</p>\n\n",
                "matched_terms": [
                    "histogram",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test the potential of our proposed PLFs for pathological speech analysis by following the training setup described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS1\" title=\"II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-A</span></a> with the CoGeN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib14\" title=\"\">14</a>]</cite> dataset, consisting of speech samples from 174 healthy Dutch speakers. The <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> parameter of equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> is set to 4 for all experiments. Subsequently, we extract the PLFs introduced in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS2\" title=\"II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a> from 122 Dutch speakers from the COPAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib15\" title=\"\">15</a>]</cite> dataset, consisting of 26 normal control speakers and 96 speakers with various speech disorders. See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.T1\" title=\"TABLE I &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> for the COPAS dataset composition. The recorded speech samples consists of the speakers reading the phonetically diverse text 'Papa en Marloes' commonly used in Dutch speech therapy. In addition, an intelligibility score based on the outcome of the Dutch Intelligibility Assessment&#160;(DIA) test&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib16\" title=\"\">16</a>]</cite> is also available.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "setup",
                    "proposed",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the predictive capabilities of our proposed PLFs for pathological speech analysis, we train both an intelligibility prediction and speech pathology classification model using the PLFs from the COPAS dataset. In addition, we report results using other commonly used features in speech pathology analysis. We employ the <span class=\"ltx_text ltx_font_italic\">emobase</span> feature set of the openSMILE library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib17\" title=\"\">17</a>]</cite>, consisting of acoustic properties and corresponding statistical derivatives of the audio signal. While being reasonably interpretable, raw acoustic properties are only of limited relevance for pathological speech analysis. Subsequently, we employ speaker embeddings from the publicly available state-of-the-art ECAPA2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib18\" title=\"\">18</a>]</cite> model. Speaker embeddings are known to capture a wide variety of speaker characterizing components but its individual features are not interpretable. Finally, we use the WER of a state-of-the-art ASR system based on the publicly available XSL-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib19\" title=\"\">19</a>]</cite> architecture, fine-tuned on the Dutch subset of the Mozilla Common Voice dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib20\" title=\"\">20</a>]</cite>. ASR-based metrics provides the possibility of a linguistic analysis of the transcribed text, although in most cases this will be limited to a word-level analysis.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the small size of the dataset, we employ a five-fold cross-validation setup. 20% of the training part of each fold is used as a validation set to tune the hyperparameters of the predictive models using grid search. We treat the model type as an additional hyperparameter to ensure a fair comparison across all input features. The model types included are: linear/logistic regression, support vector machines, decision trees and multi-layer perceptrons. Once the optimal set of hyperparameters is determined, the model is retrained using the complete training partition of the fold and evaluated on the test partition. The mean accuracy on the test partitions is reported for pathology classification with the root mean square error (RMSE) given for intelligibility prediction, with scores ranging from 0 to 100.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "setup",
                    "training",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3.T2\" title=\"TABLE II &#8227; III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes the results for pathology classification and intelligibility prediction of our proposed PLFs and other state-of-the-art features, together with a corresponding interpretability rating. For reference, the results of a weak baseline model are also given, which simply predicts the mean intelligibility score and majority pathology class of the training partitions. We observe that the baseline text-independent (T-I) features generally perform slightly worse on intelligibility prediction in comparison to our text-dependent PLF PER and XSL-R WER systems, a result often reported in other research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib5\" title=\"\">5</a>]</cite>. However, the pathology classification accuracy of respectively 59% and 43% for the PLF PER and XSL-R WER systems highlight the limited applicability of ASR transcription-based metrics for broader pathological speech analysis, given the singular nature of those features. In contrast, our proposed text-independent PLF histogram outperforms all features on intelligibility prediction and is only surpassed by the ECAPA2 embeddings on the pathology classification task. However, the PLF histograms offer superior interpretability in comparison to the latent features of the ECAPA2 embeddings, making them an overall more compelling option for pathological speech analysis.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "plf",
                    "histogram",
                    "classification",
                    "training",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain insight in how individual PLFs relate towards speech intelligibility, we also look at the Pearson correlation coefficient&#160;(PCC) of the mean frame-level PLF values of each speech sample in the COPAS dataset, together with the best performing bin feature of the corresponding PLF histogram. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T4\" title=\"TABLE IV &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, with the correlation of the best predictive PLFs depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F4\" title=\"Figure 4 &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, subdivided by speech pathology. Moderate to strong positive correlations (<math alttext=\"r\\geq 0.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8805;</mo><mn>0.4</mn></mrow><annotation encoding=\"application/x-tex\">r\\geq 0.4</annotation></semantics></math>) with intelligibility could be found for the coronal and alveolar PLFs. Both rely on the ability of tongue displacement towards the upper teeth and roof of the mouth, possibly indicating the impact of tongue mobility on speech intelligibility. A moderately strong negative correlation (<math alttext=\"r=-0.44\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.44</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.44</annotation></semantics></math>) was present for the dorsal PLF. Dorsal sounds rely on the back of the tongue which requires less precise movement than coronal phonemes and could be used as as compensation for the disability to produce the latter. In addition, nasality was moderately negatively (<math alttext=\"r=-0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.34</annotation></semantics></math>) associated with intelligibility which corroborates results from clinical research, where increased nasality as determined by perceptual ratings of SLTs and objective measurements using nasometers was associated with reduced intelligibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib21\" title=\"\">21</a>]</cite>. In addition, we observe that the histogram-based features always show higher PCCs than their corresponding mean, indicating the importance of our proposed histogram binning strategy. While a direct comparison between our phonological features and the corresponding measurements by SLTs is missing at the moment, the current results shows the viability and additional value provided by our proposed PLFs for automatic and objective pathological speech analysis.</p>\n\n",
                "matched_terms": [
                    "histogram",
                    "plf",
                    "proposed",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented weakly supervised phonological features for interpretable and objective pathological speech analysis. Experiments showed that the modeling capabilities of the proposed PLFs are competitive with other state-of-the-art features while being text-independent and providing superior interpretability. In addition, our initial analysis shows promising behavior of our PLFs in comparison to the existing clinical literature regarding the relationship between intelligibility and speech characteristics.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "proposed"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: Intelligibility PCC of mean PLF value and best histogram bin.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">PLF</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Mean</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Bin</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Nr</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">PLF</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Mean</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Bin</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Nr</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Coronal</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">0.50</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.60</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Dorsal</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">-0.44</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-0.57</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">L1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Alveolar</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.43</th>\n<td class=\"ltx_td ltx_align_left\">0.64</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Nasal</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.34</th>\n<td class=\"ltx_td ltx_align_left\">-0.54</td>\n<td class=\"ltx_td ltx_align_left\">L1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Speech</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.39</th>\n<td class=\"ltx_td ltx_align_left\">0.47</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Labial</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.29</th>\n<td class=\"ltx_td ltx_align_left\">0.52</td>\n<td class=\"ltx_td ltx_align_left\">H0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Turbulent</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.39</th>\n<td class=\"ltx_td ltx_align_left\">0.56</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Plosive</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.26</th>\n<td class=\"ltx_td ltx_align_left\">-0.58</td>\n<td class=\"ltx_td ltx_align_left\">M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mid</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.35</th>\n<td class=\"ltx_td ltx_align_left\">0.42</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Diphth.</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.20</th>\n<td class=\"ltx_td ltx_align_left\">-0.51</td>\n<td class=\"ltx_td ltx_align_left\">M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Back</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.31</th>\n<td class=\"ltx_td ltx_align_left\">0.55</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Sonorant</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.08</th>\n<td class=\"ltx_td ltx_align_left\">-0.44</td>\n<td class=\"ltx_td ltx_align_left\">H2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Low</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.29</th>\n<td class=\"ltx_td ltx_align_left\">-0.37</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">M</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Rounded</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.07</th>\n<td class=\"ltx_td ltx_align_left\">0.56</td>\n<td class=\"ltx_td ltx_align_left\">H0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Central</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.22</th>\n<td class=\"ltx_td ltx_align_left\">-0.50</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">M</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Voiced</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.05</th>\n<td class=\"ltx_td ltx_align_left\">0.44</td>\n<td class=\"ltx_td ltx_align_left\">L0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vowel</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.03</th>\n<td class=\"ltx_td ltx_align_left\">-0.48</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">M</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Lateral</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.03</th>\n<td class=\"ltx_td ltx_align_left\">0.45</td>\n<td class=\"ltx_td ltx_align_left\">H0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">High</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.03</th>\n<td class=\"ltx_td ltx_align_left\">0.57</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">H0</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Frontal</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-0.03</th>\n<td class=\"ltx_td ltx_align_left\">-0.64</td>\n<td class=\"ltx_td ltx_align_left\">M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Fricative</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">-0.01</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">-0.58</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">L2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "plosive",
            "nasal",
            "coronal",
            "voiced",
            "pcc",
            "sonorant",
            "high",
            "rounded",
            "low",
            "back",
            "value",
            "diphth",
            "plf",
            "central",
            "mean",
            "turbulent",
            "lateral",
            "speech",
            "bin",
            "mid",
            "labial",
            "alveolar",
            "fricative",
            "best",
            "histogram",
            "vowel",
            "frontal",
            "intelligibility",
            "dorsal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To gain insight in how individual PLFs relate towards speech intelligibility, we also look at the Pearson correlation coefficient&#160;(PCC) of the mean frame-level PLF values of each speech sample in the COPAS dataset, together with the best performing bin feature of the corresponding PLF histogram. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T4\" title=\"TABLE IV &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, with the correlation of the best predictive PLFs depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F4\" title=\"Figure 4 &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, subdivided by speech pathology. Moderate to strong positive correlations (<math alttext=\"r\\geq 0.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8805;</mo><mn>0.4</mn></mrow><annotation encoding=\"application/x-tex\">r\\geq 0.4</annotation></semantics></math>) with intelligibility could be found for the coronal and alveolar PLFs. Both rely on the ability of tongue displacement towards the upper teeth and roof of the mouth, possibly indicating the impact of tongue mobility on speech intelligibility. A moderately strong negative correlation (<math alttext=\"r=-0.44\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.44</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.44</annotation></semantics></math>) was present for the dorsal PLF. Dorsal sounds rely on the back of the tongue which requires less precise movement than coronal phonemes and could be used as as compensation for the disability to produce the latter. In addition, nasality was moderately negatively (<math alttext=\"r=-0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.34</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.34</annotation></semantics></math>) associated with intelligibility which corroborates results from clinical research, where increased nasality as determined by perceptual ratings of SLTs and objective measurements using nasometers was associated with reduced intelligibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib21\" title=\"\">21</a>]</cite>. In addition, we observe that the histogram-based features always show higher PCCs than their corresponding mean, indicating the importance of our proposed histogram binning strategy. While a direct comparison between our phonological features and the corresponding measurements by SLTs is missing at the moment, the current results shows the viability and additional value provided by our proposed PLFs for automatic and objective pathological speech analysis.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Paralinguistic properties of speech are essential in analyzing and choosing optimal treatment options for patients with speech disorders. However, automatic modeling of these characteristics is difficult due to the lack of labeled speech datasets describing paralinguistic properties, especially at the frame-level. In this paper, we propose a weakly supervised training method which exploits the known acoustic properties of phonemes by training an ASR model with an interpretable frame-level phonological feature bottleneck layer. Subsequently, we assess the viability of these phonological features in speech pathology analysis by developing corresponding models for intelligibility prediction and speech pathology classification. Models using our proposed phonological features perform similar to other state-of-the-art acoustic features on both tasks with a classification accuracy of 75% and a 8.43 RMSE on speech intelligibility prediction. In contrast to others, our phonological features are text-independent and highly interpretable, providing potentially useful insights for speech therapists.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of pathological speech and the subsequent treatment options are traditionally determined by a subjective assessment of speech- and language-therapists&#160;(SLTs). However, perceptual evaluation of speech by SLTs is error-prone and resource intensive, contributing towards the interest of automatic and objective speech evaluation methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib1\" title=\"\">1</a>]</cite>. In this regard, methods relying on automatic speech recognition (ASR) models have proven effective, as objective speech intelligibility metrics based on ASR transcription accuracy have shown to correlate strongly with the perceptual evaluation of speech therapists&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib4\" title=\"\">4</a>]</cite>. However, some major disadvantages are still present in ASR-based systems: the method requires a reference text and interpretability of the results is constrained to a linguistic analysis of the transcription. In addition, ASR-based metrics have only shown satisfactory results on the intelligibility regression task, covering only a small area of speech pathology analysis. Some promising results have been established by using latent representations of speech from deep neural networks (DNNs) for both pathology classification and intelligibility prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib6\" title=\"\">6</a>]</cite>. For example, speaker embeddings derived from the popular x-vector&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib7\" title=\"\">7</a>]</cite> or ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib8\" title=\"\">8</a>]</cite> models are commonly used as a text-independent alternatives for ASR-based metrics. However, models employing speaker embeddings have limited interpretability, greatly reducing their usefulness in a clinical context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To alleviate these issues, we propose a weakly supervised training method to extract frame-level phonological features (PLFs) from speech which combines the high predictive capacity of modern DNN-based features with high interpretability, essential for proper pathological speech analysis. Initially, the extraction of frame-level phonetic properties was introduced to improve the performance of downstream phoneme recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib10\" title=\"\">10</a>]</cite>. Some initial attempts have been made to use these phonetic properties for subsequent usage in pathological speech analysis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib12\" title=\"\">12</a>]</cite> but their application was limited to intelligibility prediction and lacks a comparison with current state-of-the-art ASR- and DNN-based features. To address these shortcomings, we establish the feasibility of our proposed PLFs for both intelligibility prediction and speech pathology classification and provide a comparison with current state-of-the-art features commonly used in the literature. In addition, we compare the behavior of individual PLFs with current clinical research to asses their potential as an objective paralinguistic measurement.</p>\n\n",
                "matched_terms": [
                    "high",
                    "intelligibility",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rest of the paper is organized as follows: section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2\" title=\"II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes our proposed weakly supervised PLF training setup and utterance-level feature extraction methods. Subsequently, section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3\" title=\"III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> introduces our experimental setup for intelligibility prediction and pathology classification using the proposed PLFs. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4\" title=\"IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> continues with the corresponding results and analysis and is followed by the concluding remarks in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S5\" title=\"V Conclusion &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>.</p>\n\n",
                "matched_terms": [
                    "intelligibility",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the PLF-backend is guided by three paths with distinct losses to model robust phonological features. The first path projects each acoustic embedding to the PLF vector <math alttext=\"\\boldsymbol{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119959;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{v}</annotation></semantics></math> <math alttext=\"\\in\\mathbb{R}^{F\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\in\\mathbb{R}^{F\\times 1}</annotation></semantics></math>, representing the logits of the phonological features. Subsequently, the PLF logits are converted to phone posterior probabilities by using the PLF-to-phone conversion matrix <math alttext=\"\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119924;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{M}\\in\\mathbb{R}^{P\\times F}</annotation></semantics></math> as depicted in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The values in the conversion matrix represents the expected response of our 21 defined PLFs for each phone and can range from <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> (not active) to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (active), with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><mn>0</mn></math> representing irrelevancy. Subsequently, we calculate the posterior probability <math alttext=\"P(f\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(f\\mid p)</annotation></semantics></math> of a PLF <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> having the expected value for each phoneme <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as follows:</p>\n\n",
                "matched_terms": [
                    "value",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All PLFs are updated independently during training except the PLFs related to horizontal (front, central or back) and vertical (high, mid or low) vowel position (indicated by the striped regions in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F2\" title=\"Figure 2 &#8227; II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For these two grouped PLFs, we take a weighted sum of the posterior probabilities of their corresponding attributes as calculated in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E1\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, with the weight being determined by their matching <math alttext=\"M_{p,f}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>M</mi><mrow><mi>p</mi><mo>,</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{p,f}</annotation></semantics></math> value. This allows us to define a more granular vowel position needed to model all phonemes correctly. Additionally, it also enables the ability to model positional vowel transitions within a single phoneme, as is present in e.g. diphthongs. Subsequently, we estimate the posterior log probability <math alttext=\"log(P(p\\mid\\boldsymbol{v}))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>&#8739;</mo><mi>&#119959;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">log(P(p\\mid\\boldsymbol{v}))</annotation></semantics></math> of phone <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> as:</p>\n\n",
                "matched_terms": [
                    "mid",
                    "low",
                    "high",
                    "central",
                    "vowel",
                    "back",
                    "value"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m14\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> is large, equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E1\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> will be reminiscent of a multiplicative model where each PLF must have the correct value for a corresponding phoneme. When <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m15\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> is small, the expression turns towards an additive model, with a PLF having the correct value only contributing towards evidence of a specific phoneme while being more tolerant towards incorrect PLFs. The weight of grouped PLFs in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> is fixed to 1. Finally, the parameters are updated by minimizing the corresponding negative likelihood or log-likelihood, depending on the value of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m16\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "value",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the frame-level PLFs can provide a detailed temporal view of the corresponding phonological feature as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F3\" title=\"Figure 3 &#8227; II-B1 PLF Phone Error Rate &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, an utterance-level feature representation is often necessary to describe global properties (e.g. intelligibility) of pathological speech utterances. We propose two methods to convert the <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> frame-level PLFs <math alttext=\"\\boldsymbol{V}\\in\\mathbb{R}^{F\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119933;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{V}\\in\\mathbb{R}^{F\\times T}</annotation></semantics></math> to an encompassing utterance-level feature, depending on the availability of a phonetic transcription.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our training setup of the PLFs includes phone predictions, it is possible to calculate the phone error rate&#160;(PER) if a ground-truth phonetic transcription is available. This measurement is analogues to metrics based on the word error rate&#160;(WER) of current ASR models, which are often employed in pathological speech analysis for intelligibility prediction. We also include the subcomponents of the PER (insertion, deletion and substitution rate) to allow for a more expressive feature. The phone posterior are derived from the PLF-to-phone conversion in path 1 depicted Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.F1\" title=\"Figure 1 &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the requirement of a phonetic transcription poses a limit on the applicability of the PLF phone error rate feature, we also introduce a text-independent PLF histogram feature. For each row corresponding to a PLF in <math alttext=\"\\boldsymbol{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119933;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{V}</annotation></semantics></math>, we calculate a 20-bin normalized histogram. The mean value of the three lowest (L0 - L2) and highest bins (H2 - H0) are used as our PLF histogram features, with the addition of a middle bin (M) represented by the remaining mass:</p>\n\n",
                "matched_terms": [
                    "plf",
                    "histogram",
                    "mean",
                    "value",
                    "bin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This creates a total of <math alttext=\"21\\times 7=147\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>21</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow><mo>=</mo><mn>147</mn></mrow><annotation encoding=\"application/x-tex\">21\\times 7=147</annotation></semantics></math> features in our text-independent PLF histogram. We expect that patients with a speech disorder will not be be able to use the full range of the PLFs, thus showing deviating outcomes from the healthy population. These deviations can subsequently be detected by downstream models for pathological speech analysis. For example, in speakers exhibiting hypernasality, the bin means of the nasal descriptor in a sufficiently long speech utterance are expected to be heavily skewed towards the H-bins.</p>\n\n",
                "matched_terms": [
                    "nasal",
                    "plf",
                    "histogram",
                    "speech",
                    "bin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test the potential of our proposed PLFs for pathological speech analysis by following the training setup described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS1\" title=\"II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-A</span></a> with the CoGeN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib14\" title=\"\">14</a>]</cite> dataset, consisting of speech samples from 174 healthy Dutch speakers. The <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> parameter of equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.E2\" title=\"In II-A PLF Training Setup &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> is set to 4 for all experiments. Subsequently, we extract the PLFs introduced in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.SS2\" title=\"II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II-B</span></a> from 122 Dutch speakers from the COPAS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib15\" title=\"\">15</a>]</cite> dataset, consisting of 26 normal control speakers and 96 speakers with various speech disorders. See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S2.T1\" title=\"TABLE I &#8227; II-B2 PLF Histogram &#8227; II-B PLF Feature Extraction &#8227; II Weakly Supervised Phonological Features &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> for the COPAS dataset composition. The recorded speech samples consists of the speakers reading the phonetically diverse text 'Papa en Marloes' commonly used in Dutch speech therapy. In addition, an intelligibility score based on the outcome of the Dutch Intelligibility Assessment&#160;(DIA) test&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib16\" title=\"\">16</a>]</cite> is also available.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To determine the predictive capabilities of our proposed PLFs for pathological speech analysis, we train both an intelligibility prediction and speech pathology classification model using the PLFs from the COPAS dataset. In addition, we report results using other commonly used features in speech pathology analysis. We employ the <span class=\"ltx_text ltx_font_italic\">emobase</span> feature set of the openSMILE library&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib17\" title=\"\">17</a>]</cite>, consisting of acoustic properties and corresponding statistical derivatives of the audio signal. While being reasonably interpretable, raw acoustic properties are only of limited relevance for pathological speech analysis. Subsequently, we employ speaker embeddings from the publicly available state-of-the-art ECAPA2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib18\" title=\"\">18</a>]</cite> model. Speaker embeddings are known to capture a wide variety of speaker characterizing components but its individual features are not interpretable. Finally, we use the WER of a state-of-the-art ASR system based on the publicly available XSL-R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib19\" title=\"\">19</a>]</cite> architecture, fine-tuned on the Dutch subset of the Mozilla Common Voice dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib20\" title=\"\">20</a>]</cite>. ASR-based metrics provides the possibility of a linguistic analysis of the transcribed text, although in most cases this will be limited to a word-level analysis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the small size of the dataset, we employ a five-fold cross-validation setup. 20% of the training part of each fold is used as a validation set to tune the hyperparameters of the predictive models using grid search. We treat the model type as an additional hyperparameter to ensure a fair comparison across all input features. The model types included are: linear/logistic regression, support vector machines, decision trees and multi-layer perceptrons. Once the optimal set of hyperparameters is determined, the model is retrained using the complete training partition of the fold and evaluated on the test partition. The mean accuracy on the test partitions is reported for pathology classification with the root mean square error (RMSE) given for intelligibility prediction, with scores ranging from 0 to 100.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S3.T2\" title=\"TABLE II &#8227; III Experimental Setup &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> describes the results for pathology classification and intelligibility prediction of our proposed PLFs and other state-of-the-art features, together with a corresponding interpretability rating. For reference, the results of a weak baseline model are also given, which simply predicts the mean intelligibility score and majority pathology class of the training partitions. We observe that the baseline text-independent (T-I) features generally perform slightly worse on intelligibility prediction in comparison to our text-dependent PLF PER and XSL-R WER systems, a result often reported in other research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#bib.bib5\" title=\"\">5</a>]</cite>. However, the pathology classification accuracy of respectively 59% and 43% for the PLF PER and XSL-R WER systems highlight the limited applicability of ASR transcription-based metrics for broader pathological speech analysis, given the singular nature of those features. In contrast, our proposed text-independent PLF histogram outperforms all features on intelligibility prediction and is only surpassed by the ECAPA2 embeddings on the pathology classification task. However, the PLF histograms offer superior interpretability in comparison to the latent features of the ECAPA2 embeddings, making them an overall more compelling option for pathological speech analysis.</p>\n\n",
                "matched_terms": [
                    "plf",
                    "histogram",
                    "mean",
                    "speech",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the impact of individual components of our PLF training setup, we perform an ablation study with the results given in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19879v1#S4.T3\" title=\"TABLE III &#8227; IV Results &amp; Analysis &#8227; Weakly Supervised Phonological Features for Pathological Speech Analysis\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. When we disable training of the learnable scalar matrix <math alttext=\"\\boldsymbol{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mi>&#119930;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{S}</annotation></semantics></math>, the intelligibility RMSE degrades with 6.4% relative on average. This indicates that our system benefits from deviating from the strict mapping of the fixed conversion matrix <math alttext=\"\\boldsymbol{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{M}</annotation></semantics></math>. Similarly, the 3.8% RMSE relative degradation observed when we disable the direct phone classification path hints towards the benefits of a robust acoustic embedding to guide the learning of the PLFs.</p>\n\n",
                "matched_terms": [
                    "intelligibility",
                    "plf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented weakly supervised phonological features for interpretable and objective pathological speech analysis. Experiments showed that the modeling capabilities of the proposed PLFs are competitive with other state-of-the-art features while being text-independent and providing superior interpretability. In addition, our initial analysis shows promising behavior of our PLFs in comparison to the existing clinical literature regarding the relationship between intelligibility and speech characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "intelligibility"
                ]
            }
        ]
    }
}