{
    "S4.T1": {
        "caption": "Table 1: Comprehensive evaluation of TtT framework: performance comparison, ablation study, and training strategy analysis. Dataset abbreviations: for Audio-QA — AE. (AlpacaEval), LQ. (LLaMAQuestions), TQA. (TriviaQA), WQ. (WebQuestions); for ASR — Fzh. (Fleurs-zh), A2. (AISHELL-2), A1. (AISHELL-1), WS_met. (WenetSpeech-test_meeting),\nWS_net. (WenetSpeech-test_net), Fen. (Fleurs-en).\nHigher (↑\\uparrow) is better for Audio-QA, lower (↓\\downarrow) is better for ASR.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Audio-QA (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">ASR (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">AE.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">LQ.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">TQA.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">WQ.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Fzh.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">A2.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">A1.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">WS_met.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">WS_net.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">Fen.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"11\" style=\"--ltx-bg-color:#E0FFE0;padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#E0FFE0;\">Main Results</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Qwen2.5-1.5B (AR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">17.99</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">16.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">99.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">59.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">85.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">96.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Qwen2.5-1.5B (NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">86.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">224.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">191.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">123.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">143.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">108.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-1.5B (AR&#8211;NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">15.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">23.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">3.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">7.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">44.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">14.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">16.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">52.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">41.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">49.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Qwen2.5-3B (AR)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">14.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">90.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">54.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">72.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">73.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">74.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Qwen2.5-3B (NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">11.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">68.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">212.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">160.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">89.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">111.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">83.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-3B (AR&#8211;NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">17.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">34.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">6.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">11.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">55.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">12.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">13.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">53.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">44.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">64.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"11\" style=\"--ltx-bg-color:#E0FFE0;padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#E0FFE0;\">Ablation Study</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-3B w/o BANOM</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">13.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">19.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">58.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">18.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">21.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">58.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">49.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">68.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-3B w/o PPM</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">14.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">22.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">2.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">5.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">58.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">15.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">18.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">57.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">47.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">67.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-3B w/o SST</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">14.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">10.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">1.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">56.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">25.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">31.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">64.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">56.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">62.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT-3B (AR&#8211;NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">17.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">34.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">6.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">11.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">55.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">12.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">13.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">53.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">44.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">64.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"11\" style=\"--ltx-bg-color:#E0FFE0;padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#E0FFE0;\">Training Strategy Comparison</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">TtT (AR&#8211;NAR)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">17.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">34.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">6.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">11.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">55.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">12.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">13.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">53.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">44.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">64.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Pretrain+AR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">15.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">3.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">11.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">23.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">9.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">12.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">26.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">20.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">19.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Pretrain+TtT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">26.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">40.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">11.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">21.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">18.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">6.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">5.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">27.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">19.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\">19.10</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "abbreviations",
            "qwen2515b",
            "fleurszh",
            "strategy",
            "ar–nar",
            "evaluation",
            "ttt",
            "↓downarrow",
            "comparison",
            "nar",
            "llamaquestions",
            "study",
            "fen",
            "ablation",
            "training",
            "ppm",
            "performance",
            "wsnet",
            "triviaqa",
            "wsmet",
            "audioqa",
            "aishell2",
            "sst",
            "aishell1",
            "main",
            "analysis",
            "higher",
            "asr",
            "wenetspeechtestnet",
            "lower",
            "results",
            "qwen253b",
            "comprehensive",
            "fleursen",
            "pretrainar",
            "fzh",
            "wenetspeechtestmeeting",
            "↑uparrow",
            "tqa",
            "ttt3b",
            "banom",
            "better",
            "pretrainttt",
            "alpacaeval",
            "webquestions",
            "framework",
            "dataset",
            "ttt15b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part reports the comparison of conversational understanding ability across different models on Audio-QA task. From these results we make the following observations: (1) Our proposed TtT consistently achieves the best performance on all four Audio-QA datasets. For example, at the 3B scale, TtT attains scores of 17.46, 34.68, 6.53, and 11.61 on AlpacaEval, LLaMAQuestions, TriviaQA, and WebQuestions respectively, surpassing the Qwen2.5-Base (AR) model by +3.04, +24.68, +5.93, and +10.91. This improvement stems from the hybrid AR&#8211;NAR design, which better captures the inherent asymmetry between text and audio dependencies.\n(2) Our results demonstrate a clear scaling trend shared with the baselines, where larger backbones consistently yield stronger performance. For instance, TtT-3B achieves 34.68 on LLaMAQuestions, outperforming its 1.5B counterpart which scores 23.75, reflecting an absolute gain of +10.93. This pattern aligns with the scaling laws of LLMs, reflecting that additional capacity enhances both reasoning ability and cross-modal representation learning; (3) Qwen2.5-Base (NAR) performs noticeably worse than both our hybrid framework and the AR-only baseline. Since discrete diffusion models employ an order-agnostic AO-ARM objective that ignore the inherent sequential structure of interleaved text-audio sequences. This mismatch leads to order confusion, compromising sequence coherence and weakening audio-text alignment.</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part, our proposed TtT significantly outperforms both Qwen2.5-Base (AR) and Qwen2.5-Base (NAR) on ASR benchmarks, demonstrating superior cross-modal alignment. For example, at the 3B scale, TtT achieves 12.53 and 13.65 WER on AISHELL-2 and AISHELL-1 respectively, yielding improvements of 42.41 and 58.36 absolute WER points over Qwen2.5-Base (AR). This advantage arises from the hybrid AR&#8211;NAR architecture, where NAR diffusion modeling provides efficient parallel denoising that enforces tighter alignment between acoustic frames and textual representations, while AR components ensure coherent conditioning across modalities. (2) Similar to the conversational understanding task, we observe a clear scaling trend in cross-modal alignment. Larger backbones consistently deliver better alignment, with TtT-3B achieving substantially lower WER than TtT-1.5B (e.g., 12.53 vs. 14.89 on AISHELL-2, and 13.65 vs. 16.72 on AISHELL-1). This scaling behavior reflects the increased capacity of larger models to capture fine-grained acoustic patterns and maintain robust cross-modal mappings, thereby enhancing alignment quality.</p>\n\n",
            "<p class=\"ltx_p\">To better understand the contribution of each training strategy in our hybrid AR-NAR framework, we perform an ablation study based on the full model TtT (AR-NAR). The variant w/o BANOM corresponds to removing batchwise AR &amp; NAR objective mixing from the full model, w/o PPM removes prefix preservation masking, and w/o SST removes stochastic span truncation. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> ablation study part presents the detailed results of our ablation experiments. From these results we draw the following conclusions: (1) All three training strategies have a positive impact on model performance, and removing any one of them leads to clear degradation. For instance, on the LLaMAQuestions dataset, removing SST reduces the score from 34.68 to 10.20. This drop occurs because stochastic truncation plays a crucial role in mitigating positional bias in &#10216;EOA&#10217; prediction. By forcing the model to terminate spans based on semantic content rather than fixed positional cues, SST enables more robust variable-length audio generation, and its removal undermines the model&#8217;s ability to handle flexible conversational outputs. (2) Removing BANOM yields the largest performance degradation. For example, on the AISHELL-2 dataset, the performance decreases from 12.53 to 18.58 when the strategy is removed. This is because it is essential for exposing text tokens to clean audio prefixes during training, which better matches inference conditions. Without this mechanism, the model suffers from a sharper train&#8211;test discrepancy, leading to weaker cross-modal consistency and degraded alignment quality.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> training strategy comparison part compares AR-only and AR&#8211;NAR frameworks under two different settings: (1) training directly from Qwen2.5-3B-Base without multimodal pretraining, namely TtT (AR&#8211;NAR), and (2) initialization from the multimodally aligned pretrained model, namely Pretrain+AR and Pretrain+TtT. From the table, we observe that: (1) when trained directly from Qwen2.5-3B-Base, our TtT framework achieves comparable or even superior performance to the AR-only baseline, indicating that the hybrid AR&#8211;NAR design is already competitive without pretraining; (2) when applied on top of the multimodally aligned pretrained model, Pretrain+TtT consistently matches or surpasses Pretrain+AR across both Audio-QA and ASR benchmarks. These results demonstrate that TtT not only performs strongly from scratch, but also benefits significantly when built upon large-scale multimodal alignment pretraining.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.\n\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "audioqa",
                    "ablation",
                    "training",
                    "ttt",
                    "asr",
                    "framework",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent success of LLMs has catalyzed a paradigm shift towards general-purpose Multimodal Large Language Models (MLLMs) capable of processing and generating information across diverse modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib31\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib28\" title=\"\">2023</a>)</cite>. Among these, speech-in/speech-out conversational systems have emerged as a pivotal component in facilitating natural human-AI interaction. Conventional systems typically decompose this problem into a cascaded pipeline of automatic speech recognition (ASR), LLM-driven response generation, and text-to-speech (TTS) synthesis. While effective to a degree, this modular design introduces significant latency accumulation and error propagation between modules, hindering naturalness and real-world applicability. In response, recent end-to-end approaches like Moshi <cite class=\"ltx_cite ltx_citemacro_citet\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citet\">Long et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> have sought to unify speech understanding and generation within a single model. These models are typically trained through multi-stage pipelines that involve text-to-audio tokenizer training, interleaved data construction, text-audio alignment and task-oriented supervised fine-tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib5\" title=\"\">2024</a>)</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these methods aim to generate interleaved text and speech tokens in an autoregressive manner, which are then decoded into continuous audio waveforms by a separate neural codec or diffusion-based decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Mehta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib20\" title=\"\">2024</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib14\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, this emerging paradigm faces a fundamental challenges.\nAs illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we identify a fundamental mismatch in prevailing approaches that employ a single language model to autoregressively generate both text and audio tokens. This uniform treatment applies identical autoregressive training objectives across both modalities, overlooks a critical distinction in their underlying generative processes <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib30\" title=\"\">2024</a>; Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib2\" title=\"\">2023</a>; Dang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib6\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib25\" title=\"\">2023</a>)</cite>.\nText generation inherently follows a sequential causal structure characterized by strong <span class=\"ltx_text ltx_font_bold\">target-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Box et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib3\" title=\"\">2015</a>)</cite>, where each token explicitly conditions on previously generated tokens. Consequently, an incorrect token prediction can propagate and introduce subsequent errors due to the exposure bias inherent in AR models <cite class=\"ltx_cite ltx_citemacro_citet\">Ranzato et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib23\" title=\"\">2015</a>)</cite>.\nIn contrast, audio token generation is predominantly driven by <span class=\"ltx_text ltx_font_bold\">source-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib24\" title=\"\">2020</a>)</cite>, where audio output primarily condition on the source text rather than on the preceding audio tokens. Specifically,\nwithin the current NAR span, audio tokens generation should remain faithful to the source text even when previous audio tokens are incorrectly predicted. Applying a purely AR objective to audio generation thus introduces unnecessary sequential constraints, leading to suboptimal training dynamics and magnifying error propagation. This problem can be substantially alleviated by adopting a non-autoregressive generation strategy, which aligns better with the source-dependent nature of audio modeling. Recently, discrete diffusion has emerged as a compelling alternative to AR for discrete sequence modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib34\" title=\"\">2025</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>; Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>; Sahoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib26\" title=\"\">2024</a>)</cite>. Beyond empirical gains, recent theory shows that absorbing discrete diffusion can be interpreted as modeling the conditional distributions of clean tokens and admits a tight connection to any&#8209;order AR objectives <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "better",
                    "strategy",
                    "training",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thus, we introduce Text&#8209;to&#8209;Talk (TtT), a unified audio&#8209;text MLLM that integrates AR text generation with NAR audio diffusion within a single Transformer initialized from a pretrained LLM. Text segments are trained with a standard AR cross-entropy objective, while audio segments are modeled via an NAR discrete diffusion process. During inference, the model dynamically switches between AR and NAR decoding strategies based on special control tokens. In summary, our work makes the following contributions:</p>\n\n",
                "matched_terms": [
                    "nar",
                    "ttt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify and formalize the fundamental asymmetry in dependency structures between text and audio modalities&#8212;text exhibits target-target dependencies requiring causal ordering, while audio is driven by source-target dependencies. Leveraging the any-order autoregressive nature of absorbing discrete diffusion, we establish a unified theoretical framework that proves our joint training objective provides an upper bound on the negative log-likelihood of the desired joint distribution.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments on ASR and Audio-QA benchmarks show that our proposed TtT framework substantially outperforms strong autoregressive (AR) and non-autoregressive (NAR) baselines. Our results underscore the superiority of the hybrid AR-NAR architecture, and detailed ablations validate the effectiveness of our specialized training strategies.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "audioqa",
                    "training",
                    "asr",
                    "ttt",
                    "results",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\pi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><mi>&#960;</mi><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math> is a random permutation of the token indices. Therefore, training an absorbing discrete diffusion model is equivalent to training a powerful ensemble of autoregressive models that can operate in any order. This inherent flexibility is what enables parallel, non-autoregressive generation at inference time and makes it a suitable choice for modeling source-dependent modalities like audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable efficient parallel training across all audio spans simultaneously, we apply masking operations to every audio span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math> in the sequence, rather than processing them sequentially. This parallel masking strategy significantly improves training efficiency while leveraging the time-independent nature of the denoising objective (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S2.E4\" title=\"In Time-Independent Score and the Denoising Objective. &#8227; 2.2 Absorbing Discrete Diffusion &#8227; 2 Preliminary and notation &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.F2\" title=\"Figure 2 &#8227; Training objective and upper bound analysis. &#8227; 3.3 Multimodal Factorization and a Unified Objective &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (a), our training pipeline starts from a pretrained text LLM and expands its vocabulary with discrete audio codebook tokens and control symbols (<math alttext=\"\\langle\\text{SOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>SOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{SOA}\\rangle</annotation></semantics></math>, <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math>). Each training sequence is organized as interleaved text spans and audio spans, where text spans are optimized with AR Loss while audio spans use the NAR diffusion objective.</p>\n\n",
                "matched_terms": [
                    "training",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Batchwise AR &amp; NAR Objective Mixing (BANOM)</span>: To address the contextual distribution shift between training and inference, we selectively disable NAR training for a subset of samples within each batch. Specifically, with probability <math alttext=\"p_{\\text{mix}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>mix</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{mix}}</annotation></semantics></math>, we skip the diffusion noise addition process for certain samples and compute gradients only on text tokens using AR loss. This ensures that during training, text tokens occasionally observe clean, unmasked audio spans&#8212;matching the inference scenario where text generation conditions on previously generated complete audio content rather than partially masked spans.</p>\n\n",
                "matched_terms": [
                    "banom",
                    "training",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prefix Preservation Masking (PPM)</span>: To address cross-span contextual inconsistency, we preserve clean prefixes of audio spans during diffusion training. For a fraction <math alttext=\"p_{\\text{prefix}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>prefix</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{prefix}}</annotation></semantics></math> of training samples, we randomly select a cutoff index <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and ensure that all preceding audio spans <math alttext=\"\\mathcal{A}_{&lt;m}=\\{\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{m-1}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>m</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}=\\{\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{m-1}\\}</annotation></semantics></math> remain unmasked, while applying NAR diffusion loss only to spans <math alttext=\"\\mathcal{A}_{\\geq m}=\\{\\mathcal{A}_{m},\\mathcal{A}_{m+1},\\ldots,\\mathcal{A}_{M}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&#8805;</mo><mi>m</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\geq m}=\\{\\mathcal{A}_{m},\\mathcal{A}_{m+1},\\ldots,\\mathcal{A}_{M}\\}</annotation></semantics></math>. This strategy ensures that during training, when generating span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math>, the model observes clean representations of all previous spans <math alttext=\"\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}</annotation></semantics></math>, matching the inference scenario where audio spans are generated sequentially and each span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math> conditions on fully generated, clean preceding spans <math alttext=\"\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}</annotation></semantics></math> rather than their corrupted, partially masked versions.</p>\n\n",
                "matched_terms": [
                    "ppm",
                    "strategy",
                    "nar",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Span Truncation (SST)</span>: We address the positional bias in <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> prediction by randomly truncating audio span <math alttext=\"\\mathcal{A}_{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}</annotation></semantics></math> during training. Due to disparate tokenization rates between text and audio, audio tokens significantly outnumber text tokens, resulting in fixed-size spans <math alttext=\"\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{M-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{M-1}</annotation></semantics></math> and a variable-length final span <math alttext=\"\\mathcal{A}_{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}</annotation></semantics></math>. Since all audio spans undergo simultaneous diffusion training, the model learns to predict <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> at fixed positions for early spans, creating a strong positional bias that hinders content-aware termination learning for the final span. To mitigate this, we implement stochastic truncation: with probability <math alttext=\"p_{\\text{trunc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m6\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>trunc</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{trunc}}</annotation></semantics></math>, we randomly select a truncation length <math alttext=\"k&lt;|\\mathcal{A}_{M}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k&lt;|\\mathcal{A}_{M}|</annotation></semantics></math> and create a truncated span <math alttext=\"\\mathcal{A}_{M}^{\\text{trunc}}=(a_{M,1},\\ldots,a_{M,k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi><mtext>trunc</mtext></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}^{\\text{trunc}}=(a_{M,1},\\ldots,a_{M,k})</annotation></semantics></math> by removing the original <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> token and suffix tokens <math alttext=\"(a_{M,k+1},\\ldots,a_{M,|\\mathcal{A}_{M}|})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{M,k+1},\\ldots,a_{M,|\\mathcal{A}_{M}|})</annotation></semantics></math>. This creates training samples where span termination occurs at arbitrary positions rather than fixed boundaries, forcing the model to predict <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> based on semantic content and contextual text rather than positional cues, enabling robust variable-length generation during inference.</p>\n\n",
                "matched_terms": [
                    "training",
                    "sst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.F3\" title=\"Figure 3 &#8227; 3.4 Modality-Aware Attention Mechanism &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a), our TtT framework consists of three major components: an audio encoder, an audio decoder, and a unified MLLM backbone with hybrid AR-NAR capabilities. Similar to GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, we adopt CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib10\" title=\"\">2024</a>)</cite> as both the audio encoder and decoder. During inference, the TtT model alternates between AR text generation and NAR audio synthesis within a unified framework. The generation process operates as follows: Given an input audio, it is first encoded into a sequence of discrete audio tokens through the audio encoder, which are subsequently fed into the TtT model for processing. The TtT generation process begins with AR decoding for text tokens. Once the model encounters the special token <math alttext=\"\\langle\\text{SOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>SOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{SOA}\\rangle</annotation></semantics></math>, the system switches to NAR generation mode. The NAR generation employs block-wise diffusion (detailed in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#alg1\" title=\"Algorithm 1 &#8227; A.3 Block-wise Masked Diffusion Generation For Audio Tokens &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) to generate audio token spans in parallel, returning to AR mode after predicting <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> and dropping the remaining predicted tokens after <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> within the current diffusion block, then starting a new round of AR process. The whole sequence generation terminates after the AR process generates an <math alttext=\"\\langle\\text{EOS}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOS}\\rangle</annotation></semantics></math> token. Notably, once each audio span is generated, the corresponding audio tokens can be immediately passed to the audio decoder to synthesize audio streams in parallel, ensuring low first-token latency and enabling continuous streaming audio generation without waiting for the entire sequence completion.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "nar",
                    "ttt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively train and evaluate our proposed TtT framework, we follow prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> and adopt a diverse collection of multi-task datasets, including ASR, TTS, audio chat, text chat, automated audio captioning (AAC), speech emotion classification (SEC), acoustic scene classification (ASC), and interleaved text&#8211;audio data. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T2\" title=\"Table 2 &#8227; A.4 Training dataset details &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a summary of the training datasets, with detailed examples provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS6\" title=\"A.6 Data Format of Training Data &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>. During training, we aim to construct a balanced corpus that supports effective learning across multiple tasks. Specifically, we randomly sample one million instances from the ASR dataset, the TTS dataset, and the audio chat dataset respectively. In addition, we create bilingual interleaved text and audio data, ensuring that Chinese and English are represented in approximately equal proportions. To build the audio chat corpus, we rely on the text-to-audio dataset VoiceAssistant-400K together with the text-based datasets OpenHermes-2.5 and Firefly-Train-1.1M, and we employ a TTS model, namely CosyVoice2, to convert text into synthetic audio so as to enrich the training data. To further enhance cross-modal alignment between text and audio, we follow prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>)</cite> and supplement the training corpus with interleaved text and audio data derived from the large-scale pretrained corpus FineWeb-Edu. This strategy not only expands task coverage but also strengthens the model&#8217;s ability to jointly learn from and align textual and acoustic modalities. For evaluation, we focus on two representative tasks: audio question answering (Audio-QA), which evaluates both the model&#8217;s reasoning skills and its text-to-speech capabilities in an end-to-end audio generation framework, and ASR, which measures the quality of cross-modal alignment between text and audio. The evaluation datasets are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "audioqa",
                    "strategy",
                    "evaluation",
                    "training",
                    "ttt",
                    "asr",
                    "framework",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively evaluate our model on two representative tasks, we carefully design the evaluation protocol and metrics. For Audio-QA, we introduce an ASR-LLM pipeline that transcribes the model&#8217;s spoken responses using language-specific ASR systems (Paraformer-zh for Chinese and Whisper-Large-v3 for English) and leverages a powerful LLM-as-a-Judge (Qwen3-235B-A30B) to assess semantic correctness against ground-truth answers. This approach captures the model&#8217;s ability to generate natural, accurate spoken responses in realistic conversational settings. For the ASR task, we directly measure transcription accuracy using Word Error Rate (WER), which reflects both speech recognition fidelity and cross-modal alignment within our hybrid AR-NAR framework. More details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS5.SSS1\" title=\"A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "evaluation",
                    "audioqa",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our model using the AdamW optimizer with a global batch size of 2048, a learning rate of <math alttext=\"2e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2e^{-5}</annotation></semantics></math>, and a weight decay factor of <math alttext=\"1e^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-2}</annotation></semantics></math>. The learning rate follows a cosine decay schedule with a linear warmup ratio of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Training incorporates three stochastic strategies: (1) batchwise AR &amp; NAR objective mixing with probability 0.3; (2) prefix preservation masking with ratio 0.3; (3) stochastic span truncation with probability 0.5. During inference, the model alternates between AR text decoding and NAR diffusion-based audio generation, where text decoding uses nucleus sampling with <math alttext=\"k=10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">k=10</annotation></semantics></math> and <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math>, and audio spans are generated with 200 diffusion steps, a block length of 32 tokens, and a total diffusion span length of 640 tokens under classifier-free guidance with scale 0.1. Since different training strategies lead to varying convergence speeds, reported results are based on checkpoints where training loss has converged. All experiments are conducted on 4 nodes with 8 NVIDIA A100 GPUs per node using the DeepSpeed runtime.</p>\n\n",
                "matched_terms": [
                    "results",
                    "training",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the effectiveness of our proposed TtT framework, we compare it with two representative baselines, one employing a purely AR backbone and the other adopting a purely diffusion based NAR backbone. For fairness and scalability, all three frameworks are instantiated with backbones of 1.5B and 3B parameters.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "nar",
                    "ttt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate the effectiveness of our method on top of a multimodally aligned pretrained model, we perform large-scale multimodal pretraining based on the Qwen2.5-3B-Base model. Specifically, we construct a corpus of approximately 200B tokens covering ASR, TTS, text-only data, and interleaved text&#8211;audio data. The model is trained with a standard autoregressive objective using a global batch size of 256 for 140k steps. This pretraining stage equips the backbone, namely Qwen2.5-3B-Base with strong cross-modal alignment ability before applying our hybrid AR&#8211;NAR learning framework.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "ar–nar",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented TtT, a unified multimodal framework that addresses the fundamental asymmetry in dependency structures between text and audio modalities. By integrating autoregressive generation for text with non-autoregressive discrete diffusion for audio within a single Transformer, TtT leverages the target-target dependencies of text and the source-target dependencies of audio. We established a theoretical foundation showing that our hybrid training objective provides an upper bound on the negative log-likelihood of the desired joint distribution. To mitigate train-test discrepancies inherent in this hybrid approach, we introduced three principled training strategies: Batchwise AR&#8211;NAR Objective Mixing, Prefix Preservation Masking, and Stochastic Span Truncation. Extensive experiments on ASR and Audio-QA tasks demonstrate that TtT significantly outperforms purely autoregressive or non-autoregressive baselines, achieving superior performance in conversational understanding and cross-modal alignment tasks. Our results validate that respecting the distinct generative natures of each modality is crucial for building effective speech-in/speech-out systems.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "audioqa",
                    "ar–nar",
                    "training",
                    "ttt",
                    "asr",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in end-to-end audio-language models have moved beyond traditional cascaded architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib4\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib29\" title=\"\">2023</a>)</cite> toward unified multimodal frameworks. Representative works include Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, which achieves real-time duplex speech conversation through hierarchical Transformer architectures; GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, which builds upon GLM-4-9B for robust Chinese and English speech processing; and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite>, which introduces a lightweight Multiple Cross-modal Token Prediction (MCTP) module for fast audio-text generation with significantly reduced first-token latency. More recent efforts have focused on scaling and production readiness: Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>)</cite> presents a 130B-parameter unified speech-text model with generative speech data engine and instruction-driven fine control across dialects, emotions, singing, and RAP, while Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>)</cite> features text-guided aligned speech generation with multi-codebook discretization to preserve both semantic and acoustic information. UniWav <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib17\" title=\"\">2025</a>)</cite> proposes the first unified encoder-decoder framework that jointly learns representation encoders and generative audio decoders for both discriminative and generative speech tasks.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete diffusion models have emerged as a compelling alternative to autoregressive generation, offering non-autoregressive approaches that can generate entire sequences in parallel. The foundational work of D3PMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>)</cite> generalized diffusion processes to discrete data through flexible transition matrices, with absorbing processes that progressively mask tokens proving particularly effective. This framework has since evolved through both theoretical advances and practical improvements. From a theoretical perspective, recent work has deepened our understanding of discrete diffusion dynamics. <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite> revealed that absorbing diffusion&#8217;s concrete score can be expressed as time-independent conditional probabilities, leading to RADD&#8212;a reparameterized model that removes explicit time conditioning while establishing connections to any-order autoregressive generation. Building on this foundation, <cite class=\"ltx_cite ltx_citemacro_citet\">Li &amp; Cai (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib15\" title=\"\">2025</a>)</cite> formally characterized convergence rates, proving that KL divergence decays at <math alttext=\"O(1/T)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(1/T)</annotation></semantics></math> with bounds scaling linearly with token mutual information. However, <cite class=\"ltx_cite ltx_citemacro_citet\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib11\" title=\"\">2025</a>)</cite> identified a fundamental trade-off: while masked diffusion achieves near-optimal perplexity in constant steps, sequence-level tasks like reasoning may require steps linear in sequence length. Practical advances have focused on training efficiency and application domains. <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib27\" title=\"\">2024</a>)</cite> reformulated the variational objective as a weighted integral of cross-entropy losses, unifying prior approaches while achieving state-of-the-art results that even surpass comparable autoregressive baselines. For complex reasoning tasks where autoregressive models struggle with subgoal imbalance, <cite class=\"ltx_cite ltx_citemacro_citet\">Ye et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib33\" title=\"\">2024</a>)</cite> demonstrated that Multi-Granularity Diffusion Modeling can achieve near-perfect accuracy by prioritizing harder subgoals during training. The scalability challenge has been addressed through innovative adaptation strategies. Rather than training from scratch, <cite class=\"ltx_cite ltx_citemacro_citet\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>); Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib21\" title=\"\">2025</a>)</cite> showed that pretrained autoregressive models can be efficiently converted to diffusion models via continual pre-training, maintaining competitive performance while enabling parallel generation. Meanwhile, hybrid approaches are gaining traction: <cite class=\"ltx_cite ltx_citemacro_citet\">Lovelace et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib19\" title=\"\">2024</a>)</cite> combined diffusion-based latent proposals with autoregressive decoding for controllable generation, while <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib32\" title=\"\">2025</a>)</cite> developed MMaDA, a unified multimodal diffusion foundation model that processes text, images, and reasoning within a single architecture.</p>\n\n",
                "matched_terms": [
                    "models",
                    "training",
                    "results",
                    "framework",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous evaluation frameworks such as Kimi-Audio assess Audio-QA performance using the text portion of interleaved outputs, which overlooks the fact that the audio output of an end-to-end speech model more directly reflects its ability to generate natural and semantically faithful responses. To address this limitation, we evaluate Audio-QA directly on the audio outputs of our framework by first applying an ASR model to transcribe the generated audio into text, where Whisper-Large-v3 is used for English audio and Paraformer-zh for Chinese audio, with a comparison of ASR performance across different models provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T3\" title=\"Table 3 &#8227; Audio-QA Task &#8227; A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The transcribed text is then combined with the original QA queries and the ground truth answers and passed to a large scale reasoning model, Qwen3-235B-A30B, which serves as an LLM-as-a-Judge model to determine whether the response semantically matches the reference and to provide either a correctness label or a graded score. We report the average accuracy or score on the benchmark, and this evaluation pipeline provides a more faithful assessment of our model&#8217;s audio-to-audio QA ability in realistic conversational scenarios where speech serves as the output modality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audioqa",
                    "evaluation",
                    "asr",
                    "framework",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the model&#8217;s capability in aligning speech with textual representations, we evaluate it on the ASR task, where the model generates text transcriptions from input audio and performance is measured using word error rate (WER). A lower WER indicates more accurate recognition, which reflects not only strong ASR ability but also effective cross modal consistency achieved by our hybrid AR-NAR modeling framework.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "lower",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate model performance on a diverse set of benchmarks covering both Audio Question Answering (Audio-QA) and Automatic Speech Recognition (ASR) tasks. For Audio-QA, we use four datasets: AlpacaEval, TriviaQA, and WebQuestions (English), along with ReasoningQA (Chinese), assessing cross-lingual reasoning and comprehension from speech. For ASR, we include five datasets: Fleurs-zh/en (multilingual), AISHELL-1/2, and WenetSpeech (all Chinese), covering varied domains, accents, and recording conditions to robustly measure transcription accuracy. Dataset details are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "triviaqa",
                    "audioqa",
                    "alpacaeval",
                    "asr",
                    "webquestions",
                    "dataset",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable unified training across diverse tasks, we transform all datasets into a consistent input&#8211;output format. On the one hand, this standardization allows the model to seamlessly integrate heterogeneous modalities such as speech, text, and interleaved audio&#8211;text sequences. On the other hand, a unified design is essential for supporting our training strategies, including batchwise AR &amp; NAR objective mixing, prefix preservation masking, and stochastic span truncation. These strategies rely on a shared representation to operate across modalities in a consistent way. For clarity, we provide representative examples of the adopted data formats as follow, covering ASR, TTS, audio chat, text chat, AAC, SEC, ASC, and interleaved text&#8211;audio data.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "training",
                    "nar"
                ]
            }
        ]
    },
    "A1.T2": {
        "caption": "Table 2: Summary of datasets used in training.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Samples</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task Type</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Emilia_zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">500000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TTS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Emilia_en</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">500000</td>\n<td class=\"ltx_td ltx_align_center\">TTS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AISHELL2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"8\">600000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AISHELL3</td>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CommonVoice</td>\n<td class=\"ltx_td ltx_align_center\">Chinese, English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GigaSpeech</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LibriSpeech</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MLS-Eng</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">PeopleSpeech</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoxPopuli</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">WenetSpeech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">400000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VoiceAssistant-400K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">1000000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Audio Chat</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OpenHermes-2.5</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">Audio Chat</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Firefly-Train-1.1M</td>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">Audio Chat</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MathInstruct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">262039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Text Chat</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MACS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">59282</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AAC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Clotho-v2</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">AAC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Nonspeech7k</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">SEC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalSound</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">SEC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CochlScene</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">ASC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Chinese-Fineweb-Edu (Skypile)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1500000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Interleaved Data</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FineWeb-Edu</td>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">1500000</td>\n<td class=\"ltx_td ltx_align_center\">Interleaved Data</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">6321321</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "summary",
            "type",
            "librispeech",
            "fireflytrain11m",
            "emiliazh",
            "chat",
            "cochlscene",
            "emiliaen",
            "aac",
            "tts",
            "vocalsound",
            "audio",
            "aishell3",
            "skypile",
            "macs",
            "training",
            "used",
            "text",
            "peoplespeech",
            "asc",
            "english",
            "aishell2",
            "language",
            "samples",
            "asr",
            "voiceassistant400k",
            "nonspeech7k",
            "datasets",
            "clothov2",
            "gigaspeech",
            "interleaved",
            "voxpopuli",
            "task",
            "total",
            "mathinstruct",
            "finewebedu",
            "openhermes25",
            "wenetspeech",
            "data",
            "chinesefinewebedu",
            "mlseng",
            "sec",
            "dataset",
            "chinese",
            "commonvoice"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To effectively train and evaluate our proposed TtT framework, we follow prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> and adopt a diverse collection of multi-task datasets, including ASR, TTS, audio chat, text chat, automated audio captioning (AAC), speech emotion classification (SEC), acoustic scene classification (ASC), and interleaved text&#8211;audio data. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T2\" title=\"Table 2 &#8227; A.4 Training dataset details &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a summary of the training datasets, with detailed examples provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS6\" title=\"A.6 Data Format of Training Data &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>. During training, we aim to construct a balanced corpus that supports effective learning across multiple tasks. Specifically, we randomly sample one million instances from the ASR dataset, the TTS dataset, and the audio chat dataset respectively. In addition, we create bilingual interleaved text and audio data, ensuring that Chinese and English are represented in approximately equal proportions. To build the audio chat corpus, we rely on the text-to-audio dataset VoiceAssistant-400K together with the text-based datasets OpenHermes-2.5 and Firefly-Train-1.1M, and we employ a TTS model, namely CosyVoice2, to convert text into synthetic audio so as to enrich the training data. To further enhance cross-modal alignment between text and audio, we follow prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>)</cite> and supplement the training corpus with interleaved text and audio data derived from the large-scale pretrained corpus FineWeb-Edu. This strategy not only expands task coverage but also strengthens the model&#8217;s ability to jointly learn from and align textual and acoustic modalities. For evaluation, we focus on two representative tasks: audio question answering (Audio-QA), which evaluates both the model&#8217;s reasoning skills and its text-to-speech capabilities in an end-to-end audio generation framework, and ASR, which measures the quality of cross-modal alignment between text and audio. The evaluation datasets are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.\n\n</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "language",
                    "audio",
                    "data",
                    "training",
                    "asr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent success of LLMs has catalyzed a paradigm shift towards general-purpose Multimodal Large Language Models (MLLMs) capable of processing and generating information across diverse modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib31\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib28\" title=\"\">2023</a>)</cite>. Among these, speech-in/speech-out conversational systems have emerged as a pivotal component in facilitating natural human-AI interaction. Conventional systems typically decompose this problem into a cascaded pipeline of automatic speech recognition (ASR), LLM-driven response generation, and text-to-speech (TTS) synthesis. While effective to a degree, this modular design introduces significant latency accumulation and error propagation between modules, hindering naturalness and real-world applicability. In response, recent end-to-end approaches like Moshi <cite class=\"ltx_cite ltx_citemacro_citet\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citet\">Long et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> have sought to unify speech understanding and generation within a single model. These models are typically trained through multi-stage pipelines that involve text-to-audio tokenizer training, interleaved data construction, text-audio alignment and task-oriented supervised fine-tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib5\" title=\"\">2024</a>)</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these methods aim to generate interleaved text and speech tokens in an autoregressive manner, which are then decoded into continuous audio waveforms by a separate neural codec or diffusion-based decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Mehta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib20\" title=\"\">2024</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib14\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "tts",
                    "language",
                    "audio",
                    "data",
                    "training",
                    "asr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, this emerging paradigm faces a fundamental challenges.\nAs illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we identify a fundamental mismatch in prevailing approaches that employ a single language model to autoregressively generate both text and audio tokens. This uniform treatment applies identical autoregressive training objectives across both modalities, overlooks a critical distinction in their underlying generative processes <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib30\" title=\"\">2024</a>; Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib2\" title=\"\">2023</a>; Dang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib6\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib25\" title=\"\">2023</a>)</cite>.\nText generation inherently follows a sequential causal structure characterized by strong <span class=\"ltx_text ltx_font_bold\">target-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Box et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib3\" title=\"\">2015</a>)</cite>, where each token explicitly conditions on previously generated tokens. Consequently, an incorrect token prediction can propagate and introduce subsequent errors due to the exposure bias inherent in AR models <cite class=\"ltx_cite ltx_citemacro_citet\">Ranzato et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib23\" title=\"\">2015</a>)</cite>.\nIn contrast, audio token generation is predominantly driven by <span class=\"ltx_text ltx_font_bold\">source-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib24\" title=\"\">2020</a>)</cite>, where audio output primarily condition on the source text rather than on the preceding audio tokens. Specifically,\nwithin the current NAR span, audio tokens generation should remain faithful to the source text even when previous audio tokens are incorrectly predicted. Applying a purely AR objective to audio generation thus introduces unnecessary sequential constraints, leading to suboptimal training dynamics and magnifying error propagation. This problem can be substantially alleviated by adopting a non-autoregressive generation strategy, which aligns better with the source-dependent nature of audio modeling. Recently, discrete diffusion has emerged as a compelling alternative to AR for discrete sequence modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib34\" title=\"\">2025</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>; Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>; Sahoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib26\" title=\"\">2024</a>)</cite>. Beyond empirical gains, recent theory shows that absorbing discrete diffusion can be interpreted as modeling the conditional distributions of clean tokens and admits a tight connection to any&#8209;order AR objectives <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thus, we introduce Text&#8209;to&#8209;Talk (TtT), a unified audio&#8209;text MLLM that integrates AR text generation with NAR audio diffusion within a single Transformer initialized from a pretrained LLM. Text segments are trained with a standard AR cross-entropy objective, while audio segments are modeled via an NAR discrete diffusion process. During inference, the model dynamically switches between AR and NAR decoding strategies based on special control tokens. In summary, our work makes the following contributions:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "summary",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify and formalize the fundamental asymmetry in dependency structures between text and audio modalities&#8212;text exhibits target-target dependencies requiring causal ordering, while audio is driven by source-target dependencies. Leveraging the any-order autoregressive nature of absorbing discrete diffusion, we establish a unified theoretical framework that proves our joint training objective provides an upper bound on the negative log-likelihood of the desired joint distribution.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose TtT, a hybrid AR-NAR MLLM that seamlessly integrates autoregressive text generation with discrete diffusion-based audio synthesis within a single Transformer initialized from a pretrained LLM. Our design preserves the reasoning and instruction-following capabilities of the base LLM while enabling efficient parallel audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments on ASR and Audio-QA benchmarks show that our proposed TtT framework substantially outperforms strong autoregressive (AR) and non-autoregressive (NAR) baselines. Our results underscore the superiority of the hybrid AR-NAR architecture, and detailed ablations validate the effectiveness of our specialized training strategies.</p>\n\n",
                "matched_terms": [
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider interleaved discrete text&#8211;audio sequences of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>:\n<math alttext=\"x=(x^{1},\\ldots,x^{L})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>x</mi><mi>L</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x^{1},\\ldots,x^{L})</annotation></semantics></math> with a unified discrete vocabulary\n<math alttext=\"\\mathcal{V}=\\mathcal{V}_{\\text{text}}\\cup\\mathcal{V}_{\\text{audio}}\\cup\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mtext>text</mtext></msub><mo>&#8746;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mtext>audio</mtext></msub><mo>&#8746;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}=\\mathcal{V}_{\\text{text}}\\cup\\mathcal{V}_{\\text{audio}}\\cup\\mathcal{S}</annotation></semantics></math>,\nwhere <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math> contains special tokens such as <math alttext=\"\\langle\\text{SOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>SOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{SOA}\\rangle</annotation></semantics></math> (start of audio), <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> (end of audio), <math alttext=\"\\langle\\text{EOS}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOS}\\rangle</annotation></semantics></math> (end of sequence) and the absorbing mask token <math alttext=\"[\\mathbf{M}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>&#119820;</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{M}]</annotation></semantics></math>.\nA sequence <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> is structured as a series of alternating text and audio spans: <math alttext=\"x=(\\mathcal{T}_{1},\\mathcal{A}_{1},\\ldots,\\mathcal{T}_{M},\\mathcal{A}_{M},\\langle\\text{EOS}\\rangle)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mn>1</mn></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>M</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo>,</mo><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=(\\mathcal{T}_{1},\\mathcal{A}_{1},\\ldots,\\mathcal{T}_{M},\\mathcal{A}_{M},\\langle\\text{EOS}\\rangle)</annotation></semantics></math>, where:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"UM\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">UM</annotation></semantics></math> denotes the set of unmasked (visible) tokens in the corrupted sequence.\nThis decomposition is crucial because it decouples the time-dependent dynamics from the data distribution. It implies that the model <math alttext=\"q_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">q_{\\theta}</annotation></semantics></math> does not need to learn a complex function of time <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. Instead, its sole objective is to learn to approximate the clean conditional distribution <math alttext=\"p_{0}(v|UM)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>v</mi><mo fence=\"false\">|</mo><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{0}(v|UM)</annotation></semantics></math>, which is a static, time-independent property of the data. The learning task is thus simplified to a denoising objective: given a corrupted sequence with some tokens masked, predict the original tokens for the masked positions based on the visible context.</p>\n\n",
                "matched_terms": [
                    "task",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\pi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><mi>&#960;</mi><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math> is a random permutation of the token indices. Therefore, training an absorbing discrete diffusion model is equivalent to training a powerful ensemble of autoregressive models that can operate in any order. This inherent flexibility is what enables parallel, non-autoregressive generation at inference time and makes it a suitable choice for modeling source-dependent modalities like audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce our proposed model, integrates AR generation for text and discrete diffusion for audio within a single, unified Transformer architecture.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To express the joint probability of all text tokens in the sequence, we account for the conditioning on preceding audio spans. The joint probability <math alttext=\"p_{\\theta}(x_{text})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(x_{text})</annotation></semantics></math> is therefore defined as the product of the probabilities of each text span, conditioned on all prior spans:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the theoretical foundation established in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S2.SS2\" title=\"2.2 Absorbing Discrete Diffusion &#8227; 2 Preliminary and notation &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, we apply absorbing discrete diffusion to audio spans <math alttext=\"\\mathcal{A}_{\\leq M}=\\cup_{m=1}^{M}\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&#8804;</mo><mi>M</mi></mrow></msub><mo rspace=\"0em\">=</mo><mrow><msubsup><mo lspace=\"0em\">&#8746;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\leq M}=\\cup_{m=1}^{M}\\mathcal{A}_{m}</annotation></semantics></math> while keeping text tokens <math alttext=\"\\mathcal{T}_{\\leq M}=\\cup_{m=1}^{M}\\mathcal{T}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mrow><mi/><mo>&#8804;</mo><mi>M</mi></mrow></msub><mo rspace=\"0em\">=</mo><mrow><msubsup><mo lspace=\"0em\">&#8746;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>m</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}_{\\leq M}=\\cup_{m=1}^{M}\\mathcal{T}_{m}</annotation></semantics></math> unperturbed. This design choice aligns with the fundamental difference in dependency structures: audio tokens exhibit strong <span class=\"ltx_text ltx_font_bold\">source<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>target</span> dependencies (conditioning on source text), making them well-suited for the any-order autoregressive nature of diffusion, while text tokens follow <span class=\"ltx_text ltx_font_bold\">target<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>target</span> causal dependencies, better handled by standard AR modeling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, we apply the absorbing diffusion process described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S2.SS2\" title=\"2.2 Absorbing Discrete Diffusion &#8227; 2 Preliminary and notation &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a> exclusively to audio positions. For each training sample, we sample a masking level <math alttext=\"\\lambda\\sim U([0,1])\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>&#8764;</mo><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda\\sim U([0,1])</annotation></semantics></math> and independently mask each audio token with probability <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, while preserving all text tokens. This creates corrupted sequences where audio spans contain a mixture of original tokens and mask tokens <math alttext=\"[\\mathbf{M}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>&#119820;</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{M}]</annotation></semantics></math>, but text spans remain intact.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable efficient parallel training across all audio spans simultaneously, we apply masking operations to every audio span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math> in the sequence, rather than processing them sequentially. This parallel masking strategy significantly improves training efficiency while leveraging the time-independent nature of the denoising objective (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S2.E4\" title=\"In Time-Independent Score and the Denoising Objective. &#8227; 2.2 Absorbing Discrete Diffusion &#8227; 2 Preliminary and notation &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite>, this <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>-DCE objective is mathematically equivalent to the any-order autoregressive objective. To make this connection explicit for our audio generation task, we can rewrite Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.E9\" title=\"In Training objective for audio generation. &#8227; 3.2 Absorbing discrete diffusion for audio spans &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the equivalent AO-ARM form:</p>\n\n",
                "matched_terms": [
                    "task",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\pi_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{m}</annotation></semantics></math> is a random permutation over the positions within audio span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math>, and <math alttext=\"a_{m,\\pi_{m}(&lt;j)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>m</mi><mo>,</mo><mrow><msub><mi>&#960;</mi><mi>m</mi></msub><mspace style=\"width:0.3888888888888889em;\" width=\"0.3888888888888889em\"/><mrow><mo stretchy=\"false\">(</mo><mrow><mi/><mo>&lt;</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><annotation encoding=\"application/x-tex\">a_{m,\\pi_{m}(&lt;j)}</annotation></semantics></math> denotes the audio tokens that appear before position <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> in the permuted order. This formulation makes explicit that the audio generation objective is learning to predict each audio token conditioned on an arbitrary subset of other tokens within the same span, plus the full cross-modal context from text. This any-order autoregressive nature is what enables parallel generation during inference.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having established AR modeling for text in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.SS1\" title=\"3.1 Autoregressive (AR) modeling for text &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and discrete diffusion for audio in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.SS2\" title=\"3.2 Absorbing discrete diffusion for audio spans &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we now formalize how these two paradigms can be unified within a single probabilistic framework. The key insight is to leverage the distinct dependency structures of each modality through a <em class=\"ltx_emph ltx_font_italic\">partial-order factorization</em> that respects the causal nature of text while allowing flexible ordering within audio spans.\nRecall that text tokens exhibit strong target-target dependencies requiring causal ordering, while audio tokens primarily depend on source-target relationships with their corresponding text. This suggests that within each audio span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math>, the tokens can be generated in any order as long as they condition on the appropriate cross-modal context <math alttext=\"\\mathcal{T}_{\\leq m}\\cup\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mrow><mi/><mo>&#8804;</mo><mi>m</mi></mrow></msub><mo>&#8746;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{T}_{\\leq m}\\cup\\mathcal{A}_{&lt;m}</annotation></semantics></math>. We formalize this intuition using partial orders over token positions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tokens within each audio span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math> form an antichain under <math alttext=\"\\preceq\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i3.p1.m2\" intent=\":literal\"><semantics><mo>&#10927;</mo><annotation encoding=\"application/x-tex\">\\preceq</annotation></semantics></math> (no <em class=\"ltx_emph ltx_font_italic\">mandatory</em> internal ordering), but the model is permitted to condition on previously generated tokens within the same span during training and inference under any linear extension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For any token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, let <math alttext=\"\\mathrm{Pa}(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>Pa</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Pa}(i)</annotation></semantics></math> denote its set of predecessors under this partial order. By construction, each audio token <math alttext=\"a_{m,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>m</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{m,j}</annotation></semantics></math> has predecessors <math alttext=\"\\mathrm{Pa}(a_{m,j})=\\mathcal{T}_{\\leq m}\\cup\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>Pa</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>m</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mrow><mi/><mo>&#8804;</mo><mi>m</mi></mrow></msub><mo>&#8746;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Pa}(a_{m,j})=\\mathcal{T}_{\\leq m}\\cup\\mathcal{A}_{&lt;m}</annotation></semantics></math>, while for text tokens <math alttext=\"\\mathrm{Pa}(t_{m,j})=\\mathcal{T}_{&lt;m}\\cup\\mathcal{A}_{&lt;m}\\cup t_{m,&lt;j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>Pa</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>m</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><mo>&#8746;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><mo>&#8746;</mo><msub><mi>t</mi><mrow><mi>m</mi><mo>,</mo><mrow><mi/><mo>&lt;</mo><mi>j</mi></mrow></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Pa}(t_{m,j})=\\mathcal{T}_{&lt;m}\\cup\\mathcal{A}_{&lt;m}\\cup t_{m,&lt;j}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Combining fixed-order AR for text with order-marginalized factorization for audio, our model induces the joint scoring function:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation reveals that both modalities are fundamentally autoregressive: text uses a single linear extension (left-to-right), while audio integrates over all linear extensions consistent with the partial order.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The key theoretical insight is that our combined training objective provides a tight upper bound on the negative log-likelihood of the desired joint distribution. To see this, consider the audio term:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right-hand side is precisely <math alttext=\"-\\log\\tilde{p}_{\\theta}(\\mathcal{A}_{m}\\mid\\mathcal{T}_{\\leq m},\\mathcal{A}_{&lt;m})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px4.p4.m1\" intent=\":literal\"><semantics><mrow><mo rspace=\"0.167em\">&#8722;</mo><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><msub><mover accent=\"true\"><mi>p</mi><mo>~</mo></mover><mi>&#952;</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><mo>&#8739;</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mrow><mi/><mo>&#8804;</mo><mi>m</mi></mrow></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">-\\log\\tilde{p}_{\\theta}(\\mathcal{A}_{m}\\mid\\mathcal{T}_{\\leq m},\\mathcal{A}_{&lt;m})</annotation></semantics></math> from Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.E13\" title=\"In Order-marginalized factorization for audio spans. &#8227; 3.3 Multimodal Factorization and a Unified Objective &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. The left-hand side is exactly the audio loss term for span <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px4.p4.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> in our practical training objective <math alttext=\"\\mathcal{L}_{\\text{AO}}(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px4.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>AO</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{AO}}(x)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Combining the text and audio terms according to the joint factorization in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.E14\" title=\"In Hybrid AR-NAR joint distribution. &#8227; 3.3 Multimodal Factorization and a Unified Objective &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> yields:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where the final inequality follows from combining the text equality with the audio inequality derived above, we provide a detailed derivation in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS1.SSS1\" title=\"A.1.1 Derivation of the Training Objective Upper Bound &#8227; A.1 Mathematical Derivation &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.F2\" title=\"Figure 2 &#8227; Training objective and upper bound analysis. &#8227; 3.3 Multimodal Factorization and a Unified Objective &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (a), our training pipeline starts from a pretrained text LLM and expands its vocabulary with discrete audio codebook tokens and control symbols (<math alttext=\"\\langle\\text{SOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>SOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{SOA}\\rangle</annotation></semantics></math>, <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math>). Each training sequence is organized as interleaved text spans and audio spans, where text spans are optimized with AR Loss while audio spans use the NAR diffusion objective.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its theoretical and practical advantages, the hybrid AR-NAR paradigm introduces a significant train-test discrepancies that can degrade generation quality. During training, audio spans are partially masked according to the diffusion process, while during inference, the model must generate audio and text tokens conditioned on complete text context and previously generated clean audio tokens. To bridge this gap, we propose three principled training strategies:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Batchwise AR &amp; NAR Objective Mixing (BANOM)</span>: To address the contextual distribution shift between training and inference, we selectively disable NAR training for a subset of samples within each batch. Specifically, with probability <math alttext=\"p_{\\text{mix}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>mix</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{mix}}</annotation></semantics></math>, we skip the diffusion noise addition process for certain samples and compute gradients only on text tokens using AR loss. This ensures that during training, text tokens occasionally observe clean, unmasked audio spans&#8212;matching the inference scenario where text generation conditions on previously generated complete audio content rather than partially masked spans.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prefix Preservation Masking (PPM)</span>: To address cross-span contextual inconsistency, we preserve clean prefixes of audio spans during diffusion training. For a fraction <math alttext=\"p_{\\text{prefix}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>prefix</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{prefix}}</annotation></semantics></math> of training samples, we randomly select a cutoff index <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and ensure that all preceding audio spans <math alttext=\"\\mathcal{A}_{&lt;m}=\\{\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{m-1}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>m</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}=\\{\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{m-1}\\}</annotation></semantics></math> remain unmasked, while applying NAR diffusion loss only to spans <math alttext=\"\\mathcal{A}_{\\geq m}=\\{\\mathcal{A}_{m},\\mathcal{A}_{m+1},\\ldots,\\mathcal{A}_{M}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&#8805;</mo><mi>m</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\geq m}=\\{\\mathcal{A}_{m},\\mathcal{A}_{m+1},\\ldots,\\mathcal{A}_{M}\\}</annotation></semantics></math>. This strategy ensures that during training, when generating span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math>, the model observes clean representations of all previous spans <math alttext=\"\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}</annotation></semantics></math>, matching the inference scenario where audio spans are generated sequentially and each span <math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math> conditions on fully generated, clean preceding spans <math alttext=\"\\mathcal{A}_{&lt;m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i2.p1.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{&lt;m}</annotation></semantics></math> rather than their corrupted, partially masked versions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Span Truncation (SST)</span>: We address the positional bias in <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> prediction by randomly truncating audio span <math alttext=\"\\mathcal{A}_{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}</annotation></semantics></math> during training. Due to disparate tokenization rates between text and audio, audio tokens significantly outnumber text tokens, resulting in fixed-size spans <math alttext=\"\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{M-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{M-1}</annotation></semantics></math> and a variable-length final span <math alttext=\"\\mathcal{A}_{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}</annotation></semantics></math>. Since all audio spans undergo simultaneous diffusion training, the model learns to predict <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> at fixed positions for early spans, creating a strong positional bias that hinders content-aware termination learning for the final span. To mitigate this, we implement stochastic truncation: with probability <math alttext=\"p_{\\text{trunc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m6\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>trunc</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{trunc}}</annotation></semantics></math>, we randomly select a truncation length <math alttext=\"k&lt;|\\mathcal{A}_{M}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k&lt;|\\mathcal{A}_{M}|</annotation></semantics></math> and create a truncated span <math alttext=\"\\mathcal{A}_{M}^{\\text{trunc}}=(a_{M,1},\\ldots,a_{M,k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi><mtext>trunc</mtext></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}_{M}^{\\text{trunc}}=(a_{M,1},\\ldots,a_{M,k})</annotation></semantics></math> by removing the original <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> token and suffix tokens <math alttext=\"(a_{M,k+1},\\ldots,a_{M,|\\mathcal{A}_{M}|})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>M</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>M</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{M,k+1},\\ldots,a_{M,|\\mathcal{A}_{M}|})</annotation></semantics></math>. This creates training samples where span termination occurs at arbitrary positions rather than fixed boundaries, forcing the model to predict <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> based on semantic content and contextual text rather than positional cues, enabling robust variable-length generation during inference.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable the hybrid AR-NAR generation paradigm within a single Transformer, we design a modality-aware attention mechanism that enforces different dependency structures for text and audio tokens. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.F2\" title=\"Figure 2 &#8227; Training objective and upper bound analysis. &#8227; 3.3 Multimodal Factorization and a Unified Objective &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (b), our attention design creates a step-wise pattern that handles three types of content with distinct patterns: the input prompt (system prompt and user query) follows strict causal attention where each token can only attend to previous tokens within the prompt; text tokens (<math alttext=\"\\mathcal{T}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{m}</annotation></semantics></math>) maintain strict causal attention both within spans and across spans, attending to the prompt, all previous spans, and preceding tokens within the current span; audio tokens (<math alttext=\"\\mathcal{A}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{m}</annotation></semantics></math>) use a hybrid pattern with bidirectional attention within each span to enable AO-ARM learning but causal attention across spans, attending to the prompt, all previous spans, and any token within the current audio span. This attention pattern serves two critical purposes: enabling parallel training efficiency where all audio spans can undergo noise addition simultaneously in a single forward pass, and preventing cross-span confusion by ensuring that AO-ARM objectives within each audio span do not interfere with dependencies between different spans.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S3.F3\" title=\"Figure 3 &#8227; 3.4 Modality-Aware Attention Mechanism &#8227; 3 Joint Text-AR &amp; Audio-NAR Model &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a), our TtT framework consists of three major components: an audio encoder, an audio decoder, and a unified MLLM backbone with hybrid AR-NAR capabilities. Similar to GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, we adopt CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib10\" title=\"\">2024</a>)</cite> as both the audio encoder and decoder. During inference, the TtT model alternates between AR text generation and NAR audio synthesis within a unified framework. The generation process operates as follows: Given an input audio, it is first encoded into a sequence of discrete audio tokens through the audio encoder, which are subsequently fed into the TtT model for processing. The TtT generation process begins with AR decoding for text tokens. Once the model encounters the special token <math alttext=\"\\langle\\text{SOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>SOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{SOA}\\rangle</annotation></semantics></math>, the system switches to NAR generation mode. The NAR generation employs block-wise diffusion (detailed in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#alg1\" title=\"Algorithm 1 &#8227; A.3 Block-wise Masked Diffusion Generation For Audio Tokens &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) to generate audio token spans in parallel, returning to AR mode after predicting <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> and dropping the remaining predicted tokens after <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> within the current diffusion block, then starting a new round of AR process. The whole sequence generation terminates after the AR process generates an <math alttext=\"\\langle\\text{EOS}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOS}\\rangle</annotation></semantics></math> token. Notably, once each audio span is generated, the corresponding audio tokens can be immediately passed to the audio decoder to synthesize audio streams in parallel, ensuring low first-token latency and enabling continuous streaming audio generation without waiting for the entire sequence completion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively evaluate our model on two representative tasks, we carefully design the evaluation protocol and metrics. For Audio-QA, we introduce an ASR-LLM pipeline that transcribes the model&#8217;s spoken responses using language-specific ASR systems (Paraformer-zh for Chinese and Whisper-Large-v3 for English) and leverages a powerful LLM-as-a-Judge (Qwen3-235B-A30B) to assess semantic correctness against ground-truth answers. This approach captures the model&#8217;s ability to generate natural, accurate spoken responses in realistic conversational settings. For the ASR task, we directly measure transcription accuracy using Word Error Rate (WER), which reflects both speech recognition fidelity and cross-modal alignment within our hybrid AR-NAR framework. More details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS5.SSS1\" title=\"A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our model using the AdamW optimizer with a global batch size of 2048, a learning rate of <math alttext=\"2e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2e^{-5}</annotation></semantics></math>, and a weight decay factor of <math alttext=\"1e^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-2}</annotation></semantics></math>. The learning rate follows a cosine decay schedule with a linear warmup ratio of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Training incorporates three stochastic strategies: (1) batchwise AR &amp; NAR objective mixing with probability 0.3; (2) prefix preservation masking with ratio 0.3; (3) stochastic span truncation with probability 0.5. During inference, the model alternates between AR text decoding and NAR diffusion-based audio generation, where text decoding uses nucleus sampling with <math alttext=\"k=10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">k=10</annotation></semantics></math> and <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math>, and audio spans are generated with 200 diffusion steps, a block length of 32 tokens, and a total diffusion span length of 640 tokens under classifier-free guidance with scale 0.1. Since different training strategies lead to varying convergence speeds, reported results are based on checkpoints where training loss has converged. All experiments are conducted on 4 nodes with 8 NVIDIA A100 GPUs per node using the DeepSpeed runtime.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part reports the comparison of conversational understanding ability across different models on Audio-QA task. From these results we make the following observations: (1) Our proposed TtT consistently achieves the best performance on all four Audio-QA datasets. For example, at the 3B scale, TtT attains scores of 17.46, 34.68, 6.53, and 11.61 on AlpacaEval, LLaMAQuestions, TriviaQA, and WebQuestions respectively, surpassing the Qwen2.5-Base (AR) model by +3.04, +24.68, +5.93, and +10.91. This improvement stems from the hybrid AR&#8211;NAR design, which better captures the inherent asymmetry between text and audio dependencies.\n(2) Our results demonstrate a clear scaling trend shared with the baselines, where larger backbones consistently yield stronger performance. For instance, TtT-3B achieves 34.68 on LLaMAQuestions, outperforming its 1.5B counterpart which scores 23.75, reflecting an absolute gain of +10.93. This pattern aligns with the scaling laws of LLMs, reflecting that additional capacity enhances both reasoning ability and cross-modal representation learning; (3) Qwen2.5-Base (NAR) performs noticeably worse than both our hybrid framework and the AR-only baseline. Since discrete diffusion models employ an order-agnostic AO-ARM objective that ignore the inherent sequential structure of interleaved text-audio sequences. This mismatch leads to order confusion, compromising sequence coherence and weakening audio-text alignment.</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "task",
                    "audio",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part, our proposed TtT significantly outperforms both Qwen2.5-Base (AR) and Qwen2.5-Base (NAR) on ASR benchmarks, demonstrating superior cross-modal alignment. For example, at the 3B scale, TtT achieves 12.53 and 13.65 WER on AISHELL-2 and AISHELL-1 respectively, yielding improvements of 42.41 and 58.36 absolute WER points over Qwen2.5-Base (AR). This advantage arises from the hybrid AR&#8211;NAR architecture, where NAR diffusion modeling provides efficient parallel denoising that enforces tighter alignment between acoustic frames and textual representations, while AR components ensure coherent conditioning across modalities. (2) Similar to the conversational understanding task, we observe a clear scaling trend in cross-modal alignment. Larger backbones consistently deliver better alignment, with TtT-3B achieving substantially lower WER than TtT-1.5B (e.g., 12.53 vs. 14.89 on AISHELL-2, and 13.65 vs. 16.72 on AISHELL-1). This scaling behavior reflects the increased capacity of larger models to capture fine-grained acoustic patterns and maintain robust cross-modal mappings, thereby enhancing alignment quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr",
                    "aishell2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contribution of each training strategy in our hybrid AR-NAR framework, we perform an ablation study based on the full model TtT (AR-NAR). The variant w/o BANOM corresponds to removing batchwise AR &amp; NAR objective mixing from the full model, w/o PPM removes prefix preservation masking, and w/o SST removes stochastic span truncation. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> ablation study part presents the detailed results of our ablation experiments. From these results we draw the following conclusions: (1) All three training strategies have a positive impact on model performance, and removing any one of them leads to clear degradation. For instance, on the LLaMAQuestions dataset, removing SST reduces the score from 34.68 to 10.20. This drop occurs because stochastic truncation plays a crucial role in mitigating positional bias in &#10216;EOA&#10217; prediction. By forcing the model to terminate spans based on semantic content rather than fixed positional cues, SST enables more robust variable-length audio generation, and its removal undermines the model&#8217;s ability to handle flexible conversational outputs. (2) Removing BANOM yields the largest performance degradation. For example, on the AISHELL-2 dataset, the performance decreases from 12.53 to 18.58 when the strategy is removed. This is because it is essential for exposing text tokens to clean audio prefixes during training, which better matches inference conditions. Without this mechanism, the model suffers from a sharper train&#8211;test discrepancy, leading to weaker cross-modal consistency and degraded alignment quality.</p>\n\n",
                "matched_terms": [
                    "aishell2",
                    "audio",
                    "training",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate the effectiveness of our method on top of a multimodally aligned pretrained model, we perform large-scale multimodal pretraining based on the Qwen2.5-3B-Base model. Specifically, we construct a corpus of approximately 200B tokens covering ASR, TTS, text-only data, and interleaved text&#8211;audio data. The model is trained with a standard autoregressive objective using a global batch size of 256 for 140k steps. This pretraining stage equips the backbone, namely Qwen2.5-3B-Base with strong cross-modal alignment ability before applying our hybrid AR&#8211;NAR learning framework.</p>\n\n",
                "matched_terms": [
                    "data",
                    "interleaved",
                    "tts",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> training strategy comparison part compares AR-only and AR&#8211;NAR frameworks under two different settings: (1) training directly from Qwen2.5-3B-Base without multimodal pretraining, namely TtT (AR&#8211;NAR), and (2) initialization from the multimodally aligned pretrained model, namely Pretrain+AR and Pretrain+TtT. From the table, we observe that: (1) when trained directly from Qwen2.5-3B-Base, our TtT framework achieves comparable or even superior performance to the AR-only baseline, indicating that the hybrid AR&#8211;NAR design is already competitive without pretraining; (2) when applied on top of the multimodally aligned pretrained model, Pretrain+TtT consistently matches or surpasses Pretrain+AR across both Audio-QA and ASR benchmarks. These results demonstrate that TtT not only performs strongly from scratch, but also benefits significantly when built upon large-scale multimodal alignment pretraining.</p>\n\n",
                "matched_terms": [
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented TtT, a unified multimodal framework that addresses the fundamental asymmetry in dependency structures between text and audio modalities. By integrating autoregressive generation for text with non-autoregressive discrete diffusion for audio within a single Transformer, TtT leverages the target-target dependencies of text and the source-target dependencies of audio. We established a theoretical foundation showing that our hybrid training objective provides an upper bound on the negative log-likelihood of the desired joint distribution. To mitigate train-test discrepancies inherent in this hybrid approach, we introduced three principled training strategies: Batchwise AR&#8211;NAR Objective Mixing, Prefix Preservation Masking, and Stochastic Span Truncation. Extensive experiments on ASR and Audio-QA tasks demonstrate that TtT significantly outperforms purely autoregressive or non-autoregressive baselines, achieving superior performance in conversational understanding and cross-modal alignment tasks. Our results validate that respecting the distinct generative natures of each modality is crucial for building effective speech-in/speech-out systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in end-to-end audio-language models have moved beyond traditional cascaded architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib4\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib29\" title=\"\">2023</a>)</cite> toward unified multimodal frameworks. Representative works include Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, which achieves real-time duplex speech conversation through hierarchical Transformer architectures; GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, which builds upon GLM-4-9B for robust Chinese and English speech processing; and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite>, which introduces a lightweight Multiple Cross-modal Token Prediction (MCTP) module for fast audio-text generation with significantly reduced first-token latency. More recent efforts have focused on scaling and production readiness: Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>)</cite> presents a 130B-parameter unified speech-text model with generative speech data engine and instruction-driven fine control across dialects, emotions, singing, and RAP, while Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>)</cite> features text-guided aligned speech generation with multi-codebook discretization to preserve both semantic and acoustic information. UniWav <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib17\" title=\"\">2025</a>)</cite> proposes the first unified encoder-decoder framework that jointly learns representation encoders and generative audio decoders for both discriminative and generative speech tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key limitation shared by these approaches is their reliance on uniform autoregressive objectives for both text and audio tokens, which overlooks the distinct dependency structures of these modalities. Our work addresses this gap by proposing a hybrid AR-NAR framework that respects the inherent asymmetries between text and audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete diffusion models have emerged as a compelling alternative to autoregressive generation, offering non-autoregressive approaches that can generate entire sequences in parallel. The foundational work of D3PMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>)</cite> generalized diffusion processes to discrete data through flexible transition matrices, with absorbing processes that progressively mask tokens proving particularly effective. This framework has since evolved through both theoretical advances and practical improvements. From a theoretical perspective, recent work has deepened our understanding of discrete diffusion dynamics. <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite> revealed that absorbing diffusion&#8217;s concrete score can be expressed as time-independent conditional probabilities, leading to RADD&#8212;a reparameterized model that removes explicit time conditioning while establishing connections to any-order autoregressive generation. Building on this foundation, <cite class=\"ltx_cite ltx_citemacro_citet\">Li &amp; Cai (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib15\" title=\"\">2025</a>)</cite> formally characterized convergence rates, proving that KL divergence decays at <math alttext=\"O(1/T)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(1/T)</annotation></semantics></math> with bounds scaling linearly with token mutual information. However, <cite class=\"ltx_cite ltx_citemacro_citet\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib11\" title=\"\">2025</a>)</cite> identified a fundamental trade-off: while masked diffusion achieves near-optimal perplexity in constant steps, sequence-level tasks like reasoning may require steps linear in sequence length. Practical advances have focused on training efficiency and application domains. <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib27\" title=\"\">2024</a>)</cite> reformulated the variational objective as a weighted integral of cross-entropy losses, unifying prior approaches while achieving state-of-the-art results that even surpass comparable autoregressive baselines. For complex reasoning tasks where autoregressive models struggle with subgoal imbalance, <cite class=\"ltx_cite ltx_citemacro_citet\">Ye et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib33\" title=\"\">2024</a>)</cite> demonstrated that Multi-Granularity Diffusion Modeling can achieve near-perfect accuracy by prioritizing harder subgoals during training. The scalability challenge has been addressed through innovative adaptation strategies. Rather than training from scratch, <cite class=\"ltx_cite ltx_citemacro_citet\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>); Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib21\" title=\"\">2025</a>)</cite> showed that pretrained autoregressive models can be efficiently converted to diffusion models via continual pre-training, maintaining competitive performance while enabling parallel generation. Meanwhile, hybrid approaches are gaining traction: <cite class=\"ltx_cite ltx_citemacro_citet\">Lovelace et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib19\" title=\"\">2024</a>)</cite> combined diffusion-based latent proposals with autoregressive decoding for controllable generation, while <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib32\" title=\"\">2025</a>)</cite> developed MMaDA, a unified multimodal diffusion foundation model that processes text, images, and reasoning within a single architecture.</p>\n\n",
                "matched_terms": [
                    "data",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As detailed in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#alg1\" title=\"Algorithm 1 &#8227; A.3 Block-wise Masked Diffusion Generation For Audio Tokens &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the model generates audio in fixed-size blocks of length <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>, where each block is progressively denoised over <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> steps using an absorbing discrete diffusion process. At each denoising step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model predicts tokens for all currently masked positions in parallel. The algorithm then selectively commits the most confident predictions (determined by predicted probability or random sampling) while remasking the remaining positions for further refinement. This progressive denoising continues until all positions in the current block are decoded. Crucially, if an <math alttext=\"\\langle\\text{EOA}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOA</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOA}\\rangle</annotation></semantics></math> token is generated within a block, decoding terminates immediately at that position, truncating the remainder and seamlessly returning control to the AR text generation mode.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous evaluation frameworks such as Kimi-Audio assess Audio-QA performance using the text portion of interleaved outputs, which overlooks the fact that the audio output of an end-to-end speech model more directly reflects its ability to generate natural and semantically faithful responses. To address this limitation, we evaluate Audio-QA directly on the audio outputs of our framework by first applying an ASR model to transcribe the generated audio into text, where Whisper-Large-v3 is used for English audio and Paraformer-zh for Chinese audio, with a comparison of ASR performance across different models provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T3\" title=\"Table 3 &#8227; Audio-QA Task &#8227; A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The transcribed text is then combined with the original QA queries and the ground truth answers and passed to a large scale reasoning model, Qwen3-235B-A30B, which serves as an LLM-as-a-Judge model to determine whether the response semantically matches the reference and to provide either a correctness label or a graded score. We report the average accuracy or score on the benchmark, and this evaluation pipeline provides a more faithful assessment of our model&#8217;s audio-to-audio QA ability in realistic conversational scenarios where speech serves as the output modality.</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "english",
                    "audio",
                    "asr",
                    "used",
                    "chinese",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the model&#8217;s capability in aligning speech with textual representations, we evaluate it on the ASR task, where the model generates text transcriptions from input audio and performance is measured using word error rate (WER). A lower WER indicates more accurate recognition, which reflects not only strong ASR ability but also effective cross modal consistency achieved by our hybrid AR-NAR modeling framework.</p>\n\n",
                "matched_terms": [
                    "task",
                    "audio",
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate model performance on a diverse set of benchmarks covering both Audio Question Answering (Audio-QA) and Automatic Speech Recognition (ASR) tasks. For Audio-QA, we use four datasets: AlpacaEval, TriviaQA, and WebQuestions (English), along with ReasoningQA (Chinese), assessing cross-lingual reasoning and comprehension from speech. For ASR, we include five datasets: Fleurs-zh/en (multilingual), AISHELL-1/2, and WenetSpeech (all Chinese), covering varied domains, accents, and recording conditions to robustly measure transcription accuracy. Dataset details are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "audio",
                    "wenetspeech",
                    "asr",
                    "dataset",
                    "chinese",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable unified training across diverse tasks, we transform all datasets into a consistent input&#8211;output format. On the one hand, this standardization allows the model to seamlessly integrate heterogeneous modalities such as speech, text, and interleaved audio&#8211;text sequences. On the other hand, a unified design is essential for supporting our training strategies, including batchwise AR &amp; NAR objective mixing, prefix preservation masking, and stochastic span truncation. These strategies rely on a shared representation to operate across modalities in a consistent way. For clarity, we provide representative examples of the adopted data formats as follow, covering ASR, TTS, audio chat, text chat, AAC, SEC, ASC, and interleaved text&#8211;audio data.</p>\n\n",
                "matched_terms": [
                    "aac",
                    "asc",
                    "interleaved",
                    "tts",
                    "audio",
                    "data",
                    "training",
                    "asr",
                    "chat",
                    "sec",
                    "text",
                    "datasets"
                ]
            }
        ]
    },
    "A1.T3": {
        "caption": "Table 3: WER performance of different ASR models on Chinese (zh) and English (en).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">WER-zh</span> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">WER-en</span> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">Whisper-Large-v3</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5054</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.2167</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">Paraformer-zh</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.1028</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.3946</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "english",
            "wer",
            "model",
            "weren",
            "whisperlargev3",
            "asr",
            "werzh",
            "↓downarrow",
            "chinese",
            "different",
            "paraformerzh",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Previous evaluation frameworks such as Kimi-Audio assess Audio-QA performance using the text portion of interleaved outputs, which overlooks the fact that the audio output of an end-to-end speech model more directly reflects its ability to generate natural and semantically faithful responses. To address this limitation, we evaluate Audio-QA directly on the audio outputs of our framework by first applying an ASR model to transcribe the generated audio into text, where Whisper-Large-v3 is used for English audio and Paraformer-zh for Chinese audio, with a comparison of ASR performance across different models provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T3\" title=\"Table 3 &#8227; Audio-QA Task &#8227; A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The transcribed text is then combined with the original QA queries and the ground truth answers and passed to a large scale reasoning model, Qwen3-235B-A30B, which serves as an LLM-as-a-Judge model to determine whether the response semantically matches the reference and to provide either a correctness label or a graded score. We report the average accuracy or score on the benchmark, and this evaluation pipeline provides a more faithful assessment of our model&#8217;s audio-to-audio QA ability in realistic conversational scenarios where speech serves as the output modality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.\n\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent success of LLMs has catalyzed a paradigm shift towards general-purpose Multimodal Large Language Models (MLLMs) capable of processing and generating information across diverse modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib31\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib28\" title=\"\">2023</a>)</cite>. Among these, speech-in/speech-out conversational systems have emerged as a pivotal component in facilitating natural human-AI interaction. Conventional systems typically decompose this problem into a cascaded pipeline of automatic speech recognition (ASR), LLM-driven response generation, and text-to-speech (TTS) synthesis. While effective to a degree, this modular design introduces significant latency accumulation and error propagation between modules, hindering naturalness and real-world applicability. In response, recent end-to-end approaches like Moshi <cite class=\"ltx_cite ltx_citemacro_citet\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citet\">Long et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> have sought to unify speech understanding and generation within a single model. These models are typically trained through multi-stage pipelines that involve text-to-audio tokenizer training, interleaved data construction, text-audio alignment and task-oriented supervised fine-tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib5\" title=\"\">2024</a>)</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these methods aim to generate interleaved text and speech tokens in an autoregressive manner, which are then decoded into continuous audio waveforms by a separate neural codec or diffusion-based decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Mehta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib20\" title=\"\">2024</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib14\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, this emerging paradigm faces a fundamental challenges.\nAs illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we identify a fundamental mismatch in prevailing approaches that employ a single language model to autoregressively generate both text and audio tokens. This uniform treatment applies identical autoregressive training objectives across both modalities, overlooks a critical distinction in their underlying generative processes <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>; Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib30\" title=\"\">2024</a>; Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib2\" title=\"\">2023</a>; Dang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib6\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib25\" title=\"\">2023</a>)</cite>.\nText generation inherently follows a sequential causal structure characterized by strong <span class=\"ltx_text ltx_font_bold\">target-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Box et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib3\" title=\"\">2015</a>)</cite>, where each token explicitly conditions on previously generated tokens. Consequently, an incorrect token prediction can propagate and introduce subsequent errors due to the exposure bias inherent in AR models <cite class=\"ltx_cite ltx_citemacro_citet\">Ranzato et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib23\" title=\"\">2015</a>)</cite>.\nIn contrast, audio token generation is predominantly driven by <span class=\"ltx_text ltx_font_bold\">source-target</span> dependencies <cite class=\"ltx_cite ltx_citemacro_citet\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib24\" title=\"\">2020</a>)</cite>, where audio output primarily condition on the source text rather than on the preceding audio tokens. Specifically,\nwithin the current NAR span, audio tokens generation should remain faithful to the source text even when previous audio tokens are incorrectly predicted. Applying a purely AR objective to audio generation thus introduces unnecessary sequential constraints, leading to suboptimal training dynamics and magnifying error propagation. This problem can be substantially alleviated by adopting a non-autoregressive generation strategy, which aligns better with the source-dependent nature of audio modeling. Recently, discrete diffusion has emerged as a compelling alternative to AR for discrete sequence modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib34\" title=\"\">2025</a>; Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>; Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>; Sahoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib26\" title=\"\">2024</a>)</cite>. Beyond empirical gains, recent theory shows that absorbing discrete diffusion can be interpreted as modeling the conditional distributions of clean tokens and admits a tight connection to any&#8209;order AR objectives <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\pi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><mi>&#960;</mi><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math> is a random permutation of the token indices. Therefore, training an absorbing discrete diffusion model is equivalent to training a powerful ensemble of autoregressive models that can operate in any order. This inherent flexibility is what enables parallel, non-autoregressive generation at inference time and makes it a suitable choice for modeling source-dependent modalities like audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively train and evaluate our proposed TtT framework, we follow prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> and adopt a diverse collection of multi-task datasets, including ASR, TTS, audio chat, text chat, automated audio captioning (AAC), speech emotion classification (SEC), acoustic scene classification (ASC), and interleaved text&#8211;audio data. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T2\" title=\"Table 2 &#8227; A.4 Training dataset details &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a summary of the training datasets, with detailed examples provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS6\" title=\"A.6 Data Format of Training Data &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>. During training, we aim to construct a balanced corpus that supports effective learning across multiple tasks. Specifically, we randomly sample one million instances from the ASR dataset, the TTS dataset, and the audio chat dataset respectively. In addition, we create bilingual interleaved text and audio data, ensuring that Chinese and English are represented in approximately equal proportions. To build the audio chat corpus, we rely on the text-to-audio dataset VoiceAssistant-400K together with the text-based datasets OpenHermes-2.5 and Firefly-Train-1.1M, and we employ a TTS model, namely CosyVoice2, to convert text into synthetic audio so as to enrich the training data. To further enhance cross-modal alignment between text and audio, we follow prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>)</cite> and supplement the training corpus with interleaved text and audio data derived from the large-scale pretrained corpus FineWeb-Edu. This strategy not only expands task coverage but also strengthens the model&#8217;s ability to jointly learn from and align textual and acoustic modalities. For evaluation, we focus on two representative tasks: audio question answering (Audio-QA), which evaluates both the model&#8217;s reasoning skills and its text-to-speech capabilities in an end-to-end audio generation framework, and ASR, which measures the quality of cross-modal alignment between text and audio. The evaluation datasets are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "chinese",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively evaluate our model on two representative tasks, we carefully design the evaluation protocol and metrics. For Audio-QA, we introduce an ASR-LLM pipeline that transcribes the model&#8217;s spoken responses using language-specific ASR systems (Paraformer-zh for Chinese and Whisper-Large-v3 for English) and leverages a powerful LLM-as-a-Judge (Qwen3-235B-A30B) to assess semantic correctness against ground-truth answers. This approach captures the model&#8217;s ability to generate natural, accurate spoken responses in realistic conversational settings. For the ASR task, we directly measure transcription accuracy using Word Error Rate (WER), which reflects both speech recognition fidelity and cross-modal alignment within our hybrid AR-NAR framework. More details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS5.SSS1\" title=\"A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "wer",
                    "model",
                    "whisperlargev3",
                    "asr",
                    "chinese",
                    "paraformerzh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train our model using the AdamW optimizer with a global batch size of 2048, a learning rate of <math alttext=\"2e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2e^{-5}</annotation></semantics></math>, and a weight decay factor of <math alttext=\"1e^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-2}</annotation></semantics></math>. The learning rate follows a cosine decay schedule with a linear warmup ratio of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Training incorporates three stochastic strategies: (1) batchwise AR &amp; NAR objective mixing with probability 0.3; (2) prefix preservation masking with ratio 0.3; (3) stochastic span truncation with probability 0.5. During inference, the model alternates between AR text decoding and NAR diffusion-based audio generation, where text decoding uses nucleus sampling with <math alttext=\"k=10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">k=10</annotation></semantics></math> and <math alttext=\"p=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">p=0.95</annotation></semantics></math>, and audio spans are generated with 200 diffusion steps, a block length of 32 tokens, and a total diffusion span length of 640 tokens under classifier-free guidance with scale 0.1. Since different training strategies lead to varying convergence speeds, reported results are based on checkpoints where training loss has converged. All experiments are conducted on 4 nodes with 8 NVIDIA A100 GPUs per node using the DeepSpeed runtime.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part reports the comparison of conversational understanding ability across different models on Audio-QA task. From these results we make the following observations: (1) Our proposed TtT consistently achieves the best performance on all four Audio-QA datasets. For example, at the 3B scale, TtT attains scores of 17.46, 34.68, 6.53, and 11.61 on AlpacaEval, LLaMAQuestions, TriviaQA, and WebQuestions respectively, surpassing the Qwen2.5-Base (AR) model by +3.04, +24.68, +5.93, and +10.91. This improvement stems from the hybrid AR&#8211;NAR design, which better captures the inherent asymmetry between text and audio dependencies.\n(2) Our results demonstrate a clear scaling trend shared with the baselines, where larger backbones consistently yield stronger performance. For instance, TtT-3B achieves 34.68 on LLaMAQuestions, outperforming its 1.5B counterpart which scores 23.75, reflecting an absolute gain of +10.93. This pattern aligns with the scaling laws of LLMs, reflecting that additional capacity enhances both reasoning ability and cross-modal representation learning; (3) Qwen2.5-Base (NAR) performs noticeably worse than both our hybrid framework and the AR-only baseline. Since discrete diffusion models employ an order-agnostic AO-ARM objective that ignore the inherent sequential structure of interleaved text-audio sequences. This mismatch leads to order confusion, compromising sequence coherence and weakening audio-text alignment.</p>\n\n",
                "matched_terms": [
                    "different",
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part, our proposed TtT significantly outperforms both Qwen2.5-Base (AR) and Qwen2.5-Base (NAR) on ASR benchmarks, demonstrating superior cross-modal alignment. For example, at the 3B scale, TtT achieves 12.53 and 13.65 WER on AISHELL-2 and AISHELL-1 respectively, yielding improvements of 42.41 and 58.36 absolute WER points over Qwen2.5-Base (AR). This advantage arises from the hybrid AR&#8211;NAR architecture, where NAR diffusion modeling provides efficient parallel denoising that enforces tighter alignment between acoustic frames and textual representations, while AR components ensure coherent conditioning across modalities. (2) Similar to the conversational understanding task, we observe a clear scaling trend in cross-modal alignment. Larger backbones consistently deliver better alignment, with TtT-3B achieving substantially lower WER than TtT-1.5B (e.g., 12.53 vs. 14.89 on AISHELL-2, and 13.65 vs. 16.72 on AISHELL-1). This scaling behavior reflects the increased capacity of larger models to capture fine-grained acoustic patterns and maintain robust cross-modal mappings, thereby enhancing alignment quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contribution of each training strategy in our hybrid AR-NAR framework, we perform an ablation study based on the full model TtT (AR-NAR). The variant w/o BANOM corresponds to removing batchwise AR &amp; NAR objective mixing from the full model, w/o PPM removes prefix preservation masking, and w/o SST removes stochastic span truncation. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> ablation study part presents the detailed results of our ablation experiments. From these results we draw the following conclusions: (1) All three training strategies have a positive impact on model performance, and removing any one of them leads to clear degradation. For instance, on the LLaMAQuestions dataset, removing SST reduces the score from 34.68 to 10.20. This drop occurs because stochastic truncation plays a crucial role in mitigating positional bias in &#10216;EOA&#10217; prediction. By forcing the model to terminate spans based on semantic content rather than fixed positional cues, SST enables more robust variable-length audio generation, and its removal undermines the model&#8217;s ability to handle flexible conversational outputs. (2) Removing BANOM yields the largest performance degradation. For example, on the AISHELL-2 dataset, the performance decreases from 12.53 to 18.58 when the strategy is removed. This is because it is essential for exposing text tokens to clean audio prefixes during training, which better matches inference conditions. Without this mechanism, the model suffers from a sharper train&#8211;test discrepancy, leading to weaker cross-modal consistency and degraded alignment quality.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate the effectiveness of our method on top of a multimodally aligned pretrained model, we perform large-scale multimodal pretraining based on the Qwen2.5-3B-Base model. Specifically, we construct a corpus of approximately 200B tokens covering ASR, TTS, text-only data, and interleaved text&#8211;audio data. The model is trained with a standard autoregressive objective using a global batch size of 256 for 140k steps. This pretraining stage equips the backbone, namely Qwen2.5-3B-Base with strong cross-modal alignment ability before applying our hybrid AR&#8211;NAR learning framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> training strategy comparison part compares AR-only and AR&#8211;NAR frameworks under two different settings: (1) training directly from Qwen2.5-3B-Base without multimodal pretraining, namely TtT (AR&#8211;NAR), and (2) initialization from the multimodally aligned pretrained model, namely Pretrain+AR and Pretrain+TtT. From the table, we observe that: (1) when trained directly from Qwen2.5-3B-Base, our TtT framework achieves comparable or even superior performance to the AR-only baseline, indicating that the hybrid AR&#8211;NAR design is already competitive without pretraining; (2) when applied on top of the multimodally aligned pretrained model, Pretrain+TtT consistently matches or surpasses Pretrain+AR across both Audio-QA and ASR benchmarks. These results demonstrate that TtT not only performs strongly from scratch, but also benefits significantly when built upon large-scale multimodal alignment pretraining.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented TtT, a unified multimodal framework that addresses the fundamental asymmetry in dependency structures between text and audio modalities. By integrating autoregressive generation for text with non-autoregressive discrete diffusion for audio within a single Transformer, TtT leverages the target-target dependencies of text and the source-target dependencies of audio. We established a theoretical foundation showing that our hybrid training objective provides an upper bound on the negative log-likelihood of the desired joint distribution. To mitigate train-test discrepancies inherent in this hybrid approach, we introduced three principled training strategies: Batchwise AR&#8211;NAR Objective Mixing, Prefix Preservation Masking, and Stochastic Span Truncation. Extensive experiments on ASR and Audio-QA tasks demonstrate that TtT significantly outperforms purely autoregressive or non-autoregressive baselines, achieving superior performance in conversational understanding and cross-modal alignment tasks. Our results validate that respecting the distinct generative natures of each modality is crucial for building effective speech-in/speech-out systems.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in end-to-end audio-language models have moved beyond traditional cascaded architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib4\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib29\" title=\"\">2023</a>)</cite> toward unified multimodal frameworks. Representative works include Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, which achieves real-time duplex speech conversation through hierarchical Transformer architectures; GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, which builds upon GLM-4-9B for robust Chinese and English speech processing; and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite>, which introduces a lightweight Multiple Cross-modal Token Prediction (MCTP) module for fast audio-text generation with significantly reduced first-token latency. More recent efforts have focused on scaling and production readiness: Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>)</cite> presents a 130B-parameter unified speech-text model with generative speech data engine and instruction-driven fine control across dialects, emotions, singing, and RAP, while Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>)</cite> features text-guided aligned speech generation with multi-codebook discretization to preserve both semantic and acoustic information. UniWav <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib17\" title=\"\">2025</a>)</cite> proposes the first unified encoder-decoder framework that jointly learns representation encoders and generative audio decoders for both discriminative and generative speech tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chinese",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete diffusion models have emerged as a compelling alternative to autoregressive generation, offering non-autoregressive approaches that can generate entire sequences in parallel. The foundational work of D3PMs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib1\" title=\"\">2021</a>)</cite> generalized diffusion processes to discrete data through flexible transition matrices, with absorbing processes that progressively mask tokens proving particularly effective. This framework has since evolved through both theoretical advances and practical improvements. From a theoretical perspective, recent work has deepened our understanding of discrete diffusion dynamics. <cite class=\"ltx_cite ltx_citemacro_citet\">Ou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib22\" title=\"\">2024</a>)</cite> revealed that absorbing diffusion&#8217;s concrete score can be expressed as time-independent conditional probabilities, leading to RADD&#8212;a reparameterized model that removes explicit time conditioning while establishing connections to any-order autoregressive generation. Building on this foundation, <cite class=\"ltx_cite ltx_citemacro_citet\">Li &amp; Cai (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib15\" title=\"\">2025</a>)</cite> formally characterized convergence rates, proving that KL divergence decays at <math alttext=\"O(1/T)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(1/T)</annotation></semantics></math> with bounds scaling linearly with token mutual information. However, <cite class=\"ltx_cite ltx_citemacro_citet\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib11\" title=\"\">2025</a>)</cite> identified a fundamental trade-off: while masked diffusion achieves near-optimal perplexity in constant steps, sequence-level tasks like reasoning may require steps linear in sequence length. Practical advances have focused on training efficiency and application domains. <cite class=\"ltx_cite ltx_citemacro_citet\">Shi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib27\" title=\"\">2024</a>)</cite> reformulated the variational objective as a weighted integral of cross-entropy losses, unifying prior approaches while achieving state-of-the-art results that even surpass comparable autoregressive baselines. For complex reasoning tasks where autoregressive models struggle with subgoal imbalance, <cite class=\"ltx_cite ltx_citemacro_citet\">Ye et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib33\" title=\"\">2024</a>)</cite> demonstrated that Multi-Granularity Diffusion Modeling can achieve near-perfect accuracy by prioritizing harder subgoals during training. The scalability challenge has been addressed through innovative adaptation strategies. Rather than training from scratch, <cite class=\"ltx_cite ltx_citemacro_citet\">Gong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib12\" title=\"\">2024</a>); Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib21\" title=\"\">2025</a>)</cite> showed that pretrained autoregressive models can be efficiently converted to diffusion models via continual pre-training, maintaining competitive performance while enabling parallel generation. Meanwhile, hybrid approaches are gaining traction: <cite class=\"ltx_cite ltx_citemacro_citet\">Lovelace et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib19\" title=\"\">2024</a>)</cite> combined diffusion-based latent proposals with autoregressive decoding for controllable generation, while <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib32\" title=\"\">2025</a>)</cite> developed MMaDA, a unified multimodal diffusion foundation model that processes text, images, and reasoning within a single architecture.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the model&#8217;s capability in aligning speech with textual representations, we evaluate it on the ASR task, where the model generates text transcriptions from input audio and performance is measured using word error rate (WER). A lower WER indicates more accurate recognition, which reflects not only strong ASR ability but also effective cross modal consistency achieved by our hybrid AR-NAR modeling framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate model performance on a diverse set of benchmarks covering both Audio Question Answering (Audio-QA) and Automatic Speech Recognition (ASR) tasks. For Audio-QA, we use four datasets: AlpacaEval, TriviaQA, and WebQuestions (English), along with ReasoningQA (Chinese), assessing cross-lingual reasoning and comprehension from speech. For ASR, we include five datasets: Fleurs-zh/en (multilingual), AISHELL-1/2, and WenetSpeech (all Chinese), covering varied domains, accents, and recording conditions to robustly measure transcription accuracy. Dataset details are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "asr",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable unified training across diverse tasks, we transform all datasets into a consistent input&#8211;output format. On the one hand, this standardization allows the model to seamlessly integrate heterogeneous modalities such as speech, text, and interleaved audio&#8211;text sequences. On the other hand, a unified design is essential for supporting our training strategies, including batchwise AR &amp; NAR objective mixing, prefix preservation masking, and stochastic span truncation. These strategies rely on a shared representation to operate across modalities in a consistent way. For clarity, we provide representative examples of the adopted data formats as follow, covering ASR, TTS, audio chat, text chat, AAC, SEC, ASC, and interleaved text&#8211;audio data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            }
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Evaluation datasets used for Audio-QA and ASR tasks.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task Type</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AlpacaEval</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Audio-QA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ReasoningQA</th>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">Audio-QA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TriviaQA</th>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">Audio-QA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">WebQuestions</th>\n<td class=\"ltx_td ltx_align_center\">English</td>\n<td class=\"ltx_td ltx_align_center\">Audio-QA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Fleurs-zh</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AISHELL-2</th>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AISHELL-1</th>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">WenetSpeech</th>\n<td class=\"ltx_td ltx_align_center\">Chinese</td>\n<td class=\"ltx_td ltx_align_center\">ASR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Fleurs-en</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">ASR</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "fleurszh",
            "evaluation",
            "tasks",
            "used",
            "triviaqa",
            "audioqa",
            "english",
            "aishell2",
            "language",
            "aishell1",
            "asr",
            "datasets",
            "fleursen",
            "task",
            "reasoningqa",
            "alpacaeval",
            "wenetspeech",
            "webquestions",
            "dataset",
            "chinese"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To effectively train and evaluate our proposed TtT framework, we follow prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> and adopt a diverse collection of multi-task datasets, including ASR, TTS, audio chat, text chat, automated audio captioning (AAC), speech emotion classification (SEC), acoustic scene classification (ASC), and interleaved text&#8211;audio data. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T2\" title=\"Table 2 &#8227; A.4 Training dataset details &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides a summary of the training datasets, with detailed examples provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS6\" title=\"A.6 Data Format of Training Data &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>. During training, we aim to construct a balanced corpus that supports effective learning across multiple tasks. Specifically, we randomly sample one million instances from the ASR dataset, the TTS dataset, and the audio chat dataset respectively. In addition, we create bilingual interleaved text and audio data, ensuring that Chinese and English are represented in approximately equal proportions. To build the audio chat corpus, we rely on the text-to-audio dataset VoiceAssistant-400K together with the text-based datasets OpenHermes-2.5 and Firefly-Train-1.1M, and we employ a TTS model, namely CosyVoice2, to convert text into synthetic audio so as to enrich the training data. To further enhance cross-modal alignment between text and audio, we follow prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib36\" title=\"\">2025</a>)</cite> and supplement the training corpus with interleaved text and audio data derived from the large-scale pretrained corpus FineWeb-Edu. This strategy not only expands task coverage but also strengthens the model&#8217;s ability to jointly learn from and align textual and acoustic modalities. For evaluation, we focus on two representative tasks: audio question answering (Audio-QA), which evaluates both the model&#8217;s reasoning skills and its text-to-speech capabilities in an end-to-end audio generation framework, and ASR, which measures the quality of cross-modal alignment between text and audio. The evaluation datasets are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">We evaluate model performance on a diverse set of benchmarks covering both Audio Question Answering (Audio-QA) and Automatic Speech Recognition (ASR) tasks. For Audio-QA, we use four datasets: AlpacaEval, TriviaQA, and WebQuestions (English), along with ReasoningQA (Chinese), assessing cross-lingual reasoning and comprehension from speech. For ASR, we include five datasets: Fleurs-zh/en (multilingual), AISHELL-1/2, and WenetSpeech (all Chinese), covering varied domains, accents, and recording conditions to robustly measure transcription accuracy. Dataset details are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T4\" title=\"Table 4 &#8227; A.5.2 Evaluation Datasets &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates autoregressive (AR) text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Extensive experiments across Audio-QA and ASR tasks demonstrate the effectiveness of our approach, with detailed ablation studies validating each proposed component. We will open-source our models, data and code to facilitate future research in this direction.\n\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "tasks",
                    "audioqa",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The recent success of LLMs has catalyzed a paradigm shift towards general-purpose Multimodal Large Language Models (MLLMs) capable of processing and generating information across diverse modalities <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib31\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib28\" title=\"\">2023</a>)</cite>. Among these, speech-in/speech-out conversational systems have emerged as a pivotal component in facilitating natural human-AI interaction. Conventional systems typically decompose this problem into a cascaded pipeline of automatic speech recognition (ASR), LLM-driven response generation, and text-to-speech (TTS) synthesis. While effective to a degree, this modular design introduces significant latency accumulation and error propagation between modules, hindering naturalness and real-world applicability. In response, recent end-to-end approaches like Moshi <cite class=\"ltx_cite ltx_citemacro_citet\">D&#233;fossez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citet\">Long et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite> have sought to unify speech understanding and generation within a single model. These models are typically trained through multi-stage pipelines that involve text-to-audio tokenizer training, interleaved data construction, text-audio alignment and task-oriented supervised fine-tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib9\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib5\" title=\"\">2024</a>)</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, these methods aim to generate interleaved text and speech tokens in an autoregressive manner, which are then decoded into continuous audio waveforms by a separate neural codec or diffusion-based decoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Mehta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib20\" title=\"\">2024</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib14\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments on ASR and Audio-QA benchmarks show that our proposed TtT framework substantially outperforms strong autoregressive (AR) and non-autoregressive (NAR) baselines. Our results underscore the superiority of the hybrid AR-NAR architecture, and detailed ablations validate the effectiveness of our specialized training strategies.</p>\n\n",
                "matched_terms": [
                    "audioqa",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively evaluate our model on two representative tasks, we carefully design the evaluation protocol and metrics. For Audio-QA, we introduce an ASR-LLM pipeline that transcribes the model&#8217;s spoken responses using language-specific ASR systems (Paraformer-zh for Chinese and Whisper-Large-v3 for English) and leverages a powerful LLM-as-a-Judge (Qwen3-235B-A30B) to assess semantic correctness against ground-truth answers. This approach captures the model&#8217;s ability to generate natural, accurate spoken responses in realistic conversational settings. For the ASR task, we directly measure transcription accuracy using Word Error Rate (WER), which reflects both speech recognition fidelity and cross-modal alignment within our hybrid AR-NAR framework. More details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.SS5.SSS1\" title=\"A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">A.5.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audioqa",
                    "english",
                    "task",
                    "evaluation",
                    "tasks",
                    "asr",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part reports the comparison of conversational understanding ability across different models on Audio-QA task. From these results we make the following observations: (1) Our proposed TtT consistently achieves the best performance on all four Audio-QA datasets. For example, at the 3B scale, TtT attains scores of 17.46, 34.68, 6.53, and 11.61 on AlpacaEval, LLaMAQuestions, TriviaQA, and WebQuestions respectively, surpassing the Qwen2.5-Base (AR) model by +3.04, +24.68, +5.93, and +10.91. This improvement stems from the hybrid AR&#8211;NAR design, which better captures the inherent asymmetry between text and audio dependencies.\n(2) Our results demonstrate a clear scaling trend shared with the baselines, where larger backbones consistently yield stronger performance. For instance, TtT-3B achieves 34.68 on LLaMAQuestions, outperforming its 1.5B counterpart which scores 23.75, reflecting an absolute gain of +10.93. This pattern aligns with the scaling laws of LLMs, reflecting that additional capacity enhances both reasoning ability and cross-modal representation learning; (3) Qwen2.5-Base (NAR) performs noticeably worse than both our hybrid framework and the AR-only baseline. Since discrete diffusion models employ an order-agnostic AO-ARM objective that ignore the inherent sequential structure of interleaved text-audio sequences. This mismatch leads to order confusion, compromising sequence coherence and weakening audio-text alignment.</p>\n\n",
                "matched_terms": [
                    "triviaqa",
                    "audioqa",
                    "task",
                    "alpacaeval",
                    "webquestions",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> main results part, our proposed TtT significantly outperforms both Qwen2.5-Base (AR) and Qwen2.5-Base (NAR) on ASR benchmarks, demonstrating superior cross-modal alignment. For example, at the 3B scale, TtT achieves 12.53 and 13.65 WER on AISHELL-2 and AISHELL-1 respectively, yielding improvements of 42.41 and 58.36 absolute WER points over Qwen2.5-Base (AR). This advantage arises from the hybrid AR&#8211;NAR architecture, where NAR diffusion modeling provides efficient parallel denoising that enforces tighter alignment between acoustic frames and textual representations, while AR components ensure coherent conditioning across modalities. (2) Similar to the conversational understanding task, we observe a clear scaling trend in cross-modal alignment. Larger backbones consistently deliver better alignment, with TtT-3B achieving substantially lower WER than TtT-1.5B (e.g., 12.53 vs. 14.89 on AISHELL-2, and 13.65 vs. 16.72 on AISHELL-1). This scaling behavior reflects the increased capacity of larger models to capture fine-grained acoustic patterns and maintain robust cross-modal mappings, thereby enhancing alignment quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "aishell1",
                    "asr",
                    "aishell2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contribution of each training strategy in our hybrid AR-NAR framework, we perform an ablation study based on the full model TtT (AR-NAR). The variant w/o BANOM corresponds to removing batchwise AR &amp; NAR objective mixing from the full model, w/o PPM removes prefix preservation masking, and w/o SST removes stochastic span truncation. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> ablation study part presents the detailed results of our ablation experiments. From these results we draw the following conclusions: (1) All three training strategies have a positive impact on model performance, and removing any one of them leads to clear degradation. For instance, on the LLaMAQuestions dataset, removing SST reduces the score from 34.68 to 10.20. This drop occurs because stochastic truncation plays a crucial role in mitigating positional bias in &#10216;EOA&#10217; prediction. By forcing the model to terminate spans based on semantic content rather than fixed positional cues, SST enables more robust variable-length audio generation, and its removal undermines the model&#8217;s ability to handle flexible conversational outputs. (2) Removing BANOM yields the largest performance degradation. For example, on the AISHELL-2 dataset, the performance decreases from 12.53 to 18.58 when the strategy is removed. This is because it is essential for exposing text tokens to clean audio prefixes during training, which better matches inference conditions. Without this mechanism, the model suffers from a sharper train&#8211;test discrepancy, leading to weaker cross-modal consistency and degraded alignment quality.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "aishell2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#S4.T1\" title=\"Table 1 &#8227; 4.2 Main Results &#8227; 4 Experiments &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> training strategy comparison part compares AR-only and AR&#8211;NAR frameworks under two different settings: (1) training directly from Qwen2.5-3B-Base without multimodal pretraining, namely TtT (AR&#8211;NAR), and (2) initialization from the multimodally aligned pretrained model, namely Pretrain+AR and Pretrain+TtT. From the table, we observe that: (1) when trained directly from Qwen2.5-3B-Base, our TtT framework achieves comparable or even superior performance to the AR-only baseline, indicating that the hybrid AR&#8211;NAR design is already competitive without pretraining; (2) when applied on top of the multimodally aligned pretrained model, Pretrain+TtT consistently matches or surpasses Pretrain+AR across both Audio-QA and ASR benchmarks. These results demonstrate that TtT not only performs strongly from scratch, but also benefits significantly when built upon large-scale multimodal alignment pretraining.</p>\n\n",
                "matched_terms": [
                    "audioqa",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented TtT, a unified multimodal framework that addresses the fundamental asymmetry in dependency structures between text and audio modalities. By integrating autoregressive generation for text with non-autoregressive discrete diffusion for audio within a single Transformer, TtT leverages the target-target dependencies of text and the source-target dependencies of audio. We established a theoretical foundation showing that our hybrid training objective provides an upper bound on the negative log-likelihood of the desired joint distribution. To mitigate train-test discrepancies inherent in this hybrid approach, we introduced three principled training strategies: Batchwise AR&#8211;NAR Objective Mixing, Prefix Preservation Masking, and Stochastic Span Truncation. Extensive experiments on ASR and Audio-QA tasks demonstrate that TtT significantly outperforms purely autoregressive or non-autoregressive baselines, achieving superior performance in conversational understanding and cross-modal alignment tasks. Our results validate that respecting the distinct generative natures of each modality is crucial for building effective speech-in/speech-out systems.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "audioqa",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in end-to-end audio-language models have moved beyond traditional cascaded architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib4\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib29\" title=\"\">2023</a>)</cite> toward unified multimodal frameworks. Representative works include Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib8\" title=\"\">2024</a>)</cite>, which achieves real-time duplex speech conversation through hierarchical Transformer architectures; GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib35\" title=\"\">2024</a>)</cite>, which builds upon GLM-4-9B for robust Chinese and English speech processing; and VITA-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Long et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib18\" title=\"\">2025</a>)</cite>, which introduces a lightweight Multiple Cross-modal Token Prediction (MCTP) module for fast audio-text generation with significantly reduced first-token latency. More recent efforts have focused on scaling and production readiness: Step-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib13\" title=\"\">2025</a>)</cite> presents a 130B-parameter unified speech-text model with generative speech data engine and instruction-driven fine control across dialects, emotions, singing, and RAP, while Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib16\" title=\"\">2025</a>)</cite> features text-guided aligned speech generation with multi-codebook discretization to preserve both semantic and acoustic information. UniWav <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#bib.bib17\" title=\"\">2025</a>)</cite> proposes the first unified encoder-decoder framework that jointly learns representation encoders and generative audio decoders for both discriminative and generative speech tasks.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "tasks",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous evaluation frameworks such as Kimi-Audio assess Audio-QA performance using the text portion of interleaved outputs, which overlooks the fact that the audio output of an end-to-end speech model more directly reflects its ability to generate natural and semantically faithful responses. To address this limitation, we evaluate Audio-QA directly on the audio outputs of our framework by first applying an ASR model to transcribe the generated audio into text, where Whisper-Large-v3 is used for English audio and Paraformer-zh for Chinese audio, with a comparison of ASR performance across different models provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20072v2#A1.T3\" title=\"Table 3 &#8227; Audio-QA Task &#8227; A.5.1 Evaluation Tasks &#8227; A.5 Evaluation Deatils &#8227; Appendix A Appendix &#8227; From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The transcribed text is then combined with the original QA queries and the ground truth answers and passed to a large scale reasoning model, Qwen3-235B-A30B, which serves as an LLM-as-a-Judge model to determine whether the response semantically matches the reference and to provide either a correctness label or a graded score. We report the average accuracy or score on the benchmark, and this evaluation pipeline provides a more faithful assessment of our model&#8217;s audio-to-audio QA ability in realistic conversational scenarios where speech serves as the output modality.</p>\n\n",
                "matched_terms": [
                    "audioqa",
                    "english",
                    "evaluation",
                    "asr",
                    "used",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the model&#8217;s capability in aligning speech with textual representations, we evaluate it on the ASR task, where the model generates text transcriptions from input audio and performance is measured using word error rate (WER). A lower WER indicates more accurate recognition, which reflects not only strong ASR ability but also effective cross modal consistency achieved by our hybrid AR-NAR modeling framework.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable unified training across diverse tasks, we transform all datasets into a consistent input&#8211;output format. On the one hand, this standardization allows the model to seamlessly integrate heterogeneous modalities such as speech, text, and interleaved audio&#8211;text sequences. On the other hand, a unified design is essential for supporting our training strategies, including batchwise AR &amp; NAR objective mixing, prefix preservation masking, and stochastic span truncation. These strategies rely on a shared representation to operate across modalities in a consistent way. For clarity, we provide representative examples of the adopted data formats as follow, covering ASR, TTS, audio chat, text chat, AAC, SEC, ASC, and interleaved text&#8211;audio data.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "datasets",
                    "asr"
                ]
            }
        ]
    }
}