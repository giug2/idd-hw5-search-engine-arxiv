{
    "S1.T1": {
        "caption": "Table 1. Comparison of style-controllable TTS models",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Style Control Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Control Level</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Prompt-based</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Multilingual</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">End-To-End</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Control</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">StyleSpeech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Hard tokens</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Phoneme</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LanStyleTTS-Base</td>\n<td class=\"ltx_td ltx_align_center\">Hard tokens</td>\n<td class=\"ltx_td ltx_align_center\">Phoneme</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LanStyleTTS-VITS</td>\n<td class=\"ltx_td ltx_align_center\">Hard tokens</td>\n<td class=\"ltx_td ltx_align_center\">Phoneme</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VITS</td>\n<td class=\"ltx_td ltx_align_center\">Speaker Embedding</td>\n<td class=\"ltx_td ltx_align_center\">Sentence</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Spark-TTS</td>\n<td class=\"ltx_td ltx_align_center\">Speech Prompt</td>\n<td class=\"ltx_td ltx_align_center\">Sentence</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CosyVoice</td>\n<td class=\"ltx_td ltx_align_center\">Text Prompt</td>\n<td class=\"ltx_td ltx_align_center\">Sentence</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m8\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Hard tokens + Text Prompt</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Phoneme &amp; Sentence</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m11\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.m12\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "embedding",
            "multilingual",
            "style",
            "control",
            "level",
            "comparison",
            "tts",
            "speaker",
            "sentence",
            "text",
            "cosyvoice",
            "phoneme",
            "promptbased",
            "stylespeech",
            "speech",
            "tokens",
            "ours",
            "stylecontrollable",
            "âœ“checkmark",
            "model",
            "vits",
            "sparktts",
            "prompt",
            "parastyletts",
            "paralinguistic",
            "method",
            "endtoend",
            "lanstylettsbase",
            "hard",
            "lanstylettsvits"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Recent advances in style-controllable TTS models have aimed to enhance expressiveness, multilingual capabilities, and controllability over various aspects of speech such as prosody, emotion, and speaker identity. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative models, categorized by their control method and levels of style control.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "stylecontrollable",
                    "prompt",
                    "style",
                    "parastyletts",
                    "control",
                    "paralinguistic",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-Speech (TTS) generation has made significant progress in recent years. It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools. Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Earlier TTS models such as Tacotron2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib27\" title=\"\">2017</a>)</cite>, FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib23\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib22\" title=\"\">2020</a>)</cite>, Glow-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib9\" title=\"\">2020</a>)</cite>, and VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> focused primarily on improving intelligibility and naturalness. In particular, VITS introduces a fully end-to-end architecture that unifies the acoustic model and vocoder into a single neural network. It enhances both audio quality and generation efficiency by removing the need for external modules.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "vits",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.\nSome works have attempted to control prosodic style variations across different languages. For instance, StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage. Similarly, LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.\nHowever, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation. These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "style",
                    "control",
                    "speech",
                    "paralinguistic",
                    "stylespeech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "model",
                    "style",
                    "paralinguistic",
                    "stylespeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "embedding",
                    "model",
                    "prompt",
                    "style",
                    "control",
                    "paralinguistic",
                    "speech",
                    "text",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner. The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech. Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment. Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "prompt",
                    "style",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of high computational cost and limited interpretability in LLM-based approaches. We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture. Inspired by LanStyleTTS&#8217;s use of prosody style tokens at phoneme level and VITS&#8217;s end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels. Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency. Key contributions of this work are as follows:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "tts",
                    "style",
                    "parastyletts",
                    "control",
                    "level",
                    "paralinguistic",
                    "endtoend",
                    "speech",
                    "sentence",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "stylecontrollable",
                    "model",
                    "style",
                    "control",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system is lightweight and computationally efficient, featuring an end-to-end architecture that supports expressive speech generation and is well-suited for real-time and edge-device deployment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "control",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results show that our proposed method can generate high-quality speech with performance comparable to state-of-the-art LLM-based speech generation models while achieving 30x faster inference, 8x smaller model size, and 2.5x lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "model",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference. While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling. Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.</p>\n\n",
                "matched_terms": [
                    "promptbased",
                    "vits",
                    "style",
                    "speaker",
                    "control",
                    "paralinguistic",
                    "endtoend",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with phonemes. Each phoneme is associated with its own prosody style token, enabling fine-grained, interpretable control over prosodic features. This approach is particularly effective for tonal languages such as Chinese. However, these models are not end-to-end and rely on external vocoders, limiting their efficiency. Moreover, they struggle to generalize to</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "models",
                    "style",
                    "control",
                    "endtoend",
                    "stylespeech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with individual phonemes. Each phoneme is associated with its own style token, enabling fine-grained and interpretable control over prosodic features. This design is particularly effective for tonal languages such as Chinese, where pitch and intonation are linguistically meaningful. However, these models lack the ability to control high-level paralinguistic styles such as emotion, age, or intent, which limits their expressiveness in broader speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "models",
                    "style",
                    "control",
                    "speech",
                    "paralinguistic",
                    "stylespeech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "prompt",
                    "sparktts",
                    "style",
                    "control",
                    "paralinguistic",
                    "speech",
                    "tokens",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multilingual",
                    "promptbased",
                    "model",
                    "style",
                    "prompt",
                    "speaker",
                    "control",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the IPA-based text tokenization method from LanStyleTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> to convert English and Chinese text into phonemes with accompanying prosody features such as stress (English) and tone (Chinese). More specifically, each word in English is converted to ARPAbet phonemes using the CMU Pronouncing Dictionary&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Carnegie Mellon University, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib2\" title=\"\">2023</a>)</cite>, with stress markers extracted separately to form the style token sequence. The phonemes are then mapped to IPA phonemes.\nFor Chinese, we use the <span class=\"ltx_text ltx_font_typewriter\">pypinyin</span> library to convert each character into its Pinyin form, which is then split into initials and finals. The tone is stripped from the final and used as a style token, while the remaining components are mapped to IPA phonemes.</p>\n\n",
                "matched_terms": [
                    "method",
                    "text",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first project the IPA tokens&#160;<math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and prosody style tokens&#160;<math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> into a sequence of vector representations using two distinct embedding layers. To preserve sequential information, sinusoidal positional encodings are added to the embeddings. Then, IPA and prosody style embeddings are fed into Feed-Forward Transformer (FFT) blocks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib23\" title=\"\">2019</a>)</cite> to leverage the self-attention to model long-range dependencies and contextual relationships across the token sequence. Unlike the original Transformer architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib25\" title=\"\">2017</a>)</cite>, which uses a two-layer fully connected network in its feed-forward submodule, we replace it with two one-dimensional convolutional layers to better capture local contextual dependencies between adjacent tokens.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "model",
                    "tokens",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]</annotation></semantics></math> denote the phoneme embedding sequence obtained from text tokenization. We associate each phoneme <math alttext=\"x_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">x_{l}</annotation></semantics></math> with a corresponding prosodic style embedding <math alttext=\"\\mathbf{x}_{t},\\mathbf{s}^{\\text{pho}}_{t}\\in\\mathbb{R}^{d_{1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>&#119852;</mi><mi>t</mi><mtext>pho</mtext></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>1</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{t},\\mathbf{s}^{\\text{pho}}_{t}\\in\\mathbb{R}^{d_{1}}</annotation></semantics></math>, forming the phoneme-level prosody sequence:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "text",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "model",
                    "prompt",
                    "style",
                    "paralinguistic",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is then fed into MPNet to produce the paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>para</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}</annotation></semantics></math>, which is used to condition the TTS model and guide the generation of speech with the intended paralinguistic style.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "tts",
                    "prompt",
                    "model",
                    "style",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Style in speech generation is a broad concept that encompasses both prosodic features, such as pitch and tone, and paralinguistic styles, including gender, emotion, accent, and more. In this research, we divide style into two categories with different levels of control. One is the phoneme-level style, which captures fine-grained prosodic variations such as tone and stress at the level of individual phonemes. This level strongly influences how each word is articulated. Another one is sentence-level style, which represents global characteristics of the speech. It includes emotion, age, gender, and accent. While these features shape the overall impression of the speech, they exert less direct influence on phoneme realization. To support effective control at both levels, we design specialized architectures tailored to the unique requirements of each level of style.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "style",
                    "control",
                    "level",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the phoneme embedding sequence <math alttext=\"\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]</annotation></semantics></math> and the phoneme-level prosody style sequence <math alttext=\"\\mathbf{S}^{\\text{pho}}=[\\mathbf{s}^{\\text{pho}}_{1},\\mathbf{s}^{\\text{pho}}_{2},\\ldots,\\mathbf{s}^{\\text{pho}}_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>pho</mtext></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>&#119852;</mi><mn>1</mn><mtext>pho</mtext></msubsup><mo>,</mo><msubsup><mi>&#119852;</mi><mn>2</mn><mtext>pho</mtext></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119852;</mi><mi>L</mi><mtext>pho</mtext></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{pho}}=[\\mathbf{s}^{\\text{pho}}_{1},\\mathbf{s}^{\\text{pho}}_{2},\\ldots,\\mathbf{s}^{\\text{pho}}_{L}]</annotation></semantics></math>, we apply a lightweight adapter to inject prosodic features into the phoneme representations. Following the design in LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite>, we employ a Gated Tanh Unit (GTU) fusion mechanism, defined as:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>W</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">W_{1}</annotation></semantics></math>, <math alttext=\"W_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>W</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">W_{2}</annotation></semantics></math> are learnable projection weights, <math alttext=\"b_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>b</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">b_{1}</annotation></semantics></math>, <math alttext=\"b_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>b</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">b_{2}</annotation></semantics></math> are biases, <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m5\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication, and <math alttext=\"\\sigma(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m6\" intent=\":literal\"><semantics><mrow><mi>&#963;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sigma(\\cdot)</annotation></semantics></math> is the sigmoid function. This formulation allows the prosody style to modulate the phoneme representation at a fine-grained level while preserving phonetic structure.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "level",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that paralinguistic style typically remains consistent throughout a speech, it can influence both phoneme-level and sentence-level acoustic characteristics. To capture these effects, we first apply two distinct linear layers to project paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math> into phoneme-level&#160;(<math alttext=\"\\mathbf{S}^{\\text{local}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>local</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{local}}</annotation></semantics></math>) and sentence-level&#160;(<math alttext=\"\\mathbf{S}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{global}}</annotation></semantics></math>) paralinguistic style embedding.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "prompt",
                    "style",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt Feature-wise Linear Modulation (FiLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib20\" title=\"\">2018</a>)</cite> to inject phoneme-level paralinguistic style embedding into the phoneme embeddings via sequence-wise conditioning. Specifically, given the projected style embedding <math alttext=\"\\mathbf{s}^{\\text{sent}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p3.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>sent</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{sent}}</annotation></semantics></math>, we compute the scaling and bias vectors as:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "style",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p7.m1\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication. This FiLM-based adapter integrates paralinguistic style into each phoneme within the speech.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "paralinguistic",
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding (<math alttext=\"\\mathbf{s}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p8.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{global}}</annotation></semantics></math>) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder. By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "style",
                    "parastyletts",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the variational autoencoder (VAE) framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib12\" title=\"\">2019</a>)</cite> with adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goodfellow et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib7\" title=\"\">2014</a>)</cite> and normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papamakarios et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib19\" title=\"\">2021</a>)</cite> to model expressive latent representations and decode our waveform decoder due to its fully end-to-end training, non-autoregressive inference, and high-fidelity speech generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our system, the decoder takes as input the style-integrated phoneme representations <math alttext=\"\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119831;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]</annotation></semantics></math>, which are modulated by both phoneme-level prosody and sentence-level paralinguistic style. To enforce global consistency, the sentence-level embedding <math alttext=\"\\mathbf{S}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{global}}</annotation></semantics></math> is concatenated with both the prior and posterior encodings before being passed through the normalizing flow layers.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "style",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The latent embedding <math alttext=\"\\mathbf{Z}\\in\\mathbb{R}^{N\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}\\in\\mathbb{R}^{N\\times T}</annotation></semantics></math> is first sampled from a Gaussian posterior using a variational autoencoder (VAE). The posterior distribution is conditioned on the ground-truth spectrogram <math alttext=\"\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><mi>&#119832;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Y}</annotation></semantics></math> and the sentence-level style embedding <math alttext=\"\\mathbf{S}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{global}}</annotation></semantics></math>, and is parameterized as:</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\boldsymbol{\\mu}_{\\text{post}},\\boldsymbol{\\sigma}_{\\text{post}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#120641;</mi><mtext>post</mtext></msub><mo>,</mo><msub><mi>&#120648;</mi><mtext>post</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{\\text{post}},\\boldsymbol{\\sigma}_{\\text{post}}</annotation></semantics></math> are predicted by a posterior encoder from the spectrogram and global style embedding.\nTo obtain a more expressive latent representation, we further apply a sequence of invertible normalizing flows to <math alttext=\"\\mathbf{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p5.m2\" intent=\":literal\"><semantics><mi>&#119859;</mi><annotation encoding=\"application/x-tex\">\\mathbf{z}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#120641;</mi><mtext>prior</mtext></msub><mo>,</mo><msub><mi>&#120648;</mi><mtext>prior</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}</annotation></semantics></math> are predicted by a prior encoder network, which takes as input the phoneme embeddings <math alttext=\"\\tilde{\\mathbf{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{X}}</annotation></semantics></math> and <span class=\"ltx_text ltx_font_bold\">local</span> style embedding <math alttext=\"\\mathbf{S}^{\\text{local}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>local</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{local}}</annotation></semantics></math>. These parameters define the expected distribution of latent speech features given the linguistic content and phoneme-level paralinguistic style. The KL-divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib11\" title=\"\">2013</a>)</cite> between the transformed posterior sample and the prior is minimized during training:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "style",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This structure allows the model to capture a rich and flexible latent distribution that aligns with both the local and global style information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align the phoneme sequence integrated with paralinguistic style embedding <math alttext=\"\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m1\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119831;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]</annotation></semantics></math> with the latent embedding&#160;<math alttext=\"\\mathbf{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m2\" intent=\":literal\"><semantics><mi>&#119833;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Z}</annotation></semantics></math> during training, we apply Monotonic Alignment Search (MAS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> to compute a soft alignment matrix <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L\\times T}</annotation></semantics></math>, where <math alttext=\"A_{t,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m4\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{t,j}</annotation></semantics></math> represents the attention weight between the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th phoneme and the <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m6\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame in <math alttext=\"\\mathbf{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m7\" intent=\":literal\"><semantics><mi>&#119833;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Z}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "style",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate a Stochastic Duration Predictor (SDP) to learn to predict the log-duration distribution conditioned on the phoneme and style features. During training, we minimize a log-domain Mean Squared Error (MSE) loss between the predicted and reference durations:</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It allow the model to intrinsically learn phoneme durations during training, while also capturing duration variations influenced by paralinguistic styles.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "paralinguistic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is optimized using a combination of objectives adapted from the VITS framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite>. These include a reconstruction loss <math alttext=\"\\mathcal{L}_{\\text{recon}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>recon</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{recon}}</annotation></semantics></math>, which measures the difference between the generated&#160;<math alttext=\"\\hat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{Y}</annotation></semantics></math> and ground-truth spectrogram, an adversarial loss&#160;<math alttext=\"\\mathcal{L}_{\\text{adv}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>adv</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{adv}}</annotation></semantics></math> to encourage realistic waveform generation through multi-period discriminators <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> and a feature matching loss&#160;<math alttext=\"\\mathcal{L}_{\\text{fm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>fm</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{fm}}</annotation></semantics></math> to stabilize adversarial training by aligning discriminator&#8217;s internal feature of real and generated speech:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> as the length of the text sequence and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the length of the paralinguistic style prompt.</p>\n\n",
                "matched_terms": [
                    "models",
                    "prompt",
                    "style",
                    "parastyletts",
                    "paralinguistic",
                    "control",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "model",
                    "prompt",
                    "style",
                    "parastyletts",
                    "paralinguistic",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct our experiments using a multilingual and multi-style speech corpus comprising both English and Chinese speech samples. To ensure broad coverage of paralinguistic styles, we construct a composite dataset by combining several publicly available speech datasets.\nThe final training data consists of four sources. The <span class=\"ltx_text ltx_font_bold\">Baker</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Databaker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib4\" title=\"\">2020</a>)</cite> is a single-speaker Mandarin Chinese corpus featuring a female voice. The <span class=\"ltx_text ltx_font_bold\">LJSpeech</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito and Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib8\" title=\"\">2017</a>)</cite> is a single-speaker English corpus, also featuring a female speaker, widely used in TTS research for clean and consistent English utterances. The <span class=\"ltx_text ltx_font_bold\">Emotional Speech Dataset (ESD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib29\" title=\"\">2021</a>)</cite> contributes a multilingual, multi-speaker emotional speech in both English and Chinese, covering five emotional categories and including both male and female speakers. To further enrich stylistic diversity, we curate 16 stylized character speech captions from the <span class=\"ltx_text ltx_font_bold\">Genshin Impact</span> voice dataset to capture expressive speech across different age styles.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "multilingual",
                    "speaker",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "embedding",
                    "tts",
                    "multilingual",
                    "model",
                    "style",
                    "control",
                    "paralinguistic",
                    "method",
                    "speech",
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the intelligibility and paralinguistic expressiveness of our system, we adopt both objective metrics and human perceptual testing.\nFor intelligibility assessment, we avoid using samples from our training set to ensure fair cross-model comparisons, as different baselines are trained on distinct datasets. Instead, we generate speech for two standardized corpora: the 720 Harvard Sentences<cite class=\"ltx_cite ltx_citemacro_citep\">(of&#160;Electrical and Engineers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib18\" title=\"\">1969</a>)</cite> in English and 400 phonetically balanced Mandarin TMNews sentences set from&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib3\" title=\"\">2023</a>)</cite>, both widely recognized for evaluating speech systems. The generated speech is transcribed using Whisper-Base&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib21\" title=\"\">2023</a>)</cite>, and Word Error Rate (WER) is computed by comparing the transcriptions against the ground-truth text. Lower WER scores indicate higher intelligibility and transcription consistency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "paralinguistic",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To complement the objective evaluation, we conduct a Mean Opinion Score (MOS) listening test. For each language, we randomly select ten generated samples from each model and present them to at least five bilingual listeners fluent in both English and Chinese. The listeners are asked to rate each sample along two dimensions: intelligibility (I-MOS), which reflects how easily the speech content can be understood, and naturalness (N-MOS), which reflects how human-like and fluent the speech sounds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "style",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment. All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios. To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences. Each sentence is processed individually with a batch size of one. The average inference time and peak CUDA memory usage per sample are recorded. These metrics provide a consistent and fair basis for comparing computational efficiency across models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "speech",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1:</span> Can ParaStyleTTS achieve more expressive control over speaking styles?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2:</span> Can ParaStyleTTS provide effective style control in a lightweight and resource-efficient manner?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ3:</span> Can ParaStyleTTS maintain robust style control under varying prompt formulations?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "control",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "sparktts",
                    "parastyletts",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "style",
                    "parastyletts",
                    "control",
                    "paralinguistic",
                    "speech",
                    "comparison",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the distinguishability of generated styles, we train a speaking style classifier and extract embeddings from ParaStyleTTS-generated speech conditioned on different style prompts. These embeddings are visualized using t-SNE in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, providing an external perspective on how well the model encodes paralinguistic styles in the acoustic space.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "paralinguistic",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "style",
                    "parastyletts",
                    "control",
                    "paralinguistic",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "style",
                    "parastyletts",
                    "control",
                    "endtoend",
                    "speech",
                    "text",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "model",
                    "prompt",
                    "style",
                    "parastyletts",
                    "control",
                    "level",
                    "speech",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "parastyletts",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "sparktts",
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "style",
                    "parastyletts",
                    "endtoend",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another major advantage of ParaStyleTTS is that it does not rely on LLMs for style adaptation. LLMs are computationally expensive due to their large parameter sizes and nature of autoregressive processing. ParaStyleTTS avoids this overhead by directly learning the mapping between style prompts and corresponding acoustic characteristics. With the help of a lightweight style adapter, it achieves high flexibility and fast generation while maintaining expressive control over speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to empirical improvements in runtime performance, ParaStyleTTS is also more efficient in terms of computational complexity. It processes phoneme tokens and style prompts independently using transformer encoders. The total time complexity is <math alttext=\"O(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2}+M^{2})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represent the lengths of the phoneme sequence and style prompt, respectively.\nThis complexity can be further reduced during inference by precomputing and caching the style prompt embedding. Since the prompt no longer needs to be processed at runtime, the term associated with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is eliminated. The overall complexity can be further reduced to <math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>.\nIn contrast, LLM-based models concatenate the phoneme and prompt tokens together and process them jointly. It results in a higher time complexity of <math alttext=\"O((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m6\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O((N+M)^{2})</annotation></semantics></math>. This complexity gap becomes increasingly significant with longer inputs or richer style prompts.\nBy combining architectural simplicity with both theoretical and practical efficiency, ParaStyleTTS offers a scalable and deployable solution suitable for real-time, cloud-based, and on-device TTS applications.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "models",
                    "embedding",
                    "tts",
                    "prompt",
                    "style",
                    "parastyletts",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robust style control is critical for real-world TTS applications, where prompts may vary in phrasing but still intend to express the same speaking style. A robust model must consistently generate speech that matches the intended paralinguistic style, regardless of how the prompt is formulated.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "prompt",
                    "style",
                    "control",
                    "paralinguistic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n",
                "matched_terms": [
                    "models",
                    "style",
                    "control",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "speaker",
                    "parastyletts",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prompt",
                    "style",
                    "parastyletts",
                    "control",
                    "speech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "prompt",
                    "style",
                    "parastyletts",
                    "level",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "stylecontrollable",
                    "model",
                    "style",
                    "parastyletts",
                    "control",
                    "paralinguistic",
                    "endtoend",
                    "speech",
                    "text",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "style",
                    "parastyletts",
                    "control",
                    "level",
                    "paralinguistic",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the development and writing of this paper, generative AI (GenAI) tools are used in limited, non-substantive ways. Specifically, we use GenAI tools to help refine grammar, improve clarity, and restructure paragraphs in the manuscript. All technical content, research insights, and model designs are solely authored by the authors without any GenAI-generated ideas. No GenAI tools are used to generate or modify source code, experiment design, or model training. All implementation and data handling are conducted manually using standard Python-based, PyTorch frameworks. All datasets used are publicly available human speech datasets. Data preprocessing and analysis are performed without the aid of GenAI tools.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2. Overall Performance Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">I-MOS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">N-MOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">StyleSpeech</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.33 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 28.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.18 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.94 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LanStyleTTS-Base</th>\n<td class=\"ltx_td ltx_align_center\">21.20 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 16.45</td>\n<td class=\"ltx_td ltx_align_center\">2.59 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.30</td>\n<td class=\"ltx_td ltx_align_center\">2.24 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LanStyleTTS-VITS</th>\n<td class=\"ltx_td ltx_align_center\">12.74 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.11</td>\n<td class=\"ltx_td ltx_align_center\">4.57 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.48</td>\n<td class=\"ltx_td ltx_align_center\">4.23 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VITS</th>\n<td class=\"ltx_td ltx_align_center\">19.32 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 15.88</td>\n<td class=\"ltx_td ltx_align_center\">4.28 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.38</td>\n<td class=\"ltx_td ltx_align_center\">3.82 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Spark-TTS</th>\n<td class=\"ltx_td ltx_align_center\">15.12 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 14.43</td>\n<td class=\"ltx_td ltx_align_center\">4.45 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.40</td>\n<td class=\"ltx_td ltx_align_center\">4.33 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center\">10.30 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.52</td>\n<td class=\"ltx_td ltx_align_center\">4.75 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.22</td>\n<td class=\"ltx_td ltx_align_center\">4.57 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">15.29 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 14.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.65 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.36 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.49</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "imos",
            "wer",
            "lanstylettsvits",
            "model",
            "vits",
            "sparktts",
            "overall",
            "Â±pm",
            "parastyletts",
            "stylespeech",
            "lanstylettsbase",
            "nmos",
            "comparison",
            "cosyvoice",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Earlier TTS models such as Tacotron2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib27\" title=\"\">2017</a>)</cite>, FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib23\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib22\" title=\"\">2020</a>)</cite>, Glow-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib9\" title=\"\">2020</a>)</cite>, and VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> focused primarily on improving intelligibility and naturalness. In particular, VITS introduces a fully end-to-end architecture that unifies the acoustic model and vocoder into a single neural network. It enhances both audio quality and generation efficiency by removing the need for external modules.</p>\n\n",
                "matched_terms": [
                    "model",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "stylespeech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results show that our proposed method can generate high-quality speech with performance comparable to state-of-the-art LLM-based speech generation models while achieving 30x faster inference, 8x smaller model size, and 2.5x lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "sparktts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is optimized using a combination of objectives adapted from the VITS framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite>. These include a reconstruction loss <math alttext=\"\\mathcal{L}_{\\text{recon}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>recon</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{recon}}</annotation></semantics></math>, which measures the difference between the generated&#160;<math alttext=\"\\hat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{Y}</annotation></semantics></math> and ground-truth spectrogram, an adversarial loss&#160;<math alttext=\"\\mathcal{L}_{\\text{adv}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>adv</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{adv}}</annotation></semantics></math> to encourage realistic waveform generation through multi-period discriminators <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> and a feature matching loss&#160;<math alttext=\"\\mathcal{L}_{\\text{fm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>fm</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{fm}}</annotation></semantics></math> to stabilize adversarial training by aligning discriminator&#8217;s internal feature of real and generated speech:</p>\n\n",
                "matched_terms": [
                    "model",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> as the length of the text sequence and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the length of the paralinguistic style prompt.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "parastyletts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To complement the objective evaluation, we conduct a Mean Opinion Score (MOS) listening test. For each language, we randomly select ten generated samples from each model and present them to at least five bilingual listeners fluent in both English and Chinese. The listeners are asked to rate each sample along two dimensions: intelligibility (I-MOS), which reflects how easily the speech content can be understood, and naturalness (N-MOS), which reflects how human-like and fluent the speech sounds.</p>\n\n",
                "matched_terms": [
                    "nmos",
                    "model",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment. All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios. To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences. Each sentence is processed individually with a batch size of one. The average inference time and peak CUDA memory usage per sample are recorded. These metrics provide a consistent and fair basis for comparing computational efficiency across models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "comparison",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "sparktts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to empirical improvements in runtime performance, ParaStyleTTS is also more efficient in terms of computational complexity. It processes phoneme tokens and style prompts independently using transformer encoders. The total time complexity is <math alttext=\"O(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2}+M^{2})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represent the lengths of the phoneme sequence and style prompt, respectively.\nThis complexity can be further reduced during inference by precomputing and caching the style prompt embedding. Since the prompt no longer needs to be processed at runtime, the term associated with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is eliminated. The overall complexity can be further reduced to <math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>.\nIn contrast, LLM-based models concatenate the phoneme and prompt tokens together and process them jointly. It results in a higher time complexity of <math alttext=\"O((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m6\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O((N+M)^{2})</annotation></semantics></math>. This complexity gap becomes increasingly significant with longer inputs or richer style prompts.\nBy combining architectural simplicity with both theoretical and practical efficiency, ParaStyleTTS offers a scalable and deployable solution suitable for real-time, cloud-based, and on-device TTS applications.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "parastyletts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "overall",
                    "parastyletts",
                    "cosyvoice",
                    "performance"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3. Speaking Style Expressiveness Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Emotion Acc</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Gender Acc</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Age Acc</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.00%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.88%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">54.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">100.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">57.50%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "model",
            "style",
            "age",
            "acc",
            "parastyletts",
            "gender",
            "speaking",
            "expressiveness",
            "comparison",
            "cosyvoice",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "parastyletts",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-Speech (TTS) generation has made significant progress in recent years. It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools. Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.\nSome works have attempted to control prosodic style variations across different languages. For instance, StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage. Similarly, LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.\nHowever, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation. These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "expressiveness",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner. The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech. Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment. Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of high computational cost and limited interpretability in LLM-based approaches. We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture. Inspired by LanStyleTTS&#8217;s use of prosody style tokens at phoneme level and VITS&#8217;s end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels. Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency. Key contributions of this work are as follows:</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in style-controllable TTS models have aimed to enhance expressiveness, multilingual capabilities, and controllability over various aspects of speech such as prosody, emotion, and speaker identity. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative models, categorized by their control method and levels of style control.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference. While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling. Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with individual phonemes. Each phoneme is associated with its own style token, enabling fine-grained and interpretable control over prosodic features. This design is particularly effective for tonal languages such as Chinese, where pitch and intonation are linguistically meaningful. However, these models lack the ability to control high-level paralinguistic styles such as emotion, age, or intent, which limits their expressiveness in broader speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "age",
                    "expressiveness",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "cosyvoice",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first project the IPA tokens&#160;<math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and prosody style tokens&#160;<math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> into a sequence of vector representations using two distinct embedding layers. To preserve sequential information, sinusoidal positional encodings are added to the embeddings. Then, IPA and prosody style embeddings are fed into Feed-Forward Transformer (FFT) blocks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib23\" title=\"\">2019</a>)</cite> to leverage the self-attention to model long-range dependencies and contextual relationships across the token sequence. Unlike the original Transformer architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib25\" title=\"\">2017</a>)</cite>, which uses a two-layer fully connected network in its feed-forward submodule, we replace it with two one-dimensional convolutional layers to better capture local contextual dependencies between adjacent tokens.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "age",
                    "gender",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">&#8221;A [Age] [Gender] is speaking [Accent] with [Emotion] emotion.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is then fed into MPNet to produce the paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>para</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}</annotation></semantics></math>, which is used to condition the TTS model and guide the generation of speech with the intended paralinguistic style.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Style in speech generation is a broad concept that encompasses both prosodic features, such as pitch and tone, and paralinguistic styles, including gender, emotion, accent, and more. In this research, we divide style into two categories with different levels of control. One is the phoneme-level style, which captures fine-grained prosodic variations such as tone and stress at the level of individual phonemes. This level strongly influences how each word is articulated. Another one is sentence-level style, which represents global characteristics of the speech. It includes emotion, age, gender, and accent. While these features shape the overall impression of the speech, they exert less direct influence on phoneme realization. To support effective control at both levels, we design specialized architectures tailored to the unique requirements of each level of style.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding (<math alttext=\"\\mathbf{s}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p8.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{global}}</annotation></semantics></math>) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder. By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This structure allows the model to capture a rich and flexible latent distribution that aligns with both the local and global style information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> as the length of the text sequence and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the length of the paralinguistic style prompt.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n",
                "matched_terms": [
                    "age",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "age",
                    "gender",
                    "expressiveness",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment. All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios. To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences. Each sentence is processed individually with a batch size of one. The average inference time and peak CUDA memory usage per sample are recorded. These metrics provide a consistent and fair basis for comparing computational efficiency across models.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1:</span> Can ParaStyleTTS achieve more expressive control over speaking styles?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2:</span> Can ParaStyleTTS provide effective style control in a lightweight and resource-efficient manner?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ3:</span> Can ParaStyleTTS maintain robust style control under varying prompt formulations?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the distinguishability of generated styles, we train a speaking style classifier and extract embeddings from ParaStyleTTS-generated speech conditioned on different style prompts. These embeddings are visualized using t-SNE in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, providing an external perspective on how well the model encodes paralinguistic styles in the acoustic space.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "age",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking",
                    "cosyvoice",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking",
                    "cosyvoice",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "age",
                    "parastyletts",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another major advantage of ParaStyleTTS is that it does not rely on LLMs for style adaptation. LLMs are computationally expensive due to their large parameter sizes and nature of autoregressive processing. ParaStyleTTS avoids this overhead by directly learning the mapping between style prompts and corresponding acoustic characteristics. With the help of a lightweight style adapter, it achieves high flexibility and fast generation while maintaining expressive control over speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to empirical improvements in runtime performance, ParaStyleTTS is also more efficient in terms of computational complexity. It processes phoneme tokens and style prompts independently using transformer encoders. The total time complexity is <math alttext=\"O(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2}+M^{2})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represent the lengths of the phoneme sequence and style prompt, respectively.\nThis complexity can be further reduced during inference by precomputing and caching the style prompt embedding. Since the prompt no longer needs to be processed at runtime, the term associated with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is eliminated. The overall complexity can be further reduced to <math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>.\nIn contrast, LLM-based models concatenate the phoneme and prompt tokens together and process them jointly. It results in a higher time complexity of <math alttext=\"O((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m6\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O((N+M)^{2})</annotation></semantics></math>. This complexity gap becomes increasingly significant with longer inputs or richer style prompts.\nBy combining architectural simplicity with both theoretical and practical efficiency, ParaStyleTTS offers a scalable and deployable solution suitable for real-time, cloud-based, and on-device TTS applications.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robust style control is critical for real-world TTS applications, where prompts may vary in phrasing but still intend to express the same speaking style. A robust model must consistently generate speech that matches the intended paralinguistic style, regardless of how the prompt is formulated.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "cosyvoice",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "gender",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "parastyletts",
                    "gender",
                    "speaking",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "age",
                    "parastyletts",
                    "gender",
                    "speaking",
                    "cosyvoice",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "parastyletts",
                    "speaking",
                    "cosyvoice"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4. Computational Resource Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Inference Time (ms)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Parameter Size (M)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">CUDA Memory Usage (MB)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">StyleSpeech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">463</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">LanStyleTTS-Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">53.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">61.34</td>\n<td class=\"ltx_td ltx_align_center\">480</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">LanStyleTTS-VITS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">100.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">42.52</td>\n<td class=\"ltx_td ltx_align_center\">305</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">VITS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">99.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">36.30</td>\n<td class=\"ltx_td ltx_align_center\">340</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Spark-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">7,999.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">506.63</td>\n<td class=\"ltx_td ltx_align_center\">3,854</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">CosyVoice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4,076.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">436.05</td>\n<td class=\"ltx_td ltx_align_center\">1,852</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Prompt Encoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">109.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">636</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">121.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">52.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">763</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "size",
            "inference",
            "parameter",
            "comparison",
            "time",
            "cuda",
            "encoder",
            "cosyvoice",
            "usage",
            "resource",
            "memory",
            "computational",
            "stylespeech",
            "ours",
            "model",
            "vits",
            "sparktts",
            "prompt",
            "parastyletts",
            "lanstylettsbase",
            "lanstylettsvits"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "cuda",
                    "prompt",
                    "memory",
                    "computational",
                    "parastyletts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Earlier TTS models such as Tacotron2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib27\" title=\"\">2017</a>)</cite>, FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib23\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib22\" title=\"\">2020</a>)</cite>, Glow-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib9\" title=\"\">2020</a>)</cite>, and VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> focused primarily on improving intelligibility and naturalness. In particular, VITS introduces a fully end-to-end architecture that unifies the acoustic model and vocoder into a single neural network. It enhances both audio quality and generation efficiency by removing the need for external modules.</p>\n\n",
                "matched_terms": [
                    "model",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "stylespeech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner. The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech. Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment. Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.</p>\n\n",
                "matched_terms": [
                    "time",
                    "prompt",
                    "inference",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of high computational cost and limited interpretability in LLM-based approaches. We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture. Inspired by LanStyleTTS&#8217;s use of prosody style tokens at phoneme level and VITS&#8217;s end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels. Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency. Key contributions of this work are as follows:</p>\n\n",
                "matched_terms": [
                    "computational",
                    "parastyletts",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results show that our proposed method can generate high-quality speech with performance comparable to state-of-the-art LLM-based speech generation models while achieving 30x faster inference, 8x smaller model size, and 2.5x lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "cuda",
                    "usage",
                    "model",
                    "memory",
                    "size",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference. While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling. Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "sparktts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is then fed into MPNet to produce the paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>para</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}</annotation></semantics></math>, which is used to condition the TTS model and guide the generation of speech with the intended paralinguistic style.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding (<math alttext=\"\\mathbf{s}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p8.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{global}}</annotation></semantics></math>) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder. By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the variational autoencoder (VAE) framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib12\" title=\"\">2019</a>)</cite> with adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goodfellow et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib7\" title=\"\">2014</a>)</cite> and normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papamakarios et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib19\" title=\"\">2021</a>)</cite> to model expressive latent representations and decode our waveform decoder due to its fully end-to-end training, non-autoregressive inference, and high-fidelity speech generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is optimized using a combination of objectives adapted from the VITS framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite>. These include a reconstruction loss <math alttext=\"\\mathcal{L}_{\\text{recon}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>recon</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{recon}}</annotation></semantics></math>, which measures the difference between the generated&#160;<math alttext=\"\\hat{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{Y}</annotation></semantics></math> and ground-truth spectrogram, an adversarial loss&#160;<math alttext=\"\\mathcal{L}_{\\text{adv}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>adv</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{adv}}</annotation></semantics></math> to encourage realistic waveform generation through multi-period discriminators <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m4\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> and a feature matching loss&#160;<math alttext=\"\\mathcal{L}_{\\text{fm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>fm</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{fm}}</annotation></semantics></math> to stabilize adversarial training by aligning discriminator&#8217;s internal feature of real and generated speech:</p>\n\n",
                "matched_terms": [
                    "model",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> as the length of the text sequence and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the length of the paralinguistic style prompt.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "parastyletts",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "time",
                    "prompt",
                    "model",
                    "parastyletts",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on four NVIDIA V100 GPUs with a batch size of 32 for up to 700k steps. We adopt the AdamW optimizer<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib14\" title=\"\">2017</a>)</cite>, using the same hyperparameters and learning rate schedule as VITS<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment. All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios. To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences. Each sentence is processed individually with a batch size of one. The average inference time and peak CUDA memory usage per sample are recorded. These metrics provide a consistent and fair basis for comparing computational efficiency across models.</p>\n\n",
                "matched_terms": [
                    "time",
                    "cuda",
                    "usage",
                    "model",
                    "memory",
                    "size",
                    "computational",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "usage",
                    "resource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ3:</span> Can ParaStyleTTS maintain robust style control under varying prompt formulations?</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "sparktts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "comparison",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "time",
                    "cuda",
                    "model",
                    "prompt",
                    "resource",
                    "memory",
                    "size",
                    "parastyletts",
                    "inference",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "time",
                    "cuda",
                    "usage",
                    "model",
                    "prompt",
                    "memory",
                    "parastyletts",
                    "inference",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "time",
                    "cuda",
                    "usage",
                    "model",
                    "sparktts",
                    "memory",
                    "size",
                    "parastyletts",
                    "inference",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another major advantage of ParaStyleTTS is that it does not rely on LLMs for style adaptation. LLMs are computationally expensive due to their large parameter sizes and nature of autoregressive processing. ParaStyleTTS avoids this overhead by directly learning the mapping between style prompts and corresponding acoustic characteristics. With the help of a lightweight style adapter, it achieves high flexibility and fast generation while maintaining expressive control over speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "parameter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to empirical improvements in runtime performance, ParaStyleTTS is also more efficient in terms of computational complexity. It processes phoneme tokens and style prompts independently using transformer encoders. The total time complexity is <math alttext=\"O(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2}+M^{2})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represent the lengths of the phoneme sequence and style prompt, respectively.\nThis complexity can be further reduced during inference by precomputing and caching the style prompt embedding. Since the prompt no longer needs to be processed at runtime, the term associated with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is eliminated. The overall complexity can be further reduced to <math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>.\nIn contrast, LLM-based models concatenate the phoneme and prompt tokens together and process them jointly. It results in a higher time complexity of <math alttext=\"O((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m6\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O((N+M)^{2})</annotation></semantics></math>. This complexity gap becomes increasingly significant with longer inputs or richer style prompts.\nBy combining architectural simplicity with both theoretical and practical efficiency, ParaStyleTTS offers a scalable and deployable solution suitable for real-time, cloud-based, and on-device TTS applications.</p>\n\n",
                "matched_terms": [
                    "time",
                    "prompt",
                    "computational",
                    "parastyletts",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robust style control is critical for real-world TTS applications, where prompts may vary in phrasing but still intend to express the same speaking style. A robust model must consistently generate speech that matches the intended paralinguistic style, regardless of how the prompt is formulated.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "prompt",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "usage",
                    "model",
                    "memory",
                    "size",
                    "computational",
                    "parastyletts",
                    "inference",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5. Emotion Per-Class Accuracy Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Happy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Angry</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Sad</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Neutral</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Surprise</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">57.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">82.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.00%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">72.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">55.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">27.50</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "happy",
            "surprise",
            "model",
            "sad",
            "neutral",
            "perclass",
            "parastyletts",
            "angry",
            "accuracy",
            "comparison",
            "cosyvoice",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "sad",
                    "neutral",
                    "angry",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "comparison",
                    "cosyvoice",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "surprise",
                    "model",
                    "neutral",
                    "emotion",
                    "angry",
                    "sad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "parastyletts",
                    "cosyvoice",
                    "angry"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "perclass",
                    "parastyletts",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parastyletts",
                    "accuracy",
                    "cosyvoice",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            }
        ]
    },
    "S5.T6": {
        "caption": "Table 6. Age Per-Class Accuracy Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Child</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Teenager</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">YoungAdult</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Adult</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">82.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">72.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">50.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">25.00</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "youngadult",
            "teenager",
            "child",
            "model",
            "age",
            "perclass",
            "parastyletts",
            "adult",
            "accuracy",
            "comparison",
            "cosyvoice"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "age",
                    "parastyletts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "age",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "age",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n",
                "matched_terms": [
                    "age",
                    "teenager",
                    "adult",
                    "child"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "age",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "age",
                    "parastyletts",
                    "comparison",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "age",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "age",
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "perclass",
                    "parastyletts",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "age",
                    "parastyletts",
                    "accuracy",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            }
        ]
    },
    "S5.T7": {
        "caption": "Table 7. Gender Per-Class Accuracy Comparison",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Male</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Female</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ParaStyleTTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">100.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">100.00</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "model",
            "perclass",
            "parastyletts",
            "gender",
            "female",
            "accuracy",
            "male",
            "comparison",
            "cosyvoice"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "female",
                    "male",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct our experiments using a multilingual and multi-style speech corpus comprising both English and Chinese speech samples. To ensure broad coverage of paralinguistic styles, we construct a composite dataset by combining several publicly available speech datasets.\nThe final training data consists of four sources. The <span class=\"ltx_text ltx_font_bold\">Baker</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Databaker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib4\" title=\"\">2020</a>)</cite> is a single-speaker Mandarin Chinese corpus featuring a female voice. The <span class=\"ltx_text ltx_font_bold\">LJSpeech</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito and Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib8\" title=\"\">2017</a>)</cite> is a single-speaker English corpus, also featuring a female speaker, widely used in TTS research for clean and consistent English utterances. The <span class=\"ltx_text ltx_font_bold\">Emotional Speech Dataset (ESD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib29\" title=\"\">2021</a>)</cite> contributes a multilingual, multi-speaker emotional speech in both English and Chinese, covering five emotional categories and including both male and female speakers. To further enrich stylistic diversity, we curate 16 stylized character speech captions from the <span class=\"ltx_text ltx_font_bold\">Genshin Impact</span> voice dataset to capture expressive speech across different age styles.</p>\n\n",
                "matched_terms": [
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n",
                "matched_terms": [
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "female",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S4.T2\" title=\"Table 2 &#8227; 4.3. Training &#8227; 4. Experiment &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> compares the speech quality generated by ParaStyleTTS with that of other models.\nParaStyleTTS closely matches the perceptual quality of CosyVoice and outperforms Spark-TTS and other non-LLM-based baselines. It achieves an intelligibility MOS of 4.64 and a naturalness MOS of 4.36, ranking as the second-best model in subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "gender",
                    "comparison",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "female",
                    "male",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T4\" title=\"Table 4 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents a resource-based comparison of TTS models in terms of inference speed, model size, and CUDA memory usage, which are three key metrics that affect real-time responsiveness, storage cost, and hardware requirements. All models are evaluated on a single NVIDIA RTX 3060 Ti GPU. The reported results correspond to generating a speech segment approximately two seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CUDA memory usage remains unchanged at 763 MB because the reported value reflects peak memory usage at any given point during inference. In our setup, the prompt encoder and the main ParaStyleTTS model run sequentially, not in parallel. Since they do not overlap in execution, the total memory usage at any one time is determined by the larger of the two. Because ParaStyleTTS uses more memory than the prompt encoder, the overall peak memory remains at 763 MB.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, LLM-based TTS models such as CosyVoice and Spark-TTS are significantly more resource-intensive. Among them, CosyVoice is the most lightweight, yet it still requires over 4000 ms of inference time, more than 400 million parameters, and 1852 MB of CUDA memory. Compared to CosyVoice, ParaStyleTTS achieves over <span class=\"ltx_text ltx_font_bold\">30x</span> faster inference, up to <span class=\"ltx_text ltx_font_bold\">8x</span> smaller model size, and <span class=\"ltx_text ltx_font_bold\">2.5x</span> lower CUDA memory usage.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "gender",
                    "male",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parastyletts",
                    "gender",
                    "female",
                    "male",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "perclass",
                    "parastyletts",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "parastyletts",
                    "gender",
                    "accuracy",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "parastyletts",
                    "model",
                    "cosyvoice"
                ]
            }
        ]
    },
    "S7.T8": {
        "caption": "Table 8. Distribution of Training Data by Speaking Style",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Category</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Count</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\">Gender</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Female</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">51,646</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">69.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Male</th>\n<td class=\"ltx_td ltx_align_right\">34,360</td>\n<td class=\"ltx_td ltx_align_right\">38.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\">Age</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Child</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1,463</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Teenager</th>\n<td class=\"ltx_td ltx_align_right\">9,107</td>\n<td class=\"ltx_td ltx_align_right\">13.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Youngadult</th>\n<td class=\"ltx_td ltx_align_right\">9,012</td>\n<td class=\"ltx_td ltx_align_right\">13.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adult</th>\n<td class=\"ltx_td ltx_align_right\">66,424</td>\n<td class=\"ltx_td ltx_align_right\">79.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\">Emotion</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Angry</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">7,062</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">5.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Sad</th>\n<td class=\"ltx_td ltx_align_right\">7,005</td>\n<td class=\"ltx_td ltx_align_right\">6.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Neutral</th>\n<td class=\"ltx_td ltx_align_right\">57,972</td>\n<td class=\"ltx_td ltx_align_right\">84.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Happy</th>\n<td class=\"ltx_td ltx_align_right\">6,560</td>\n<td class=\"ltx_td ltx_align_right\">5.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Surprise</th>\n<td class=\"ltx_td ltx_align_right\">7,407</td>\n<td class=\"ltx_td ltx_align_right\">6.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\">Language</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Chinese</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">41,475</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">47.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">English</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">44,531</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">60.60</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "style",
            "speaking",
            "adult",
            "angry",
            "hours",
            "surprise",
            "age",
            "gender",
            "category",
            "training",
            "male",
            "value",
            "distribution",
            "count",
            "english",
            "happy",
            "language",
            "emotion",
            "youngadult",
            "teenager",
            "child",
            "neutral",
            "data",
            "female",
            "chinese",
            "sad"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-Speech (TTS) generation has made significant progress in recent years. It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools. Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.\nSome works have attempted to control prosodic style variations across different languages. For instance, StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage. Similarly, LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.\nHowever, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation. These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "chinese",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "language",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner. The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech. Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment. Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of high computational cost and limited interpretability in LLM-based approaches. We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture. Inspired by LanStyleTTS&#8217;s use of prosody style tokens at phoneme level and VITS&#8217;s end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels. Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency. Key contributions of this work are as follows:</p>\n\n",
                "matched_terms": [
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in style-controllable TTS models have aimed to enhance expressiveness, multilingual capabilities, and controllability over various aspects of speech such as prosody, emotion, and speaker identity. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative models, categorized by their control method and levels of style control.</p>\n\n",
                "matched_terms": [
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference. While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling. Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speaking",
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with phonemes. Each phoneme is associated with its own prosody style token, enabling fine-grained, interpretable control over prosodic features. This approach is particularly effective for tonal languages such as Chinese. However, these models are not end-to-end and rely on external vocoders, limiting their efficiency. Moreover, they struggle to generalize to</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with individual phonemes. Each phoneme is associated with its own style token, enabling fine-grained and interpretable control over prosodic features. This design is particularly effective for tonal languages such as Chinese, where pitch and intonation are linguistically meaningful. However, these models lack the ability to control high-level paralinguistic styles such as emotion, age, or intent, which limits their expressiveness in broader speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "age",
                    "chinese",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "style",
                    "training",
                    "speaking",
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the IPA-based text tokenization method from LanStyleTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> to convert English and Chinese text into phonemes with accompanying prosody features such as stress (English) and tone (Chinese). More specifically, each word in English is converted to ARPAbet phonemes using the CMU Pronouncing Dictionary&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Carnegie Mellon University, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib2\" title=\"\">2023</a>)</cite>, with stress markers extracted separately to form the style token sequence. The phonemes are then mapped to IPA phonemes.\nFor Chinese, we use the <span class=\"ltx_text ltx_font_typewriter\">pypinyin</span> library to convert each character into its Pinyin form, which is then split into initials and finals. The tone is stripped from the final and used as a style token, while the remaining components are mapped to IPA phonemes.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our IPA dictionary comprises 81 phonemes, 5 tone markers for Chinese, and 3 stress markers for English. In addition, we include three special tokens: <span class=\"ltx_text ltx_font_typewriter\">[START]</span> to denote the beginning of a sequence, <span class=\"ltx_text ltx_font_typewriter\">[END]</span> for the end of a sequence, and <span class=\"ltx_text ltx_font_typewriter\">[|]</span> as a word boundary marker.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">&#8221;A [Age] [Gender] is speaking [Accent] with [Emotion] emotion.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Style in speech generation is a broad concept that encompasses both prosodic features, such as pitch and tone, and paralinguistic styles, including gender, emotion, accent, and more. In this research, we divide style into two categories with different levels of control. One is the phoneme-level style, which captures fine-grained prosodic variations such as tone and stress at the level of individual phonemes. This level strongly influences how each word is articulated. Another one is sentence-level style, which represents global characteristics of the speech. It includes emotion, age, gender, and accent. While these features shape the overall impression of the speech, they exert less direct influence on phoneme realization. To support effective control at both levels, we design specialized architectures tailored to the unique requirements of each level of style.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding (<math alttext=\"\\mathbf{s}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p8.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{global}}</annotation></semantics></math>) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder. By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.</p>\n\n",
                "matched_terms": [
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The latent embedding <math alttext=\"\\mathbf{Z}\\in\\mathbb{R}^{N\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>N</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}\\in\\mathbb{R}^{N\\times T}</annotation></semantics></math> is first sampled from a Gaussian posterior using a variational autoencoder (VAE). The posterior distribution is conditioned on the ground-truth spectrogram <math alttext=\"\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><mi>&#119832;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Y}</annotation></semantics></math> and the sentence-level style embedding <math alttext=\"\\mathbf{S}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{global}}</annotation></semantics></math>, and is parameterized as:</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#120641;</mi><mtext>prior</mtext></msub><mo>,</mo><msub><mi>&#120648;</mi><mtext>prior</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}</annotation></semantics></math> are predicted by a prior encoder network, which takes as input the phoneme embeddings <math alttext=\"\\tilde{\\mathbf{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{X}}</annotation></semantics></math> and <span class=\"ltx_text ltx_font_bold\">local</span> style embedding <math alttext=\"\\mathbf{S}^{\\text{local}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>local</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{local}}</annotation></semantics></math>. These parameters define the expected distribution of latent speech features given the linguistic content and phoneme-level paralinguistic style. The KL-divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib11\" title=\"\">2013</a>)</cite> between the transformed posterior sample and the prior is minimized during training:</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This structure allows the model to capture a rich and flexible latent distribution that aligns with both the local and global style information.</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align the phoneme sequence integrated with paralinguistic style embedding <math alttext=\"\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m1\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119831;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}=[\\hat{x}_{1},\\hat{x}_{2},\\ldots,\\hat{x}_{L}]</annotation></semantics></math> with the latent embedding&#160;<math alttext=\"\\mathbf{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m2\" intent=\":literal\"><semantics><mi>&#119833;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Z}</annotation></semantics></math> during training, we apply Monotonic Alignment Search (MAS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> to compute a soft alignment matrix <math alttext=\"\\mathbf{A}\\in\\mathbb{R}^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#119808;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{A}\\in\\mathbb{R}^{L\\times T}</annotation></semantics></math>, where <math alttext=\"A_{t,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m4\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{t,j}</annotation></semantics></math> represents the attention weight between the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th phoneme and the <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m6\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th frame in <math alttext=\"\\mathbf{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m7\" intent=\":literal\"><semantics><mi>&#119833;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Z}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate a Stochastic Duration Predictor (SDP) to learn to predict the log-duration distribution conditioned on the phoneme and style features. During training, we minimize a log-domain Mean Squared Error (MSE) loss between the predicted and reference durations:</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct our experiments using a multilingual and multi-style speech corpus comprising both English and Chinese speech samples. To ensure broad coverage of paralinguistic styles, we construct a composite dataset by combining several publicly available speech datasets.\nThe final training data consists of four sources. The <span class=\"ltx_text ltx_font_bold\">Baker</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Databaker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib4\" title=\"\">2020</a>)</cite> is a single-speaker Mandarin Chinese corpus featuring a female voice. The <span class=\"ltx_text ltx_font_bold\">LJSpeech</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito and Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib8\" title=\"\">2017</a>)</cite> is a single-speaker English corpus, also featuring a female speaker, widely used in TTS research for clean and consistent English utterances. The <span class=\"ltx_text ltx_font_bold\">Emotional Speech Dataset (ESD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib29\" title=\"\">2021</a>)</cite> contributes a multilingual, multi-speaker emotional speech in both English and Chinese, covering five emotional categories and including both male and female speakers. To further enrich stylistic diversity, we curate 16 stylized character speech captions from the <span class=\"ltx_text ltx_font_bold\">Genshin Impact</span> voice dataset to capture expressive speech across different age styles.</p>\n\n",
                "matched_terms": [
                    "english",
                    "age",
                    "data",
                    "training",
                    "female",
                    "male",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "english",
                    "happy",
                    "style",
                    "speaking",
                    "female",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the intelligibility and paralinguistic expressiveness of our system, we adopt both objective metrics and human perceptual testing.\nFor intelligibility assessment, we avoid using samples from our training set to ensure fair cross-model comparisons, as different baselines are trained on distinct datasets. Instead, we generate speech for two standardized corpora: the 720 Harvard Sentences<cite class=\"ltx_cite ltx_citemacro_citep\">(of&#160;Electrical and Engineers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib18\" title=\"\">1969</a>)</cite> in English and 400 phonetically balanced Mandarin TMNews sentences set from&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib3\" title=\"\">2023</a>)</cite>, both widely recognized for evaluating speech systems. The generated speech is transcribed using Whisper-Base&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib21\" title=\"\">2023</a>)</cite>, and Word Error Rate (WER) is computed by comparing the transcriptions against the ground-truth text. Lower WER scores indicate higher intelligibility and transcription consistency.</p>\n\n",
                "matched_terms": [
                    "training",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To complement the objective evaluation, we conduct a Mean Opinion Score (MOS) listening test. For each language, we randomly select ten generated samples from each model and present them to at least five bilingual listeners fluent in both English and Chinese. The listeners are asked to rate each sample along two dimensions: intelligibility (I-MOS), which reflects how easily the speech content can be understood, and naturalness (N-MOS), which reflects how human-like and fluent the speech sounds.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the distinguishability of generated styles, we train a speaking style classifier and extract embeddings from ParaStyleTTS-generated speech conditioned on different style prompts. These embeddings are visualized using t-SNE in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, providing an external perspective on how well the model encodes paralinguistic styles in the acoustic space.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "surprise",
                    "sad",
                    "age",
                    "neutral",
                    "emotion",
                    "female",
                    "male",
                    "angry"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "language",
                    "style",
                    "training",
                    "speaking",
                    "angry"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "training",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another major advantage of ParaStyleTTS is that it does not rely on LLMs for style adaptation. LLMs are computationally expensive due to their large parameter sizes and nature of autoregressive processing. ParaStyleTTS avoids this overhead by directly learning the mapping between style prompts and corresponding acoustic characteristics. With the help of a lightweight style adapter, it achieves high flexibility and fast generation while maintaining expressive control over speaking style.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robust style control is critical for real-world TTS applications, where prompts may vary in phrasing but still intend to express the same speaking style. A robust model must consistently generate speech that matches the intended paralinguistic style, regardless of how the prompt is formulated.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n",
                "matched_terms": [
                    "child",
                    "surprise",
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "male",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n",
                "matched_terms": [
                    "male",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "style",
                    "gender",
                    "speaking",
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "style",
                    "age",
                    "gender",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "training",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the development and writing of this paper, generative AI (GenAI) tools are used in limited, non-substantive ways. Specifically, we use GenAI tools to help refine grammar, improve clarity, and restructure paragraphs in the manuscript. All technical content, research insights, and model designs are solely authored by the authors without any GenAI-generated ideas. No GenAI tools are used to generate or modify source code, experiment design, or model training. All implementation and data handling are conducted manually using standard Python-based, PyTorch frameworks. All datasets used are publicly available human speech datasets. Data preprocessing and analysis are performed without the aid of GenAI tools.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            }
        ]
    },
    "S7.T9": {
        "caption": "Table 9. Text Prompts Used for Gender-specific Style Control",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Gender</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Prompt</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A female speaker is talking.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are listening to a woman speak.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">This voice belongs to a female speaker.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A young woman is speaking in a calm tone.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A woman is narrating this sentence.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A girl is speaking softly.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You&#8217;re hearing the voice of a lady.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A lady is giving this speech.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">This is the voice of a female child.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Female</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A girl is talking in Chinese.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A male speaker is talking.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You are hearing a man&#8217;s voice.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">This voice belongs to a male speaker.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A man is speaking in a confident tone.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A male narrator is delivering the sentence.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A man is speaking cheerfully.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">You&#8217;re hearing the voice of a gentleman.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A gentleman is giving this speech.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">This is the voice of a male.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Male</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">A gentleman is talking in Chinese.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "youâ€™re",
            "you",
            "lady",
            "prompts",
            "style",
            "genderspecific",
            "control",
            "listening",
            "speaking",
            "man",
            "delivering",
            "gentleman",
            "narrator",
            "young",
            "voice",
            "girl",
            "speaker",
            "gender",
            "calm",
            "used",
            "sentence",
            "text",
            "softly",
            "male",
            "belongs",
            "tone",
            "narrating",
            "manâ€™s",
            "hearing",
            "speech",
            "talking",
            "confident",
            "speak",
            "giving",
            "child",
            "prompt",
            "woman",
            "cheerfully",
            "female",
            "chinese"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To further investigate robustness against prompt variation, we conduct a controlled experiment using gender as a proxy attribute. Gender is ideal for this task due to its perceptual clarity and ease of evaluation. We design a set of prompts with varied phrasing but identical semantic meaning (e.g., &#8221;A male speaker is talking&#8221;, &#8221;A man is talking&#8221;, etc.) and use them to guide speech generation for both ParaStyleTTS and CosyVoice. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T9\" title=\"Table 9 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix section shows the full details of evaluated prompts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry. While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility. More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.\nIn this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone. ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling. It allows fine-grained and robust control over factors such as emotion, gender, and age. Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.\nExperimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation. Demo can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://parastyletts.github.io/ParaStyleTTS_Demo/\" title=\"\">https://parastyletts.github.io/ParaStyleTTS_Demo/</a>. Code can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/haoweilou/ParaStyleTTS\" title=\"\">https://github.com/haoweilou/ParaStyleTTS</a>.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "control",
                    "gender",
                    "speaking",
                    "speech",
                    "used",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-Speech (TTS) generation has made significant progress in recent years. It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools. Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.\nSome works have attempted to control prosodic style variations across different languages. For instance, StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage. Similarly, LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.\nHowever, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation. These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.</p>\n\n",
                "matched_terms": [
                    "tone",
                    "style",
                    "voice",
                    "gender",
                    "control",
                    "speech",
                    "chinese",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles. Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker&#8217;s emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib28\" title=\"\">2024</a>)</cite> demonstrate strong capabilities in natural language understanding and text generation. These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.\nCosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite> explores the use of LLMs to enable paralinguistic control in speech.\nIn CosyVoice, a descriptive style prompt (e.g., &#8221;a young woman speaking angrily&#8221;) is concatenated with the text input and processed by an LLM. The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder. This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt. While this approach allows for flexible and expressive synthesis, it also introduces several limitations.</p>\n\n",
                "matched_terms": [
                    "young",
                    "prompt",
                    "woman",
                    "style",
                    "control",
                    "speaking",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner. The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech. Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment. Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "style",
                    "control",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of high computational cost and limited interpretability in LLM-based approaches. We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture. Inspired by LanStyleTTS&#8217;s use of prosody style tokens at phoneme level and VITS&#8217;s end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels. Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency. Key contributions of this work are as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "sentence",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaking",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "control",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in style-controllable TTS models have aimed to enhance expressiveness, multilingual capabilities, and controllability over various aspects of speech such as prosody, emotion, and speaker identity. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S1.T1\" title=\"Table 1 &#8227; 1. Introduction &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides a comparative overview of representative models, categorized by their control method and levels of style control.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib10\" title=\"\">2021</a>)</cite> adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference. While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling. Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.</p>\n\n",
                "matched_terms": [
                    "style",
                    "speaker",
                    "control",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with phonemes. Each phoneme is associated with its own prosody style token, enabling fine-grained, interpretable control over prosodic features. This approach is particularly effective for tonal languages such as Chinese. However, these models are not end-to-end and rely on external vocoders, limiting their efficiency. Moreover, they struggle to generalize to</p>\n\n",
                "matched_terms": [
                    "control",
                    "style",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StyleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib15\" title=\"\">2024</a>)</cite> and LanStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> introduce phoneme-level style control mechanisms by aligning prosodic style tokens with individual phonemes. Each phoneme is associated with its own style token, enabling fine-grained and interpretable control over prosodic features. This design is particularly effective for tonal languages such as Chinese, where pitch and intonation are linguistically meaningful. However, these models lack the ability to control high-level paralinguistic styles such as emotion, age, or intent, which limits their expressiveness in broader speech generation scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "style",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recent models, including Spark-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib26\" title=\"\">2025</a>)</cite> and CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib5\" title=\"\">2024</a>)</cite>, leverage either speech or text-based prompt with LLMs to control speaking style. CosyVoice introduces a text-prompted system that enables paralinguistic control. Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "style",
                    "control",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this approach provides flexibility and multilingual generalization, it also introduces notable limitations. Our experiments show that CosyVoice is highly sensitive to prompt phrasing. For example, altering the prompt from &#8220;a female speaker&#8221; to &#8220;a female speaker is speaking Chinese&#8221; can lead to\na speech generated with an incorrect speaking style. In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker. This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts. A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "voice",
                    "speaker",
                    "control",
                    "speaking",
                    "speech",
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the IPA-based text tokenization method from LanStyleTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib16\" title=\"\">2025</a>)</cite> to convert English and Chinese text into phonemes with accompanying prosody features such as stress (English) and tone (Chinese). More specifically, each word in English is converted to ARPAbet phonemes using the CMU Pronouncing Dictionary&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Carnegie Mellon University, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib2\" title=\"\">2023</a>)</cite>, with stress markers extracted separately to form the style token sequence. The phonemes are then mapped to IPA phonemes.\nFor Chinese, we use the <span class=\"ltx_text ltx_font_typewriter\">pypinyin</span> library to convert each character into its Pinyin form, which is then split into initials and finals. The tone is stripped from the final and used as a style token, while the remaining components are mapped to IPA phonemes.</p>\n\n",
                "matched_terms": [
                    "tone",
                    "style",
                    "used",
                    "chinese",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our IPA dictionary comprises 81 phonemes, 5 tone markers for Chinese, and 3 stress markers for English. In addition, we include three special tokens: <span class=\"ltx_text ltx_font_typewriter\">[START]</span> to denote the beginning of a sequence, <span class=\"ltx_text ltx_font_typewriter\">[END]</span> for the end of a sequence, and <span class=\"ltx_text ltx_font_typewriter\">[|]</span> as a word boundary marker.</p>\n\n",
                "matched_terms": [
                    "tone",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},x_{2},\\ldots,x_{L}]</annotation></semantics></math> denote the phoneme embedding sequence obtained from text tokenization. We associate each phoneme <math alttext=\"x_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">x_{l}</annotation></semantics></math> with a corresponding prosodic style embedding <math alttext=\"\\mathbf{x}_{t},\\mathbf{s}^{\\text{pho}}_{t}\\in\\mathbb{R}^{d_{1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#119857;</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>&#119852;</mi><mi>t</mi><mtext>pho</mtext></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>1</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{t},\\mathbf{s}^{\\text{pho}}_{t}\\in\\mathbb{R}^{d_{1}}</annotation></semantics></math>, forming the phoneme-level prosody sequence:</p>\n\n",
                "matched_terms": [
                    "text",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding is denoted as <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math>, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent. To obtain this embedding, we employ a pre-trained MPNet model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib24\" title=\"\">2020</a>)</cite> to encode descriptive paralinguistic prompts into <math alttext=\"d_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">d_{2}</annotation></semantics></math>-dimensional embedding.\nFor each speech sample in our dataset, we construct a text prompt using the following template:</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "gender",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">&#8221;A [Age] [Gender] is speaking [Accent] with [Emotion] emotion.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "gender",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is then fed into MPNet to produce the paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>para</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}</annotation></semantics></math>, which is used to condition the TTS model and guide the generation of speech with the intended paralinguistic style.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Style in speech generation is a broad concept that encompasses both prosodic features, such as pitch and tone, and paralinguistic styles, including gender, emotion, accent, and more. In this research, we divide style into two categories with different levels of control. One is the phoneme-level style, which captures fine-grained prosodic variations such as tone and stress at the level of individual phonemes. This level strongly influences how each word is articulated. Another one is sentence-level style, which represents global characteristics of the speech. It includes emotion, age, gender, and accent. While these features shape the overall impression of the speech, they exert less direct influence on phoneme realization. To support effective control at both levels, we design specialized architectures tailored to the unique requirements of each level of style.</p>\n\n",
                "matched_terms": [
                    "tone",
                    "style",
                    "gender",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that paralinguistic style typically remains consistent throughout a speech, it can influence both phoneme-level and sentence-level acoustic characteristics. To capture these effects, we first apply two distinct linear layers to project paralinguistic prompt embedding <math alttext=\"\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mtext>para</mtext></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mn>2</mn></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{para}}\\in\\mathbb{R}^{d_{2}}</annotation></semantics></math> into phoneme-level&#160;(<math alttext=\"\\mathbf{S}^{\\text{local}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>local</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{local}}</annotation></semantics></math>) and sentence-level&#160;(<math alttext=\"\\mathbf{S}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{global}}</annotation></semantics></math>) paralinguistic style embedding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p7.m1\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication. This FiLM-based adapter integrates paralinguistic style into each phoneme within the speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence-level paralinguistic style embedding (<math alttext=\"\\mathbf{s}^{\\text{global}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p8.m1\" intent=\":literal\"><semantics><msup><mi>&#119852;</mi><mtext>global</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{s}^{\\text{global}}</annotation></semantics></math>) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder. By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#120641;</mi><mtext>prior</mtext></msub><mo>,</mo><msub><mi>&#120648;</mi><mtext>prior</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}_{\\text{prior}},\\boldsymbol{\\sigma}_{\\text{prior}}</annotation></semantics></math> are predicted by a prior encoder network, which takes as input the phoneme embeddings <math alttext=\"\\tilde{\\mathbf{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{X}}</annotation></semantics></math> and <span class=\"ltx_text ltx_font_bold\">local</span> style embedding <math alttext=\"\\mathbf{S}^{\\text{local}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p9.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext>local</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{\\text{local}}</annotation></semantics></math>. These parameters define the expected distribution of latent speech features given the linguistic content and phoneme-level paralinguistic style. The KL-divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib11\" title=\"\">2013</a>)</cite> between the transformed posterior sample and the prior is minimized during training:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> as the length of the text sequence and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> as the length of the paralinguistic style prompt.</p>\n\n",
                "matched_terms": [
                    "text",
                    "control",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter. The time complexity is <math alttext=\"\\mathbf{O}(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2})</annotation></semantics></math>. Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional <math alttext=\"\\mathbf{O}(M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>M</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(M^{2})</annotation></semantics></math> in computation. In total, this results in a combined time complexity of <math alttext=\"\\mathbf{O}(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(N^{2}+M^{2})</annotation></semantics></math>.\nIn contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length <math alttext=\"N+M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N+M</annotation></semantics></math>, which is jointly encoded by a large transformer or LLM model. This yields a total time complexity of <math alttext=\"\\mathbf{O}((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}((N+M)^{2})</annotation></semantics></math>.\nAs a result, LLM-style fusion introduces an additional cross-attention cost of <math alttext=\"\\mathbf{O}(NM)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119822;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{O}(NM)</annotation></semantics></math>, making it less computationally efficient.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "text",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct our experiments using a multilingual and multi-style speech corpus comprising both English and Chinese speech samples. To ensure broad coverage of paralinguistic styles, we construct a composite dataset by combining several publicly available speech datasets.\nThe final training data consists of four sources. The <span class=\"ltx_text ltx_font_bold\">Baker</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Databaker, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib4\" title=\"\">2020</a>)</cite> is a single-speaker Mandarin Chinese corpus featuring a female voice. The <span class=\"ltx_text ltx_font_bold\">LJSpeech</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ito and Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib8\" title=\"\">2017</a>)</cite> is a single-speaker English corpus, also featuring a female speaker, widely used in TTS research for clean and consistent English utterances. The <span class=\"ltx_text ltx_font_bold\">Emotional Speech Dataset (ESD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib29\" title=\"\">2021</a>)</cite> contributes a multilingual, multi-speaker emotional speech in both English and Chinese, covering five emotional categories and including both male and female speakers. To further enrich stylistic diversity, we curate 16 stylized character speech captions from the <span class=\"ltx_text ltx_font_bold\">Genshin Impact</span> voice dataset to capture expressive speech across different age styles.</p>\n\n",
                "matched_terms": [
                    "male",
                    "voice",
                    "speaker",
                    "speech",
                    "female",
                    "used",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It covers <span class=\"ltx_text ltx_font_bold\">two languages</span> (English and Chinese), <span class=\"ltx_text ltx_font_bold\">two genders</span> (male and female), and <span class=\"ltx_text ltx_font_bold\">four age categories</span> (child, teenager, young adult, and adult). Emotion labels span <span class=\"ltx_text ltx_font_bold\">five classes</span>: neutral, happy, sad, angry, and surprised.\nThe final training dataset has 86k speech samples, with around 108.30 hours of training data. The dataset contains speech from 38 speakers.\nMore details about our dataset can be found in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S7.T8\" title=\"Table 8 &#8227; 7. Limitations and Future Work &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in the appendix.</p>\n\n",
                "matched_terms": [
                    "child",
                    "young",
                    "speech",
                    "female",
                    "male",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speech recordings are resampled to 22.05 kH. To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS1\" title=\"3.1. Text Tokenization &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model. In addition, we generate a paralinguistic style caption (e.g., &#8221;A young female is speaking English with happy emotion&#8221;), which is then encoded into a style embedding to guide paralinguistic style control.</p>\n\n",
                "matched_terms": [
                    "young",
                    "style",
                    "control",
                    "speaking",
                    "speech",
                    "female",
                    "chinese",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the intelligibility and paralinguistic expressiveness of our system, we adopt both objective metrics and human perceptual testing.\nFor intelligibility assessment, we avoid using samples from our training set to ensure fair cross-model comparisons, as different baselines are trained on distinct datasets. Instead, we generate speech for two standardized corpora: the 720 Harvard Sentences<cite class=\"ltx_cite ltx_citemacro_citep\">(of&#160;Electrical and Engineers, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib18\" title=\"\">1969</a>)</cite> in English and 400 phonetically balanced Mandarin TMNews sentences set from&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib3\" title=\"\">2023</a>)</cite>, both widely recognized for evaluating speech systems. The generated speech is transcribed using Whisper-Base&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib21\" title=\"\">2023</a>)</cite>, and Word Error Rate (WER) is computed by comparing the transcriptions against the ground-truth text. Lower WER scores indicate higher intelligibility and transcription consistency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To complement the objective evaluation, we conduct a Mean Opinion Score (MOS) listening test. For each language, we randomly select ten generated samples from each model and present them to at least five bilingual listeners fluent in both English and Chinese. The listeners are asked to rate each sample along two dimensions: intelligibility (I-MOS), which reflects how easily the speech content can be understood, and naturalness (N-MOS), which reflects how human-like and fluent the speech sounds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chinese",
                    "listening"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers. For emotion evaluation, we adopt Emotion2Vec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib17\" title=\"\">2023</a>)</cite>, a state-of-the-art, open-source model for emotion recognition in speech. We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.\nFor age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib6\" title=\"\">2023</a>)</cite>. This model evaluates whether age and gender styles in the generated speech can be reliably identified.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gender",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment. All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios. To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences. Each sentence is processed individually with a batch size of one. The average inference time and peak CUDA memory usage per sample are recorded. These metrics provide a consistent and fair basis for comparing computational efficiency across models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the performance of ParaStyleTTS in terms of speech quality, resource usage, speaking style controllability, and robustness.\nOur research is guided by the following three research questions (RQs):</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1:</span> Can ParaStyleTTS achieve more expressive control over speaking styles?</p>\n\n",
                "matched_terms": [
                    "control",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2:</span> Can ParaStyleTTS provide effective style control in a lightweight and resource-efficient manner?</p>\n\n",
                "matched_terms": [
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ3:</span> Can ParaStyleTTS maintain robust style control under varying prompt formulations?</p>\n\n",
                "matched_terms": [
                    "control",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T3\" title=\"Table 3 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comparison of expressive speaking style control across different models. The results show that ParaStyleTTS surpasses CosyVoice in all evaluated paralinguistic dimensions, including emotion, gender, and age control. Specifically, speech generated by ParaStyleTTS achieves classification accuracies of 54.00% for emotion, 100.00% for gender, and 57.50% for age. In contrast, CosyVoice obtains only 47.50%, 75.00%, and 21.88% for the same categories, respectively.</p>\n\n",
                "matched_terms": [
                    "style",
                    "gender",
                    "control",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further evaluate the distinguishability of generated styles, we train a speaking style classifier and extract embeddings from ParaStyleTTS-generated speech conditioned on different style prompts. These embeddings are visualized using t-SNE in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, providing an external perspective on how well the model encodes paralinguistic styles in the acoustic space.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompts",
                    "speaking",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, age-related embeddings form well-separated clusters, with only minor overlap between Teenagers and Young Adults. This is expected, as teenagers and young adults are relatively close in age and therefore tend to share similar vocal characteristics. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows clearly distinct clusters for Male and Female, which confirms that the model captures gender-specific acoustic features. In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F2.sf3\" title=\"In Figure 2 &#8227; 5.2. Speaking Style Controllability &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>, most emotion classes form coherent and distinguishable clusters. However, Happy and Surprise overlap noticeably, likely because both involve elevated pitch, faster speech, and high energy. These similarities reduce the model&#8217;s ability to distinguish them.\nBy contrast, Sad, Neutral, and Angry are easier to separate. Each of these emotions shows more distinct acoustic patterns, such as slower pace, flat intonation, or sharper articulation.</p>\n\n",
                "matched_terms": [
                    "young",
                    "genderspecific",
                    "speech",
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that ParaStyleTTS can successfully control the speaking style of generated speech. It outperforms the CosyVoice across all three evaluated paralinguistic styles and produces embeddings with high distinguishability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaking",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This improvement can be attributed to ParaStyleTTS&#8217;s dedicated two-level style modeling architecture and its end-to-end training design.\nUnlike CosyVoice, which relies on large language models (LLMs) to infer and apply speaking styles from a semantic perspective, ParaStyleTTS adopts an acoustic-centric learning paradigm. In CosyVoice, style control is driven by semantics understanding. LLMs interpret the meaning of style prompts like happy or angry based on semantics meaning and then rely on a vocoder to generate speech. The overall process follows a pipeline from text to semantics, and from semantics to acoustics, where the acoustic characteristics of different speaking styles are modeled only indirectly.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "style",
                    "control",
                    "speaking",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS learns speaking styles directly from acoustic features through supervised training with style prompts. As shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S3.SS5\" title=\"3.5. Latent Embedding Learning &#8227; 3. Method &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>, the latent embeddings are explicitly conditioned on style prompts, enabling the model to form direct associations between each prompt and its corresponding acoustic characteristics. This approach allows for more accurate and fine-grained control over style expression.\nThe two-level architecture further strengthens this capability by applying the style prompt at both the phoneme level (capturing prosody and speech rate) and the sentence level (capturing broader attributes such as emotion and age). This design enables ParaStyleTTS to generate speech that is\nacoustically consistent with the intended speaking style.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "control",
                    "speaking",
                    "speech",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the table, ParaStyleTTS demonstrates significant resource efficiency. When the prompt encoder is included, the model requires 140 ms of inference time, has 150 million parameters, and uses 763 MB of CUDA memory. However, due to the design of ParaStyleTTS, the prompt encoder can be decoupled during inference by precomputing and caching the style embeddings in a production environment. Without the prompt encoder, the runtime model size and inference time are reduced to just 52 million parameters and 121 ms, respectively.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This efficiency comes from the end-to-end and efficient design of ParaStyleTTS.\nLLM-based approaches, such as CosyVoice, rely on a multi-stage pipeline for speech generation. Beyond using LLMs for style fusion, CosyVoice also requires a flow-matching-based vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#bib.bib13\" title=\"\">2022</a>)</cite> to convert the LLM output into a waveform. Each of these components adds to the overall computational load and system complexity.\nParaStyleTTS, on the other hand, leverages a fully end-to-end architecture that directly generates waveforms with no need for any additional modules. Hence, it reduces both latency and space consumption.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another major advantage of ParaStyleTTS is that it does not rely on LLMs for style adaptation. LLMs are computationally expensive due to their large parameter sizes and nature of autoregressive processing. ParaStyleTTS avoids this overhead by directly learning the mapping between style prompts and corresponding acoustic characteristics. With the help of a lightweight style adapter, it achieves high flexibility and fast generation while maintaining expressive control over speaking style.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "speaking",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to empirical improvements in runtime performance, ParaStyleTTS is also more efficient in terms of computational complexity. It processes phoneme tokens and style prompts independently using transformer encoders. The total time complexity is <math alttext=\"O(N^{2}+M^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2}+M^{2})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m3\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> represent the lengths of the phoneme sequence and style prompt, respectively.\nThis complexity can be further reduced during inference by precomputing and caching the style prompt embedding. Since the prompt no longer needs to be processed at runtime, the term associated with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m4\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is eliminated. The overall complexity can be further reduced to <math alttext=\"O(N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(N^{2})</annotation></semantics></math>.\nIn contrast, LLM-based models concatenate the phoneme and prompt tokens together and process them jointly. It results in a higher time complexity of <math alttext=\"O((N+M)^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p8.m6\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O((N+M)^{2})</annotation></semantics></math>. This complexity gap becomes increasingly significant with longer inputs or richer style prompts.\nBy combining architectural simplicity with both theoretical and practical efficiency, ParaStyleTTS offers a scalable and deployable solution suitable for real-time, cloud-based, and on-device TTS applications.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robust style control is critical for real-world TTS applications, where prompts may vary in phrasing but still intend to express the same speaking style. A robust model must consistently generate speech that matches the intended paralinguistic style, regardless of how the prompt is formulated.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "control",
                    "speaking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a clear gap in style accuracy and robustness between models in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T5\" title=\"Table 5 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T6\" title=\"Table 6 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.T7\" title=\"Table 7 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. CosyVoice frequently fails to produce speech aligned with the intended emotion, age, or gender, especially for underrepresented or subtle styles. For instance, CosyVoice achieves only 5.00% accuracy for Surprise, 0.00% for Child, and 50.00% for Male. These inconsistencies indicate that its style control is fragile and often fails to match the prompted speaking style.</p>\n\n",
                "matched_terms": [
                    "child",
                    "style",
                    "gender",
                    "control",
                    "speaking",
                    "speech",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ParaStyleTTS consistently generates speech with clearly distinguishable gender characteristics across all phrasings. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, embeddings of Male and Female speech form two well-separated clusters, indicating that the model maintains stable control regardless of prompt formulation.\nCosyVoice, on the other hand, shows weak robustness in this setup. While it performs reliably on Female prompts, it fails to maintain consistency for Male prompts: 5 out of 10 male-prompted samples are perceptually identified as female. This inconsistency is reflected in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18308v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4. Robustness Style Control &#8227; 5. Result &amp; Discussion &#8227; ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, where multiple Male samples are mis-clustered near the Female region. It highlights CosyVoice&#8217;s failure to robustly represent style under prompt variation. CosyVoice relies on LLMs to interpret the semantic information from prompts and infer speaking styles. This text-to-style pathway maps semantic meaning to acoustic characteristics through multiple indirect, black-box stages, including semantic interpretation, content-style fusion, and waveform generation. As a result, even small changes in prompt phrasing can cause fluctuations in the LLM&#8217;s latent representations, leading to unintended variations in the generated speech. Since CosyVoice lacks an explicit control mechanism for aligning acoustic output with the intended style, its control becomes brittle and highly sensitive to the prompt formulation.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "prompt",
                    "style",
                    "control",
                    "gender",
                    "speaking",
                    "speech",
                    "female",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, ParaStyleTTS adopts an acoustic-centric, prompt-to-acoustics learning paradigm. It learns to directly associate style prompts with acoustic features through supervised training, enabling a more grounded and consistent style representation. This results in clear and disentangled mappings between the prompt and the generated output, making the system robust to differences in prompt wording. These findings show that ParaStyleTTS not only achieves higher per-class accuracy across various paralinguistic styles but also maintains robust and consistent style expression under varied prompt formulations. This level of robustness is essential for real-world TTS applications, especially in interactive or open-ended environments, where users may express the same speaking style with a different prompt formulation.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "speaking",
                    "prompt",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, we propose a novel style-controllable TTS model, ParaStyleTTS, that enables efficient, robust, and expressive control over speaking style. ParaStyleTTS introduces a novel two-level style modeling architecture that captures both local prosodic and global paralinguistic styles and supports flexible control over speaking styles such as emotion, gender, and age. It adopts an acoustic-centric, end-to-end design that can generate high-quality speech directly from input text and style prompts.\nExperiments demonstrate that ParaStyleTTS outperforms LLM-based baselines in both style accuracy and computational efficiency. It achieves over 30x faster inference, up to 8x smaller model size, and 2.5x lower memory usage compared to CosyVoice, while maintaining consistent and expressive style control.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "style",
                    "control",
                    "gender",
                    "speaking",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While ParaStyleTTS demonstrates strong performance in naturalness and paralinguistic style control, it still falls slightly behind CosyVoice in overall intelligibility and subjective naturalness. In future work, we plan to expand the training dataset by incorporating a greater variety of speakers, speaking styles, and languages to help bridge this gap. Additionally, the current model supports only three paralinguistic styles. We aim to extend controllability to a broader range of paralinguistic styles, such as personality, speaking tone, and energy level, to enable a more comprehensive control of speaking style in TTS model.</p>\n\n",
                "matched_terms": [
                    "tone",
                    "speaking",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the development and writing of this paper, generative AI (GenAI) tools are used in limited, non-substantive ways. Specifically, we use GenAI tools to help refine grammar, improve clarity, and restructure paragraphs in the manuscript. All technical content, research insights, and model designs are solely authored by the authors without any GenAI-generated ideas. No GenAI tools are used to generate or modify source code, experiment design, or model training. All implementation and data handling are conducted manually using standard Python-based, PyTorch frameworks. All datasets used are publicly available human speech datasets. Data preprocessing and analysis are performed without the aid of GenAI tools.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            }
        ]
    }
}