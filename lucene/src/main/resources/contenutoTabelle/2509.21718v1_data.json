{
    "S3.T1": {
        "source_file": "Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization",
        "caption": "Table 1: Comparison of Offline vs Online Preference Optimization for English TTS Synthesis (with and without CFG). Results reported with 95%95\\% confidence intervals averaged across 55 inference runs.",
        "body": "Model\nCFG\nCER (↓\\downarrow)\nSSIM (↑\\uparrow)\nSquim-MOS (↑\\uparrow)\n\n\n\n\nBase\nNo\n2.68±1.132.68\\pm 1.13\n0.637±0.0080.637\\pm 0.008\n4.35±0.024.35\\pm 0.02\n\n\nBase + DPO\nNo\n0.89±0.150.89\\pm 0.15\n0.667±0.0030.667\\pm 0.003\n4.40±0.014.40\\pm 0.01\n\n\nBase + GRPO\nNo\n0.56±0.24\\mathbf{0.56\\pm 0.24}\n0.759±0.002\\mathbf{0.759\\pm 0.002}\n4.39±0.014.39\\pm 0.01\n\n\nBase\nYes\n0.57±0.110.57\\pm 0.11\n0.720±0.0040.720\\pm 0.004\n4.41±0.014.41\\pm 0.01\n\n\nBase + DPO\nYes\n0.55±0.100.55\\pm 0.10\n0.729±0.0030.729\\pm 0.003\n4.41±0.014.41\\pm 0.01\n\n\nBase + GRPO\nYes\n0.53±0.16\\mathbf{0.53\\pm 0.16}\n0.783±0.005\\mathbf{0.783\\pm 0.005}\n4.40±0.014.40\\pm 0.01",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CFG</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SSIM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Squim-MOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">No</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"2.68\\pm 1.13\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mrow><mn>2.68</mn><mo>&#177;</mo><mn>1.13</mn></mrow><annotation encoding=\"application/x-tex\">2.68\\pm 1.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.637\\pm 0.008\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mrow><mn>0.637</mn><mo>&#177;</mo><mn>0.008</mn></mrow><annotation encoding=\"application/x-tex\">0.637\\pm 0.008</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.35\\pm 0.02\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mrow><mn>4.35</mn><mo>&#177;</mo><mn>0.02</mn></mrow><annotation encoding=\"application/x-tex\">4.35\\pm 0.02</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Base + DPO</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">No</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.89\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mrow><mn>0.89</mn><mo>&#177;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">0.89\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.667\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mrow><mn>0.667</mn><mo>&#177;</mo><mn>0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.667\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.40\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mn>4.40</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">4.40\\pm 0.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Base + GRPO</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">No</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.56\\pm 0.24}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.56</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.24</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.56\\pm 0.24}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.759\\pm 0.002}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.759</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.002</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.759\\pm 0.002}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.39\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mrow><mn>4.39</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">4.39\\pm 0.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Yes</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.57\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mrow><mn>0.57</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">0.57\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.720\\pm 0.004\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mrow><mn>0.720</mn><mo>&#177;</mo><mn>0.004</mn></mrow><annotation encoding=\"application/x-tex\">0.720\\pm 0.004</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.41\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mrow><mn>4.41</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">4.41\\pm 0.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Base + DPO</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Yes</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.55\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mrow><mn>0.55</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">0.55\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.729\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mrow><mn>0.729</mn><mo>&#177;</mo><mn>0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.729\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.41\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mrow><mn>4.41</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">4.41\\pm 0.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Base + GRPO</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Yes</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.53\\pm 0.16}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.53</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.16</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.53\\pm 0.16}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.783\\pm 0.005}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.783</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.783\\pm 0.005}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.40\\pm 0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mrow><mn>4.40</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">4.40\\pm 0.01</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "confidence",
            "intervals",
            "055±010055pm",
            "053±016mathbf053pm",
            "↓downarrow",
            "online",
            "yes",
            "0720±00040720pm",
            "ssim",
            "cfg",
            "across",
            "0783±0005mathbf0783pm",
            "english",
            "tts",
            "base",
            "057±011057pm",
            "runs",
            "0637±00080637pm",
            "0667±00030667pm",
            "results",
            "synthesis",
            "averaged",
            "model",
            "optimization",
            "440±001440pm",
            "435±002435pm",
            "089±015089pm",
            "grpo",
            "441±001441pm",
            "preference",
            "without",
            "0729±00030729pm",
            "cer",
            "↑uparrow",
            "squimmos",
            "268±113268pm",
            "0759±0002mathbf0759pm",
            "reported",
            "offline",
            "056±024mathbf056pm",
            "439±001439pm",
            "dpo",
            "inference",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Beyond intelligibility, GRPO also enhances SSIM and PESQ scores. GRPO benefits not only unseen languages but also those seen during baseline model training as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Offline vs Online Preference Optimization &#8227; 3 Experiments &#8227; Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>\n(see project webpage for more details).\nOverall, combining small-scale fine-tuning with preference alignment enables strong, scalable adaptation to new languages with minimal data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Developing high-quality text-to-speech&#160;(TTS) systems for low-resource languages is challenging due to the scarcity of paired text and speech data. In contrast, automatic speech recognition&#160;(ASR) models for such languages are often more accessible, owing to large-scale multilingual pre-training efforts.\nWe propose a framework based on Group Relative Policy Optimization&#160;(GRPO) to adapt an autoregressive, multilingual TTS model to new languages.\nOur method first establishes a language-agnostic foundation for TTS synthesis by training a multilingual baseline with International Phonetic Alphabet (IPA) tokens. Next, we fine-tune this model on limited paired data of the new languages to capture the target language&#8217;s prosodic features.\nFinally, we apply GRPO to optimize the model using only unpaired text and speaker prompts, guided by a multi-objective reward from pretrained ASR, speaker verification, and audio quality estimation models.\nExperiments demonstrate that this pipeline produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone.\nFurthermore, our GRPO-based framework also improves TTS performance in high-resource languages, surpassing offline alignment methods such as Direct Preference Optimization&#160;(DPO) yielding superior intelligibility, speaker similarity, and audio quality.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Project Webpage: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"grpotts.github.io\" title=\"\">grpotts.github.io</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "offline",
                    "model",
                    "grpo",
                    "optimization",
                    "tts",
                    "preference",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nText-to-Speech, Preference Optimization, Speech Synthesis, Low-resource language, Multilingual</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "preference",
                    "optimization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in neural text-to-speech (TTS) synthesis have led to highly natural and intelligible systems, enabled by large-scale datasets and powerful generative models such as autoregressive transformers and diffusion models. Particularly, large language model (LLM) based TTS approaches have become increasingly common, leveraging the strong representation learning capabilities of LLMs to improve prosody, context adherence and naturalness of generated speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>]</cite>.\nHowever, the development of TTS for low-resource languages remains a pressing challenge. Collecting high-quality paired text&#8211;speech corpora is expensive and often infeasible for many underrepresented languages.\nThis data scarcity creates a substantial performance gap compared to high-resource languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, automatic speech recognition (ASR) systems for low-resource languages have benefited significantly from large-scale multilingual pretraining and transfer learning. Past work such as Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib9\" title=\"\">9</a>]</cite> and Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib10\" title=\"\">10</a>]</cite> have made robust ASR models widely available, even for languages with limited training resources.\nThis asymmetry suggests an opportunity: while curating extensive TTS data is difficult, one may still obtain a reliable ASR model for a given low-resource language, which can serve as a feedback signal for improving TTS models.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A parallel line of research in natural language processing has shown that reinforcement learning (RL) can effectively align generative models with task-specific objectives. Reinforcement learning from human feedback (RLHF) has emerged as a central paradigm for large language models (LLMs), where an external &#8220;judge&#8221; (human or automated) provides preference signals that guide the model towards producing more useful outputs. Recently, similar ideas have been explored in speech generation.\nOffline preference optimization methods such as Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib11\" title=\"\">11</a>]</cite> have been applied to TTS synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib12\" title=\"\">12</a>]</cite>, but these approaches lack the dynamic feedback loop provided by online RL methods.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "offline",
                    "model",
                    "optimization",
                    "tts",
                    "preference",
                    "online",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we investigate whether <span class=\"ltx_text ltx_font_italic\">ASR models can be leveraged as a reward function for learning TTS in low-resource settings</span>.\nSpecifically, we introduce a framework based on <span class=\"ltx_text ltx_font_italic\">Group Relative Policy Optimization (GRPO)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib13\" title=\"\">13</a>]</cite>, where an autoregressive TTS model trained across multiple languages is adapted to new languages using reinforcement learning. Our method proceeds in three stages: 1) We train a baseline autoregressive TTS model on a set of source languages, using the International Phonetic Alphabet (IPA) as the text representation. 2) The baseline model is adapted to a new target language using a small amount of available paired data, capturing its phonetic and prosodic patterns. 3) We generate text and speaker prompts in the target language and optimize the TTS model across multiple objectives using GRPO, guided by reward metrics from ASR, speaker verification, and PESQ&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib14\" title=\"\">14</a>]</cite> estimation models to improve intelligibility, speaker consistency, and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "grpo",
                    "optimization",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments show that this pipeline of fine-tuning followed by GRPO with automatic reward signals, yields intelligible and speaker-consistent TTS for new low-resource languages, significantly outperforming fine-tuning alone. Furthermore, we show that GRPO provides strong preference alignment even in <span class=\"ltx_text ltx_font_italic\">high-resource (seen) languages</span>, outperforming offline alignment methods such as DPO.\nTo the best of our knowledge, this is the first work to extend GRPO to token-based TTS synthesis with multi-objective rewards.\nThese findings suggest that online reinforcement learning offers a general and effective mechanism for aligning TTS models with perceptual and linguistic objectives.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "offline",
                    "grpo",
                    "tts",
                    "preference",
                    "online",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines our proposed framework for adapting a multilingual text-to-speech (TTS) model to a low-resource language using the three-step framework described above.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our synthesis framework is based on Koel-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>]</cite>, an autoregressive encoder-decoder TTS model that operates on the low-frame rate (<math alttext=\"21.5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mn>21.5</mn><annotation encoding=\"application/x-tex\">21.5</annotation></semantics></math> FPS) audio codec representation given by NanoCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib15\" title=\"\">15</a>]</cite>.\nKoel-TTS comprises an autoregressive (AR) transformer decoder conditioned on text encodings from a non-autoregressive (NAR) transformer encoder using cross-attention (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>a).\nSpecifically, given an input text sequence and acoustic codes of a reference speaker audio, the TTS model autoregressively generates the acoustic codes corresponding to the input text and speaker.\nTo build a foundation model capable of generalizing to new languages, we first pretrain the Koel-TTS model on a multilingual dataset comprising six languages &#8212; English, Dutch, Italian, Spanish, French, and German.\nA key aspect of our pretraining strategy is the use of the International Phonetic Alphabet (IPA) for text representation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib16\" title=\"\">16</a>]</cite>.\nIPA provides a universal, standardized representation of speech sounds.\nThe IPA text is tokenized using a byte-level tokenizer, which naturally handles the full range of IPA symbols with just <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> tokens.\nThis phonetic-based approach enables the model to learn a mapping from universal acoustic units to speech, which is crucial for zero-shot or few-shot adaptation to a new language.\nThe baseline model is trained on triplets of <em class=\"ltx_emph ltx_font_italic\">context audio, IPA transcript, and target audio</em> and optimized using only the next-frame prediction loss with a parallel head&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "synthesis",
                    "english",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After pretraining, we adapt the baseline model to a target low-resource language through supervised fine-tuning. This stage uses a limited amount of paired data (i.e., a few hours of speech and corresponding transcripts) from the target language.\nTo prevent catastrophic forgetting of the knowledge acquired during pretraining, we employ a mixed-data training strategy.\nThe fine-tuning dataset is a combination of the original pretraining data and the new low-resource data.\nWe upsample the low-resource data, ensuring that each training batch contains a high proportion of samples from the target language.\nThis approach allows the model to learn the specific phonetic and prosodic characteristics of the new language while retaining its general synthesis capabilities for languages seen during baseline model training.\nThe resulting fine-tuned model serves as the reference policy for the subsequent reinforcement learning stage.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While fine-tuning adapts the model to the target language, its quality is often limited by the small data size. To further enhance intelligibility and speaker similarity, we adapt Group Relative Policy Optimization (GRPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib13\" title=\"\">13</a>]</cite>, an online reinforcement learning algorithm designed for policy alignment. This procedure is depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b).</p>\n\n",
                "matched_terms": [
                    "grpo",
                    "model",
                    "optimization",
                    "online"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction:</span>\nThe GRPO training process requires prompts from which the model generates speech.\nEach prompt consists of a speaker reference audio and a text transcript to be synthesized.\nFor low-resource scenarios, a significant advantage of this setup is that the content of the speaker audio prompt does not need to match the text transcript.\nThis decouples the need for paired data, allowing us to leverage readily available, non-parallel speech and text data for creating a set of prompts. Specifically, we include <math alttext=\"15k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>15</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">15k</annotation></semantics></math> text and speaker reference audio pairs as the prompts for each language in our dataset. During GRPO, we construct prompts for all languages, including those seen during baseline model training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reward Function:</span>\nFor each generated audio sample, we compute a reward signal <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> that quantifies its preference.\nThe total reward is a weighted combination of three components: transcription accuracy, measured by Character Error Rate&#160;(CER), speaker similarity, measured by a cosine similarity score&#160;(SSIM) and perceptual speech quality, measured by a neural PESQ estimator:</p>\n\n",
                "matched_terms": [
                    "cer",
                    "ssim",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reward signal is well-scaled and interpretable, we map the raw CER and SSIM scores to a normalized reward value between <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><mn>0</mn></math> and <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> using a piecewise linear function. This function is defined by three key points: the worst possible score maps to 0, the best possible score maps to 1, and the average score observed from the baseline model maps to 0.5. The reward for intermediate values is linearly interpolated.\nTo optimize for PESQ, we normalize the raw PESQ score between <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><mn>0</mn></math> and <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> by setting <math alttext=\"R_{\\textit{pesq}}=\\textit{PESQ}/4.5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m5\" intent=\":literal\"><semantics><mrow><msub><mi>R</mi><mtext class=\"ltx_mathvariant_italic\">pesq</mtext></msub><mo>=</mo><mrow><mtext class=\"ltx_mathvariant_italic\">PESQ</mtext><mo>/</mo><mn>4.5</mn></mrow></mrow><annotation encoding=\"application/x-tex\">R_{\\textit{pesq}}=\\textit{PESQ}/4.5</annotation></semantics></math>.\nThe maximum CER and minimum PESQ values are clipped at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> and <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m7\" intent=\":literal\"><mn>0</mn></math> respectively before normalization.\nFor measuring CER, we use the multilingual Whisper Large V3 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib9\" title=\"\">9</a>]</cite>. SSIM is computed between the context audio and the generated audio using the Titanet-large model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib17\" title=\"\">17</a>]</cite>. PESQ is estimated using the reference-free neural PESQ estimator from torchaudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib14\" title=\"\">14</a>]</cite>. The precise reward normalization functions used in our framework are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#S2.F2\" title=\"Figure 2 &#8227; 2.3 Refinement with GRPO &#8227; 2 Methodology &#8227; Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "model",
                    "ssim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GRPO Objective:</span>\nGRPO optimizes the policy <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation></semantics></math> (our TTS model) to maximize the expected reward from generated samples&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib13\" title=\"\">13</a>]</cite>. For a given text and context audio input <math alttext=\"x=(x_{\\textit{text}},x_{\\textit{audio}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext class=\"ltx_mathvariant_italic\">text</mtext></msub><mo>,</mo><msub><mi>x</mi><mtext class=\"ltx_mathvariant_italic\">audio</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{\\textit{text}},x_{\\textit{audio}})</annotation></semantics></math>, the model&#8217;s response distribution <math alttext=\"\\pi(y|x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#960;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi(y|x)</annotation></semantics></math> encompasses a range of potential outputs <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> with varying reward values.\nFor each prompt <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>, we sample <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m6\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> responses <math alttext=\"\\{y_{i,k}\\}_{k=1}^{K}\\sim\\pi_{\\theta_{\\text{old}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m7\" intent=\":literal\"><semantics><mrow><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mo>&#8764;</mo><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mtext>old</mtext></msub></msub></mrow><annotation encoding=\"application/x-tex\">\\{y_{i,k}\\}_{k=1}^{K}\\sim\\pi_{\\theta_{\\text{old}}}</annotation></semantics></math> and obtain rewards <math alttext=\"r_{i,k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m8\" intent=\":literal\"><semantics><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">r_{i,k}</annotation></semantics></math>.\nA group baseline is defined as</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We omit the KL penalty to the reference model proposed in the original GRPO formulation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib13\" title=\"\">13</a>]</cite> since we find that relying solely on the group-relative advantages stabilizes learning and speeds up training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Model Training:</span> Our baseline TTS model is trained on English and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> European languages. For English, the training corpus consists of approximately <math alttext=\"18k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>18</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">18k</annotation></semantics></math> hours of TTS data from the following datasets: <em class=\"ltx_emph ltx_font_italic\">train-clean-360</em> and <em class=\"ltx_emph ltx_font_italic\">train-clean-100</em> subsets of LibriTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib18\" title=\"\">18</a>]</cite>, HiFiTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib19\" title=\"\">19</a>]</cite>, a <math alttext=\"17k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>17</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">17k</annotation></semantics></math>-hour subset of the LibriVox MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib20\" title=\"\">20</a>]</cite> and a proprietary, 2-speaker, 63-hour dataset. For European languages, we use <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> languages from the CML dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib21\" title=\"\">21</a>]</cite> that contains <math alttext=\"1{,}562\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>562</mn></mrow><annotation encoding=\"application/x-tex\">1{,}562</annotation></semantics></math> hours of German, <math alttext=\"642\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>642</mn><annotation encoding=\"application/x-tex\">642</annotation></semantics></math> hours of Dutch, <math alttext=\"476\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mn>476</mn><annotation encoding=\"application/x-tex\">476</annotation></semantics></math> hours of Spanish, <math alttext=\"283\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mn>283</mn><annotation encoding=\"application/x-tex\">283</annotation></semantics></math> hours of French, <math alttext=\"131\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mn>131</mn><annotation encoding=\"application/x-tex\">131</annotation></semantics></math> hours of Italian speech data.\nTo enable Classifier Free Guidance (CFG) during inference, we follow the methodology proposed in Koel-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>]</cite> to randomly drop out conditioning input during training.</p>\n\n",
                "matched_terms": [
                    "cfg",
                    "model",
                    "tts",
                    "english",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Alignment using GRPO:</span>\nGRPO is performed for a maximum of <math alttext=\"2k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">2k</annotation></semantics></math> mini-batch iterations where each mini-batch contains <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math> text and context audio prompts, at a fixed learning rate of <span class=\"ltx_text ltx_font_typewriter\">2e-7</span>. For each prompt, we generate <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> audios using multinomial sampling at temperature <math alttext=\"0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mn>0.7</mn><annotation encoding=\"application/x-tex\">0.7</annotation></semantics></math>.\nDuring GRPO, CFG is used for inference with a probability of <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> and CFG scale of <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>, to improve model alignment both with and without CFG.\nWe validate the model every <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> iterations on a validation set containing the new languages, and choose the checkpoint with the best <math alttext=\"R_{\\textit{CER}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext class=\"ltx_mathvariant_italic\">CER</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\textit{CER}}</annotation></semantics></math> on this validation set.</p>\n\n",
                "matched_terms": [
                    "cfg",
                    "model",
                    "grpo",
                    "without",
                    "preference",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation:</span>\nWe evaluate the generated speech along three primary dimensions: intelligibility, speaker similarity, and audio quality.\nIntelligibility is quantified using ASR-derived character error rate (CER) using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib9\" title=\"\">9</a>]</cite>. Speaker similarity is measured by computing the cosine similarity (SSIM) between embeddings of synthesized utterances and their corresponding context audio. Embeddings are obtained with Titanet-Small&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib17\" title=\"\">17</a>]</cite>, distinct from the Titanet-Large model applied during preference alignment. Audio quality is measured using a reference-free neural PESQ estimator&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib14\" title=\"\">14</a>]</cite> and naturalness is evaluated using Squim-MOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib14\" title=\"\">14</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "preference",
                    "cer",
                    "squimmos",
                    "ssim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a direct comparison with the DPO approach introduced in Koel-TTS, we apply GRPO to the <span class=\"ltx_text ltx_font_italic\">Koel-TTS 380M</span> English TTS model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#bib.bib6\" title=\"\">6</a>]</cite>, using the same prompt dataset. For consistency, we exclude the PESQ reward term, ensuring the setup is directly aligned with the conditions reported for DPO. Our results show that GRPO consistently achieves higher intelligibility and speaker similarity scores while maintaining comparable naturalness. This demonstrates that online preference optimization, which leverages continuous feedback and iterative improvement, is more effective than offline alignment techniques such as DPO.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "offline",
                    "model",
                    "optimization",
                    "grpo",
                    "tts",
                    "english",
                    "preference",
                    "online",
                    "dpo",
                    "comparison",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the performance of the baseline, fine-tuned, and preference-aligned models on low-resource languages in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21718v1#S2.F3\" title=\"Figure 3 &#8227; 2.3 Refinement with GRPO &#8227; 2 Methodology &#8227; Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The baseline model (solid gray), which was not trained on these languages, shows limited generalizability despite IPA tokenization, evident from its high CER scores. Interestingly, applying our multi-objective GRPO directly on this baseline (dashed gray) substantially reduces CER and boosts SSIM, even without access to paired text&#8211;speech data in the target language. This suggests that preference alignment alone can bridge part of the gap in learning TTS for new languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "grpo",
                    "tts",
                    "preference",
                    "without",
                    "cer",
                    "ssim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning on small amounts of target language data further amplifies performance gains. Models trained with just <math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> minutes of supervision already outperform the baseline by a wide margin, with steady improvements as the fine-tuning budget increases to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> hour and <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> hours. Notably, aligning fine-tuned TTS checkpoints with the proposed GRPO objective consistently lowers CER even further, highlighting the complementary benefits of preference alignment.\nFor example, with only <math alttext=\"30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mn>30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math> minutes of Portuguese data, CER drops from <math alttext=\"33.00\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mn>33.00</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">33.00\\%</annotation></semantics></math> in the baseline to <math alttext=\"3.94\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>3.94</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.94\\%</annotation></semantics></math> after fine-tuning and GRPO, which is a reduction of more than <math alttext=\"8\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mn>8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">8\\times</annotation></semantics></math>. Similar trends hold across Hindi and Polish, underscoring the robustness of this approach across typologically diverse languages.</p>\n\n",
                "matched_terms": [
                    "across",
                    "grpo",
                    "tts",
                    "preference",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a GRPO-based framework for adapting multilingual TTS generation models to low-resource languages by leveraging a pretrained ASR model for the new languages. When combining supervised fine-tuning on limited paired data with online preference optimization using multi-objective rewards, our approach delivers consistent gains in intelligibility, speaker similarity, and perceptual quality. Beyond low-resource adaptation, we also demonstrate that GRPO guided by automatic reward functions for TTS, provides stronger preference alignment than offline methods, even for widely used, data-rich languages, highlighting its broad utility for TTS synthesis.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "offline",
                    "model",
                    "optimization",
                    "grpo",
                    "tts",
                    "preference",
                    "online"
                ]
            }
        ]
    }
}