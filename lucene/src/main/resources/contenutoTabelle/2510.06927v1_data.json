{
    "A1.T1": {
        "caption": "Table 1: WER of MaskGCT for the cross-sentence task on different variants of the LibriSpeech test-clean.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">Subset Variant</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">40 utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">2.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">1234 utterances&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">4.22</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "wer",
            "task",
            "variants",
            "variant",
            "2024b",
            "librispeech",
            "yang",
            "maskgct",
            "2025c",
            "wang",
            "subset",
            "utterances",
            "different",
            "crosssentence"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To demonstrate this issue, we evaluate the open-sourced MaskGCT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/amphion/MaskGCT\" title=\"\">https://huggingface.co/amphion/MaskGCT</a></span></span></span> on two commonly used variants of the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset. WER is computed between ASR transcription of synthesized audio and the ground-truth text, using the HuBERT-Large ASR model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib30\" title=\"\">2021</a>)</cite>. The WER differs significantly across the two versions, ranging from 2.63 to 4.22, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A1.T1\" title=\"Table 1 &#8227; Appendix A Case Study on Variants of LibriSpeech test-clean Subsets &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. This observation argues the importance of clearly reporting dataset versions and evaluation protocols to ensure fair and reproducible comparisons.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) technology has made rapid progress, driven by advances in generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib59\" title=\"\">2018</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib40\" title=\"\">2019</a>); Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib37\" title=\"\">2021</a>); Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib56\" title=\"\">2021</a>); Jeong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib31\" title=\"\">2021</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> and the growing computational power and data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib81\" title=\"\">2019</a>); Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib50\" title=\"\">2015</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib43\" title=\"\">2024</a>); Kang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib34\" title=\"\">2024</a>); He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib29\" title=\"\">2024</a>)</cite>. Modern TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>); Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib21\" title=\"\">2024b</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite> produce speech that is increasingly natural, expressive, and human-indistinguishable, offering broad benefits across accessibility, education, content creation, and voice-based human-computer interaction.\nAt the same time, these capabilities are inherently dual-use.\nRealistic voice synthesis and voice cloning have already been exploited in misinformation campaigns, telecom fraud via audio deepfakes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib73\" title=\"\">2025</a>)</cite>.\nMoreover, biased training data can reinforce societal inequities&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pinhanez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib51\" title=\"\">2024</a>)</cite>,\nyielding uneven quality across demographic groups and reinforcing harmful stereotypes.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis technology enters a transformative era with the rise of fully neural, end-to-end architectures that significantly enhance speech naturalness and simplify the synthesis process. WaveNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Van Den&#160;Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib64\" title=\"\">2016</a>)</cite> generates high-quality raw audio by learning the long-range patterns in sound. Building on this, Tacotron&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib72\" title=\"\">2017</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib59\" title=\"\">2018</a>)</cite> uses attention-based sequence-to-sequence networks to turn text into mel-spectrograms, which a neural vocoder then converts into final waveforms. These models eliminate the need for hand-crafted linguistic features and complex alignment procedures, producing speech with more natural prosody and near-human quality. The introduction of Transformer-based models marks a further breakthrough&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib57\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib56\" title=\"\">2021</a>)</cite>. In parallel, more diverse generative modeling approaches begin to emerge, including variational&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib39\" title=\"\">2020</a>); Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib37\" title=\"\">2021</a>)</cite>, adversarial&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib44\" title=\"\">2018</a>)</cite>, and flow-based models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Miao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib46\" title=\"\">2020</a>); Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib36\" title=\"\">2020</a>)</cite>, which unify acoustic modeling and waveform generation within a single probabilistic framework. These models reflect a broader trend toward integrated, data-driven, and highly expressive TTS systems capable of capturing the variability and richness of natural speech across different speakers, styles, and diverse contexts.</p>\n\n",
                "matched_terms": [
                    "different",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The landscape of TTS has been fundamentally transformed by the emergence of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rombach et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib58\" title=\"\">2022</a>); Ramesh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib53\" title=\"\">2021</a>)</cite> and LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib49\" title=\"\">2024</a>); Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib7\" title=\"\">2023</a>)</cite>. Recent TTS systems leverage powerful sequence modeling to achieve unprecedented generalization, naturalness, and flexibility. Foundation models such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> and its subsequent extensions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>); Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib28\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib19\" title=\"\">2025</a>); Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite> redefine TTS as a conditional sequence modeling task over audio tokens, enabling zero-shot capabilities such as voice cloning and style transfer. In parallel, probabilistic generative methods, particularly diffusion models and flow-matching models, have advanced the field by enabling AR synthesis with high-fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib32\" title=\"\">2025</a>)</cite>, NAR synthesis with explicit duration controllability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Together, these developments mark a shift toward more unified, scalable, and general-purpose TTS systems.</p>\n\n",
                "matched_terms": [
                    "task",
                    "2024b",
                    "yang",
                    "2025c",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluation remains the primary choice for assessing perceptual quality in TTS, with MOS serving as the dominant protocol. MOS employs a five-point absolute category rating scale to rate individual utterances. Variants such as CMOS and MUSHRA are used for pairwise or comparative assessments. Although broadly regarded as the gold standard, these methods fall short in terms of sensitivity, consistency, and practical feasibility.\nOne major drawback of MOS stems from its limited resolution. As the quality of synthetic speech continues to improve, MOS scores tend to saturate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib70\" title=\"\">2025c</a>)</cite>. This ceiling effect obscures perceptual differences between high-performing systems, making it increasingly difficult to distinguish among them.\nAnother issue arises from the inherent variability in subjective ratings. Factors such as listener bias, contextual framing, playback conditions, and even day-to-day mood can introduce substantial noise. Without rigorous rater calibration and experimental controls, evaluations become unreliable.\nMoreover, the high cost associated with subjective evaluations presents a practical barrier. The process of recruiting a large and diverse pool of listeners, along with the need to ensure controlled testing conditions, demands considerable time and resources. These requirements often limit the feasibility and scale of such evaluations.</p>\n\n",
                "matched_terms": [
                    "variants",
                    "wang",
                    "2025c",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote fidelity and accuracy in TTS evaluation, we propose the following actionable recommendations, grounded in a reevaluation of current practices:\n(1) <span class=\"ltx_text ltx_font_italic\">greater attention to perceptual validity and uncertainty in objective metrics.</span>\nInterpretation of objective score differences should be approached with caution due to their nonlinear scaling, diminishing returns, domain-specific biases, and inherent uncertainty.\nWe advocate reporting uncertainty estimates of predicted MOS, particularly under out-of-distribution scenarios.\nWithout explicit consideration of uncertainty and prediction errors, small differences in predicted MOS should not be interpreted as genuine performance gains;\n(2) <span class=\"ltx_text ltx_font_italic\">development of practical, discriminative, and scalable evaluation protocols.</span>\nWe encourage the development of improved subjective and objective evaluation metrics, as exemplified by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib70\" title=\"\">2025c</a>)</cite> for subjective evaluation to address score saturation, reduce environmental inconsistencies, and enhance interpretability, and by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib77\" title=\"\">2025b</a>)</cite> for objective evaluation to better correlate with human perception;\n(3) <span class=\"ltx_text ltx_font_italic\">expanding the evaluation scope to underexplored dimensions.</span>\nWe advocate for a broader evaluation scope that includes dimensions such as long-form coherence, emotional expressiveness, punctuation sensitivity, and polyphonic word disambiguation.</p>\n\n",
                "matched_terms": [
                    "2025c",
                    "wang",
                    "yang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge to comparability stems from the inconsistent usage of evaluation datasets. The most commonly used test set, LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib50\" title=\"\">2015</a>)</cite> test-clean, is employed in divergent ways across various TTS studies. For example, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> utilizes 1234 utterances for zero-shot evaluation, while NatureSpeech3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite> employ only 40 utterance subsets, and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite> uses 1127 utterances with punctuation and capitalization. Such disparities in test set size significantly influence evaluation metrics like WER, as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A1\" title=\"Appendix A Case Study on Variants of LibriSpeech test-clean Subsets &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, making cross-study comparisons unreliable.\nFurthermore, most TTS studies do not release their prompt speech lists, while a few&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> only describe how the prompt lists are constructed. However, the sequence of prompt speech can impact performance, making results difficult to reproduce or compare.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "wer",
                    "2024b",
                    "librispeech",
                    "maskgct",
                    "utterances",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inference tasks to evaluate zero-shot TTS are also fragmented. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> introduced two tasks, <span class=\"ltx_text ltx_font_italic\">Continuation</span>, which uses the first three seconds of an utterance as a prompt and continues the speech, and <span class=\"ltx_text ltx_font_italic\">Cross-Sentence</span>, which prompts with a full utterance from the same speaker. However, later work such as E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>)</cite> redefines the <span class=\"ltx_text ltx_font_italic\">Continuation</span> task by using the last three seconds of a truncated segment as the prompt. These inconsistencies in task definition lead to incomparable evaluation results across different works.</p>\n\n",
                "matched_terms": [
                    "task",
                    "different",
                    "wang",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The computation of SIM scores also varies across studies. SIM-o measures the similarity between the synthesized speech and the original prompt, while SIM-r measures the similarity between the synthesized speech and the reconstructed prompt. SIM-r is not comparable across systems using different reconstruction methods. Even for SIM-o, evaluation practices differ. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> excludes the prompt segment from the synthesized audio when computing similarity, whereas VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite> includes the prompt in both the synthesized and reference speech. As detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A2\" title=\"Appendix B Case Study on Inconsistencies in SIM-o Evaluation Protocols &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, this leads to incomparability across different works.</p>\n\n",
                "matched_terms": [
                    "different",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To advance comparability, standardization, and transferability in TTS evaluation, we propose the following actionable recommendations:\n(1) <span class=\"ltx_text ltx_font_italic\">clear distinctions between comparable and incomparable results in evaluation reporting.</span>\nMetrics derived under different datasets, tasks, or configurations must not be treated as interchangeable. Any deviations should be reported explicitly to avoid misleading comparisons;\n(2) <span class=\"ltx_text ltx_font_italic\">stick to existing standardized evaluation protocols.</span>\nWhen formal standards such as ITU-T P.808 for MOS are available, researchers should adhere to them consistently. In the absence of formal standards, alignment with widely adopted practices is encouraged to promote practical convergence across studies;\n(3) <span class=\"ltx_text ltx_font_italic\">ensuring transparency in evaluation reporting.</span>\nEvaluation details should be disclosed, including but not limited to dataset splits, prompt lists, inference task definitions, metric configurations, human listening test procedures for MOS, and measurement setups for RTF;\n(4) <span class=\"ltx_text ltx_font_italic\">development of transferable metrics.</span>\nModel-based evaluation, including the use of LLMs as judges, offers a scalable and cost-effective alternative to human evaluation. We advocate for the creation and validation of metrics whose scores are reliably comparable across systems and studies without requiring simultaneous re-evaluation.</p>\n\n",
                "matched_terms": [
                    "task",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multiple versions of the LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset are used across recent TTS works, which leads to inconsistencies in reported results. One version contains 1234 utterances and is used in systems such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, MELLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>)</cite>, and PALLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite>. Another version contains 40 utterances and is used in works including NatureSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Other subsets, such as the one used in F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite>, also exist. These differences cause substantial variation in WER evaluations even for the same model.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "wer",
                    "2024b",
                    "librispeech",
                    "yang",
                    "maskgct",
                    "2025c",
                    "subset",
                    "utterances",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, there are two practices for computing SIM-o for the continuation task. One approach, adopted by VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, computes speaker similarity between the first 3-second ground-truth speech prompt and the remaining synthesized speech, excluding the prompt. Alternatively, another approach, as used in VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, computes the similarity between the full synthesized speech, including the prompt and the entire ground-truth speech.</p>\n\n",
                "matched_terms": [
                    "task",
                    "wang"
                ]
            }
        ]
    },
    "A2.T2": {
        "caption": "Table 2: SIM-o scores with or without prompt for the continuation task on the LibriSpeech test-clean.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">Protocol</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIM-o</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Without Prompt&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">0.754</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">With Prompt&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">0.905</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "simo",
            "without",
            "task",
            "protocol",
            "2023a",
            "prompt",
            "librispeech",
            "chen",
            "continuation",
            "wang",
            "scores"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A2.T2\" title=\"Table 2 &#8227; Appendix B Case Study on Inconsistencies in SIM-o Evaluation Protocols &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this difference using a representative case. These practices result in substantial differences in SIM-o scores, with an absolute value difference of up to 0.151. This case argues the necessity of clearly specifying the SIM-o computation method when reporting speaker similarity results for the continuation task.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) technology has made rapid progress, driven by advances in generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib59\" title=\"\">2018</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib40\" title=\"\">2019</a>); Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib37\" title=\"\">2021</a>); Ren et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib56\" title=\"\">2021</a>); Jeong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib31\" title=\"\">2021</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> and the growing computational power and data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib81\" title=\"\">2019</a>); Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib50\" title=\"\">2015</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib43\" title=\"\">2024</a>); Kang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib34\" title=\"\">2024</a>); He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib29\" title=\"\">2024</a>)</cite>. Modern TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>); Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib21\" title=\"\">2024b</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite> produce speech that is increasingly natural, expressive, and human-indistinguishable, offering broad benefits across accessibility, education, content creation, and voice-based human-computer interaction.\nAt the same time, these capabilities are inherently dual-use.\nRealistic voice synthesis and voice cloning have already been exploited in misinformation campaigns, telecom fraud via audio deepfakes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib73\" title=\"\">2025</a>)</cite>.\nMoreover, biased training data can reinforce societal inequities&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pinhanez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib51\" title=\"\">2024</a>)</cite>,\nyielding uneven quality across demographic groups and reinforcing harmful stereotypes.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "2023a",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The landscape of TTS has been fundamentally transformed by the emergence of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rombach et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib58\" title=\"\">2022</a>); Ramesh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib53\" title=\"\">2021</a>)</cite> and LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib49\" title=\"\">2024</a>); Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib7\" title=\"\">2023</a>)</cite>. Recent TTS systems leverage powerful sequence modeling to achieve unprecedented generalization, naturalness, and flexibility. Foundation models such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> and its subsequent extensions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>); Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib28\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib19\" title=\"\">2025</a>); Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite> redefine TTS as a conditional sequence modeling task over audio tokens, enabling zero-shot capabilities such as voice cloning and style transfer. In parallel, probabilistic generative methods, particularly diffusion models and flow-matching models, have advanced the field by enabling AR synthesis with high-fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib32\" title=\"\">2025</a>)</cite>, NAR synthesis with explicit duration controllability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Together, these developments mark a shift toward more unified, scalable, and general-purpose TTS systems.</p>\n\n",
                "matched_terms": [
                    "task",
                    "chen",
                    "2023a",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation of modern TTS systems has increasingly adopted a dual-track framework that combines subjective and objective measures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib20\" title=\"\">2024a</a>); Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib3\" title=\"\">2024</a>)</cite>. CMOS and SMOS are now widely used to assess perceived naturalness and speaker similarity, forming the core of human evaluation protocols. On the objective side, metrics such as word error rate, speaker embedding similarity, and model-based predictions of speech quality have become standard practice. Many recent approaches rely on pre-trained automatic speech recognition models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib52\" title=\"\">2022</a>); Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib30\" title=\"\">2021</a>); Gulati et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib26\" title=\"\">2020</a>)</cite>, speaker verification models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib12\" title=\"\">2022</a>)</cite>, and perceptual quality prediction models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baba et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib6\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib67\" title=\"\">2023b</a>)</cite> to provide consistent and scalable assessments. This shift reflects a broader trend toward using neural models as evaluation tools. Despite these advances, current evaluation practices largely overlook the ethical and societal implications of highly realistic speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lv et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib42\" title=\"\">2022</a>); Yi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib79\" title=\"\">2024</a>)</cite>. Issues such as identity spoofing, misinformation, and unauthorized voice replication are typically addressed only in brief disclaimers. There remains a critical need for standardized methodologies that integrate safety and misuse considerations into the core evaluation of speech generation systems.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "2023a",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Predicted MOS scores are generated by models trained on human ratings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cooper and Yamagishi (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib16\" title=\"\">2021</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib41\" title=\"\">2025</a>)</cite> collected following ITU-T P.808&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Naderi and Cutler (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib48\" title=\"\">2020</a>)</cite>.\nWhile these models offer a scalable alternative to human evaluation, they struggle with generalization and uncertainty estimation, primarily due to limitations in the diversity of training data and model representational power.\nPrior works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib68\" title=\"\">2025b</a>); Cooper et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib15\" title=\"\">2022</a>)</cite> have shown that existing MOS prediction models often produce inconsistent results even on in-domain data, and their performance degrades significantly when applied to out-of-domain data.\nA typical example of domain mismatch is the widespread use of DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Reddy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib54\" title=\"\">2021</a>); Cumlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib17\" title=\"\">2024</a>); Reddy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib55\" title=\"\">2022</a>)</cite>, which is trained on speech enhancement data yet commonly employed to evaluate synthesized speech.\nMoreover, MOS prediction models generally lack uncertainty estimation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib69\" title=\"\">2024a</a>)</cite>, as they typically provide only point estimates without associated confidence intervals, making it difficult to assess the reliability of the predicted quality scores. This remains rarely examined in current research.</p>\n\n",
                "matched_terms": [
                    "without",
                    "wang",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluation remains the primary choice for assessing perceptual quality in TTS, with MOS serving as the dominant protocol. MOS employs a five-point absolute category rating scale to rate individual utterances. Variants such as CMOS and MUSHRA are used for pairwise or comparative assessments. Although broadly regarded as the gold standard, these methods fall short in terms of sensitivity, consistency, and practical feasibility.\nOne major drawback of MOS stems from its limited resolution. As the quality of synthetic speech continues to improve, MOS scores tend to saturate&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib70\" title=\"\">2025c</a>)</cite>. This ceiling effect obscures perceptual differences between high-performing systems, making it increasingly difficult to distinguish among them.\nAnother issue arises from the inherent variability in subjective ratings. Factors such as listener bias, contextual framing, playback conditions, and even day-to-day mood can introduce substantial noise. Without rigorous rater calibration and experimental controls, evaluations become unreliable.\nMoreover, the high cost associated with subjective evaluations presents a practical barrier. The process of recruiting a large and diverse pool of listeners, along with the need to ensure controlled testing conditions, demands considerable time and resources. These requirements often limit the feasibility and scale of such evaluations.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scores",
                    "wang",
                    "protocol"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote fidelity and accuracy in TTS evaluation, we propose the following actionable recommendations, grounded in a reevaluation of current practices:\n(1) <span class=\"ltx_text ltx_font_italic\">greater attention to perceptual validity and uncertainty in objective metrics.</span>\nInterpretation of objective score differences should be approached with caution due to their nonlinear scaling, diminishing returns, domain-specific biases, and inherent uncertainty.\nWe advocate reporting uncertainty estimates of predicted MOS, particularly under out-of-distribution scenarios.\nWithout explicit consideration of uncertainty and prediction errors, small differences in predicted MOS should not be interpreted as genuine performance gains;\n(2) <span class=\"ltx_text ltx_font_italic\">development of practical, discriminative, and scalable evaluation protocols.</span>\nWe encourage the development of improved subjective and objective evaluation metrics, as exemplified by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib70\" title=\"\">2025c</a>)</cite> for subjective evaluation to address score saturation, reduce environmental inconsistencies, and enhance interpretability, and by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib77\" title=\"\">2025b</a>)</cite> for objective evaluation to better correlate with human perception;\n(3) <span class=\"ltx_text ltx_font_italic\">expanding the evaluation scope to underexplored dimensions.</span>\nWe advocate for a broader evaluation scope that includes dimensions such as long-form coherence, emotional expressiveness, punctuation sensitivity, and polyphonic word disambiguation.</p>\n\n",
                "matched_terms": [
                    "without",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge to comparability stems from the inconsistent usage of evaluation datasets. The most commonly used test set, LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib50\" title=\"\">2015</a>)</cite> test-clean, is employed in divergent ways across various TTS studies. For example, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> utilizes 1234 utterances for zero-shot evaluation, while NatureSpeech3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite> employ only 40 utterance subsets, and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite> uses 1127 utterances with punctuation and capitalization. Such disparities in test set size significantly influence evaluation metrics like WER, as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A1\" title=\"Appendix A Case Study on Variants of LibriSpeech test-clean Subsets &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, making cross-study comparisons unreliable.\nFurthermore, most TTS studies do not release their prompt speech lists, while a few&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> only describe how the prompt lists are constructed. However, the sequence of prompt speech can impact performance, making results difficult to reproduce or compare.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "chen",
                    "2023a",
                    "prompt",
                    "librispeech",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inference tasks to evaluate zero-shot TTS are also fragmented. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> introduced two tasks, <span class=\"ltx_text ltx_font_italic\">Continuation</span>, which uses the first three seconds of an utterance as a prompt and continues the speech, and <span class=\"ltx_text ltx_font_italic\">Cross-Sentence</span>, which prompts with a full utterance from the same speaker. However, later work such as E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>)</cite> redefines the <span class=\"ltx_text ltx_font_italic\">Continuation</span> task by using the last three seconds of a truncated segment as the prompt. These inconsistencies in task definition lead to incomparable evaluation results across different works.</p>\n\n",
                "matched_terms": [
                    "task",
                    "2023a",
                    "prompt",
                    "continuation",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The computation of SIM scores also varies across studies. SIM-o measures the similarity between the synthesized speech and the original prompt, while SIM-r measures the similarity between the synthesized speech and the reconstructed prompt. SIM-r is not comparable across systems using different reconstruction methods. Even for SIM-o, evaluation practices differ. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> excludes the prompt segment from the synthesized audio when computing similarity, whereas VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite> includes the prompt in both the synthesized and reference speech. As detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A2\" title=\"Appendix B Case Study on Inconsistencies in SIM-o Evaluation Protocols &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, this leads to incomparability across different works.</p>\n\n",
                "matched_terms": [
                    "simo",
                    "chen",
                    "2023a",
                    "prompt",
                    "wang",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Widely adopted MOS evaluations frequently depart from recommended standards. While ITU-T P.808&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Naderi and Cutler (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib48\" title=\"\">2020</a>)</cite> provides detailed protocols for conducting listening tests, many studies refer to MOS without reporting essential details, including rating scale definitions, rater calibration, playback conditions, and whether listeners rated naturalness or overall quality. Such inconsistencies reduce the reliability and comparability of MOS scores.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To advance comparability, standardization, and transferability in TTS evaluation, we propose the following actionable recommendations:\n(1) <span class=\"ltx_text ltx_font_italic\">clear distinctions between comparable and incomparable results in evaluation reporting.</span>\nMetrics derived under different datasets, tasks, or configurations must not be treated as interchangeable. Any deviations should be reported explicitly to avoid misleading comparisons;\n(2) <span class=\"ltx_text ltx_font_italic\">stick to existing standardized evaluation protocols.</span>\nWhen formal standards such as ITU-T P.808 for MOS are available, researchers should adhere to them consistently. In the absence of formal standards, alignment with widely adopted practices is encouraged to promote practical convergence across studies;\n(3) <span class=\"ltx_text ltx_font_italic\">ensuring transparency in evaluation reporting.</span>\nEvaluation details should be disclosed, including but not limited to dataset splits, prompt lists, inference task definitions, metric configurations, human listening test procedures for MOS, and measurement setups for RTF;\n(4) <span class=\"ltx_text ltx_font_italic\">development of transferable metrics.</span>\nModel-based evaluation, including the use of LLMs as judges, offers a scalable and cost-effective alternative to human evaluation. We advocate for the creation and validation of metrics whose scores are reliably comparable across systems and studies without requiring simultaneous re-evaluation.</p>\n\n",
                "matched_terms": [
                    "without",
                    "prompt",
                    "task",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multiple versions of the LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset are used across recent TTS works, which leads to inconsistencies in reported results. One version contains 1234 utterances and is used in systems such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, MELLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>)</cite>, and PALLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite>. Another version contains 40 utterances and is used in works including NatureSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Other subsets, such as the one used in F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite>, also exist. These differences cause substantial variation in WER evaluations even for the same model.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "chen",
                    "2023a",
                    "librispeech",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIM-o is defined as the cosine similarity between speaker embeddings extracted from original speech and synthesized speech. Commonly, SIM-o is computed using WavLM-TDNN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification#pre-trained-models\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification#pre-trained-models</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib12\" title=\"\">2022</a>)</cite>, where the score ranges within <math alttext=\"[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1,1]</annotation></semantics></math>, with higher values indicating greater speaker similarity.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "simo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, there are two practices for computing SIM-o for the continuation task. One approach, adopted by VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, computes speaker similarity between the first 3-second ground-truth speech prompt and the remaining synthesized speech, excluding the prompt. Alternatively, another approach, as used in VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, computes the similarity between the full synthesized speech, including the prompt and the entire ground-truth speech.</p>\n\n",
                "matched_terms": [
                    "simo",
                    "task",
                    "chen",
                    "2023a",
                    "prompt",
                    "continuation",
                    "wang"
                ]
            }
        ]
    },
    "A3.T3": {
        "caption": "Table 3: Comparison of WER and WER-MOS for the continuation task on the LibriSpeech test-clean.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">System</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER (%)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER-MOS (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Ground Truth</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">1.61</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">1.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">MELLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">1.47</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">1.05</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "melle",
            "testclean",
            "wer",
            "task",
            "librispeech",
            "meng",
            "continuation",
            "truth",
            "system",
            "wermos",
            "comparison",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A3.T3\" title=\"Table 3 &#8227; Appendix C Comparison between WER and Perceived Intelligibility &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports both objective WER and subjective WER-MOS results. While the synthesized speech from MELLE achieves a lower WER compared with the ground-truth recordings, the perceptual WER-MOS difference is marginal. This finding argues that, at already low error rates, further reductions in WER yield negligible perceptual improvement.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The landscape of TTS has been fundamentally transformed by the emergence of generative models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rombach et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib58\" title=\"\">2022</a>); Ramesh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib53\" title=\"\">2021</a>)</cite> and LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib49\" title=\"\">2024</a>); Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib7\" title=\"\">2023</a>)</cite>. Recent TTS systems leverage powerful sequence modeling to achieve unprecedented generalization, naturalness, and flexibility. Foundation models such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> and its subsequent extensions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>); Han et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib28\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib19\" title=\"\">2025</a>); Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite> redefine TTS as a conditional sequence modeling task over audio tokens, enabling zero-shot capabilities such as voice cloning and style transfer. In parallel, probabilistic generative methods, particularly diffusion models and flow-matching models, have advanced the field by enabling AR synthesis with high-fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib66\" title=\"\">2025a</a>); Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib32\" title=\"\">2025</a>)</cite>, NAR synthesis with explicit duration controllability&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Together, these developments mark a shift toward more unified, scalable, and general-purpose TTS systems.</p>\n\n",
                "matched_terms": [
                    "task",
                    "meng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge to comparability stems from the inconsistent usage of evaluation datasets. The most commonly used test set, LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib50\" title=\"\">2015</a>)</cite> test-clean, is employed in divergent ways across various TTS studies. For example, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> utilizes 1234 utterances for zero-shot evaluation, while NatureSpeech3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite> employ only 40 utterance subsets, and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite> uses 1127 utterances with punctuation and capitalization. Such disparities in test set size significantly influence evaluation metrics like WER, as detailed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A1\" title=\"Appendix A Case Study on Variants of LibriSpeech test-clean Subsets &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, making cross-study comparisons unreliable.\nFurthermore, most TTS studies do not release their prompt speech lists, while a few&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> only describe how the prompt lists are constructed. However, the sequence of prompt speech can impact performance, making results difficult to reproduce or compare.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inference tasks to evaluate zero-shot TTS are also fragmented. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite> introduced two tasks, <span class=\"ltx_text ltx_font_italic\">Continuation</span>, which uses the first three seconds of an utterance as a prompt and continues the speech, and <span class=\"ltx_text ltx_font_italic\">Cross-Sentence</span>, which prompts with a full utterance from the same speaker. However, later work such as E2 TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Eskimez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib22\" title=\"\">2024</a>)</cite> redefines the <span class=\"ltx_text ltx_font_italic\">Continuation</span> task by using the last three seconds of a truncated segment as the prompt. These inconsistencies in task definition lead to incomparable evaluation results across different works.</p>\n\n",
                "matched_terms": [
                    "task",
                    "continuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multiple versions of the LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset are used across recent TTS works, which leads to inconsistencies in reported results. One version contains 1234 utterances and is used in systems such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, MELLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>)</cite>, and PALLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib78\" title=\"\">2025c</a>)</cite>. Another version contains 40 utterances and is used in works including NatureSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ju et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib33\" title=\"\">2024</a>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib71\" title=\"\">2024b</a>)</cite>. Other subsets, such as the one used in F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib13\" title=\"\">2025</a>)</cite>, also exist. These differences cause substantial variation in WER evaluations even for the same model.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "testclean",
                    "wer",
                    "librispeech",
                    "meng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate this issue, we evaluate the open-sourced MaskGCT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/amphion/MaskGCT\" title=\"\">https://huggingface.co/amphion/MaskGCT</a></span></span></span> on two commonly used variants of the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset. WER is computed between ASR transcription of synthesized audio and the ground-truth text, using the HuBERT-Large ASR model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib30\" title=\"\">2021</a>)</cite>. The WER differs significantly across the two versions, ranging from 2.63 to 4.22, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A1.T1\" title=\"Table 1 &#8227; Appendix A Case Study on Variants of LibriSpeech test-clean Subsets &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. This observation argues the importance of clearly reporting dataset versions and evaluation protocols to ensure fair and reproducible comparisons.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, there are two practices for computing SIM-o for the continuation task. One approach, adopted by VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib65\" title=\"\">2023a</a>)</cite>, computes speaker similarity between the first 3-second ground-truth speech prompt and the remaining synthesized speech, excluding the prompt. Alternatively, another approach, as used in VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib11\" title=\"\">2024</a>)</cite>, computes the similarity between the full synthesized speech, including the prompt and the entire ground-truth speech.</p>\n\n",
                "matched_terms": [
                    "task",
                    "continuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#A2.T2\" title=\"Table 2 &#8227; Appendix B Case Study on Inconsistencies in SIM-o Evaluation Protocols &#8227; Towards Responsible Evaluation for Text-to-Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this difference using a representative case. These practices result in substantial differences in SIM-o scores, with an absolute value difference of up to 0.151. This case argues the necessity of clearly specifying the SIM-o computation method when reporting speaker similarity results for the continuation task.</p>\n\n",
                "matched_terms": [
                    "task",
                    "continuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine the relationship between WER and perceptual intelligibility, we conduct an analysis. We perform ASR on both synthesized speech from MELLE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Meng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06927v1#bib.bib45\" title=\"\">2025</a>)</cite> and ground-truth speech using the Conformer-Transducer model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span>. We recruit 10 graduate students with research experience in TTS as raters and conduct a MOS test focusing on perceived intelligibility, denoted as WER-MOS. Subjective ratings are computed by manually transcribing each sample and calculating its corresponding WER.</p>\n\n",
                "matched_terms": [
                    "meng",
                    "melle",
                    "wermos",
                    "wer"
                ]
            }
        ]
    }
}