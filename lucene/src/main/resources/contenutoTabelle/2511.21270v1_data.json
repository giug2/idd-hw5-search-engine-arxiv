{
    "S3.T1": {
        "source_file": "Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale",
        "caption": "Table 1. Comparison of different methods on SEED test sets (test-zh, test-en). For each subset we report WER (↓ lower is better), SIM (↑ higher is better).",
        "body": "Method\ntest-zh\ntest-en\ntest-hard\n\n\n\n\n\nCER↓\\downarrow\n\n\nSIM↑\\uparrow\n\n\nWER↓\\downarrow\n\n\nSIM↑\\uparrow\n\n\nCER↓\\downarrow\n\n\nSIM↑\\uparrow\n\n\nMOS↑\\uparrow\n\n\n\nSeed-TTS\n1.12\n0.796\n2.62\n0.714\n7.59\n0.776\n-\n\n\nFireRedTTS\n1.51\n0.635\n3.82\n0.460\n17.45\n0.621\n3.53\n\n\nMaskGCT\n2.27\n0.774\n2.62\n0.714\n10.27\n0.748\n-\n\n\nF5-TTS\n1.56\n0.741\n1.83\n0.615\n8.67\n0.713\n3.94\n\n\nSpark-TTS\n1.20\n0.672\n1.98\n0.584\n-\n-\n4.01\n\n\nCosyVoice\n3.63\n0.723\n4.29\n0.609\n11.75\n0.709\n3.89\n\n\nCosyVoice2\n1.45\n0.748\n2.57\n0.652\n6.83\n0.724\n3.98\n\n\nCosyVoice3\n1.12\n0.781\n2.21\n0.720\n5.83\n0.758\n4.07\n\n\nLLaSA-8B\n1.59\n0.684\n2.97\n0.574\n11.09\n0.660\n3.67\n\n\nLLaSA+SFT\n1.51\n0.688\n2.89\n0.582\n10.63\n0.674\n3.76\n\n\nLLaSA+RL\n1.10\n0.758\n2.12\n0.672\n6.04\n0.731\n4.12\n\n\nLLaSA+RL+FM\n1.08\n0.790\n2.08\n0.733\n5.98\n0.775\n4.21",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">test-zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">test-en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">test-hard</span></td>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Seed-TTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.796</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.714</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.776</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">FireRedTTS</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.51</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.635</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.460</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">17.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.621</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">MaskGCT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.774</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.714</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.748</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">F5-TTS</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.56</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.741</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.615</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.713</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Spark-TTS</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.672</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">1.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.584</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.723</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.609</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.709</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.748</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.652</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.724</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice3</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.781</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.720</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">5.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLaSA-8B</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.684</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.574</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.660</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLaSA+SFT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.688</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.582</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.674</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLaSA+RL</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">1.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.672</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.731</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">4.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLaSA+RL+FM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.790</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.733</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">5.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.775</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.21</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cosyvoice2",
            "wer",
            "llasarlfm",
            "lower",
            "cosyvoice",
            "testhard",
            "llasasft",
            "methods",
            "f5tts",
            "different",
            "testen",
            "each",
            "maskgct",
            "wer↓downarrow",
            "sets",
            "cer↓downarrow",
            "method",
            "fireredtts",
            "cosyvoice3",
            "sim",
            "comparison",
            "seed",
            "llasarl",
            "sim↑uparrow",
            "test",
            "higher",
            "report",
            "testzh",
            "better",
            "mos↑uparrow",
            "subset",
            "seedtts",
            "sparktts",
            "llasa8b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.21270v1#S3.T1\" title=\"Table 1 &#8227; 3.2. Main Results &#8227; 3. Experiments &#8227; Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports results on the SEED<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> benchmark, evaluating both objective metrics (CER/WER, SIM) and subjective naturalness through MOS, where 100 randomly sampled utterances were rated by 10 participants. We compare against several systems including Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025maskgct</span>)</cite>, F5-TTS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025f5</span>)</cite>, Spark-TTS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025spark</span>)</cite>, CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2025cosyvoice</span>)</cite>, and the LLaSA<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ye2025llasa</span>)</cite> baseline, an SFT-only variant, and a post-RL Flow Matching (FM)<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mehta2024matcha</span>)</cite> refinement model. Our GRPO-optimized LLaSA achieves the best CER on <span class=\"ltx_text ltx_font_italic\">test-zh</span> and competitive results across languages, outperforming all open-source single-codebook TTS LLMs (e.g., Spark-TTS, CosyVoice/2) and the SFT-only counterpart, showing the higher sample efficiency of RL relative to supervised fine-tuning. Despite CosyVoice3 benefiting from a hybrid architecture and substantially larger training data (1M h vs. our 250k h), our model still attains lower CER and comparable SIM.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">On the challenging <span class=\"ltx_text ltx_font_italic\">test-hard</span> split, GRPO delivers notable gains in both CER and SIM, demonstrating improved robustness under difficult scenarios. Importantly, our method also achieves the highest MOS, indicating strong alignment with human preference. Adding FM after RL further improves performance, especially SIM and MOS, confirming that our RL optimization strengthens the intrinsic AR policy in a manner complementary to acoustic refinement.</p>\n\n",
                "matched_terms": [
                    "sim",
                    "testhard",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.21270v1#S3.T2\" title=\"Table 2 &#8227; 3.4. Ablation Study &#8227; 3. Experiments &#8227; Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the incremental effect of each reward term across both objective (CER/WER, SIM) and subjective (MOS) metrics. <math alttext=\"R_{\\text{intl}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>intl</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{intl}}</annotation></semantics></math> &amp; <math alttext=\"R_{\\text{sim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>sim</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{sim}}</annotation></semantics></math> establish strong initial gains, while the length penalty further reduces duration mismatch. <math alttext=\"R_{ent}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>R</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{ent}</annotation></semantics></math> yields notable improvements in both stability and MOS, indicating smoother and more natural token dynamics. <math alttext=\"R_{pro}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>R</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{pro}</annotation></semantics></math> delivers the largest additional boost across all metrics&#8212;especially in MOS&#8212;showing that explicit rhythmic supervision not only improves prosodic structure but also better aligns the model&#8217;s outputs with human perceptual preferences.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sim",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a GRPO-based RL framework that directly improves the intrinsic autoregressive policy of single-codebook TTS LLMs. By integrating objective, rule-based, and LLM-assisted prosody rewards, our method enhances prosodic stability, speaker similarity, and naturalness across model and data scales. These gains further compound with flow-matching refinement, indicating that our RL optimization strengthens the core AR decoder rather than overlapping with downstream token-refinement methods.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "method"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale",
        "caption": "Table 2. Ablation study on the contribution of different reward components. Lower CER/WER and higher SIM/MOS indicate better performance.",
        "body": "Method\nzh\nen\n\n\n\n\nCER↓\\downarrow\n\n\nSIM↑\\uparrow\n\n\nMOS↑\\uparrow\n\n\nWER↑\\uparrow\n\n\nSIM↓\\downarrow\n\n\nMOS↑\\uparrow\n\n\n\n\n\nLLaSA\n1.59\n0.684\n3.68\n2.97\n0.574\n3.57\n\n\n\n+ Ri​n​t​lR_{intl} & Rs​i​mR_{sim}\n\n1.31\n0.719\n3.77\n2.66\n0.623\n3.69\n\n\n\n+ Rl​e​nR_{len}\n\n1.23\n0.738\n3.81\n2.48\n0.647\n3.75\n\n\n\n+ Re​n​tR_{ent}\n\n1.12\n0.751\n4.01\n2.25\n0.668\n3.90\n\n\n\n+ Rp​r​oR_{pro} (Full)\n\n1.10\n0.758\n4.25\n2.12\n0.672\n4.12",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLaSA</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.684</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.574</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><math alttext=\"R_{intl}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">l</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{intl}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span><math alttext=\"R_{sim}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">m</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{sim}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.31</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.719</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.623</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><math alttext=\"R_{len}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">n</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{len}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.738</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.647</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.75</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><math alttext=\"R_{ent}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">t</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{ent}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.751</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.668</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><math alttext=\"R_{pro}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">o</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{pro}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (Full)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.672</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "components",
            "llasa",
            "simmos",
            "study",
            "lower",
            "contribution",
            "rp​r​orpro",
            "different",
            "ri​n​t​lrintl",
            "cer↓downarrow",
            "method",
            "reward",
            "ablation",
            "performance",
            "rl​e​nrlen",
            "rs​i​mrsim",
            "cerwer",
            "sim↑uparrow",
            "higher",
            "wer↑uparrow",
            "indicate",
            "sim↓downarrow",
            "better",
            "mos↑uparrow",
            "re​n​trent",
            "full"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.21270v1#S3.T2\" title=\"Table 2 &#8227; 3.4. Ablation Study &#8227; 3. Experiments &#8227; Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the incremental effect of each reward term across both objective (CER/WER, SIM) and subjective (MOS) metrics. <math alttext=\"R_{\\text{intl}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>intl</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{intl}}</annotation></semantics></math> &amp; <math alttext=\"R_{\\text{sim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>R</mi><mtext>sim</mtext></msub><annotation encoding=\"application/x-tex\">R_{\\text{sim}}</annotation></semantics></math> establish strong initial gains, while the length penalty further reduces duration mismatch. <math alttext=\"R_{ent}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>R</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{ent}</annotation></semantics></math> yields notable improvements in both stability and MOS, indicating smoother and more natural token dynamics. <math alttext=\"R_{pro}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>R</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">R_{pro}</annotation></semantics></math> delivers the largest additional boost across all metrics&#8212;especially in MOS&#8212;showing that explicit rhythmic supervision not only improves prosodic structure but also better aligns the model&#8217;s outputs with human perceptual preferences.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.</p>\n\n",
                "matched_terms": [
                    "method",
                    "reward"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a multi-reward GRPO framework that integrates objective metrics with three rule-based rewards: a length penalty for duration consistency, an entropy reward for stable decoding, and an LLM-annotated Prosody Alignment Reward for explicit rhythm supervision. To obtain prosodic templates, a reasoning LLM (e.g., DeepSeek-R1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2025deepseek</span>)</cite>) generates multiple plausible pause patterns via in-context learning offline; these templates then guide online optimization toward natural rhythm. We further conduct a systematic scaling study across 1K&#8211;1M data and 1B&#8211;8B models, revealing a strong correlation between RL effectiveness and data scale. Experiments show robust gains in prosodic stability, speaker similarity, and naturalness. Moreover, adding a Flow Matching refinement module after RL continues to yield improvements, confirming that our method strengthens the intrinsic AR policy in a manner complementary to acoustic refiners.</p>\n\n",
                "matched_terms": [
                    "study",
                    "reward",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The reward function <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> is decomposed into several interpretable components that capture different aspects of synthesis quality:</p>\n\n",
                "matched_terms": [
                    "components",
                    "reward",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{i}</annotation></semantics></math> are tunable scaling coefficients that balance the contribution of different reward terms. The policy parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> are updated via GRPO to maximize <math alttext=\"\\mathcal{J}(\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119973;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#952;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{J}(\\theta)</annotation></semantics></math> under the combined signal.</p>\n\n",
                "matched_terms": [
                    "contribution",
                    "reward",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.21270v1#S3.T1\" title=\"Table 1 &#8227; 3.2. Main Results &#8227; 3. Experiments &#8227; Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports results on the SEED<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> benchmark, evaluating both objective metrics (CER/WER, SIM) and subjective naturalness through MOS, where 100 randomly sampled utterances were rated by 10 participants. We compare against several systems including Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025maskgct</span>)</cite>, F5-TTS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025f5</span>)</cite>, Spark-TTS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025spark</span>)</cite>, CosyVoice<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2025cosyvoice</span>)</cite>, and the LLaSA<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ye2025llasa</span>)</cite> baseline, an SFT-only variant, and a post-RL Flow Matching (FM)<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mehta2024matcha</span>)</cite> refinement model. Our GRPO-optimized LLaSA achieves the best CER on <span class=\"ltx_text ltx_font_italic\">test-zh</span> and competitive results across languages, outperforming all open-source single-codebook TTS LLMs (e.g., Spark-TTS, CosyVoice/2) and the SFT-only counterpart, showing the higher sample efficiency of RL relative to supervised fine-tuning. Despite CosyVoice3 benefiting from a hybrid architecture and substantially larger training data (1M h vs. our 250k h), our model still attains lower CER and comparable SIM.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "cerwer",
                    "llasa",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the challenging <span class=\"ltx_text ltx_font_italic\">test-hard</span> split, GRPO delivers notable gains in both CER and SIM, demonstrating improved robustness under difficult scenarios. Importantly, our method also achieves the highest MOS, indicating strong alignment with human preference. Adding FM after RL further improves performance, especially SIM and MOS, confirming that our RL optimization strengthens the intrinsic AR policy in a manner complementary to acoustic refinement.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance"
                ]
            }
        ]
    }
}