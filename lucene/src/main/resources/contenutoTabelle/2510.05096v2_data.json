{
    "S2.T1": {
        "caption": "Table 1: Comparison of Paper2VideoÂ with existing benchmarks. Top: existing natural video generation; Button: recent Agents for research works.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Benchmarks</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Inputs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Outputs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Subtitle</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Slides</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Cursor</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:-0.25pt 4.0pt;\">Speaker</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">Face</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">Voice</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#696969;\">Natural Video Generation</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">VBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib14\" title=\"\">2024a</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Short Vid.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">VBench<math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math><math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib15\" title=\"\">2024b</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Text&amp;Image</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Short Vid.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">Talkinghead&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib30\" title=\"\">2024</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Audio&amp;Image</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Short Vid.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">MovieBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib34\" title=\"\">2025a</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Text&amp;Audio&amp;Image</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Long Vid.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#696969;\">Multimodal Agent for Research</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">Paper2Poster&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Paper</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Poster</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Doc.&amp;Template</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Slide</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\">PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Doc.&amp;Template</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">Audio&amp;Long Vid.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\">Paper2Video&#160;(Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\">Paper&amp;Image&amp;Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\">Audio&amp;Long Vid.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#0071BC;\">&#10003;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "natural",
            "inputs",
            "2024b",
            "recent",
            "shi",
            "research",
            "paper2video",
            "subtitle",
            "comparison",
            "paper2poster",
            "talkinghead",
            "textimage",
            "pang",
            "pptagent",
            "2024a",
            "huang",
            "doctemplate",
            "slides",
            "face",
            "works",
            "generation",
            "speaker",
            "voice",
            "short",
            "long",
            "tan",
            "slide",
            "textaudioimage",
            "paperimageaudio",
            "vid",
            "text",
            "agents",
            "presentagent",
            "button",
            "vbench",
            "cursor",
            "moviebench",
            "top",
            "agent",
            "zheng",
            "2025a",
            "poster",
            "audiolong",
            "multimodal",
            "ours",
            "outputs",
            "existing",
            "video",
            "audioimage",
            "paper",
            "benchmarks"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video.\nUnlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\">Paper2Video</span>, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics&#8212;Meta Similarity, PresentArena, <span class=\"ltx_text ltx_font_italic\">PresentQuiz</span>, and <span class=\"ltx_text ltx_font_italic\">IP Memory</span>&#8212;to measure how videos convey the paper&#8217;s information to the audience and affect the work impact. Building on this foundation, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective <span class=\"ltx_text ltx_font_italic\">Tree Search Visual Choice</span>, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video&#160;demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/showlab/Paper2Video\" title=\"\">https://github.com/showlab/Paper2Video</a>.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "multimodal",
                    "existing",
                    "natural",
                    "cursor",
                    "inputs",
                    "agent",
                    "video",
                    "slides",
                    "speaker",
                    "research",
                    "generation",
                    "short",
                    "paper2video",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos are widely used in research communication, serving as a crucial and effective means to bridge researchers, as many conferences require them as an essential material for submission. However, the manual creation of such a video is highly labor-intensive, requiring slide design, subtitle writing, per-slide recording, and careful editing, which on average may take several hours to produce a <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> to <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> minute video for a scientific paper. Despite some prior works on slide and poster generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib29\" title=\"\">2021</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>], Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite> and other AI4Research tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>], Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>], Seo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib25\" title=\"\">2025</a>], Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite>, automatic academic presentation video generation is a superproblem of them, a practical yet more challenging direction.</p>\n\n",
                "matched_terms": [
                    "pang",
                    "zheng",
                    "video",
                    "works",
                    "generation",
                    "research",
                    "poster",
                    "slide",
                    "paper",
                    "subtitle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike natural video generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Zhang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib37\" title=\"\">2025</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, presentation video exhibits distinctive characteristics, including multi-sensory integration, multi-figure conditioning, and high text density, which highlight the limitations of current natural video generation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>. Specifically, academic presentation video generation faces several crucial challenges:\n<span class=\"ltx_text ltx_font_italic\">a.</span> It originates from long-context papers that contain dense text as well as multiple figures and tables;\n<span class=\"ltx_text ltx_font_italic\">b.</span> It requires the coordination of multiple aligned channels, including slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, subtitling, text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib6\" title=\"\">2024b</a>]</cite>, cursor control, and talking head generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>], Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite>;\n<span class=\"ltx_text ltx_font_italic\">c.</span> It lacks well-defined evaluation metrics: what constitutes a good presentation video, particularly in terms of knowledge conveyance and audience accessibility. Even for the state-of-the-art end-to-end video&#8211;audio generation model Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, notable limitations remain in video length, clarity of dense on-screen text, and multi-modal long-document condition.\nIn this work, we try to solve these two core problems as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "natural",
                    "cursor",
                    "zheng",
                    "2024b",
                    "video",
                    "generation",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable comprehensive evaluation of academic presentation video generation, we present the <span class=\"ltx_text ltx_font_bold\">Paper2Video</span> Benchmark, comprising <math alttext=\"101\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>101</mn><annotation encoding=\"application/x-tex\">101</annotation></semantics></math> paired research papers and author-recorded presentation videos from recent conferences, together with original slides and speaker identity metadata. Based on this benchmark, we develop a suite of metrics to comprehensively evaluate generation quality from multiple dimensions:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Meta Similarity &#8212; We employ a VLM to evaluate the alignment of generated slides and subtitles with human-designed counterparts.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> PresentArena &#8212; We use a VideoLLM as a proxy audience to perform double-order pairwise comparisons between generated and human-made videos.\nNotably, the primary purpose of a presentation is to <span class=\"ltx_text ltx_font_italic\">effectively convey the information contained in the paper</span>. To this end, we introduce <span class=\"ltx_text ltx_font_bold\">(iii) PresentQuiz</span>, which treats the VideoLLMs as the audience and requires them to answer paper-derived questions given the videos. Furthermore, another important purpose of presentation video is to <span class=\"ltx_text ltx_font_italic\">enhance the visibility and impact of the author&#8217;s work</span>. Motivated by real-conference interactions, we introduce <span class=\"ltx_text ltx_font_bold\">(iv) IP Memory</span>, which measures how well an audience can associate authors and works after watching presentation videos.</p>\n\n",
                "matched_terms": [
                    "video",
                    "recent",
                    "slides",
                    "works",
                    "research",
                    "generation",
                    "speaker",
                    "paper2video",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively generate ready-to-use academic presentation videos, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework that enables academic presentation video generation from research papers and speaker identity. It integrates subsequent key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> Slide Generation. Instead of adopting the commonly used format (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, pptx, XML) from a template slide as in &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we employ LaTeX code for slide generation from sketch, given its formal suitability for academic use and higher efficiency. Specifically, we employ a state-of-the-art Coder to generate code and introduce an effective <span class=\"ltx_text ltx_font_bold\">focused debugging</span> strategy, which iteratively narrows the scope and resolves compilation errors using feedback that indicates the relevant rows.\nTo address the insensitivity of LLMs to fine-grained numerical adjustments, we propose a novel method called <span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice</span>. This approach systematically explores parameter variations to generate multiple branches, which are then concatenated into a single figure. A VLM is then tasked with selecting the optimal branch, thereby effectively improving element layouts such as figure and font size.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitling and Cursor Grounding. We generate subtitles and cursor prompts for each sentence based on the slides. Then we achieve cursor spatial-temporal alignment using <span class=\"ltx_text ltx_font_bold\">Computer-use grounding model</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> models and WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> respectively. <span class=\"ltx_text ltx_font_bold\">(iii)</span> Speech Synthesis and Talking-head Rendering. We synthesize personalized speech via text-to-speech models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib5\" title=\"\">2024a</a>]</cite> and produce talking-head videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> for author presentations. Inspired by human recording practice and the independence between each slide, we <span class=\"ltx_text ltx_font_bold\">parallelize generation</span> across slides, achieving a speedup of more than <math alttext=\"\\mathbf{6\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>&#120788;</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6\\times}</annotation></semantics></math>.\nWe will open-source all our data and codebase to empower the research community.\n</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "cursor",
                    "zheng",
                    "2024a",
                    "video",
                    "slides",
                    "speaker",
                    "research",
                    "generation",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Paper2Video, the first high-quality benchmark of <math alttext=\"101\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>101</mn><annotation encoding=\"application/x-tex\">101</annotation></semantics></math> papers with author-recorded presentation videos, slides, and speaker metadata, together with evaluation metrics: Meta Similarity, PresentArena, PresentQuiz, and IP Memory.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "paper2video",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose PaperTalker, the first multi-agent framework for academic presentation video generation. It introduces three key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> tree search visual choice for fine-grained slide generation; <span class=\"ltx_text ltx_font_bold\">(ii)</span> a GUI-grounding model coupled with WhisperX for spatial-temporal aligned cursor grounding; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> slide-wise parallel generation to improve efficiency.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "generation",
                    "slide",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in video diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib14\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib15\" title=\"\">b</a>]</cite> have substantially improved <span class=\"ltx_text ltx_font_italic\">natural</span> video generation in terms of length, quality, and controllability. However, these <span class=\"ltx_text ltx_font_bold\">end-to-end</span> diffusion models still struggle to produce long videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, several minutes), handle multiple shots, and support conditioning on multiple images&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>.\nMoreover, most existing approaches generate only video without aligned audio, leaving a gap for real-world applications. To address these limitations, recent works leverage <span class=\"ltx_text ltx_font_bold\">multi-agent</span> collaboration to generate multi-shot, long video&#8211;audio pairs and enable multi-image conditioning.\nSpecifically, for natural videos, MovieAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib35\" title=\"\">2025b</a>]</cite> adopts a hierarchical CoT planning strategy and leverages LLMs to simulate the roles of a director, screenwriter, storyboard artist, and location manager, thereby enabling long-form movie generation. Alternatively, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> targets presentation video generation but merely combines PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite> with text-to-speech to produce narrated slides.\nHowever, it lacks personalization (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, mechanical speech and absence of a presenter) and fails to generate academic-style slides (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, missing opening and outline slides), thereby limiting its applicability in academic contexts. Our work addresses these limitations and enables ready-to-use academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "natural",
                    "pptagent",
                    "zheng",
                    "2024a",
                    "video",
                    "recent",
                    "huang",
                    "shi",
                    "slides",
                    "works",
                    "generation",
                    "long",
                    "presentagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many useful tasks have been explored under the umbrella of AI for Research (AI4Research)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>]</cite>, which aims to support the full scholarly workflow spanning text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dasigi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib9\" title=\"\">2021</a>]</cite>, static visuals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and dynamic video&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>. With the breakthrough of LLMs in text generation and the Internet search ability, extensive efforts have been devoted to academic writing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>]</cite> and literature surveying&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Katz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib16\" title=\"\">2024</a>], DeYoung et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib11\" title=\"\">2021</a>], Lu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib18\" title=\"\">2020</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>]</cite>, substantially improving research efficiency. Besides, some works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Starace et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib28\" title=\"\">2025</a>], Xiang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib36\" title=\"\">2025</a>]</cite> benchmark AI agents&#8217; end-to-end ability to replicate top-performing ML papers, while others leverage agents to enable idea proposal&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shojaee et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib27\" title=\"\">2025</a>]</cite> and data-driven scientific inspiration&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib7\" title=\"\">2024c</a>], Mitchener et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib21\" title=\"\">2025</a>]</cite>.\nTo further enhance productivity, a growing number of work focuses on the automatic visual design of figures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib33\" title=\"\">2023</a>]</cite>, slides&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and charts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib13\" title=\"\">2024</a>]</cite>. More recently, Paper2Agent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite> has reimagined research papers as interactive and reliable AI agents, designed to assist readers in understanding scientific works. However, very few studies have investigated video generation for scientific purposes, leaving this area relatively underexplored. Our work belongs to one of the pioneering efforts in this direction, initiating a systematic study on academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "pang",
                    "zheng",
                    "video",
                    "slides",
                    "shi",
                    "works",
                    "research",
                    "generation",
                    "text",
                    "agents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a research paper and the author&#8217;s identity information, our goal is to automatically synthesize an academic presentation video that faithfully conveys the paper&#8217;s core contributions in an audience-friendly manner.\nWe identify that a perfect presentation video is usually required to integrate four coordinated components:\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">i</span>) slides</span> contain well-organized, visually oriented, expressive figures and tables with concise text description;\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">ii</span>) synchronized subtitles and speech</span> are semantically aligned with the slides, including supplementary details;\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">iii</span>) presenter</span> should exhibit natural yet professional facial expressions, ideally accompanied by appropriate gestures;\nand <span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">iv</span>) a cursor indicator</span> serves as an attentional anchor, helping the audience focus and follow the narration.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "natural",
                    "video",
                    "slides",
                    "research",
                    "paper",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task poses several distinctive challenges:\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">a<span class=\"ltx_text ltx_font_upright\">. Multi-modal Long-Context Understanding.</span></span> Research papers span many pages with dense text, equations, figures, and tables.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">b<span class=\"ltx_text ltx_font_upright\">. Multi-turn Agent Tasks.</span></span> It is challenging to solve this task with a single end-to-end model, as it requires multi-channel generation and alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, slides, cursors, and presenter).\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">c<span class=\"ltx_text ltx_font_upright\">. Personalized Presenter Synthesis.</span></span>\nAchieving high-quality, identity-preserving, and lip-synchronous talking-head video remains time-consuming, and even more challenging when jointly modeling voice, face, and gesture.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">d<span class=\"ltx_text ltx_font_upright\">. Spatial-Temporal-Grounding.</span></span>\nProducing cursor trajectories synchronized with narration and slide content demands precise alignment between linguistic units and visual anchors.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "multimodal",
                    "cursor",
                    "agent",
                    "video",
                    "slides",
                    "face",
                    "voice",
                    "generation",
                    "research",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Source.</span> We use AI conference papers as the data source for two reasons: (i) they offer high-quality, diverse content across subfields with rich text, figures, and tables; and (ii) the field&#8217;s rapid growth and open-sharing culture provide plentiful, polished author-recorded presentations and slides on YouTube and SlidesLive. However, complete metadata are often unavailable (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, presentation videos, slides, presenter images, and voice samples). We thus manually select papers with relatively complete metadata and supplement missing fields by sourcing presenter images from authors&#8217; websites. Overall, we curate 101 peer-reviewed conference papers from the past three years: 41 from machine learning (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, NeurIPS, ICLR, ICML), 40 from computer vision (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, CVPR, ICCV, ECCV), and 20 from natural language processing(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, ACL, EMNLP, NAACL).\nEach instance includes the paper&#8217;s full <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project and a matched, author-recorded presentation video comprising the slide and talking-head streams with speaker identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, portrait and voice sample). For 40% of the data, we additionally collect the original slide files (PDF), enabling direct, reference-based evaluation of slide generation.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "natural",
                    "video",
                    "slides",
                    "voice",
                    "speaker",
                    "generation",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Statistics.</span>\nOverall, Paper2Video covers 101 paper-video pairs spanning diverse topics as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S3.F2\" title=\"Figure 2 &#8227; 3.2 Data Curation &#8227; 3 Paper2Video Benchmark &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (a), ensuring broad coverage across fields. The paper contains <math alttext=\"13.3K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>13.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">13.3K</annotation></semantics></math> words(<math alttext=\"3.3K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>3.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">3.3K</annotation></semantics></math> tokens), 44.7 figures, and 28.7 pages on average, serving as multi-modal long document inputs. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S3.F2\" title=\"Figure 2 &#8227; 3.2 Data Curation &#8227; 3 Paper2Video Benchmark &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (b) and (c), we also report the distributions of slides per presentation and video durations in Paper2Video. On average, presentations contain 16 slides and last 6min&#160;15s, with some samples reaching up to 14 minutes. Although Paper2Video comprises 101 curated presentations, the benchmark is designed to evaluate long-horizon agentic tasks rather than mere video generation.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "inputs",
                    "video",
                    "slides",
                    "generation",
                    "long",
                    "paper2video",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">(b)&#160;Slides per video</p>\n\n",
                "matched_terms": [
                    "video",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike natural video generation, academic presentation videos serve a highly specialized role: they are not merely about visual fidelity but about communicating scholarship. This makes it difficult to directly apply conventional metrics from video synthesis (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, FVD, IS, or CLIP-based similarity). Instead, their value lies in how well they <span class=\"ltx_text ltx_font_italic\">disseminate research, amplify scholarly visibility.</span></p>\n\n",
                "matched_terms": [
                    "generation",
                    "research",
                    "natural",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity</span> <span class=\"ltx_text ltx_font_italic\">&#8211; How video like human-made?</span>\nAs we have the ground-truth human-made presentation videos with original slides, we evaluate how well the generated intermediate assets (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;speech, slides, and subtitles) aligned with the ones created by authors, which serves as the pseudo ground-truth.\n(i) For each slide, we pair the slide image with its corresponding subtitles and submit both the generated pair and the human-made pair to the VLMs to obtain a similarity score on a five-point scale.\n(ii) To further assess speech(<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;vocal timbre), we uniformly sample a ten-second segment from the presentation audio, encode the generated and human-recorded audio with a speaking embedding model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite>, and compute the cosine similarity between the embeddings to measure speech similarity.</p>\n\n",
                "matched_terms": [
                    "video",
                    "slide",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz</span> <span class=\"ltx_text ltx_font_italic\">&#8211; How videos conveys the paper knowledge?</span> Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we evaluate information coverage using a multiple-choice quiz on the presentation video. We first generate a set of questions with four options and the corresponding correct answers from the source paper. Then we ask the VideoLLMs to watch the presentation and answer each question. Overall accuracy serves as the metric, with higher accuracy indicating better information coverage.</p>\n\n",
                "matched_terms": [
                    "video",
                    "paper",
                    "pang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to ablate the contribution of each component, we evaluate both the quality and the gains provided by individual components (<span class=\"ltx_text ltx_font_italic\">e.g.,</span>&#160;slides, cursor, and presenter). Notably, to further assess presentation videos from the user perspective, we conduct human studies to evaluate the results.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overview.</span>\nTo address these challenges and liberate researchers from the burdensome task of manual video preparation, we introduce PaperTalker, a multi-agent framework designed to automatically generate presentation videos directly from academic papers.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F4\" title=\"Figure 4 &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, to decouple the different roles, making the method scalable and flexible, the pipeline comprises four builders:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Slide builder. Given the paper, we first synthesize slides with <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#160;code and refine them with compilation feedback to correct grammar and optimize layout;\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitle builder. The slides are then processed by a VLM to generate subtitles and sentence-level visual-focus prompts; <span class=\"ltx_text ltx_font_bold\">(iii)</span> Cursor builder. These prompts are then grounded into on-screen cursor coordinates and synchronized with the narration.\n<span class=\"ltx_text ltx_font_bold\">(iv)</span> Talker builder. Given the voice sample and the portrait of the speaker, text-to-speech and talking-head modules generate a realistic, personalized talker video.\nFor clarity, we denote the paper document, author portrait, and voice sample as <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>, <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "cursor",
                    "video",
                    "voice",
                    "slides",
                    "speaker",
                    "slide",
                    "paper",
                    "subtitle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A prerequisite for producing a presentation video is the creation of the slides. Despite there being some existing works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we target the generation of academic slides with fine-grained layouts and formal structure from scratch.\nRather than selecting a template and iteratively editing it with VLMs, we generate slides directly from a paper&#8217;s <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project by prompting the model to write Beamer code. We adopt Beamer for three reasons: <span class=\"ltx_text ltx_font_bold\">(i)</span> <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#8217;s declarative typesetting automatically arranges text block and figures from their parameters without explicitly planing the positions; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Beamer is compact and expressive, representing the same content in fewer lines than XML-based formats; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> Beamer provides well-designed, formally configured styles (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, page numbers, section headers, hyperlinks) that are well suited to academic slide design.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "zheng",
                    "video",
                    "slides",
                    "works",
                    "generation",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the paper <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> as input, the LLM first produces a draft slide code. We compile this code to collect diagnostics(<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;errors and warnings). Then, we use the error information to elicit a repaired correct code.\nThis procedure ensures that the generated Beamer code is grammatically correct and effectively leverages and faithfully covers its content.</p>\n\n",
                "matched_terms": [
                    "paper",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> can automatically arrange the location of the contents in the slides, the generated slides could sometimes still suffer from inappropriate layouts (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, overflow) due to the unsuitable parameters for figure or text font size. However, as the compilation warning signals potential layout issues, we are able to first use them to identify the slides that require refinement.</p>\n\n",
                "matched_terms": [
                    "text",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice.</span> After localizing the slides that require refinement, the key challenge is how to adjust their layouts effectively. As LLMs/VLMs fail to perceive real-time visual feedback like human designers, we observe that prompting the them to directly tune numeric layout parameters (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, font sizes, margins, figure scales) is ineffective: the models are largely <span class=\"ltx_text ltx_font_bold\">insensitive</span> to small numeric changes, yielding unstable and inefficient refinement, consistent with limitations of the parameter-editing strategy in PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>. To address this limitation, we introduce a <em class=\"ltx_emph ltx_font_italic\">visual-selection</em> module for overflowed slides. The module first constructs the neighborhoods of layout variants for the current slide by rule-based adjusting the figure and text parameters, renders each variant to an image, and then uses the VLMs as a judge to score the candidates and select the one with the best layout. Specifically, for text-only slides, we sweep the font size; for slides with figures, we first vary the figure scaling factors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 1.25, 0.75, 0.5, 0.25) and then reduce the font size, details shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F5\" title=\"Figure 5 &#8227; 4.1 Slide Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. These edits are straightforward in <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> Beamer, whose structured syntax automatically reflows content as parameter changes. This module <span class=\"ltx_text ltx_font_bold\">decouples discrete layout search from semantic reasoning</span> and reliably resolves overflow cases with minimal time and tokens.</p>\n\n",
                "matched_terms": [
                    "zheng",
                    "pptagent",
                    "slides",
                    "slide",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After fixing the errors and adjusting the parameters, we compile the slide code to obtain the finalized slides <math alttext=\"\\mathcal{S}_{i},i=1,\\dots,n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo>,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i},i=1,\\dots,n</annotation></semantics></math> with fine-grained layouts, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> indicates the number of slides.</p>\n\n",
                "matched_terms": [
                    "slide",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the speech should follow the slides, given the generated slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, we rasterize them into images and pass them to a VLM, which produces sentence-level subtitles <math alttext=\"T_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">T_{i}^{j}</annotation></semantics></math> and its corresponding visual-focus prompt <math alttext=\"P_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>P</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">P_{i}^{j}</annotation></semantics></math>. The visual-focus prompt serves as an intermediate representation linking speech to the cursor, enabling precise temporal and spatial alignment of the cursor with the narration in order to improve audience guidance, which will be discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.SS4\" title=\"4.4 Cursor Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "slide",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The presenter video is vital for audience engagement and conveying the researcher&#8217;s scholarly identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, face and voice). Given the subtitles <math alttext=\"\\mathcal{T}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}</annotation></semantics></math>, the author&#8217;s portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and a short voice sample <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, our objective is to synthesize a presenter video that delivers the slide content in the author&#8217;s voice, with faithful identity preservation and lip&#8211;audio synchronization.</p>\n\n",
                "matched_terms": [
                    "video",
                    "voice",
                    "face",
                    "short",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subtitle-to-Speech.</span> Given subtitles and voice sample, we use F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib6\" title=\"\">2024b</a>]</cite> to generate speech audio per slide, <math alttext=\"\\widetilde{\\mathcal{A}}_{i}=\\operatorname{TTS}\\!\\left(\\{\\,T_{i}^{j}\\,\\}_{j=1}^{m_{i}}|\\mathcal{A}\\right),i=1,\\ldots,n,\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><mpadded style=\"width:1.947em;\" width=\"1.947em\"><mi>TTS</mi></mpadded><mo>&#8289;</mo><mrow><mo>(</mo><mrow><msubsup><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><msubsup><mi>T</mi><mi>i</mi><mi>j</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>i</mi></msub></msubsup><mo fence=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\mathcal{A}}_{i}=\\operatorname{TTS}\\!\\left(\\{\\,T_{i}^{j}\\,\\}_{j=1}^{m_{i}}|\\mathcal{A}\\right),i=1,\\ldots,n,</annotation></semantics></math>\n&#160;where <math alttext=\"m_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">m_{i}</annotation></semantics></math> is the number of the sentences in <math alttext=\"\\mathcal{T}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "slide",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel Talkinghead Generation.</span>\nTo balance fidelity and efficiency, we use Hallo2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite> for head-only synthesis and employ FantasyTalking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> to support talking generation with upper-body articulation.\nA persistent challenge is the long generation time: generating only a few minutes of talking-head video typically takes several hours, and some models(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, FantasyTalking) do not yet natively support long-video generation.\nInspired by the common practice of slide-by-slide recording and the independence between each slide, we synthesize the presenter video on a per-slide basis. Specifically, for each slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, given the audio condition <math alttext=\"\\widetilde{\\mathcal{A}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\widetilde{\\mathcal{A}}_{i}</annotation></semantics></math> and portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, we generate an independent clip <math alttext=\"\\mathcal{V}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}</annotation></semantics></math> and execute these jobs in parallel, markedly reducing generation time: <math alttext=\"\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><mo>=</mo><mrow><mpadded style=\"width:0.484em;\" width=\"0.484em\"><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mpadded><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,</annotation></semantics></math>&#160;where <math alttext=\"\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math> represents the talking-head generation model.\nThis design is justified because slide transitions are hard scene changes, and the temporal continuity of the presenter across adjacent slides is unnecessary.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "video",
                    "slides",
                    "generation",
                    "slide",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spatial-Temporal Grounding.</span>\nIn practice, presenters leverage the cursor as an attentional guide: a well-aligned cursor trajectory minimizes extraneous cognitive load, helps the audience track the presentation, and keeps focus on the key content. However, automatic cursor-trajectory grounding is nontrivial, requiring simultaneous alignment to the timing of speech and the visual semantics of the slides. To simplify the task, we assume that the cursor will stay still within a sentence and only move between the sentences. Thus, we estimate a per-sentence cursor location and time span. For spatial alignment, motivated by strong computer-use models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> which simulate user interaction with the screenshot, we propose to ground the cursor location <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> for each sentence with the visual focus prompt <math alttext=\"\\mathcal{P}_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}^{j}</annotation></semantics></math> by UI-TARS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite>. To achieve precise temporal alignment, we then use WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> to extract word-level timestamps and align them with the corresponding sentence in the subtitles to derive the start and end times <math alttext=\"(t_{s},t_{e})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>s</mi></msub><mo>,</mo><msub><mi>t</mi><mi>e</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{s},t_{e})</annotation></semantics></math> of each cursor segment.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of baselines: <span class=\"ltx_text ltx_font_bold\">(i)</span> End-to-end Methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, where natural video generation models produce the presentation video directly from a prompt generated by paper; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Multi-Agent Frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, which combine slide generation with text-to-speech generation and compose them into a presentation video; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> PaperTalker, our method and its variants. For the VLM and VideoLLM, we choose <span class=\"ltx_text ltx_font_italic\">GPT-4.1</span> and <span class=\"ltx_text ltx_font_italic\">Gemini-2.5-Flash</span>, respectively, for a favorable efficiency and performance trade-off. We perform inference using eight NVIDIA RTX A6000 GPUs.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "zheng",
                    "video",
                    "shi",
                    "generation",
                    "slide",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity.</span> We evaluate the alignment of the generated slides, subtitles, and speech with corresponding human-authored ones. For speech, we randomly sample a 10-second audio segment from the video generated by each method and compute the cosine similarity between its embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite> and those of the author&#8217;s speech. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest scores in both speech and content similarity, demonstrating that its outputs <span class=\"ltx_text ltx_font_bold\">align most closely with human creation</span> among all baselines. We attribute this performance to personalized TTS and our slide-generation design: <span class=\"ltx_text ltx_font_bold\">(i)</span> adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span>, which provides formal, academically styled templates while <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> automatically arranges content within each slide; and <span class=\"ltx_text ltx_font_bold\">(ii)</span> a tree search visual choice layout refinement that further enforces fine-grained slide layouts as commonly observed in human-authored slides.</p>\n\n",
                "matched_terms": [
                    "video",
                    "outputs",
                    "slide",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz.</span> To assess information coverage, we conduct a VideoQA evaluation. Following prior work on posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we construct QA sets by prompting an LLM to generate questions targeting (i) fine-grained details and (ii) higher-level understanding of the paper. The videos and QA sets are then fed into a VideoLLM to conduct the quiz.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker achieves superior performance across both aspects, outperforming HumanMade and PresentAgent despite shorter video length. This indicates that PaperTalker produces videos that are <span class=\"ltx_text ltx_font_bold\">more informative within shorter durations</span>. Furthermore, the absence of the talker or cursor results in performance degradation, as the cursor trajectory potentially <span class=\"ltx_text ltx_font_bold\">guides the attention and supports accurate grounding of the key contents</span> for the VideoLLMs during inference, referring to Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span> for more details.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "pang",
                    "video",
                    "paper",
                    "presentagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IP Memory.</span> We evaluate the degree to which the generated presentation videos facilitate audience retention of the work, thereby assessing their memorability and lasting impact. PaperTalker achieves the highest recall accuracy. This improvement mainly stems from the inclusion of an engaging talker with the author&#8217;s figure and voice, which <span class=\"ltx_text ltx_font_bold\">significantly helps the audience retain the video content</span>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency Analysis.</span> As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Qualitative Analysis &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, PaperTalker achieves the lowest cost. This efficiency stems from our slide-generation design: adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span> <span class=\"ltx_text ltx_font_bold\">reduces token usage</span> for slide creation, and our tree search visual choice module which is a <span class=\"ltx_text ltx_font_bold\">lightweight post-processing step</span>. Runtime is further reduced by our parallel talking-head generation mechanism. By contrast, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> incurs higher token costs due to frequent refinement queries during slide editing.</p>\n\n",
                "matched_terms": [
                    "talkinghead",
                    "shi",
                    "generation",
                    "slide",
                    "presentagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F7\" title=\"Figure 7 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, PaperTalker produces presentation videos that <span class=\"ltx_text ltx_font_bold\">most closely align with the human-made</span> ones. While Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite> renders a high-quality speaker in front of the screen, it is constrained by short duration (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 8s) and blurred text. Besides, PresentAgent<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> typically suffers from the absence of the presenter and slide-design errors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, overflow, incorrect title, incomplete author lists, and institutions).</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "short",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">What benefits are brought by Cursor Highlight?</span>\nMotivated by the observation that a cursor typically helps audiences locate the relevant region, we hypothesize that a visible cursor, by providing an explicit spatial cue, facilitates content grounding for VLMs. To evaluate this, we design a localization QA task: for each subtitle sentence and its corresponding slide, a VLM generates a four-option multiple-choice question about the sentence&#8217;s corresponding position on the slide. The VLMs are then prompted to answer using slide screenshots, with or without the cursor, and accuracy is measured as the metric. As shown in Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span>, the accuracy is much higher with the cursor highlight, corroborating its <span class=\"ltx_text ltx_font_bold\">importance for the audience&#8217;s visual grounding accessibility</span> of presentation videos.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "subtitle",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">How does tree search visual choice improve slide quality?</span> To assess the contribution of the tree-search visual choice module, we conduct an ablation experiment, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T5\" title=\"Table 5 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In line with prior work on slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we assess the generated slides using a VLM on a 1&#8211;5 scale across content, design, and coherence. The results show a pronounced decline in design quality when layout refinement is removed, highlighting <span class=\"ltx_text ltx_font_bold\">the tree-search visual choice module as a key component for slide creation</span> (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;resolving overfull issues), referring to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#A2.F8\" title=\"Figure 8 &#8227; B.2 Results of Tree Search Visual Choice &#8227; Appendix B Experiment &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for visualization.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "slide",
                    "zheng",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work tackles the long-standing bottleneck of presentation video generation by agent automation.\nWith Paper2Video, we provide the first comprehensive benchmark and well-designed metrics to rigorously evaluate presentation videos in terms of quality, knowledge coverage, and academic memorability.\nOur proposed PaperTalker&#160;framework demonstrates that automated generation of ready-to-use academic presentation videos is both feasible and effective, producing outputs that closely approximate author-recorded presentations while significantly reducing production time by 6 times.\nWe hope our work advances AI for Research and supports scalable scholarly communication.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "agent",
                    "video",
                    "generation",
                    "research",
                    "paper2video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To operationalize this, we construct video&#8211;question pairs by sampling a five-second clip from each presentation video and selecting a corresponding understanding-level question from PresentQuiz. A VideoLLM serves as the audience proxy: it is presented with four randomly sampled video&#8211;question pairs, where the videos and questions are shuffled, together with an image of one speaker as the query. The model is then asked to identify the relevant question to pose, and the accuracy quantifies the IP Memory score. Higher recall accuracy indicates that the generated results are more impressive and hold greater potential for lasting impact.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#A2.F8\" title=\"Figure 8 &#8227; B.2 Results of Tree Search Visual Choice &#8227; Appendix B Experiment &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the slides before and after applying tree search visual choice refinement. The refinement <span class=\"ltx_text ltx_font_bold\">resolves the overfull issues</span> and substantially improves slide quality, indicating that this module <span class=\"ltx_text ltx_font_bold\">plays a crucial role in layout adjustment</span>.</p>\n\n",
                "matched_terms": [
                    "slide",
                    "slides"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Detailed evaluation result of Paper2Video across three baselines. PaperTalkâ\\text{PaperTalk}^{*} represents a simple version without presenter and cursor. Bold and Underline indicates the best and the second. NA means the results of method are not applicable to the metric.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Similarity<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Arena<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PresentQuiz Acc.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">IP Memory<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Avg. Duration(s)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Content</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Detail</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Under.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HumanMade</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">5.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.738</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.908</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">375.15</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">Wan2.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>]</cite></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">NA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">NA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.1%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.251</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.551</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">11.5%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">4.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite></span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.133</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">NA</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.2%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.367</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.585</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">31.3%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">8.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PresentAgent<sub class=\"ltx_sub\">QWEN</sub>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PresentAgent<sub class=\"ltx_sub\">GPT4.1</sub>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.045</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.0%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.548</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.654</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">12.5%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">430.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PaperTalk<sub class=\"ltx_sub\">QWEN</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\text{PaperTalk\\textsubscript{GPT4.1}}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><msup><mrow><mtext>PaperTalk</mtext><mtext><sub class=\"ltx_sub\">GPT4.1</sub></mtext></mrow><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\text{PaperTalk\\textsubscript{GPT4.1}}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.646</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.2%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.835</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.949</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">37.5%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">234.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PaperTalk<sub class=\"ltx_sub\">GPT4.1</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.646</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">17.0</span>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.842</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.951</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">234.36</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "presentquiz",
            "applicable",
            "second",
            "content",
            "shi",
            "baselines",
            "deepmind",
            "evaluation",
            "similarityâuparrow",
            "paper2video",
            "not",
            "durations",
            "underline",
            "memoryâuparrow",
            "arenaâuparrow",
            "veo3",
            "presentagentqwen",
            "papertalkgpt41âtextpapertalktextsubscriptgpt41",
            "represents",
            "detail",
            "humanmade",
            "papertalkâtextpapertalk",
            "across",
            "avg",
            "cursor",
            "without",
            "result",
            "bold",
            "wan22",
            "wan",
            "simple",
            "under",
            "results",
            "indicates",
            "speech",
            "metric",
            "version",
            "means",
            "presenter",
            "accâuparrow",
            "three",
            "papertalkqwen",
            "presentagentgpt41",
            "best",
            "papertalkgpt41",
            "method",
            "detailed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity.</span> We evaluate the alignment of the generated slides, subtitles, and speech with corresponding human-authored ones. For speech, we randomly sample a 10-second audio segment from the video generated by each method and compute the cosine similarity between its embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite> and those of the author&#8217;s speech. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest scores in both speech and content similarity, demonstrating that its outputs <span class=\"ltx_text ltx_font_bold\">align most closely with human creation</span> among all baselines. We attribute this performance to personalized TTS and our slide-generation design: <span class=\"ltx_text ltx_font_bold\">(i)</span> adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span>, which provides formal, academically styled templates while <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> automatically arranges content within each slide; and <span class=\"ltx_text ltx_font_bold\">(ii)</span> a tree search visual choice layout refinement that further enforces fine-grained slide layouts as commonly observed in human-authored slides.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentArena.</span> We compare the presentation videos generated by each method against the human-made videos. As an automatic evaluator, we prompt the VideoLLMs as a judge to determine which presentation is better with respect to clarity, delivery, and engagement. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest pairwise winning rate among all baselines, indicating that our method produces presentation videos with <span class=\"ltx_text ltx_font_bold\">superior overall perceived quality</span>. Notably, PaperTalker outperforms its variants without the talker and cursor by 1.8%, highlighting the gains introduced by these components and implying that the VideoLLM <span class=\"ltx_text ltx_font_bold\">favors presentation videos with a talker presenting</span>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz.</span> To assess information coverage, we conduct a VideoQA evaluation. Following prior work on posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we construct QA sets by prompting an LLM to generate questions targeting (i) fine-grained details and (ii) higher-level understanding of the paper. The videos and QA sets are then fed into a VideoLLM to conduct the quiz.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker achieves superior performance across both aspects, outperforming HumanMade and PresentAgent despite shorter video length. This indicates that PaperTalker produces videos that are <span class=\"ltx_text ltx_font_bold\">more informative within shorter durations</span>. Furthermore, the absence of the talker or cursor results in performance degradation, as the cursor trajectory potentially <span class=\"ltx_text ltx_font_bold\">guides the attention and supports accurate grounding of the key contents</span> for the VideoLLMs during inference, referring to Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span> for more details.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video.\nUnlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\">Paper2Video</span>, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics&#8212;Meta Similarity, PresentArena, <span class=\"ltx_text ltx_font_italic\">PresentQuiz</span>, and <span class=\"ltx_text ltx_font_italic\">IP Memory</span>&#8212;to measure how videos convey the paper&#8217;s information to the audience and affect the work impact. Building on this foundation, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective <span class=\"ltx_text ltx_font_italic\">Tree Search Visual Choice</span>, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video&#160;demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/showlab/Paper2Video\" title=\"\">https://github.com/showlab/Paper2Video</a>.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "cursor",
                    "baselines",
                    "evaluation",
                    "paper2video",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike natural video generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Zhang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib37\" title=\"\">2025</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, presentation video exhibits distinctive characteristics, including multi-sensory integration, multi-figure conditioning, and high text density, which highlight the limitations of current natural video generation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>. Specifically, academic presentation video generation faces several crucial challenges:\n<span class=\"ltx_text ltx_font_italic\">a.</span> It originates from long-context papers that contain dense text as well as multiple figures and tables;\n<span class=\"ltx_text ltx_font_italic\">b.</span> It requires the coordination of multiple aligned channels, including slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, subtitling, text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib6\" title=\"\">2024b</a>]</cite>, cursor control, and talking head generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>], Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite>;\n<span class=\"ltx_text ltx_font_italic\">c.</span> It lacks well-defined evaluation metrics: what constitutes a good presentation video, particularly in terms of knowledge conveyance and audience accessibility. Even for the state-of-the-art end-to-end video&#8211;audio generation model Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, notable limitations remain in video length, clarity of dense on-screen text, and multi-modal long-document condition.\nIn this work, we try to solve these two core problems as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "veo3",
                    "wan",
                    "deepmind",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable comprehensive evaluation of academic presentation video generation, we present the <span class=\"ltx_text ltx_font_bold\">Paper2Video</span> Benchmark, comprising <math alttext=\"101\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>101</mn><annotation encoding=\"application/x-tex\">101</annotation></semantics></math> paired research papers and author-recorded presentation videos from recent conferences, together with original slides and speaker identity metadata. Based on this benchmark, we develop a suite of metrics to comprehensively evaluate generation quality from multiple dimensions:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Meta Similarity &#8212; We employ a VLM to evaluate the alignment of generated slides and subtitles with human-designed counterparts.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> PresentArena &#8212; We use a VideoLLM as a proxy audience to perform double-order pairwise comparisons between generated and human-made videos.\nNotably, the primary purpose of a presentation is to <span class=\"ltx_text ltx_font_italic\">effectively convey the information contained in the paper</span>. To this end, we introduce <span class=\"ltx_text ltx_font_bold\">(iii) PresentQuiz</span>, which treats the VideoLLMs as the audience and requires them to answer paper-derived questions given the videos. Furthermore, another important purpose of presentation video is to <span class=\"ltx_text ltx_font_italic\">enhance the visibility and impact of the author&#8217;s work</span>. Motivated by real-conference interactions, we introduce <span class=\"ltx_text ltx_font_bold\">(iv) IP Memory</span>, which measures how well an audience can associate authors and works after watching presentation videos.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "evaluation",
                    "humanmade",
                    "paper2video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively generate ready-to-use academic presentation videos, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework that enables academic presentation video generation from research papers and speaker identity. It integrates subsequent key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> Slide Generation. Instead of adopting the commonly used format (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, pptx, XML) from a template slide as in &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we employ LaTeX code for slide generation from sketch, given its formal suitability for academic use and higher efficiency. Specifically, we employ a state-of-the-art Coder to generate code and introduce an effective <span class=\"ltx_text ltx_font_bold\">focused debugging</span> strategy, which iteratively narrows the scope and resolves compilation errors using feedback that indicates the relevant rows.\nTo address the insensitivity of LLMs to fine-grained numerical adjustments, we propose a novel method called <span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice</span>. This approach systematically explores parameter variations to generate multiple branches, which are then concatenated into a single figure. A VLM is then tasked with selecting the optimal branch, thereby effectively improving element layouts such as figure and font size.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitling and Cursor Grounding. We generate subtitles and cursor prompts for each sentence based on the slides. Then we achieve cursor spatial-temporal alignment using <span class=\"ltx_text ltx_font_bold\">Computer-use grounding model</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> models and WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> respectively. <span class=\"ltx_text ltx_font_bold\">(iii)</span> Speech Synthesis and Talking-head Rendering. We synthesize personalized speech via text-to-speech models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib5\" title=\"\">2024a</a>]</cite> and produce talking-head videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> for author presentations. Inspired by human recording practice and the independence between each slide, we <span class=\"ltx_text ltx_font_bold\">parallelize generation</span> across slides, achieving a speedup of more than <math alttext=\"\\mathbf{6\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>&#120788;</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6\\times}</annotation></semantics></math>.\nWe will open-source all our data and codebase to empower the research community.\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "cursor",
                    "method",
                    "indicates",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Paper2Video, the first high-quality benchmark of <math alttext=\"101\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>101</mn><annotation encoding=\"application/x-tex\">101</annotation></semantics></math> papers with author-recorded presentation videos, slides, and speaker metadata, together with evaluation metrics: Meta Similarity, PresentArena, PresentQuiz, and IP Memory.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "evaluation",
                    "paper2video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose PaperTalker, the first multi-agent framework for academic presentation video generation. It introduces three key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> tree search visual choice for fine-grained slide generation; <span class=\"ltx_text ltx_font_bold\">(ii)</span> a GUI-grounding model coupled with WhisperX for spatial-temporal aligned cursor grounding; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> slide-wise parallel generation to improve efficiency.</p>\n\n",
                "matched_terms": [
                    "three",
                    "cursor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on Paper2Video confirm the effectiveness of PaperTalker, which outperforms human-made presentations by 10% in PresentQuiz accuracy and achieves comparable ratings in user studies, indicating that its quality approaches that of human-created content.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "content",
                    "paper2video",
                    "results",
                    "humanmade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in video diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib14\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib15\" title=\"\">b</a>]</cite> have substantially improved <span class=\"ltx_text ltx_font_italic\">natural</span> video generation in terms of length, quality, and controllability. However, these <span class=\"ltx_text ltx_font_bold\">end-to-end</span> diffusion models still struggle to produce long videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, several minutes), handle multiple shots, and support conditioning on multiple images&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>.\nMoreover, most existing approaches generate only video without aligned audio, leaving a gap for real-world applications. To address these limitations, recent works leverage <span class=\"ltx_text ltx_font_bold\">multi-agent</span> collaboration to generate multi-shot, long video&#8211;audio pairs and enable multi-image conditioning.\nSpecifically, for natural videos, MovieAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib35\" title=\"\">2025b</a>]</cite> adopts a hierarchical CoT planning strategy and leverages LLMs to simulate the roles of a director, screenwriter, storyboard artist, and location manager, thereby enabling long-form movie generation. Alternatively, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> targets presentation video generation but merely combines PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite> with text-to-speech to produce narrated slides.\nHowever, it lacks personalization (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, mechanical speech and absence of a presenter) and fails to generate academic-style slides (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, missing opening and outline slides), thereby limiting its applicability in academic contexts. Our work addresses these limitations and enables ready-to-use academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "without",
                    "presenter",
                    "wan",
                    "shi",
                    "deepmind",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many useful tasks have been explored under the umbrella of AI for Research (AI4Research)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>]</cite>, which aims to support the full scholarly workflow spanning text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dasigi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib9\" title=\"\">2021</a>]</cite>, static visuals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and dynamic video&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>. With the breakthrough of LLMs in text generation and the Internet search ability, extensive efforts have been devoted to academic writing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>]</cite> and literature surveying&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Katz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib16\" title=\"\">2024</a>], DeYoung et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib11\" title=\"\">2021</a>], Lu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib18\" title=\"\">2020</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>]</cite>, substantially improving research efficiency. Besides, some works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Starace et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib28\" title=\"\">2025</a>], Xiang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib36\" title=\"\">2025</a>]</cite> benchmark AI agents&#8217; end-to-end ability to replicate top-performing ML papers, while others leverage agents to enable idea proposal&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shojaee et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib27\" title=\"\">2025</a>]</cite> and data-driven scientific inspiration&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib7\" title=\"\">2024c</a>], Mitchener et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib21\" title=\"\">2025</a>]</cite>.\nTo further enhance productivity, a growing number of work focuses on the automatic visual design of figures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib33\" title=\"\">2023</a>]</cite>, slides&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and charts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib13\" title=\"\">2024</a>]</cite>. More recently, Paper2Agent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite> has reimagined research papers as interactive and reliable AI agents, designed to assist readers in understanding scientific works. However, very few studies have investigated video generation for scientific purposes, leaving this area relatively underexplored. Our work belongs to one of the pioneering efforts in this direction, initiating a systematic study on academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "under",
                    "shi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a research paper and the author&#8217;s identity information, our goal is to automatically synthesize an academic presentation video that faithfully conveys the paper&#8217;s core contributions in an audience-friendly manner.\nWe identify that a perfect presentation video is usually required to integrate four coordinated components:\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">i</span>) slides</span> contain well-organized, visually oriented, expressive figures and tables with concise text description;\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">ii</span>) synchronized subtitles and speech</span> are semantically aligned with the slides, including supplementary details;\n<span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">iii</span>) presenter</span> should exhibit natural yet professional facial expressions, ideally accompanied by appropriate gestures;\nand <span class=\"ltx_text ltx_font_bold\">(<span class=\"ltx_text ltx_font_italic\">iv</span>) a cursor indicator</span> serves as an attentional anchor, helping the audience focus and follow the narration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "cursor",
                    "presenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task poses several distinctive challenges:\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">a<span class=\"ltx_text ltx_font_upright\">. Multi-modal Long-Context Understanding.</span></span> Research papers span many pages with dense text, equations, figures, and tables.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">b<span class=\"ltx_text ltx_font_upright\">. Multi-turn Agent Tasks.</span></span> It is challenging to solve this task with a single end-to-end model, as it requires multi-channel generation and alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, slides, cursors, and presenter).\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">c<span class=\"ltx_text ltx_font_upright\">. Personalized Presenter Synthesis.</span></span>\nAchieving high-quality, identity-preserving, and lip-synchronous talking-head video remains time-consuming, and even more challenging when jointly modeling voice, face, and gesture.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">d<span class=\"ltx_text ltx_font_upright\">. Spatial-Temporal-Grounding.</span></span>\nProducing cursor trajectories synchronized with narration and slide content demands precise alignment between linguistic units and visual anchors.</p>\n\n",
                "matched_terms": [
                    "content",
                    "cursor",
                    "presenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Source.</span> We use AI conference papers as the data source for two reasons: (i) they offer high-quality, diverse content across subfields with rich text, figures, and tables; and (ii) the field&#8217;s rapid growth and open-sharing culture provide plentiful, polished author-recorded presentations and slides on YouTube and SlidesLive. However, complete metadata are often unavailable (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, presentation videos, slides, presenter images, and voice samples). We thus manually select papers with relatively complete metadata and supplement missing fields by sourcing presenter images from authors&#8217; websites. Overall, we curate 101 peer-reviewed conference papers from the past three years: 41 from machine learning (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, NeurIPS, ICLR, ICML), 40 from computer vision (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, CVPR, ICCV, ECCV), and 20 from natural language processing(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, ACL, EMNLP, NAACL).\nEach instance includes the paper&#8217;s full <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project and a matched, author-recorded presentation video comprising the slide and talking-head streams with speaker identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, portrait and voice sample). For 40% of the data, we additionally collect the original slide files (PDF), enabling direct, reference-based evaluation of slide generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "content",
                    "presenter",
                    "three",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Statistics.</span>\nOverall, Paper2Video covers 101 paper-video pairs spanning diverse topics as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S3.F2\" title=\"Figure 2 &#8227; 3.2 Data Curation &#8227; 3 Paper2Video Benchmark &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (a), ensuring broad coverage across fields. The paper contains <math alttext=\"13.3K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>13.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">13.3K</annotation></semantics></math> words(<math alttext=\"3.3K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>3.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">3.3K</annotation></semantics></math> tokens), 44.7 figures, and 28.7 pages on average, serving as multi-modal long document inputs. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S3.F2\" title=\"Figure 2 &#8227; 3.2 Data Curation &#8227; 3 Paper2Video Benchmark &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (b) and (c), we also report the distributions of slides per presentation and video durations in Paper2Video. On average, presentations contain 16 slides and last 6min&#160;15s, with some samples reaching up to 14 minutes. Although Paper2Video comprises 101 curated presentations, the benchmark is designed to evaluate long-horizon agentic tasks rather than mere video generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "paper2video",
                    "durations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity</span> <span class=\"ltx_text ltx_font_italic\">&#8211; How video like human-made?</span>\nAs we have the ground-truth human-made presentation videos with original slides, we evaluate how well the generated intermediate assets (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;speech, slides, and subtitles) aligned with the ones created by authors, which serves as the pseudo ground-truth.\n(i) For each slide, we pair the slide image with its corresponding subtitles and submit both the generated pair and the human-made pair to the VLMs to obtain a similarity score on a five-point scale.\n(ii) To further assess speech(<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;vocal timbre), we uniformly sample a ten-second segment from the presentation audio, encode the generated and human-recorded audio with a speaking embedding model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite>, and compute the cosine similarity between the embeddings to measure speech similarity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "humanmade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz</span> <span class=\"ltx_text ltx_font_italic\">&#8211; How videos conveys the paper knowledge?</span> Following prior work <cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we evaluate information coverage using a multiple-choice quiz on the presentation video. We first generate a set of questions with four options and the corresponding correct answers from the source paper. Then we ask the VideoLLMs to watch the presentation and answer each question. Overall accuracy serves as the metric, with higher accuracy indicating better information coverage.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to ablate the contribution of each component, we evaluate both the quality and the gains provided by individual components (<span class=\"ltx_text ltx_font_italic\">e.g.,</span>&#160;slides, cursor, and presenter). Notably, to further assess presentation videos from the user perspective, we conduct human studies to evaluate the results.</p>\n\n",
                "matched_terms": [
                    "presenter",
                    "cursor",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overview.</span>\nTo address these challenges and liberate researchers from the burdensome task of manual video preparation, we introduce PaperTalker, a multi-agent framework designed to automatically generate presentation videos directly from academic papers.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F4\" title=\"Figure 4 &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, to decouple the different roles, making the method scalable and flexible, the pipeline comprises four builders:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Slide builder. Given the paper, we first synthesize slides with <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#160;code and refine them with compilation feedback to correct grammar and optimize layout;\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitle builder. The slides are then processed by a VLM to generate subtitles and sentence-level visual-focus prompts; <span class=\"ltx_text ltx_font_bold\">(iii)</span> Cursor builder. These prompts are then grounded into on-screen cursor coordinates and synchronized with the narration.\n<span class=\"ltx_text ltx_font_bold\">(iv)</span> Talker builder. Given the voice sample and the portrait of the speaker, text-to-speech and talking-head modules generate a realistic, personalized talker video.\nFor clarity, we denote the paper document, author portrait, and voice sample as <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>, <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "cursor",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A prerequisite for producing a presentation video is the creation of the slides. Despite there being some existing works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we target the generation of academic slides with fine-grained layouts and formal structure from scratch.\nRather than selecting a template and iteratively editing it with VLMs, we generate slides directly from a paper&#8217;s <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project by prompting the model to write Beamer code. We adopt Beamer for three reasons: <span class=\"ltx_text ltx_font_bold\">(i)</span> <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#8217;s declarative typesetting automatically arranges text block and figures from their parameters without explicitly planing the positions; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Beamer is compact and expressive, representing the same content in fewer lines than XML-based formats; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> Beamer provides well-designed, formally configured styles (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, page numbers, section headers, hyperlinks) that are well suited to academic slide design.</p>\n\n",
                "matched_terms": [
                    "content",
                    "three",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice.</span> After localizing the slides that require refinement, the key challenge is how to adjust their layouts effectively. As LLMs/VLMs fail to perceive real-time visual feedback like human designers, we observe that prompting the them to directly tune numeric layout parameters (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, font sizes, margins, figure scales) is ineffective: the models are largely <span class=\"ltx_text ltx_font_bold\">insensitive</span> to small numeric changes, yielding unstable and inefficient refinement, consistent with limitations of the parameter-editing strategy in PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>. To address this limitation, we introduce a <em class=\"ltx_emph ltx_font_italic\">visual-selection</em> module for overflowed slides. The module first constructs the neighborhoods of layout variants for the current slide by rule-based adjusting the figure and text parameters, renders each variant to an image, and then uses the VLMs as a judge to score the candidates and select the one with the best layout. Specifically, for text-only slides, we sweep the font size; for slides with figures, we first vary the figure scaling factors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 1.25, 0.75, 0.5, 0.25) and then reduce the font size, details shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F5\" title=\"Figure 5 &#8227; 4.1 Slide Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. These edits are straightforward in <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> Beamer, whose structured syntax automatically reflows content as parameter changes. This module <span class=\"ltx_text ltx_font_bold\">decouples discrete layout search from semantic reasoning</span> and reliably resolves overflow cases with minimal time and tokens.</p>\n\n",
                "matched_terms": [
                    "content",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the speech should follow the slides, given the generated slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, we rasterize them into images and pass them to a VLM, which produces sentence-level subtitles <math alttext=\"T_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">T_{i}^{j}</annotation></semantics></math> and its corresponding visual-focus prompt <math alttext=\"P_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>P</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">P_{i}^{j}</annotation></semantics></math>. The visual-focus prompt serves as an intermediate representation linking speech to the cursor, enabling precise temporal and spatial alignment of the cursor with the narration in order to improve audience guidance, which will be discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.SS4\" title=\"4.4 Cursor Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "cursor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The presenter video is vital for audience engagement and conveying the researcher&#8217;s scholarly identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, face and voice). Given the subtitles <math alttext=\"\\mathcal{T}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}</annotation></semantics></math>, the author&#8217;s portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and a short voice sample <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, our objective is to synthesize a presenter video that delivers the slide content in the author&#8217;s voice, with faithful identity preservation and lip&#8211;audio synchronization.</p>\n\n",
                "matched_terms": [
                    "content",
                    "presenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel Talkinghead Generation.</span>\nTo balance fidelity and efficiency, we use Hallo2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite> for head-only synthesis and employ FantasyTalking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> to support talking generation with upper-body articulation.\nA persistent challenge is the long generation time: generating only a few minutes of talking-head video typically takes several hours, and some models(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, FantasyTalking) do not yet natively support long-video generation.\nInspired by the common practice of slide-by-slide recording and the independence between each slide, we synthesize the presenter video on a per-slide basis. Specifically, for each slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, given the audio condition <math alttext=\"\\widetilde{\\mathcal{A}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\widetilde{\\mathcal{A}}_{i}</annotation></semantics></math> and portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, we generate an independent clip <math alttext=\"\\mathcal{V}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}</annotation></semantics></math> and execute these jobs in parallel, markedly reducing generation time: <math alttext=\"\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><mo>=</mo><mrow><mpadded style=\"width:0.484em;\" width=\"0.484em\"><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mpadded><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,</annotation></semantics></math>&#160;where <math alttext=\"\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math> represents the talking-head generation model.\nThis design is justified because slide transitions are hard scene changes, and the temporal continuity of the presenter across adjacent slides is unnecessary.</p>\n\n",
                "matched_terms": [
                    "presenter",
                    "across",
                    "represents",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spatial-Temporal Grounding.</span>\nIn practice, presenters leverage the cursor as an attentional guide: a well-aligned cursor trajectory minimizes extraneous cognitive load, helps the audience track the presentation, and keeps focus on the key content. However, automatic cursor-trajectory grounding is nontrivial, requiring simultaneous alignment to the timing of speech and the visual semantics of the slides. To simplify the task, we assume that the cursor will stay still within a sentence and only move between the sentences. Thus, we estimate a per-sentence cursor location and time span. For spatial alignment, motivated by strong computer-use models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> which simulate user interaction with the screenshot, we propose to ground the cursor location <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> for each sentence with the visual focus prompt <math alttext=\"\\mathcal{P}_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}^{j}</annotation></semantics></math> by UI-TARS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite>. To achieve precise temporal alignment, we then use WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> to extract word-level timestamps and align them with the corresponding sentence in the subtitles to derive the start and end times <math alttext=\"(t_{s},t_{e})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>s</mi></msub><mo>,</mo><msub><mi>t</mi><mi>e</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{s},t_{e})</annotation></semantics></math> of each cursor segment.</p>\n\n",
                "matched_terms": [
                    "content",
                    "speech",
                    "cursor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of baselines: <span class=\"ltx_text ltx_font_bold\">(i)</span> End-to-end Methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, where natural video generation models produce the presentation video directly from a prompt generated by paper; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Multi-Agent Frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, which combine slide generation with text-to-speech generation and compose them into a presentation video; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> PaperTalker, our method and its variants. For the VLM and VideoLLM, we choose <span class=\"ltx_text ltx_font_italic\">GPT-4.1</span> and <span class=\"ltx_text ltx_font_italic\">Gemini-2.5-Flash</span>, respectively, for a favorable efficiency and performance trade-off. We perform inference using eight NVIDIA RTX A6000 GPUs.</p>\n\n",
                "matched_terms": [
                    "three",
                    "wan",
                    "baselines",
                    "deepmind",
                    "shi",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Evaluation.</span>\nTo further assess the quality of the generated presentations from the user perspective, we conducted a human evaluation in which ten participants were provided with each paper along with its corresponding presentation videos generated by different methods. Participants were asked to rank the videos according to their preferences(<math alttext=\"1\\text{(worse)}-5\\text{(best)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(worse)</mtext></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(best)</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">1\\text{(worse)}-5\\text{(best)}</annotation></semantics></math>).\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F6\" title=\"Figure 6 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, human-made videos achieve the highest score, with PaperTalker ranking second and outperforming all other baselines. This demonstrates that presentation videos generated by PaperTalker <span class=\"ltx_text ltx_font_bold\">gain consistently favor from human users</span> over other baselines and <span class=\"ltx_text ltx_font_bold\">comparable to human-made</span>.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "evaluation",
                    "humanmade",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F7\" title=\"Figure 7 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, PaperTalker produces presentation videos that <span class=\"ltx_text ltx_font_bold\">most closely align with the human-made</span> ones. While Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite> renders a high-quality speaker in front of the screen, it is constrained by short duration (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 8s) and blurred text. Besides, PresentAgent<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> typically suffers from the absence of the presenter and slide-design errors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, overflow, incorrect title, incomplete author lists, and institutions).</p>\n\n",
                "matched_terms": [
                    "presenter",
                    "deepmind",
                    "veo3",
                    "humanmade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">What benefits are brought by Cursor Highlight?</span>\nMotivated by the observation that a cursor typically helps audiences locate the relevant region, we hypothesize that a visible cursor, by providing an explicit spatial cue, facilitates content grounding for VLMs. To evaluate this, we design a localization QA task: for each subtitle sentence and its corresponding slide, a VLM generates a four-option multiple-choice question about the sentence&#8217;s corresponding position on the slide. The VLMs are then prompted to answer using slide screenshots, with or without the cursor, and accuracy is measured as the metric. As shown in Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span>, the accuracy is much higher with the cursor highlight, corroborating its <span class=\"ltx_text ltx_font_bold\">importance for the audience&#8217;s visual grounding accessibility</span> of presentation videos.</p>\n\n",
                "matched_terms": [
                    "content",
                    "metric",
                    "cursor",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">How does tree search visual choice improve slide quality?</span> To assess the contribution of the tree-search visual choice module, we conduct an ablation experiment, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T5\" title=\"Table 5 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In line with prior work on slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we assess the generated slides using a VLM on a 1&#8211;5 scale across content, design, and coherence. The results show a pronounced decline in design quality when layout refinement is removed, highlighting <span class=\"ltx_text ltx_font_bold\">the tree-search visual choice module as a key component for slide creation</span> (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;resolving overfull issues), referring to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#A2.F8\" title=\"Figure 8 &#8227; B.2 Results of Tree Search Visual Choice &#8227; Appendix B Experiment &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for visualization.</p>\n\n",
                "matched_terms": [
                    "content",
                    "across",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To operationalize this, we construct video&#8211;question pairs by sampling a five-second clip from each presentation video and selecting a corresponding understanding-level question from PresentQuiz. A VideoLLM serves as the audience proxy: it is presented with four randomly sampled video&#8211;question pairs, where the videos and questions are shuffled, together with an image of one speaker as the query. The model is then asked to identify the relevant question to pose, and the accuracy quantifies the IP Memory score. Higher recall accuracy indicates that the generated results are more impressive and hold greater potential for lasting impact.</p>\n\n",
                "matched_terms": [
                    "presentquiz",
                    "results",
                    "indicates"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Generation cost for each method.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Token (K) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Time (min.)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Cost ($)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">NA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Wan2.2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">NA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.280</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">241</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.003</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">PaperTalker (w/o Talker)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.001</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">PaperTalker (w/o Par.)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">287.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.001</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">PaperTalker</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">48.1 <span class=\"ltx_text ltx_font_bold\">(6<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.001</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "time",
            "6Ãtimes",
            "par",
            "token",
            "veo3",
            "wan22",
            "wan",
            "shi",
            "deepmind",
            "generation",
            "presentagent",
            "each",
            "talker",
            "method",
            "cost",
            "âdownarrow",
            "minâdownarrow",
            "papertalker"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency Analysis.</span> As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Qualitative Analysis &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, PaperTalker achieves the lowest cost. This efficiency stems from our slide-generation design: adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span> <span class=\"ltx_text ltx_font_bold\">reduces token usage</span> for slide creation, and our tree search visual choice module which is a <span class=\"ltx_text ltx_font_bold\">lightweight post-processing step</span>. Runtime is further reduced by our parallel talking-head generation mechanism. By contrast, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> incurs higher token costs due to frequent refinement queries during slide editing.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video.\nUnlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\">Paper2Video</span>, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics&#8212;Meta Similarity, PresentArena, <span class=\"ltx_text ltx_font_italic\">PresentQuiz</span>, and <span class=\"ltx_text ltx_font_italic\">IP Memory</span>&#8212;to measure how videos convey the paper&#8217;s information to the audience and affect the work impact. Building on this foundation, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective <span class=\"ltx_text ltx_font_italic\">Tree Search Visual Choice</span>, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video&#160;demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/showlab/Paper2Video\" title=\"\">https://github.com/showlab/Paper2Video</a>.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "generation",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike natural video generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Zhang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib37\" title=\"\">2025</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, presentation video exhibits distinctive characteristics, including multi-sensory integration, multi-figure conditioning, and high text density, which highlight the limitations of current natural video generation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>. Specifically, academic presentation video generation faces several crucial challenges:\n<span class=\"ltx_text ltx_font_italic\">a.</span> It originates from long-context papers that contain dense text as well as multiple figures and tables;\n<span class=\"ltx_text ltx_font_italic\">b.</span> It requires the coordination of multiple aligned channels, including slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, subtitling, text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib6\" title=\"\">2024b</a>]</cite>, cursor control, and talking head generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>], Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite>;\n<span class=\"ltx_text ltx_font_italic\">c.</span> It lacks well-defined evaluation metrics: what constitutes a good presentation video, particularly in terms of knowledge conveyance and audience accessibility. Even for the state-of-the-art end-to-end video&#8211;audio generation model Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, notable limitations remain in video length, clarity of dense on-screen text, and multi-modal long-document condition.\nIn this work, we try to solve these two core problems as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "deepmind",
                    "veo3",
                    "wan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively generate ready-to-use academic presentation videos, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework that enables academic presentation video generation from research papers and speaker identity. It integrates subsequent key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> Slide Generation. Instead of adopting the commonly used format (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, pptx, XML) from a template slide as in &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we employ LaTeX code for slide generation from sketch, given its formal suitability for academic use and higher efficiency. Specifically, we employ a state-of-the-art Coder to generate code and introduce an effective <span class=\"ltx_text ltx_font_bold\">focused debugging</span> strategy, which iteratively narrows the scope and resolves compilation errors using feedback that indicates the relevant rows.\nTo address the insensitivity of LLMs to fine-grained numerical adjustments, we propose a novel method called <span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice</span>. This approach systematically explores parameter variations to generate multiple branches, which are then concatenated into a single figure. A VLM is then tasked with selecting the optimal branch, thereby effectively improving element layouts such as figure and font size.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitling and Cursor Grounding. We generate subtitles and cursor prompts for each sentence based on the slides. Then we achieve cursor spatial-temporal alignment using <span class=\"ltx_text ltx_font_bold\">Computer-use grounding model</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> models and WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> respectively. <span class=\"ltx_text ltx_font_bold\">(iii)</span> Speech Synthesis and Talking-head Rendering. We synthesize personalized speech via text-to-speech models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib5\" title=\"\">2024a</a>]</cite> and produce talking-head videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> for author presentations. Inspired by human recording practice and the independence between each slide, we <span class=\"ltx_text ltx_font_bold\">parallelize generation</span> across slides, achieving a speedup of more than <math alttext=\"\\mathbf{6\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>&#120788;</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6\\times}</annotation></semantics></math>.\nWe will open-source all our data and codebase to empower the research community.\n</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "generation",
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose PaperTalker, the first multi-agent framework for academic presentation video generation. It introduces three key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> tree search visual choice for fine-grained slide generation; <span class=\"ltx_text ltx_font_bold\">(ii)</span> a GUI-grounding model coupled with WhisperX for spatial-temporal aligned cursor grounding; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> slide-wise parallel generation to improve efficiency.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in video diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib14\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib15\" title=\"\">b</a>]</cite> have substantially improved <span class=\"ltx_text ltx_font_italic\">natural</span> video generation in terms of length, quality, and controllability. However, these <span class=\"ltx_text ltx_font_bold\">end-to-end</span> diffusion models still struggle to produce long videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, several minutes), handle multiple shots, and support conditioning on multiple images&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>.\nMoreover, most existing approaches generate only video without aligned audio, leaving a gap for real-world applications. To address these limitations, recent works leverage <span class=\"ltx_text ltx_font_bold\">multi-agent</span> collaboration to generate multi-shot, long video&#8211;audio pairs and enable multi-image conditioning.\nSpecifically, for natural videos, MovieAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib35\" title=\"\">2025b</a>]</cite> adopts a hierarchical CoT planning strategy and leverages LLMs to simulate the roles of a director, screenwriter, storyboard artist, and location manager, thereby enabling long-form movie generation. Alternatively, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> targets presentation video generation but merely combines PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite> with text-to-speech to produce narrated slides.\nHowever, it lacks personalization (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, mechanical speech and absence of a presenter) and fails to generate academic-style slides (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, missing opening and outline slides), thereby limiting its applicability in academic contexts. Our work addresses these limitations and enables ready-to-use academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "shi",
                    "wan",
                    "deepmind",
                    "generation",
                    "presentagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many useful tasks have been explored under the umbrella of AI for Research (AI4Research)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>]</cite>, which aims to support the full scholarly workflow spanning text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dasigi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib9\" title=\"\">2021</a>]</cite>, static visuals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and dynamic video&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>. With the breakthrough of LLMs in text generation and the Internet search ability, extensive efforts have been devoted to academic writing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>]</cite> and literature surveying&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Katz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib16\" title=\"\">2024</a>], DeYoung et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib11\" title=\"\">2021</a>], Lu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib18\" title=\"\">2020</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>]</cite>, substantially improving research efficiency. Besides, some works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Starace et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib28\" title=\"\">2025</a>], Xiang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib36\" title=\"\">2025</a>]</cite> benchmark AI agents&#8217; end-to-end ability to replicate top-performing ML papers, while others leverage agents to enable idea proposal&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shojaee et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib27\" title=\"\">2025</a>]</cite> and data-driven scientific inspiration&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib7\" title=\"\">2024c</a>], Mitchener et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib21\" title=\"\">2025</a>]</cite>.\nTo further enhance productivity, a growing number of work focuses on the automatic visual design of figures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib33\" title=\"\">2023</a>]</cite>, slides&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and charts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib13\" title=\"\">2024</a>]</cite>. More recently, Paper2Agent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite> has reimagined research papers as interactive and reliable AI agents, designed to assist readers in understanding scientific works. However, very few studies have investigated video generation for scientific purposes, leaving this area relatively underexplored. Our work belongs to one of the pioneering efforts in this direction, initiating a systematic study on academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "shi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Source.</span> We use AI conference papers as the data source for two reasons: (i) they offer high-quality, diverse content across subfields with rich text, figures, and tables; and (ii) the field&#8217;s rapid growth and open-sharing culture provide plentiful, polished author-recorded presentations and slides on YouTube and SlidesLive. However, complete metadata are often unavailable (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, presentation videos, slides, presenter images, and voice samples). We thus manually select papers with relatively complete metadata and supplement missing fields by sourcing presenter images from authors&#8217; websites. Overall, we curate 101 peer-reviewed conference papers from the past three years: 41 from machine learning (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, NeurIPS, ICLR, ICML), 40 from computer vision (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, CVPR, ICCV, ECCV), and 20 from natural language processing(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, ACL, EMNLP, NAACL).\nEach instance includes the paper&#8217;s full <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project and a matched, author-recorded presentation video comprising the slide and talking-head streams with speaker identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, portrait and voice sample). For 40% of the data, we additionally collect the original slide files (PDF), enabling direct, reference-based evaluation of slide generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overview.</span>\nTo address these challenges and liberate researchers from the burdensome task of manual video preparation, we introduce PaperTalker, a multi-agent framework designed to automatically generate presentation videos directly from academic papers.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F4\" title=\"Figure 4 &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, to decouple the different roles, making the method scalable and flexible, the pipeline comprises four builders:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Slide builder. Given the paper, we first synthesize slides with <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#160;code and refine them with compilation feedback to correct grammar and optimize layout;\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitle builder. The slides are then processed by a VLM to generate subtitles and sentence-level visual-focus prompts; <span class=\"ltx_text ltx_font_bold\">(iii)</span> Cursor builder. These prompts are then grounded into on-screen cursor coordinates and synchronized with the narration.\n<span class=\"ltx_text ltx_font_bold\">(iv)</span> Talker builder. Given the voice sample and the portrait of the speaker, text-to-speech and talking-head modules generate a realistic, personalized talker video.\nFor clarity, we denote the paper document, author portrait, and voice sample as <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>, <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "papertalker",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice.</span> After localizing the slides that require refinement, the key challenge is how to adjust their layouts effectively. As LLMs/VLMs fail to perceive real-time visual feedback like human designers, we observe that prompting the them to directly tune numeric layout parameters (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, font sizes, margins, figure scales) is ineffective: the models are largely <span class=\"ltx_text ltx_font_bold\">insensitive</span> to small numeric changes, yielding unstable and inefficient refinement, consistent with limitations of the parameter-editing strategy in PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>. To address this limitation, we introduce a <em class=\"ltx_emph ltx_font_italic\">visual-selection</em> module for overflowed slides. The module first constructs the neighborhoods of layout variants for the current slide by rule-based adjusting the figure and text parameters, renders each variant to an image, and then uses the VLMs as a judge to score the candidates and select the one with the best layout. Specifically, for text-only slides, we sweep the font size; for slides with figures, we first vary the figure scaling factors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 1.25, 0.75, 0.5, 0.25) and then reduce the font size, details shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F5\" title=\"Figure 5 &#8227; 4.1 Slide Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. These edits are straightforward in <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> Beamer, whose structured syntax automatically reflows content as parameter changes. This module <span class=\"ltx_text ltx_font_bold\">decouples discrete layout search from semantic reasoning</span> and reliably resolves overflow cases with minimal time and tokens.</p>\n\n",
                "matched_terms": [
                    "time",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel Talkinghead Generation.</span>\nTo balance fidelity and efficiency, we use Hallo2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite> for head-only synthesis and employ FantasyTalking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> to support talking generation with upper-body articulation.\nA persistent challenge is the long generation time: generating only a few minutes of talking-head video typically takes several hours, and some models(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, FantasyTalking) do not yet natively support long-video generation.\nInspired by the common practice of slide-by-slide recording and the independence between each slide, we synthesize the presenter video on a per-slide basis. Specifically, for each slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, given the audio condition <math alttext=\"\\widetilde{\\mathcal{A}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\widetilde{\\mathcal{A}}_{i}</annotation></semantics></math> and portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, we generate an independent clip <math alttext=\"\\mathcal{V}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}</annotation></semantics></math> and execute these jobs in parallel, markedly reducing generation time: <math alttext=\"\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><mo>=</mo><mrow><mpadded style=\"width:0.484em;\" width=\"0.484em\"><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mpadded><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,</annotation></semantics></math>&#160;where <math alttext=\"\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math> represents the talking-head generation model.\nThis design is justified because slide transitions are hard scene changes, and the temporal continuity of the presenter across adjacent slides is unnecessary.</p>\n\n",
                "matched_terms": [
                    "time",
                    "generation",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spatial-Temporal Grounding.</span>\nIn practice, presenters leverage the cursor as an attentional guide: a well-aligned cursor trajectory minimizes extraneous cognitive load, helps the audience track the presentation, and keeps focus on the key content. However, automatic cursor-trajectory grounding is nontrivial, requiring simultaneous alignment to the timing of speech and the visual semantics of the slides. To simplify the task, we assume that the cursor will stay still within a sentence and only move between the sentences. Thus, we estimate a per-sentence cursor location and time span. For spatial alignment, motivated by strong computer-use models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> which simulate user interaction with the screenshot, we propose to ground the cursor location <math alttext=\"(x,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,y)</annotation></semantics></math> for each sentence with the visual focus prompt <math alttext=\"\\mathcal{P}_{i}^{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>i</mi><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{P}_{i}^{j}</annotation></semantics></math> by UI-TARS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite>. To achieve precise temporal alignment, we then use WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> to extract word-level timestamps and align them with the corresponding sentence in the subtitles to derive the start and end times <math alttext=\"(t_{s},t_{e})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>s</mi></msub><mo>,</mo><msub><mi>t</mi><mi>e</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t_{s},t_{e})</annotation></semantics></math> of each cursor segment.</p>\n\n",
                "matched_terms": [
                    "time",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of baselines: <span class=\"ltx_text ltx_font_bold\">(i)</span> End-to-end Methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, where natural video generation models produce the presentation video directly from a prompt generated by paper; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Multi-Agent Frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, which combine slide generation with text-to-speech generation and compose them into a presentation video; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> PaperTalker, our method and its variants. For the VLM and VideoLLM, we choose <span class=\"ltx_text ltx_font_italic\">GPT-4.1</span> and <span class=\"ltx_text ltx_font_italic\">Gemini-2.5-Flash</span>, respectively, for a favorable efficiency and performance trade-off. We perform inference using eight NVIDIA RTX A6000 GPUs.</p>\n\n",
                "matched_terms": [
                    "shi",
                    "wan",
                    "deepmind",
                    "generation",
                    "method",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity.</span> We evaluate the alignment of the generated slides, subtitles, and speech with corresponding human-authored ones. For speech, we randomly sample a 10-second audio segment from the video generated by each method and compute the cosine similarity between its embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite> and those of the author&#8217;s speech. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest scores in both speech and content similarity, demonstrating that its outputs <span class=\"ltx_text ltx_font_bold\">align most closely with human creation</span> among all baselines. We attribute this performance to personalized TTS and our slide-generation design: <span class=\"ltx_text ltx_font_bold\">(i)</span> adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span>, which provides formal, academically styled templates while <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> automatically arranges content within each slide; and <span class=\"ltx_text ltx_font_bold\">(ii)</span> a tree search visual choice layout refinement that further enforces fine-grained slide layouts as commonly observed in human-authored slides.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentArena.</span> We compare the presentation videos generated by each method against the human-made videos. As an automatic evaluator, we prompt the VideoLLMs as a judge to determine which presentation is better with respect to clarity, delivery, and engagement. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest pairwise winning rate among all baselines, indicating that our method produces presentation videos with <span class=\"ltx_text ltx_font_bold\">superior overall perceived quality</span>. Notably, PaperTalker outperforms its variants without the talker and cursor by 1.8%, highlighting the gains introduced by these components and implying that the VideoLLM <span class=\"ltx_text ltx_font_bold\">favors presentation videos with a talker presenting</span>.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "talker",
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz.</span> To assess information coverage, we conduct a VideoQA evaluation. Following prior work on posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we construct QA sets by prompting an LLM to generate questions targeting (i) fine-grained details and (ii) higher-level understanding of the paper. The videos and QA sets are then fed into a VideoLLM to conduct the quiz.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker achieves superior performance across both aspects, outperforming HumanMade and PresentAgent despite shorter video length. This indicates that PaperTalker produces videos that are <span class=\"ltx_text ltx_font_bold\">more informative within shorter durations</span>. Furthermore, the absence of the talker or cursor results in performance degradation, as the cursor trajectory potentially <span class=\"ltx_text ltx_font_bold\">guides the attention and supports accurate grounding of the key contents</span> for the VideoLLMs during inference, referring to Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span> for more details.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "presentagent",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">IP Memory.</span> We evaluate the degree to which the generated presentation videos facilitate audience retention of the work, thereby assessing their memorability and lasting impact. PaperTalker achieves the highest recall accuracy. This improvement mainly stems from the inclusion of an engaging talker with the author&#8217;s figure and voice, which <span class=\"ltx_text ltx_font_bold\">significantly helps the audience retain the video content</span>.</p>\n\n",
                "matched_terms": [
                    "talker",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Evaluation.</span>\nTo further assess the quality of the generated presentations from the user perspective, we conducted a human evaluation in which ten participants were provided with each paper along with its corresponding presentation videos generated by different methods. Participants were asked to rank the videos according to their preferences(<math alttext=\"1\\text{(worse)}-5\\text{(best)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(worse)</mtext></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(best)</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">1\\text{(worse)}-5\\text{(best)}</annotation></semantics></math>).\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F6\" title=\"Figure 6 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, human-made videos achieve the highest score, with PaperTalker ranking second and outperforming all other baselines. This demonstrates that presentation videos generated by PaperTalker <span class=\"ltx_text ltx_font_bold\">gain consistently favor from human users</span> over other baselines and <span class=\"ltx_text ltx_font_bold\">comparable to human-made</span>.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F7\" title=\"Figure 7 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, PaperTalker produces presentation videos that <span class=\"ltx_text ltx_font_bold\">most closely align with the human-made</span> ones. While Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite> renders a high-quality speaker in front of the screen, it is constrained by short duration (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 8s) and blurred text. Besides, PresentAgent<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> typically suffers from the absence of the presenter and slide-design errors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, overflow, incorrect title, incomplete author lists, and institutions).</p>\n\n",
                "matched_terms": [
                    "deepmind",
                    "veo3",
                    "papertalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work tackles the long-standing bottleneck of presentation video generation by agent automation.\nWith Paper2Video, we provide the first comprehensive benchmark and well-designed metrics to rigorously evaluate presentation videos in terms of quality, knowledge coverage, and academic memorability.\nOur proposed PaperTalker&#160;framework demonstrates that automated generation of ready-to-use academic presentation videos is both feasible and effective, producing outputs that closely approximate author-recorded presentations while significantly reducing production time by 6 times.\nWe hope our work advances AI for Research and supports scalable scholarly communication.</p>\n\n",
                "matched_terms": [
                    "time",
                    "generation",
                    "papertalker"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Ablation study on cursor.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Accuracy<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PaperTalker (w/o Cursor)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.084</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">PaperTalker</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.633</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cursor",
            "study",
            "ablation",
            "accuracyâuparrow",
            "method",
            "papertalker"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video.\nUnlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\">Paper2Video</span>, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics&#8212;Meta Similarity, PresentArena, <span class=\"ltx_text ltx_font_italic\">PresentQuiz</span>, and <span class=\"ltx_text ltx_font_italic\">IP Memory</span>&#8212;to measure how videos convey the paper&#8217;s information to the audience and affect the work impact. Building on this foundation, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective <span class=\"ltx_text ltx_font_italic\">Tree Search Visual Choice</span>, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video&#160;demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/showlab/Paper2Video\" title=\"\">https://github.com/showlab/Paper2Video</a>.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively generate ready-to-use academic presentation videos, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework that enables academic presentation video generation from research papers and speaker identity. It integrates subsequent key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> Slide Generation. Instead of adopting the commonly used format (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, pptx, XML) from a template slide as in &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we employ LaTeX code for slide generation from sketch, given its formal suitability for academic use and higher efficiency. Specifically, we employ a state-of-the-art Coder to generate code and introduce an effective <span class=\"ltx_text ltx_font_bold\">focused debugging</span> strategy, which iteratively narrows the scope and resolves compilation errors using feedback that indicates the relevant rows.\nTo address the insensitivity of LLMs to fine-grained numerical adjustments, we propose a novel method called <span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice</span>. This approach systematically explores parameter variations to generate multiple branches, which are then concatenated into a single figure. A VLM is then tasked with selecting the optimal branch, thereby effectively improving element layouts such as figure and font size.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitling and Cursor Grounding. We generate subtitles and cursor prompts for each sentence based on the slides. Then we achieve cursor spatial-temporal alignment using <span class=\"ltx_text ltx_font_bold\">Computer-use grounding model</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> models and WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> respectively. <span class=\"ltx_text ltx_font_bold\">(iii)</span> Speech Synthesis and Talking-head Rendering. We synthesize personalized speech via text-to-speech models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib5\" title=\"\">2024a</a>]</cite> and produce talking-head videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> for author presentations. Inspired by human recording practice and the independence between each slide, we <span class=\"ltx_text ltx_font_bold\">parallelize generation</span> across slides, achieving a speedup of more than <math alttext=\"\\mathbf{6\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>&#120788;</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6\\times}</annotation></semantics></math>.\nWe will open-source all our data and codebase to empower the research community.\n</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose PaperTalker, the first multi-agent framework for academic presentation video generation. It introduces three key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> tree search visual choice for fine-grained slide generation; <span class=\"ltx_text ltx_font_bold\">(ii)</span> a GUI-grounding model coupled with WhisperX for spatial-temporal aligned cursor grounding; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> slide-wise parallel generation to improve efficiency.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overview.</span>\nTo address these challenges and liberate researchers from the burdensome task of manual video preparation, we introduce PaperTalker, a multi-agent framework designed to automatically generate presentation videos directly from academic papers.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F4\" title=\"Figure 4 &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, to decouple the different roles, making the method scalable and flexible, the pipeline comprises four builders:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Slide builder. Given the paper, we first synthesize slides with <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#160;code and refine them with compilation feedback to correct grammar and optimize layout;\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitle builder. The slides are then processed by a VLM to generate subtitles and sentence-level visual-focus prompts; <span class=\"ltx_text ltx_font_bold\">(iii)</span> Cursor builder. These prompts are then grounded into on-screen cursor coordinates and synchronized with the narration.\n<span class=\"ltx_text ltx_font_bold\">(iv)</span> Talker builder. Given the voice sample and the portrait of the speaker, text-to-speech and talking-head modules generate a realistic, personalized talker video.\nFor clarity, we denote the paper document, author portrait, and voice sample as <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>, <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of baselines: <span class=\"ltx_text ltx_font_bold\">(i)</span> End-to-end Methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, where natural video generation models produce the presentation video directly from a prompt generated by paper; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Multi-Agent Frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, which combine slide generation with text-to-speech generation and compose them into a presentation video; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> PaperTalker, our method and its variants. For the VLM and VideoLLM, we choose <span class=\"ltx_text ltx_font_italic\">GPT-4.1</span> and <span class=\"ltx_text ltx_font_italic\">Gemini-2.5-Flash</span>, respectively, for a favorable efficiency and performance trade-off. We perform inference using eight NVIDIA RTX A6000 GPUs.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity.</span> We evaluate the alignment of the generated slides, subtitles, and speech with corresponding human-authored ones. For speech, we randomly sample a 10-second audio segment from the video generated by each method and compute the cosine similarity between its embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite> and those of the author&#8217;s speech. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest scores in both speech and content similarity, demonstrating that its outputs <span class=\"ltx_text ltx_font_bold\">align most closely with human creation</span> among all baselines. We attribute this performance to personalized TTS and our slide-generation design: <span class=\"ltx_text ltx_font_bold\">(i)</span> adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span>, which provides formal, academically styled templates while <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> automatically arranges content within each slide; and <span class=\"ltx_text ltx_font_bold\">(ii)</span> a tree search visual choice layout refinement that further enforces fine-grained slide layouts as commonly observed in human-authored slides.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentArena.</span> We compare the presentation videos generated by each method against the human-made videos. As an automatic evaluator, we prompt the VideoLLMs as a judge to determine which presentation is better with respect to clarity, delivery, and engagement. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest pairwise winning rate among all baselines, indicating that our method produces presentation videos with <span class=\"ltx_text ltx_font_bold\">superior overall perceived quality</span>. Notably, PaperTalker outperforms its variants without the talker and cursor by 1.8%, highlighting the gains introduced by these components and implying that the VideoLLM <span class=\"ltx_text ltx_font_bold\">favors presentation videos with a talker presenting</span>.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz.</span> To assess information coverage, we conduct a VideoQA evaluation. Following prior work on posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we construct QA sets by prompting an LLM to generate questions targeting (i) fine-grained details and (ii) higher-level understanding of the paper. The videos and QA sets are then fed into a VideoLLM to conduct the quiz.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker achieves superior performance across both aspects, outperforming HumanMade and PresentAgent despite shorter video length. This indicates that PaperTalker produces videos that are <span class=\"ltx_text ltx_font_bold\">more informative within shorter durations</span>. Furthermore, the absence of the talker or cursor results in performance degradation, as the cursor trajectory potentially <span class=\"ltx_text ltx_font_bold\">guides the attention and supports accurate grounding of the key contents</span> for the VideoLLMs during inference, referring to Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span> for more details.</p>\n\n",
                "matched_terms": [
                    "papertalker",
                    "cursor"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Evaluation result on slide quality.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Method</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Content (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Design (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Coherence (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">HumanMade</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.43</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.85</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">\n<math alttext=\"\\text{PPTAgent}_{\\text{Qwen7B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\" intent=\":literal\"><semantics><msub><mtext>PPTAgent</mtext><mtext>Qwen7B</mtext></msub><annotation encoding=\"application/x-tex\">\\text{PPTAgent}_{\\text{Qwen7B}}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">3.43</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.57</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"\\text{PaperTalker}_{\\text{Qwen7B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m5\" intent=\":literal\"><semantics><msub><mtext>PaperTalker</mtext><mtext>Qwen7B</mtext></msub><annotation encoding=\"application/x-tex\">\\text{PaperTalker}_{\\text{Qwen7B}}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">4.00</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.53</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">\n<math alttext=\"\\text{PPTAgent}_{\\text{GPT4.1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m6\" intent=\":literal\"><semantics><msub><mtext>PPTAgent</mtext><mtext>GPT4.1</mtext></msub><annotation encoding=\"application/x-tex\">\\text{PPTAgent}_{\\text{GPT4.1}}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">4.07</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.02</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">\n<math alttext=\"\\text{PaperTalker}_{\\text{GPT4.1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m7\" intent=\":literal\"><semantics><msub><mtext>PaperTalker</mtext><mtext>GPT4.1</mtext></msub><annotation encoding=\"application/x-tex\">\\text{PaperTalker}_{\\text{GPT4.1}}</annotation></semantics></math>(w/o Tree Search)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">4.33</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.73</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"\\text{PaperTalker}_{\\text{GPT4.1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m8\" intent=\":literal\"><semantics><msub><mtext>PaperTalker</mtext><mtext>GPT4.1</mtext></msub><annotation encoding=\"application/x-tex\">\\text{PaperTalker}_{\\text{GPT4.1}}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.34</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.85</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "coherence",
            "âuparrow",
            "pptagentqwen7btextpptagenttextqwen7b",
            "papertalkergpt41textpapertalkertextgpt41wo",
            "design",
            "zheng",
            "content",
            "result",
            "pptagentgpt41textpptagenttextgpt41",
            "papertalkerqwen7btextpapertalkertextqwen7b",
            "evaluation",
            "papertalkergpt41textpapertalkertextgpt41",
            "tree",
            "slide",
            "method",
            "search",
            "humanmade",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">How does tree search visual choice improve slide quality?</span> To assess the contribution of the tree-search visual choice module, we conduct an ablation experiment, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T5\" title=\"Table 5 &#8227; 5.4 Key Ablations &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In line with prior work on slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we assess the generated slides using a VLM on a 1&#8211;5 scale across content, design, and coherence. The results show a pronounced decline in design quality when layout refinement is removed, highlighting <span class=\"ltx_text ltx_font_bold\">the tree-search visual choice module as a key component for slide creation</span> (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;resolving overfull issues), referring to Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#A2.F8\" title=\"Figure 8 &#8227; B.2 Results of Tree Search Visual Choice &#8227; Appendix B Experiment &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for visualization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video.\nUnlike natural video, presentation video generation involves distinctive challenges: long-context inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker.\nTo address these challenges, we introduce <span class=\"ltx_text ltx_font_bold\">Paper2Video</span>, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics&#8212;Meta Similarity, PresentArena, <span class=\"ltx_text ltx_font_italic\">PresentQuiz</span>, and <span class=\"ltx_text ltx_font_italic\">IP Memory</span>&#8212;to measure how videos convey the paper&#8217;s information to the audience and affect the work impact. Building on this foundation, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective <span class=\"ltx_text ltx_font_italic\">Tree Search Visual Choice</span>, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video&#160;demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/showlab/Paper2Video\" title=\"\">https://github.com/showlab/Paper2Video</a>.</p>\n\n",
                "matched_terms": [
                    "design",
                    "evaluation",
                    "tree",
                    "slide",
                    "search"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Academic presentation videos are widely used in research communication, serving as a crucial and effective means to bridge researchers, as many conferences require them as an essential material for submission. However, the manual creation of such a video is highly labor-intensive, requiring slide design, subtitle writing, per-slide recording, and careful editing, which on average may take several hours to produce a <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> to <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> minute video for a scientific paper. Despite some prior works on slide and poster generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib29\" title=\"\">2021</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>], Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite> and other AI4Research tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>], Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>], Seo et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib25\" title=\"\">2025</a>], Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite>, automatic academic presentation video generation is a superproblem of them, a practical yet more challenging direction.</p>\n\n",
                "matched_terms": [
                    "slide",
                    "design",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike natural video generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Zhang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib37\" title=\"\">2025</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, presentation video exhibits distinctive characteristics, including multi-sensory integration, multi-figure conditioning, and high text density, which highlight the limitations of current natural video generation models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>. Specifically, academic presentation video generation faces several crucial challenges:\n<span class=\"ltx_text ltx_font_italic\">a.</span> It originates from long-context papers that contain dense text as well as multiple figures and tables;\n<span class=\"ltx_text ltx_font_italic\">b.</span> It requires the coordination of multiple aligned channels, including slide generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, subtitling, text-to-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib6\" title=\"\">2024b</a>]</cite>, cursor control, and talking head generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>], Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite>;\n<span class=\"ltx_text ltx_font_italic\">c.</span> It lacks well-defined evaluation metrics: what constitutes a good presentation video, particularly in terms of knowledge conveyance and audience accessibility. Even for the state-of-the-art end-to-end video&#8211;audio generation model Veo3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, notable limitations remain in video length, clarity of dense on-screen text, and multi-modal long-document condition.\nIn this work, we try to solve these two core problems as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "slide",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable comprehensive evaluation of academic presentation video generation, we present the <span class=\"ltx_text ltx_font_bold\">Paper2Video</span> Benchmark, comprising <math alttext=\"101\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>101</mn><annotation encoding=\"application/x-tex\">101</annotation></semantics></math> paired research papers and author-recorded presentation videos from recent conferences, together with original slides and speaker identity metadata. Based on this benchmark, we develop a suite of metrics to comprehensively evaluate generation quality from multiple dimensions:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Meta Similarity &#8212; We employ a VLM to evaluate the alignment of generated slides and subtitles with human-designed counterparts.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> PresentArena &#8212; We use a VideoLLM as a proxy audience to perform double-order pairwise comparisons between generated and human-made videos.\nNotably, the primary purpose of a presentation is to <span class=\"ltx_text ltx_font_italic\">effectively convey the information contained in the paper</span>. To this end, we introduce <span class=\"ltx_text ltx_font_bold\">(iii) PresentQuiz</span>, which treats the VideoLLMs as the audience and requires them to answer paper-derived questions given the videos. Furthermore, another important purpose of presentation video is to <span class=\"ltx_text ltx_font_italic\">enhance the visibility and impact of the author&#8217;s work</span>. Motivated by real-conference interactions, we introduce <span class=\"ltx_text ltx_font_bold\">(iv) IP Memory</span>, which measures how well an audience can associate authors and works after watching presentation videos.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "humanmade",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively generate ready-to-use academic presentation videos, we propose <span class=\"ltx_text ltx_font_bold\">PaperTalker</span>, the first multi-agent framework that enables academic presentation video generation from research papers and speaker identity. It integrates subsequent key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> Slide Generation. Instead of adopting the commonly used format (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, pptx, XML) from a template slide as in &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we employ LaTeX code for slide generation from sketch, given its formal suitability for academic use and higher efficiency. Specifically, we employ a state-of-the-art Coder to generate code and introduce an effective <span class=\"ltx_text ltx_font_bold\">focused debugging</span> strategy, which iteratively narrows the scope and resolves compilation errors using feedback that indicates the relevant rows.\nTo address the insensitivity of LLMs to fine-grained numerical adjustments, we propose a novel method called <span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice</span>. This approach systematically explores parameter variations to generate multiple branches, which are then concatenated into a single figure. A VLM is then tasked with selecting the optimal branch, thereby effectively improving element layouts such as figure and font size.\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitling and Cursor Grounding. We generate subtitles and cursor prompts for each sentence based on the slides. Then we achieve cursor spatial-temporal alignment using <span class=\"ltx_text ltx_font_bold\">Computer-use grounding model</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib17\" title=\"\">2025</a>], Qin et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib23\" title=\"\">2025</a>]</cite> models and WhisperX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib1\" title=\"\">2023</a>]</cite> respectively. <span class=\"ltx_text ltx_font_bold\">(iii)</span> Speech Synthesis and Talking-head Rendering. We synthesize personalized speech via text-to-speech models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib5\" title=\"\">2024a</a>]</cite> and produce talking-head videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> for author presentations. Inspired by human recording practice and the independence between each slide, we <span class=\"ltx_text ltx_font_bold\">parallelize generation</span> across slides, achieving a speedup of more than <math alttext=\"\\mathbf{6\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mrow><mn>&#120788;</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6\\times}</annotation></semantics></math>.\nWe will open-source all our data and codebase to empower the research community.\n</p>\n\n",
                "matched_terms": [
                    "zheng",
                    "tree",
                    "slide",
                    "method",
                    "search"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose PaperTalker, the first multi-agent framework for academic presentation video generation. It introduces three key modules: <span class=\"ltx_text ltx_font_bold\">(i)</span> tree search visual choice for fine-grained slide generation; <span class=\"ltx_text ltx_font_bold\">(ii)</span> a GUI-grounding model coupled with WhisperX for spatial-temporal aligned cursor grounding; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> slide-wise parallel generation to improve efficiency.</p>\n\n",
                "matched_terms": [
                    "search",
                    "tree",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results on Paper2Video confirm the effectiveness of PaperTalker, which outperforms human-made presentations by 10% in PresentQuiz accuracy and achieves comparable ratings in user studies, indicating that its quality approaches that of human-created content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "humanmade",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in video diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Blattmann et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib2\" title=\"\">2023</a>], Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], Huang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib14\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib15\" title=\"\">b</a>]</cite> have substantially improved <span class=\"ltx_text ltx_font_italic\">natural</span> video generation in terms of length, quality, and controllability. However, these <span class=\"ltx_text ltx_font_bold\">end-to-end</span> diffusion models still struggle to produce long videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>], Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, several minutes), handle multiple shots, and support conditioning on multiple images&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib19\" title=\"\">2025</a>]</cite>.\nMoreover, most existing approaches generate only video without aligned audio, leaving a gap for real-world applications. To address these limitations, recent works leverage <span class=\"ltx_text ltx_font_bold\">multi-agent</span> collaboration to generate multi-shot, long video&#8211;audio pairs and enable multi-image conditioning.\nSpecifically, for natural videos, MovieAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib35\" title=\"\">2025b</a>]</cite> adopts a hierarchical CoT planning strategy and leverages LLMs to simulate the roles of a director, screenwriter, storyboard artist, and location manager, thereby enabling long-form movie generation. Alternatively, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> targets presentation video generation but merely combines PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite> with text-to-speech to produce narrated slides.\nHowever, it lacks personalization (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, mechanical speech and absence of a presenter) and fails to generate academic-style slides (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, missing opening and outline slides), thereby limiting its applicability in academic contexts. Our work addresses these limitations and enables ready-to-use academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many useful tasks have been explored under the umbrella of AI for Research (AI4Research)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib4\" title=\"\">2025</a>]</cite>, which aims to support the full scholarly workflow spanning text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dasigi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib9\" title=\"\">2021</a>]</cite>, static visuals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and dynamic video&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite>. With the breakthrough of LLMs in text generation and the Internet search ability, extensive efforts have been devoted to academic writing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chamoun et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib3\" title=\"\">2024</a>]</cite> and literature surveying&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Katz et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib16\" title=\"\">2024</a>], DeYoung et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib11\" title=\"\">2021</a>], Lu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib18\" title=\"\">2020</a>], Goldsack et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib12\" title=\"\">2022</a>]</cite>, substantially improving research efficiency. Besides, some works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Starace et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib28\" title=\"\">2025</a>], Xiang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib36\" title=\"\">2025</a>]</cite> benchmark AI agents&#8217; end-to-end ability to replicate top-performing ML papers, while others leverage agents to enable idea proposal&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shojaee et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib27\" title=\"\">2025</a>]</cite> and data-driven scientific inspiration&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib7\" title=\"\">2024c</a>], Mitchener et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib21\" title=\"\">2025</a>]</cite>.\nTo further enhance productivity, a growing number of work focuses on the automatic visual design of figures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib33\" title=\"\">2023</a>]</cite>, slides&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, and charts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib13\" title=\"\">2024</a>]</cite>. More recently, Paper2Agent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Miao et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib20\" title=\"\">2025</a>]</cite> has reimagined research papers as interactive and reliable AI agents, designed to assist readers in understanding scientific works. However, very few studies have investigated video generation for scientific purposes, leaving this area relatively underexplored. Our work belongs to one of the pioneering efforts in this direction, initiating a systematic study on academic presentation video generation.</p>\n\n",
                "matched_terms": [
                    "search",
                    "design",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task poses several distinctive challenges:\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">a<span class=\"ltx_text ltx_font_upright\">. Multi-modal Long-Context Understanding.</span></span> Research papers span many pages with dense text, equations, figures, and tables.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">b<span class=\"ltx_text ltx_font_upright\">. Multi-turn Agent Tasks.</span></span> It is challenging to solve this task with a single end-to-end model, as it requires multi-channel generation and alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, slides, cursors, and presenter).\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">c<span class=\"ltx_text ltx_font_upright\">. Personalized Presenter Synthesis.</span></span>\nAchieving high-quality, identity-preserving, and lip-synchronous talking-head video remains time-consuming, and even more challenging when jointly modeling voice, face, and gesture.\n<span class=\"ltx_text ltx_font_bold ltx_font_italic\">d<span class=\"ltx_text ltx_font_upright\">. Spatial-Temporal-Grounding.</span></span>\nProducing cursor trajectories synchronized with narration and slide content demands precise alignment between linguistic units and visual anchors.</p>\n\n",
                "matched_terms": [
                    "content",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Source.</span> We use AI conference papers as the data source for two reasons: (i) they offer high-quality, diverse content across subfields with rich text, figures, and tables; and (ii) the field&#8217;s rapid growth and open-sharing culture provide plentiful, polished author-recorded presentations and slides on YouTube and SlidesLive. However, complete metadata are often unavailable (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, presentation videos, slides, presenter images, and voice samples). We thus manually select papers with relatively complete metadata and supplement missing fields by sourcing presenter images from authors&#8217; websites. Overall, we curate 101 peer-reviewed conference papers from the past three years: 41 from machine learning (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, NeurIPS, ICLR, ICML), 40 from computer vision (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, CVPR, ICCV, ECCV), and 20 from natural language processing(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, ACL, EMNLP, NAACL).\nEach instance includes the paper&#8217;s full <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project and a matched, author-recorded presentation video comprising the slide and talking-head streams with speaker identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, portrait and voice sample). For 40% of the data, we additionally collect the original slide files (PDF), enabling direct, reference-based evaluation of slide generation.</p>\n\n",
                "matched_terms": [
                    "content",
                    "evaluation",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity</span> <span class=\"ltx_text ltx_font_italic\">&#8211; How video like human-made?</span>\nAs we have the ground-truth human-made presentation videos with original slides, we evaluate how well the generated intermediate assets (<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;speech, slides, and subtitles) aligned with the ones created by authors, which serves as the pseudo ground-truth.\n(i) For each slide, we pair the slide image with its corresponding subtitles and submit both the generated pair and the human-made pair to the VLMs to obtain a similarity score on a five-point scale.\n(ii) To further assess speech(<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;vocal timbre), we uniformly sample a ten-second segment from the presentation audio, encode the generated and human-recorded audio with a speaking embedding model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite>, and compute the cosine similarity between the embeddings to measure speech similarity.</p>\n\n",
                "matched_terms": [
                    "humanmade",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overview.</span>\nTo address these challenges and liberate researchers from the burdensome task of manual video preparation, we introduce PaperTalker, a multi-agent framework designed to automatically generate presentation videos directly from academic papers.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F4\" title=\"Figure 4 &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, to decouple the different roles, making the method scalable and flexible, the pipeline comprises four builders:\n<span class=\"ltx_text ltx_font_bold\">(i)</span> Slide builder. Given the paper, we first synthesize slides with <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#160;code and refine them with compilation feedback to correct grammar and optimize layout;\n<span class=\"ltx_text ltx_font_bold\">(ii)</span> Subtitle builder. The slides are then processed by a VLM to generate subtitles and sentence-level visual-focus prompts; <span class=\"ltx_text ltx_font_bold\">(iii)</span> Cursor builder. These prompts are then grounded into on-screen cursor coordinates and synchronized with the narration.\n<span class=\"ltx_text ltx_font_bold\">(iv)</span> Talker builder. Given the voice sample and the portrait of the speaker, text-to-speech and talking-head modules generate a realistic, personalized talker video.\nFor clarity, we denote the paper document, author portrait, and voice sample as <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>, <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "slide",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A prerequisite for producing a presentation video is the creation of the slides. Despite there being some existing works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, we target the generation of academic slides with fine-grained layouts and formal structure from scratch.\nRather than selecting a template and iteratively editing it with VLMs, we generate slides directly from a paper&#8217;s <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> project by prompting the model to write Beamer code. We adopt Beamer for three reasons: <span class=\"ltx_text ltx_font_bold\">(i)</span> <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span>&#8217;s declarative typesetting automatically arranges text block and figures from their parameters without explicitly planing the positions; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Beamer is compact and expressive, representing the same content in fewer lines than XML-based formats; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> Beamer provides well-designed, formally configured styles (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, page numbers, section headers, hyperlinks) that are well suited to academic slide design.</p>\n\n",
                "matched_terms": [
                    "content",
                    "slide",
                    "design",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the paper <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> as input, the LLM first produces a draft slide code. We compile this code to collect diagnostics(<span class=\"ltx_text ltx_font_italic\">i.e.,</span>&#160;errors and warnings). Then, we use the error information to elicit a repaired correct code.\nThis procedure ensures that the generated Beamer code is grammatically correct and effectively leverages and faithfully covers its content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tree Search Visual Choice.</span> After localizing the slides that require refinement, the key challenge is how to adjust their layouts effectively. As LLMs/VLMs fail to perceive real-time visual feedback like human designers, we observe that prompting the them to directly tune numeric layout parameters (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, font sizes, margins, figure scales) is ineffective: the models are largely <span class=\"ltx_text ltx_font_bold\">insensitive</span> to small numeric changes, yielding unstable and inefficient refinement, consistent with limitations of the parameter-editing strategy in PPTAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>. To address this limitation, we introduce a <em class=\"ltx_emph ltx_font_italic\">visual-selection</em> module for overflowed slides. The module first constructs the neighborhoods of layout variants for the current slide by rule-based adjusting the figure and text parameters, renders each variant to an image, and then uses the VLMs as a judge to score the candidates and select the one with the best layout. Specifically, for text-only slides, we sweep the font size; for slides with figures, we first vary the figure scaling factors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 1.25, 0.75, 0.5, 0.25) and then reduce the font size, details shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S4.F5\" title=\"Figure 5 &#8227; 4.1 Slide Builder &#8227; 4 PaperTalker Agent &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. These edits are straightforward in <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> Beamer, whose structured syntax automatically reflows content as parameter changes. This module <span class=\"ltx_text ltx_font_bold\">decouples discrete layout search from semantic reasoning</span> and reliably resolves overflow cases with minimal time and tokens.</p>\n\n",
                "matched_terms": [
                    "zheng",
                    "content",
                    "tree",
                    "slide",
                    "search"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The presenter video is vital for audience engagement and conveying the researcher&#8217;s scholarly identity (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, face and voice). Given the subtitles <math alttext=\"\\mathcal{T}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{T}_{i}</annotation></semantics></math>, the author&#8217;s portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, and a short voice sample <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, our objective is to synthesize a presenter video that delivers the slide content in the author&#8217;s voice, with faithful identity preservation and lip&#8211;audio synchronization.</p>\n\n",
                "matched_terms": [
                    "content",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Parallel Talkinghead Generation.</span>\nTo balance fidelity and efficiency, we use Hallo2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cui et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib8\" title=\"\">2024</a>]</cite> for head-only synthesis and employ FantasyTalking&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib32\" title=\"\">2025</a>]</cite> to support talking generation with upper-body articulation.\nA persistent challenge is the long generation time: generating only a few minutes of talking-head video typically takes several hours, and some models(<span class=\"ltx_text ltx_font_italic\">e.g.</span>, FantasyTalking) do not yet natively support long-video generation.\nInspired by the common practice of slide-by-slide recording and the independence between each slide, we synthesize the presenter video on a per-slide basis. Specifically, for each slide <math alttext=\"\\mathcal{S}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}</annotation></semantics></math>, given the audio condition <math alttext=\"\\widetilde{\\mathcal{A}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\widetilde{\\mathcal{A}}_{i}</annotation></semantics></math> and portrait <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, we generate an independent clip <math alttext=\"\\mathcal{V}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}</annotation></semantics></math> and execute these jobs in parallel, markedly reducing generation time: <math alttext=\"\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>i</mi></msub><mo>=</mo><mrow><mpadded style=\"width:0.484em;\" width=\"0.484em\"><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mpadded><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><mi>i</mi></msub><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}_{i}=\\mathcal{G}\\!\\left(\\widetilde{\\mathcal{A}}_{i},\\mathcal{I}\\right),i=1,\\ldots,n,</annotation></semantics></math>&#160;where <math alttext=\"\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><annotation encoding=\"application/x-tex\">\\mathcal{G}</annotation></semantics></math> represents the talking-head generation model.\nThis design is justified because slide transitions are hard scene changes, and the temporal continuity of the presenter across adjacent slides is unnecessary.</p>\n\n",
                "matched_terms": [
                    "design",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of baselines: <span class=\"ltx_text ltx_font_bold\">(i)</span> End-to-end Methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wan et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib31\" title=\"\">2025</a>], DeepMind [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib10\" title=\"\">2025</a>]</cite>, where natural video generation models produce the presentation video directly from a prompt generated by paper; <span class=\"ltx_text ltx_font_bold\">(ii)</span> Multi-Agent Frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>], Zheng et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib38\" title=\"\">2025</a>]</cite>, which combine slide generation with text-to-speech generation and compose them into a presentation video; and <span class=\"ltx_text ltx_font_bold\">(iii)</span> PaperTalker, our method and its variants. For the VLM and VideoLLM, we choose <span class=\"ltx_text ltx_font_italic\">GPT-4.1</span> and <span class=\"ltx_text ltx_font_italic\">Gemini-2.5-Flash</span>, respectively, for a favorable efficiency and performance trade-off. We perform inference using eight NVIDIA RTX A6000 GPUs.</p>\n\n",
                "matched_terms": [
                    "slide",
                    "method",
                    "zheng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Meta Similarity.</span> We evaluate the alignment of the generated slides, subtitles, and speech with corresponding human-authored ones. For speech, we randomly sample a 10-second audio segment from the video generated by each method and compute the cosine similarity between its embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ravanelli et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib24\" title=\"\">2024</a>]</cite> and those of the author&#8217;s speech. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest scores in both speech and content similarity, demonstrating that its outputs <span class=\"ltx_text ltx_font_bold\">align most closely with human creation</span> among all baselines. We attribute this performance to personalized TTS and our slide-generation design: <span class=\"ltx_text ltx_font_bold\">(i)</span> adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span>, which provides formal, academically styled templates while <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> automatically arranges content within each slide; and <span class=\"ltx_text ltx_font_bold\">(ii)</span> a tree search visual choice layout refinement that further enforces fine-grained slide layouts as commonly observed in human-authored slides.</p>\n\n",
                "matched_terms": [
                    "design",
                    "content",
                    "tree",
                    "slide",
                    "method",
                    "search"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentArena.</span> We compare the presentation videos generated by each method against the human-made videos. As an automatic evaluator, we prompt the VideoLLMs as a judge to determine which presentation is better with respect to clarity, delivery, and engagement. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker attains the highest pairwise winning rate among all baselines, indicating that our method produces presentation videos with <span class=\"ltx_text ltx_font_bold\">superior overall perceived quality</span>. Notably, PaperTalker outperforms its variants without the talker and cursor by 1.8%, highlighting the gains introduced by these components and implying that the VideoLLM <span class=\"ltx_text ltx_font_bold\">favors presentation videos with a talker presenting</span>.</p>\n\n",
                "matched_terms": [
                    "humanmade",
                    "method",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PresentQuiz.</span> To assess information coverage, we conduct a VideoQA evaluation. Following prior work on posters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pang et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib22\" title=\"\">2025</a>]</cite>, we construct QA sets by prompting an LLM to generate questions targeting (i) fine-grained details and (ii) higher-level understanding of the paper. The videos and QA sets are then fed into a VideoLLM to conduct the quiz.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, PaperTalker achieves superior performance across both aspects, outperforming HumanMade and PresentAgent despite shorter video length. This indicates that PaperTalker produces videos that are <span class=\"ltx_text ltx_font_bold\">more informative within shorter durations</span>. Furthermore, the absence of the talker or cursor results in performance degradation, as the cursor trajectory potentially <span class=\"ltx_text ltx_font_bold\">guides the attention and supports accurate grounding of the key contents</span> for the VideoLLMs during inference, referring to Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span> for more details.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "humanmade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Evaluation.</span>\nTo further assess the quality of the generated presentations from the user perspective, we conducted a human evaluation in which ten participants were provided with each paper along with its corresponding presentation videos generated by different methods. Participants were asked to rank the videos according to their preferences(<math alttext=\"1\\text{(worse)}-5\\text{(best)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(worse)</mtext></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>(best)</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">1\\text{(worse)}-5\\text{(best)}</annotation></semantics></math>).\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.F6\" title=\"Figure 6 &#8227; 5.2 Main Results &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, human-made videos achieve the highest score, with PaperTalker ranking second and outperforming all other baselines. This demonstrates that presentation videos generated by PaperTalker <span class=\"ltx_text ltx_font_bold\">gain consistently favor from human users</span> over other baselines and <span class=\"ltx_text ltx_font_bold\">comparable to human-made</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "humanmade",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency Analysis.</span> As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Qualitative Analysis &#8227; 5 Experiments &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, PaperTalker achieves the lowest cost. This efficiency stems from our slide-generation design: adopting <span class=\"ltx_text ltx_font_italic\">Beamer</span> <span class=\"ltx_text ltx_font_bold\">reduces token usage</span> for slide creation, and our tree search visual choice module which is a <span class=\"ltx_text ltx_font_bold\">lightweight post-processing step</span>. Runtime is further reduced by our parallel talking-head generation mechanism. By contrast, PresentAgent&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi et&#160;al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#bib.bib26\" title=\"\">2025</a>]</cite> incurs higher token costs due to frequent refinement queries during slide editing.</p>\n\n",
                "matched_terms": [
                    "search",
                    "tree",
                    "design",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">What benefits are brought by Cursor Highlight?</span>\nMotivated by the observation that a cursor typically helps audiences locate the relevant region, we hypothesize that a visible cursor, by providing an explicit spatial cue, facilitates content grounding for VLMs. To evaluate this, we design a localization QA task: for each subtitle sentence and its corresponding slide, a VLM generates a four-option multiple-choice question about the sentence&#8217;s corresponding position on the slide. The VLMs are then prompted to answer using slide screenshots, with or without the cursor, and accuracy is measured as the metric. As shown in Table&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:table:cursor</span>, the accuracy is much higher with the cursor highlight, corroborating its <span class=\"ltx_text ltx_font_bold\">importance for the audience&#8217;s visual grounding accessibility</span> of presentation videos.</p>\n\n",
                "matched_terms": [
                    "content",
                    "design",
                    "slide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05096v2#A2.F8\" title=\"Figure 8 &#8227; B.2 Results of Tree Search Visual Choice &#8227; Appendix B Experiment &#8227; Paper2Video: Automatic Video Generation from Scientific Papers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates the slides before and after applying tree search visual choice refinement. The refinement <span class=\"ltx_text ltx_font_bold\">resolves the overfull issues</span> and substantially improves slide quality, indicating that this module <span class=\"ltx_text ltx_font_bold\">plays a crucial role in layout adjustment</span>.</p>\n\n",
                "matched_terms": [
                    "search",
                    "tree",
                    "slide",
                    "quality"
                ]
            }
        ]
    }
}