{
    "S3.T1": {
        "caption": "TABLE I: Performance comparison with test speech watermarking methods. Best performance in each column is highlighted in bold.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Models</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">BPS(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PESQ(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SNR(dB)(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"15\">BER(%)(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MEAN</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ND</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">GN</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MF</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MP3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">LP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">DS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">QT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">AS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">EA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">FS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">RA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">PU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">PD</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">WavMark</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">32</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AudioSeal</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n<td class=\"ltx_td ltx_align_center\">27.53</td>\n<td class=\"ltx_td ltx_align_center\">18.66</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center\">52.12</td>\n<td class=\"ltx_td ltx_align_center\">59.50</td>\n<td class=\"ltx_td ltx_align_center\">47.09</td>\n<td class=\"ltx_td ltx_align_center\">50.69</td>\n<td class=\"ltx_td ltx_align_center\">51.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">WavMark-16bps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n<td class=\"ltx_td ltx_align_center\">41.56</td>\n<td class=\"ltx_td ltx_align_center\">2.43</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">1.14</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">1.26</td>\n<td class=\"ltx_td ltx_align_center\">1.32</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">3.25</td>\n<td class=\"ltx_td ltx_align_center\">3.06</td>\n<td class=\"ltx_td ltx_align_center\">9.56</td>\n<td class=\"ltx_td ltx_align_center\">5.23</td>\n<td class=\"ltx_td ltx_align_center\">6.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Teacher model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">45.75</span></td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">0.13</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.85</td>\n<td class=\"ltx_td ltx_align_center\">0.13</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\">1.11</td>\n<td class=\"ltx_td ltx_align_center\">1.20</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\">1.04</td>\n<td class=\"ltx_td ltx_align_center\">0.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Teacher-woMSSL</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n<td class=\"ltx_td ltx_align_center\">40.34</td>\n<td class=\"ltx_td ltx_align_center\">1.23</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center\">2.16</td>\n<td class=\"ltx_td ltx_align_center\">0.18</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center\">1.24</td>\n<td class=\"ltx_td ltx_align_center\">0.24</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\">2.04</td>\n<td class=\"ltx_td ltx_align_center\">6.79</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center\">2.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Teacher-woCMFM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">4.30</td>\n<td class=\"ltx_td ltx_align_center\">42.25</td>\n<td class=\"ltx_td ltx_align_center\">1.77</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\">1.68</td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.90</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">1.02</td>\n<td class=\"ltx_td ltx_align_center\">1.86</td>\n<td class=\"ltx_td ltx_align_center\">1.86</td>\n<td class=\"ltx_td ltx_align_center\">7.03</td>\n<td class=\"ltx_td ltx_align_center\">3.43</td>\n<td class=\"ltx_td ltx_align_center\">4.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DKD Student</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td class=\"ltx_td ltx_align_center\">4.08</td>\n<td class=\"ltx_td ltx_align_center\">44.29</td>\n<td class=\"ltx_td ltx_align_center\">1.94</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">2.41</td>\n<td class=\"ltx_td ltx_align_center\">1.50</td>\n<td class=\"ltx_td ltx_align_center\">0.91</td>\n<td class=\"ltx_td ltx_align_center\">2.93</td>\n<td class=\"ltx_td ltx_align_center\">6.38</td>\n<td class=\"ltx_td ltx_align_center\">0.39</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.20</td>\n<td class=\"ltx_td ltx_align_center\">3.13</td>\n<td class=\"ltx_td ltx_align_center\">3.19</td>\n<td class=\"ltx_td ltx_align_center\">2.93</td>\n<td class=\"ltx_td ltx_align_center\">4.10</td>\n<td class=\"ltx_td ltx_align_center\">2.54</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">PKDMark</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">16</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "student",
            "column",
            "teacher",
            "snrdb↑uparrow",
            "each",
            "comparison",
            "teacherwocmfm",
            "bps↑uparrow",
            "wavmark16bps",
            "methods",
            "test",
            "wavmark",
            "performance",
            "watermarking",
            "dkd",
            "bold",
            "highlighted",
            "mean",
            "speech",
            "mp3",
            "audioseal",
            "ber↓downarrow",
            "pesq↑uparrow",
            "model",
            "best",
            "pkdmark",
            "teacherwomssl"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate imperceptibility, synthesized speech samples from both the teacher and student models are compared against the original recordings. PESQ scores reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> indicate minimal quality differences between the teacher (4.33) and student (4.30) models. Furthermore, a CMOS test yields a score of -0.04 when comparing original recordings with student model samples, suggesting that PKDMark is nearly indistinguishable from the original speech. These results confirm that the distillation process effectively preserves high imperceptibility.</p>\n\n",
            "<p class=\"ltx_p\">The robustness is evaluated under various distortions, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>. The tested distortions include: 1) no distortion (ND); 2) Gaussian noise (GN); 3) median filtering (MF); 4) MP3 compression at 64kbps (MP3); 5) low pass filtering with 4kHz stop band (LP); 6) down sampling to 8kHz (DS); 7) quantization to 9bit (QT); 8) amplitude scaling (AS); 9) echo addition with 100ms delay (EA); 10) slow speed to 0.9x (SS) ; 11) fast speed to 1.1x (FS); 12) reverberation attack with RT60 = 200ms (RA); 13) pitch up shifting by +10% (PU); 14) pitch down shifting by -10% (PD). Distortions 1) to 9) are basic distortions, while 10) to 14) are advanced attacks, which pose challenges even for some deep learning-based watermark methods<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib31\" title=\"\">31</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib32\" title=\"\">32</a>]</cite>. Two open-source models are selected for comparison: WavMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite> and AudioSeal <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib16\" title=\"\">16</a>]</cite>. To make a fair comparison, WavMark-16bps is trained at a 16bps bit rate, with the remaining settings kept the same as the original WavMark.</p>\n\n",
            "<p class=\"ltx_p\">The results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> demonstrate that PKDMark achieves an average BER of 0.51% across all various distortions, which is comparable to the teacher model&#8217;s average BER of 0.56%. Furthermore, PKDMark exhibits superior robustness compared to WavMark and WavMark-16bps across all test distortions, especially under advanced attacks. In particular, PKDMark consistently exceeds WavMark (8. 58%) and WavMark-16bps (2. 43%) in all types of distortions.</p>\n\n",
            "<p class=\"ltx_p\">To further evaluate watermark reliability, watermark speech detection is performed on a test dataset of 1000 TTS samples, each lasting between 1 and 2 seconds and embedded with an 8-bit synchronization code from the 16-bit capacity. The same distortions in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> are applied. A simple detection strategy is used: if at least 7 out of 8 bits match, the sample is classified as watermarked speech; otherwise, it is classified as original speech. The average detection F1 score reaches 99.6% across test distortions. These results confirm the robustness of our approach in preserving critical watermark message and ensuring reliable detection even under advanced manipulations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of speech generative models, unauthorized voice cloning poses significant privacy and security risks. Speech watermarking offers a viable solution for tracing sources and preventing misuse. Current watermarking technologies fall mainly into two categories: DSP-based methods and deep learning-based methods. DSP-based methods are efficient but vulnerable to attacks, whereas deep learning-based methods offer robust protection at the expense of significantly higher computational cost. To improve the computational efficiency and enhance the robustness, we propose PKDMark, a lightweight deep learning-based speech watermarking method that leverages progressive knowledge distillation (PKD). Our approach proceeds in two stages: (1) training a high-performance teacher model using an invertible neural network-based architecture, and (2) transferring the teacher&#8217;s capabilities to a compact student model through progressive knowledge distillation. This process reduces computational costs by 93.6% while maintaining high level of robust performance and imperceptibility. Experimental results demonstrate that our distilled model achieves an average detection F1 score of 99.6% with a PESQ of 4.30 in advanced distortions, enabling efficient speech watermarking for real-time speech synthesis applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "student",
                    "teacher",
                    "watermarking",
                    "model",
                    "pkdmark",
                    "methods",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of deep learning-based generative models, text-to-speech (TTS) and voice cloning technologies have reached unprecedented levels of realism. While these innovations enable high-fidelity speech synthesis, they also introduce significant challenges in copyright protection, content authentication, and privacy security. The increasing difficulty in distinguishing synthetic speech from natural speech raises concerns about misinformation, deepfake attacks, and unauthorized content reproduction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio watermarking has emerged as a proactive and effective solution by embedding traceable, imperceptible signals within speech, enabling robust identification and authentication <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib1\" title=\"\">1</a>]</cite>.\nTraditional audio watermarking methods relied on digital signal processing (DSP) to embed watermarks in time domain or transformation domains, demonstrating resilience against common distortions such as noise addition or lossy compression. However, their reliance on handcrafted features makes them susceptible to advanced manipulations, including reverberation and desynchronization attacks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib2\" title=\"\">2</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "speech",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, researchers have explored deep learning-based methods that enhance the imperceptibility, robustness, and capacity of audio watermarks. These methods generally fall into two categories: generative watermarking and post-hoc watermarking.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generative watermarking techniques directly embed watermarks into the speech synthesis process, ensuring that all generated speech inherently carries a watermark. While this approach eliminates the need for additional processing, the embedded watermark is model-specific and lacks adaptability across different synthesis models. For example, TraceableSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib3\" title=\"\">3</a>]</cite> embeds watermarking into a TTS model, ensuring high imperceptibility while remaining resilient to audio clipping attacks. Similarly, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib4\" title=\"\">4</a>]</cite> introduces a method for watermarking audio generative models at the latent representation level, ensuring robust detection. To enhance flexibility, HiFi-GANw <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib5\" title=\"\">5</a>]</cite> fine-tunes a HiFi-GAN vocoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib6\" title=\"\">6</a>]</cite> by jointly optimizing speech quality and watermark extraction loss, enabling robust watermark retrieval across various synthesis models. Building on the collaborative watermarking framework, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib7\" title=\"\">7</a>]</cite> enhances the robustness to both traditional and neural audio codecs by employing a HiFi-GAN vocoder and an anti-spoofing detector. In another generative framework, GROOT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib8\" title=\"\">8</a>]</cite> embeds watermarks during audio synthesis using diffusion models, achieving high extraction accuracy even under compound post-processing attacks.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, post-hoc watermarking applies watermarking techniques after speech synthesis, offering flexible integration but requiring additional processing. As a representative example, WavMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite> introduces an invertible neural network (INN)-based approach, allowing flexible watermark embedding and detection with high robustness. Extending this approach, DRAW <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib10\" title=\"\">10</a>]</cite> enhances resilience against desynchronization and replay attacks in a dual-decoder framework. While early approaches mainly ensured basic robustness, recent work increasingly targets advanced manipulations. DeAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib11\" title=\"\">11</a>]</cite> and DeepAWR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib12\" title=\"\">12</a>]</cite> employ deep learning to improve the robustness against re-recording distortions. To further mitigate manipulation attacks, a method proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib13\" title=\"\">13</a>]</cite> adopts an encoder-decoder framework in the discrete wavelet transform (DWT) domain. Additionally, a multi-scale transformer model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib14\" title=\"\">14</a>]</cite> extracts acoustic features to strengthen resistance against desynchronization. MaskMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib15\" title=\"\">15</a>]</cite> embeds a secret key vector through a multiplicative spectrogram mask to improve robustness against neural network-based transformations. Along with efforts on robustness, efficiency has also been explored. AudioSeal <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib16\" title=\"\">16</a>]</cite> employs an EnCodec<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib17\" title=\"\">17</a>]</cite>-based generator&#8211;detector architecture for fast, localized watermark detection at the sample level. By integrating psychoacoustic masking with a compact model design, SilentCipher <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib18\" title=\"\">18</a>]</cite> enhances message capacity and robustness while maintaining imperceptibility. Operating on discrete latent representations, DiscreteWM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib19\" title=\"\">19</a>]</cite> employs vector-quantized autoencoders to embed watermarks, enhancing robustness while offering flexible capacity and efficient detection.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "model",
                    "speech",
                    "audioseal",
                    "wavmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning&#8211;based speech watermarking offers strong robustness but often comes with high computational costs, limiting its practicality in real-time applications. In contrast, traditional DSP-based watermarking is computationally efficient but lacks resilience against sophisticated attacks. To enhance both efficiency and robustness, we propose PKDMark, a Progressive Knowledge Distillation (PKD) framework that combines the robustness of deep models with the efficiency of lightweight architectures, enabling practical deployment in real-world speech synthesis and transmission systems. The key contributions of the paper are summarized as follows:\n</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "models",
                    "pkdmark",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) The robustness of an INN-based speech watermarking system against advanced signal distortions is enhanced by introducing a complex domain message feature map and a multi-objective optimization strategy that jointly improves perceptual quality and decoding accuracy.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) A PKD-based method is introduced to transfer the watermarking capability from a teacher model with strong robustness and high perceptual quality. Through progressive distillation, the lightweight student model incrementally learns these capabilities, enabling efficient real-time watermark embedding.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "student",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) Furthermore, the proposed framework exhibits strong scalability and generalizability, offering a viable solution for a broad class of deep learning&#8211;based post hoc watermarking systems beyond speech synthesis.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">WavMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite> is a deep learning-based audio watermarking framework built on an invertible neural network (INN), enabling shared parameters for both watermark embedding and decoding. It encodes up to 32 bits per second while maintaining high imperceptibility and a low average bit error rate (BER) of 0.48% across ten types of attacks. WavMark further supports automatic localization, allowing precise watermark detection without external reference. Its robustness and domain versatility make it well-suited for voice cloning detection, copyright protection, and broadcast authentication.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "wavmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Knowledge distillation (KD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib20\" title=\"\">20</a>]</cite> is a well-known neural model compression method where a smaller student model is generally supervised by a larger teacher model to obtain a competitive performance.\nProgressive knowledge distillation (PKD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib21\" title=\"\">21</a>]</cite> is an extension of traditional knowledge distillation, designed to enhance the knowledge transfer process from teacher to student. Instead of transferring all knowledge at once, PKD gradually refines the student model over intermediate learning stages. This approach has been demonstrated to be effective in improving the performance of large language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib22\" title=\"\">22</a>]</cite>, image classification and generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib25\" title=\"\">25</a>]</cite> and information retrieval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib26\" title=\"\">26</a>]</cite>. Different from prior PKD studies, the proposed PKD approach explores knowledge distillation in the context of speech watermarking for the first time. By jointly optimizing both the teacher and student models, it enables more effective knowledge transfer and improves the student model&#8217;s ability to embed robust watermarks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "student",
                    "teacher",
                    "watermarking",
                    "model",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proposed method adopts an <span class=\"ltx_text ltx_font_italic\">encoder&#8211;attack&#8211;decoder</span> pipeline to enable robust speech watermarking, where an intermediate attack layer simulates real-world distortions to enforce resilience against manipulations. An overview of the proposed method framework is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The process begins by converting the input host speech signal <math alttext=\"x(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x(t)</annotation></semantics></math> into a complex-valued spectrogram <math alttext=\"X\\in\\mathbb{C}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8450;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{C}^{T\\times F}</annotation></semantics></math> via Short-Time Fourier Transform (STFT), where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> denotes the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins. A binary message <math alttext=\"m\\in\\{0,1\\}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">m\\in\\{0,1\\}^{K}</annotation></semantics></math> of length <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> is mapped and expanded to a complex feature map <math alttext=\"M\\in\\mathbb{C}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8712;</mo><msup><mi>&#8450;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">M\\in\\mathbb{C}^{T\\times F}</annotation></semantics></math>. The encoder network fuses <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> to generate a watermarked spectrogram <math alttext=\"X_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">X_{w}</annotation></semantics></math>, which is subsequently transformed back to the time domain using inverse STFT, yielding the watermarked speech signal <math alttext=\"x_{w}(t)=\\mathrm{ISTFT}(X_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>x</mi><mi>w</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ISTFT</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}(t)=\\mathrm{ISTFT}(X_{w})</annotation></semantics></math>. To simulate real&#8208;world distortions, an attack layer applies various of distortions before message decoding, ensuring watermark resilience against different types of manipulations. The distorted speech <math alttext=\"x_{a}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>a</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{a}(t)</annotation></semantics></math> is then transformed into a complex STFT spectrogram <math alttext=\"\\hat{X}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>^</mo></mover><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\hat{X}_{a}</annotation></semantics></math> and passed through the decoder to recover a decoded feature map <math alttext=\"\\hat{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m14\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>M</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{M}</annotation></semantics></math>. Finally, the decoded message <math alttext=\"\\hat{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m15\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>m</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{m}</annotation></semantics></math> is obtained by reducing <math alttext=\"\\hat{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m16\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>M</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{M}</annotation></semantics></math> along the temporal dimension and feeding the reduced vector through two feedforward layers.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this foundation, our main contribution is a two&#8208;stage training strategy that enhances robustness, imperceptibility, and efficiency. In the first stage, a teacher model is trained for end-to-end audio watermarking using an invertible neural network (INN), guided by a multi-objective optimization scheme. In the second stage, a lightweight student model is trained via progressive knowledge distillation (PKD), where supervision gradually transitions from the teacher&#8217;s guidance to the student&#8217;s own predictions. This design enables efficient watermark embedding while preserving the strong robustness and high perceptual quality of the teacher watermarking system. For watermark detection, the teacher decoder is retained to ensure accurate and reliable message recovery.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "student",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The teacher model is trained by jointly optimizing the encoder and decoder to balance imperceptibility and robustness, with an adversarial discriminator further enhancing speech fidelity. Compared to WavMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite>, our approach introduces a novel message feature map in the complex STFT domain, along with a multi-objective joint optimization strategy. By exploiting both amplitude and phase information, it alleviates distortions caused by phase&#8211;amplitude mismatches during ISTFT reconstruction. The message feature map embeds information in both the real and imaginary components, ensuring alignment with the time and frequency structures in the complex STFT domain of the original speech. Additionally, the multi-objective optimization ensures an optimal balance between imperceptibility and robustness, resulting in superior performance of the teacher model compared to WavMark.</p>\n\n",
                "matched_terms": [
                    "teacher",
                    "model",
                    "performance",
                    "speech",
                    "wavmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The teacher model utilizes an invertible neural network (INN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite> in which the encoder and decoder share parameters, ensuring strict reversibility in both forward and inverse processes. Training procedure follows an encoder-attack-decoder framework, incorporating a signal processor, message feature mapping, encoder, decoder, attack layer, and discriminator. The detailed modules and training flow are as follows.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Message Feature Map:</span>\nAn embedding layer maps each bit to a corresponding message embedding vector. A two-layer feedforward neural network transforms the embedding vectors to align with the frequency dimension <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> of complex STFT spectrogram <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> of the host speech. The hidden vectors are then summed and averaged, forming a compact representation <math alttext=\"\\mathbf{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m3\" intent=\":literal\"><semantics><mi>&#119841;</mi><annotation encoding=\"application/x-tex\">\\mathbf{h}</annotation></semantics></math>. Finally, the transformed representation is repeated along the time and complex axes, ensuring it matches the time-frequency dimension of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, resulting in a complex message feature map.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training of the teacher model, multiple loss terms are introduced and jointly optimized to balance imperceptibility, robustness, and decoding accuracy:</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce model size and improve inference efficiency, direct knowledge distillation (DKD) is initially explored by jointly training the student encoder with the teacher decoder. However, experiments (detailed in Section IV.E) show that this approach falls short of the teacher model&#8217;s performance, as it primarily transfers knowledge at the output layer while overlooking feature representation learning. To address this, we propose a progressive knowledge distillation (PKD) method in training, which gradually transferrs watermark knowledge from the teacher to the student model.</p>\n\n",
                "matched_terms": [
                    "student",
                    "teacher",
                    "dkd",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall PKD framework is illustrated in Figure 2. Specifically, the student encoder output <math alttext=\"x_{w}^{S}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>S</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{S}(t)</annotation></semantics></math> and the teacher encoder output <math alttext=\"x_{w}^{T}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>T</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{T}(t)</annotation></semantics></math> are linearly combined as follows:</p>\n\n",
                "matched_terms": [
                    "student",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> denotes the mixing factor of student model, with n representing the training step. The combined output <math alttext=\"X_{\\text{com}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>com</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{com}}</annotation></semantics></math> is decoded by the teacher decoder. During training, <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> gradually increases from 0 to 1. Initially, When <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 0, the training framework is equivalent to the teacher model training framework. Conversely, when <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 1, it corresponds to the direct knowledge distillation framework.</p>\n\n",
                "matched_terms": [
                    "student",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adopting a progressive strategy to increase <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> enables the student model to strike a better balance between perceptual quality and decoding robustness. In the early training stage, a smaller <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> ensures that gradient updates are primarily guided by the teacher network, providing a stable supervisory signal. This guidance helps the student network build a strong initial representation and prevents it from getting trapped in local optima.\nAs training progresses and the student network improves, gradually increasing <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> shifts the gradient updates to rely more on the student&#8217;s own outputs. This transition encourages the student model to improve its decoding performance by optimizing its own representations while still benefiting from subtle guidance from the teacher network. In the later stage, when <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 1, the student model has effectively learned the teacher&#8217;s representations and can be jointly optimized with the teacher decoder to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "student",
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The student model adopts the same attack layer, discriminator, message feature mapping, and adversarial loss as the teacher model. The main points of the joint optimization strategy are as follows:</p>\n\n",
                "matched_terms": [
                    "student",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perceptual Loss:</span>\nDifferent from the first stage, the student perceptual loss is computed as the MSE between the watermark speech of student model <math alttext=\"x_{w}^{S}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>S</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{S}(t)</annotation></semantics></math> and the watermark speech of teacher model <math alttext=\"x_{w}^{T}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>T</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{T}(t)</annotation></semantics></math>. This alignment ensures that the student model&#8217;s watermark embedding closely follows the teacher model&#8217;s representation in the time-frequency domain, enhancing perceptual similarity and embedding consistency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "student",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain training stability, the teacher encoder and decoder&#8217;s parameters are initially kept fixed, with updates applied only to the student encoder. Once the student model gradually aligns with the teacher&#8217;s performance as <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> reaches 1, a final global fine-tuning step is performed. This approach effectively leverages knowledge from both the teacher encoder and decoder, allowing the student model to converge efficiently while preserving speech quality.</p>\n\n",
                "matched_terms": [
                    "student",
                    "teacher",
                    "model",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation</span>.\nThe PKD framework begins with training a teacher model, followed by iterative PKD training to obtain a compact student model. The embedding capacity is set to 16 BPS, which is sufficient for speech synthesis applications. The teacher model comprises 8 INN blocks, each containing a 5-layer ResNet module with 32 channels per layer, while the student model is significantly smaller, with 2 INN blocks and 16 channels per layer. During PKD training, the mixing factor <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> is linearly increased from 0.1 to 1 over 40k steps.</p>\n\n",
                "matched_terms": [
                    "student",
                    "teacher",
                    "model",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics</span>.\nImperceptibility and robustness are assessed using both objective and subjective metrics. For imperceptibility, signal-to-noise ratio (SNR) quantifies watermarking distortion, while perceptual evaluation of speech quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib30\" title=\"\">30</a>]</cite> measures perceptual quality. Additionally, the subjective comparison mean opinion score (CMOS) captures human perception of quality differences. Robustness is evaluated using bit error rate (BER) to assess the reliability of extracted information and detection F1 score to measure accuracy in distinguishing between original and watermarked speech.</p>\n\n",
                "matched_terms": [
                    "watermarking",
                    "mean",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to AudioSeal, PKDMark achieves significantly better robustness under advanced distortions. While AudioSeal obtains perfect BERs (0.00%) in several basic distortions, its performance degrades drastically under more complex attacks such as time stretching and pitch shifting, resulting in a high average BER of 18.66%. In contrast, PKDMark maintains consistently low BERs in most distortion scenarios, highlighting its strong generalization capability under challenging conditions.</p>\n\n",
                "matched_terms": [
                    "audioseal",
                    "pkdmark",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S4.T2\" title=\"TABLE II &#8227; IV-E Ablation study &#8227; IV Experiments &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> provides a summary of the efficiency gains achieved by PKDMark in terms of computational cost and real-time performance. The computational cost is measured in floating point operations per second (FLOPS). The teacher model requires 36.0 GFLOPS, whereas PKDMark reduces this to 2.3 GFLOPS, resulting in a 93.6% reduction. Real-time performance is evaluated across both GPU and CPU configurations. All evaluations are conducted using sequential single-threaded processing, without leveraging parallel or multi-threaded inference. On an NVIDIA T4 GPU, PKDMark achieves a real-time factor (RTF) of 0.006, representing an 84.7% improvement over the teacher model&#8217;s RTF of 0.039. On an Intel i9-13900 CPU, PKDMark achieves an RTF of 0.020, marking an 87.1% improvement compared to the teacher model&#8217;s 0.155. These results highlight the efficiency and practical deployment potential of PKDMark for real-time speech synthesis across diverse hardware environments.</p>\n\n",
                "matched_terms": [
                    "teacher",
                    "model",
                    "pkdmark",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of the proposed components is assessed through a series of ablation experiments, with results summarized in Table 1.\nFor the teacher model, Teacher-woCMFM removes the proposed complex message feature map from the teacher model, resulting in an increased BER of 1.77% and reduced SNR of 42.25 dB. Teacher-woMSSL, which removes the multi-scale STFT loss, yields an increased BER of 1.23% and reduced SNR of 40.34 dB. These findings demonstrate that both the proposed complex feature map and multi-objective joint optimization are essential to improve the robustness and quality of the watermark.</p>\n\n",
                "matched_terms": [
                    "teacherwomssl",
                    "model",
                    "teacherwocmfm",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the distillation strategy, PKD Student (PKDMark) and DKD Student are trained under identical configurations, differing only in the distillation method. As shown in Table 1, PKD Student achieves superior performance in PESQ (4.30 vs. 4.08) and BER (0.51% vs. 1.94%). These results suggest that the PKD strategy enables more effective knowledge transfer, leading to a better balance between perceptual quality and watermark robustness.</p>\n\n",
                "matched_terms": [
                    "dkd",
                    "student",
                    "pkdmark",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose PKDMark, an efficient speech watermarking method for real-time speech synthesis via progressive knowledge distillation (PKD). Our approach enhances an invertible neural network (INN)-based teacher model to embed watermarks that preserve high fidelity and demonstrate robustness against 14 types of distortions and attacks. By transferring watermarking knowledge from the teacher model to a compact student model, we achieve a significant reduction of 93.6% in computational cost. Ablation studies further confirm that the proposed enhancements contribute to superior performance relative to the baseline model, with PKD outperforming the DKD approach in both perceptual quality and robustness. These results demonstrate the effectiveness of the proposed PKDMark method, showing that it achieves superior robustness and imperceptibility particularly under advanced attacks compared to the two speech watermarking methods evaluated in this study. Future work will focus on expanding watermark capacity and strengthening robustness to more advanced adversarial conditions, such as neural codec compression and neural denoising attacks.</p>\n\n",
                "matched_terms": [
                    "student",
                    "teacher",
                    "watermarking",
                    "dkd",
                    "model",
                    "pkdmark",
                    "methods",
                    "speech",
                    "performance"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Computational cost and real-time performance of PKDMark.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">GFLOPS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">RTF (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">T4 (GPU)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">i9-13900 (CPU)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Teacher Model</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.155</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PKDMark</th>\n<td class=\"ltx_td ltx_align_center\">2.3</td>\n<td class=\"ltx_td ltx_align_center\">0.006</td>\n<td class=\"ltx_td ltx_align_center\">0.020</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Reduction (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">93.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">84.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">87.1%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reduction",
            "teacher",
            "gflops",
            "cpu",
            "model",
            "computational",
            "i913900",
            "pkdmark",
            "gpu",
            "realtime",
            "cost",
            "↓downarrow",
            "rtf",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S4.T2\" title=\"TABLE II &#8227; IV-E Ablation study &#8227; IV Experiments &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> provides a summary of the efficiency gains achieved by PKDMark in terms of computational cost and real-time performance. The computational cost is measured in floating point operations per second (FLOPS). The teacher model requires 36.0 GFLOPS, whereas PKDMark reduces this to 2.3 GFLOPS, resulting in a 93.6% reduction. Real-time performance is evaluated across both GPU and CPU configurations. All evaluations are conducted using sequential single-threaded processing, without leveraging parallel or multi-threaded inference. On an NVIDIA T4 GPU, PKDMark achieves a real-time factor (RTF) of 0.006, representing an 84.7% improvement over the teacher model&#8217;s RTF of 0.039. On an Intel i9-13900 CPU, PKDMark achieves an RTF of 0.020, marking an 87.1% improvement compared to the teacher model&#8217;s 0.155. These results highlight the efficiency and practical deployment potential of PKDMark for real-time speech synthesis across diverse hardware environments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid advancement of speech generative models, unauthorized voice cloning poses significant privacy and security risks. Speech watermarking offers a viable solution for tracing sources and preventing misuse. Current watermarking technologies fall mainly into two categories: DSP-based methods and deep learning-based methods. DSP-based methods are efficient but vulnerable to attacks, whereas deep learning-based methods offer robust protection at the expense of significantly higher computational cost. To improve the computational efficiency and enhance the robustness, we propose PKDMark, a lightweight deep learning-based speech watermarking method that leverages progressive knowledge distillation (PKD). Our approach proceeds in two stages: (1) training a high-performance teacher model using an invertible neural network-based architecture, and (2) transferring the teacher&#8217;s capabilities to a compact student model through progressive knowledge distillation. This process reduces computational costs by 93.6% while maintaining high level of robust performance and imperceptibility. Experimental results demonstrate that our distilled model achieves an average detection F1 score of 99.6% with a PESQ of 4.30 in advanced distortions, enabling efficient speech watermarking for real-time speech synthesis applications.</p>\n\n",
                "matched_terms": [
                    "teacher",
                    "model",
                    "computational",
                    "pkdmark",
                    "realtime",
                    "cost",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning&#8211;based speech watermarking offers strong robustness but often comes with high computational costs, limiting its practicality in real-time applications. In contrast, traditional DSP-based watermarking is computationally efficient but lacks resilience against sophisticated attacks. To enhance both efficiency and robustness, we propose PKDMark, a Progressive Knowledge Distillation (PKD) framework that combines the robustness of deep models with the efficiency of lightweight architectures, enabling practical deployment in real-world speech synthesis and transmission systems. The key contributions of the paper are summarized as follows:\n</p>\n\n",
                "matched_terms": [
                    "computational",
                    "pkdmark",
                    "realtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) A PKD-based method is introduced to transfer the watermarking capability from a teacher model with strong robustness and high perceptual quality. Through progressive distillation, the lightweight student model incrementally learns these capabilities, enabling efficient real-time watermark embedding.</p>\n\n",
                "matched_terms": [
                    "model",
                    "realtime",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Knowledge distillation (KD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib20\" title=\"\">20</a>]</cite> is a well-known neural model compression method where a smaller student model is generally supervised by a larger teacher model to obtain a competitive performance.\nProgressive knowledge distillation (PKD) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib21\" title=\"\">21</a>]</cite> is an extension of traditional knowledge distillation, designed to enhance the knowledge transfer process from teacher to student. Instead of transferring all knowledge at once, PKD gradually refines the student model over intermediate learning stages. This approach has been demonstrated to be effective in improving the performance of large language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib22\" title=\"\">22</a>]</cite>, image classification and generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib25\" title=\"\">25</a>]</cite> and information retrieval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib26\" title=\"\">26</a>]</cite>. Different from prior PKD studies, the proposed PKD approach explores knowledge distillation in the context of speech watermarking for the first time. By jointly optimizing both the teacher and student models, it enables more effective knowledge transfer and improves the student model&#8217;s ability to embed robust watermarks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this foundation, our main contribution is a two&#8208;stage training strategy that enhances robustness, imperceptibility, and efficiency. In the first stage, a teacher model is trained for end-to-end audio watermarking using an invertible neural network (INN), guided by a multi-objective optimization scheme. In the second stage, a lightweight student model is trained via progressive knowledge distillation (PKD), where supervision gradually transitions from the teacher&#8217;s guidance to the student&#8217;s own predictions. This design enables efficient watermark embedding while preserving the strong robustness and high perceptual quality of the teacher watermarking system. For watermark detection, the teacher decoder is retained to ensure accurate and reliable message recovery.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The teacher model is trained by jointly optimizing the encoder and decoder to balance imperceptibility and robustness, with an adversarial discriminator further enhancing speech fidelity. Compared to WavMark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite>, our approach introduces a novel message feature map in the complex STFT domain, along with a multi-objective joint optimization strategy. By exploiting both amplitude and phase information, it alleviates distortions caused by phase&#8211;amplitude mismatches during ISTFT reconstruction. The message feature map embeds information in both the real and imaginary components, ensuring alignment with the time and frequency structures in the complex STFT domain of the original speech. Additionally, the multi-objective optimization ensures an optimal balance between imperceptibility and robustness, resulting in superior performance of the teacher model compared to WavMark.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The teacher model utilizes an invertible neural network (INN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#bib.bib9\" title=\"\">9</a>]</cite> in which the encoder and decoder share parameters, ensuring strict reversibility in both forward and inverse processes. Training procedure follows an encoder-attack-decoder framework, incorporating a signal processor, message feature mapping, encoder, decoder, attack layer, and discriminator. The detailed modules and training flow are as follows.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the training of the teacher model, multiple loss terms are introduced and jointly optimized to balance imperceptibility, robustness, and decoding accuracy:</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce model size and improve inference efficiency, direct knowledge distillation (DKD) is initially explored by jointly training the student encoder with the teacher decoder. However, experiments (detailed in Section IV.E) show that this approach falls short of the teacher model&#8217;s performance, as it primarily transfers knowledge at the output layer while overlooking feature representation learning. To address this, we propose a progressive knowledge distillation (PKD) method in training, which gradually transferrs watermark knowledge from the teacher to the student model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> denotes the mixing factor of student model, with n representing the training step. The combined output <math alttext=\"X_{\\text{com}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>com</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{com}}</annotation></semantics></math> is decoded by the teacher decoder. During training, <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> gradually increases from 0 to 1. Initially, When <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 0, the training framework is equivalent to the teacher model training framework. Conversely, when <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 1, it corresponds to the direct knowledge distillation framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adopting a progressive strategy to increase <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> enables the student model to strike a better balance between perceptual quality and decoding robustness. In the early training stage, a smaller <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> ensures that gradient updates are primarily guided by the teacher network, providing a stable supervisory signal. This guidance helps the student network build a strong initial representation and prevents it from getting trapped in local optima.\nAs training progresses and the student network improves, gradually increasing <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> shifts the gradient updates to rely more on the student&#8217;s own outputs. This transition encourages the student model to improve its decoding performance by optimizing its own representations while still benefiting from subtle guidance from the teacher network. In the later stage, when <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> = 1, the student model has effectively learned the teacher&#8217;s representations and can be jointly optimized with the teacher decoder to further enhance performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The student model adopts the same attack layer, discriminator, message feature mapping, and adversarial loss as the teacher model. The main points of the joint optimization strategy are as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perceptual Loss:</span>\nDifferent from the first stage, the student perceptual loss is computed as the MSE between the watermark speech of student model <math alttext=\"x_{w}^{S}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>S</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{S}(t)</annotation></semantics></math> and the watermark speech of teacher model <math alttext=\"x_{w}^{T}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>w</mi><mi>T</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{w}^{T}(t)</annotation></semantics></math>. This alignment ensures that the student model&#8217;s watermark embedding closely follows the teacher model&#8217;s representation in the time-frequency domain, enhancing perceptual similarity and embedding consistency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain training stability, the teacher encoder and decoder&#8217;s parameters are initially kept fixed, with updates applied only to the student encoder. Once the student model gradually aligns with the teacher&#8217;s performance as <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> reaches 1, a final global fine-tuning step is performed. This approach effectively leverages knowledge from both the teacher encoder and decoder, allowing the student model to converge efficiently while preserving speech quality.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation</span>.\nThe PKD framework begins with training a teacher model, followed by iterative PKD training to obtain a compact student model. The embedding capacity is set to 16 BPS, which is sufficient for speech synthesis applications. The teacher model comprises 8 INN blocks, each containing a 5-layer ResNet module with 32 channels per layer, while the student model is significantly smaller, with 2 INN blocks and 16 channels per layer. During PKD training, the mixing factor <math alttext=\"\\lambda(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(n)</annotation></semantics></math> is linearly increased from 0.1 to 1 over 40k steps.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate imperceptibility, synthesized speech samples from both the teacher and student models are compared against the original recordings. PESQ scores reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> indicate minimal quality differences between the teacher (4.33) and student (4.30) models. Furthermore, a CMOS test yields a score of -0.04 when comparing original recordings with student model samples, suggesting that PKDMark is nearly indistinguishable from the original speech. These results confirm that the distillation process effectively preserves high imperceptibility.</p>\n\n",
                "matched_terms": [
                    "pkdmark",
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19812v1#S3.T1\" title=\"TABLE I &#8227; III-C1 Progressive Knowledge Distillation &#8227; III-C Student Model &#8227; III Method &#8227; Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> demonstrate that PKDMark achieves an average BER of 0.51% across all various distortions, which is comparable to the teacher model&#8217;s average BER of 0.56%. Furthermore, PKDMark exhibits superior robustness compared to WavMark and WavMark-16bps across all test distortions, especially under advanced attacks. In particular, PKDMark consistently exceeds WavMark (8. 58%) and WavMark-16bps (2. 43%) in all types of distortions.</p>\n\n",
                "matched_terms": [
                    "pkdmark",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to AudioSeal, PKDMark achieves significantly better robustness under advanced distortions. While AudioSeal obtains perfect BERs (0.00%) in several basic distortions, its performance degrades drastically under more complex attacks such as time stretching and pitch shifting, resulting in a high average BER of 18.66%. In contrast, PKDMark maintains consistently low BERs in most distortion scenarios, highlighting its strong generalization capability under challenging conditions.</p>\n\n",
                "matched_terms": [
                    "pkdmark",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effectiveness of the proposed components is assessed through a series of ablation experiments, with results summarized in Table 1.\nFor the teacher model, Teacher-woCMFM removes the proposed complex message feature map from the teacher model, resulting in an increased BER of 1.77% and reduced SNR of 42.25 dB. Teacher-woMSSL, which removes the multi-scale STFT loss, yields an increased BER of 1.23% and reduced SNR of 40.34 dB. These findings demonstrate that both the proposed complex feature map and multi-objective joint optimization are essential to improve the robustness and quality of the watermark.</p>\n\n",
                "matched_terms": [
                    "model",
                    "teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the distillation strategy, PKD Student (PKDMark) and DKD Student are trained under identical configurations, differing only in the distillation method. As shown in Table 1, PKD Student achieves superior performance in PESQ (4.30 vs. 4.08) and BER (0.51% vs. 1.94%). These results suggest that the PKD strategy enables more effective knowledge transfer, leading to a better balance between perceptual quality and watermark robustness.</p>\n\n",
                "matched_terms": [
                    "pkdmark",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose PKDMark, an efficient speech watermarking method for real-time speech synthesis via progressive knowledge distillation (PKD). Our approach enhances an invertible neural network (INN)-based teacher model to embed watermarks that preserve high fidelity and demonstrate robustness against 14 types of distortions and attacks. By transferring watermarking knowledge from the teacher model to a compact student model, we achieve a significant reduction of 93.6% in computational cost. Ablation studies further confirm that the proposed enhancements contribute to superior performance relative to the baseline model, with PKD outperforming the DKD approach in both perceptual quality and robustness. These results demonstrate the effectiveness of the proposed PKDMark method, showing that it achieves superior robustness and imperceptibility particularly under advanced attacks compared to the two speech watermarking methods evaluated in this study. Future work will focus on expanding watermark capacity and strengthening robustness to more advanced adversarial conditions, such as neural codec compression and neural denoising attacks.</p>\n\n",
                "matched_terms": [
                    "reduction",
                    "teacher",
                    "model",
                    "computational",
                    "pkdmark",
                    "realtime",
                    "cost",
                    "performance"
                ]
            }
        ]
    }
}