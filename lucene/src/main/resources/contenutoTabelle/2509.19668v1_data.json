{
    "S6.T1": {
        "source_file": "Selective Classifier-free Guidance for Zero-shot Text-to-speech",
        "caption": "Table 1: Comparison of state-of-the-art zero-shot TTS models. Italicized results are experimentally obtained; others are reported.",
        "body": "Model\nLibriSpeech\nSeed-TTS-en\nSeed-TTS-zh\n\n\n\nSIM\nWER\nSIM\nWER\nSIM\nWER\n\n\nF5-TTS (Base)\n0.675\n0.020\n0.679\n0.018\n0.763\n0.017\n\n\nF5-TTS (def_text)\n0.682\n0.022\n0.690\n0.018\n0.764\n0.019\n\n\nCosyVoice 2 (Base)\n0.661\n0.025\n0.660\n0.024\n0.753\n0.017\n\n\nCosyVoice 2 (input_text)\n0.671\n0.025\n0.666\n0.026\n0.763\n0.018\n\n\n\nMinimax-Speech [7]\n\n\n\n0.738\n0.019\n0.799\n0.010\n\n\n\nSeed-TTS [23]\n\n\n\n0.762\n0.022\n0.796\n0.011\n\n\n\nCosyVoice 3-1.5B [5]\n\n\n\n0.720\n0.022\n0.781\n0.012",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">LibriSpeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Seed-TTS-en</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Seed-TTS-zh</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS (Base)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.675</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.020</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.679</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.018</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.763</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.017</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS (def_text)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.682</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.690</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.018</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.764</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.019</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2 (Base)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.661</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.025</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.660</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.753</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.017</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2 (input_text)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.671</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.025</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.666</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.026</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.763</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.018</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Minimax-Speech&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.738</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.019</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.799</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.010</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Seed-TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.762</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.796</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.011</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 3-1.5B&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_border_b ltx_border_r\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.720</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.781</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.012</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "deftext",
            "cosyvoice",
            "seedtts",
            "librispeech",
            "sim",
            "f5tts",
            "seedttsen",
            "stateoftheart",
            "315b",
            "tts",
            "zeroshot",
            "base",
            "wer",
            "experimentally",
            "results",
            "seedttszh",
            "obtained",
            "model",
            "reported",
            "models",
            "others",
            "inputtext",
            "comparison",
            "minimaxspeech",
            "italicized"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Even though our proposed CFG strategy can improve SIM for F5-TTS and CosyVoice 2, these improvements do not completely close the gap of state-of-the-art results reported by other closed-source models as listed in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S6.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 6 Discussion &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, given that our methods can find significant improvements with only an inference-time hyperparameter sweep, they are added gains without difficulties of training large models or collecting large, high-quality datasets.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In zero-shot text-to-speech, achieving a balance between fidelity to the target speaker and adherence to text content remains a challenge. While classifier-free guidance (CFG) strategies have shown promising results in image generation, their application to speech synthesis are underexplored. Separating the conditions used for CFG enables trade-offs between different desired characteristics in speech synthesis. In this paper, we evaluate the adaptability of CFG strategies originally developed for image generation to speech synthesis and extend separated-condition CFG approaches for this domain. Our results show that CFG strategies effective in image generation generally fail to improve speech synthesis. We also find that we can improve speaker similarity while limiting degradation of text adherence by applying standard CFG during early timesteps and switching to selective CFG only in later timesteps. Surprisingly, we observe that the effectiveness of a selective CFG strategy is highly text-representation dependent, as differences between the two languages of English and Mandarin can lead to different results even with the same model.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Classifier-free guidance (CFG)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a key component of iteratively denoising generative models such as diffusion or flow matching.\nFlow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;originally tested in image generation&#8212;has been used successfully in zero-shot text-to-speech (TTS) beginning with Voicebox&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Other state-of-the-art (SOTA) zero-shot TTS models continue to utilize flow matching such as F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MegaTTS 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Minimax-Speech &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "models",
                    "tts",
                    "zeroshot",
                    "f5tts",
                    "stateoftheart",
                    "minimaxspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Techniques inspired by CFG are also used in both diffusion and flow matching&#8211;based image generation models, such as weight schedules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, different types of negative prompting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, modifications to the CFG algorithm&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or converting CFG to training-time target modification&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The last technique of training-time target modification has been already been applied to speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, but the other methods have not. We reduce this gap in the literature by evaluating several weight schedules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the methods proposed in CFG-Zero* on a zero-shot TTS model.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, we find that model and language differences affect which CFG strategies are effective. Our proposed strategy is effective for F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in English, but for Mandarin with the same model, selective CFG does not improve speaker similarity. With CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we find that selective CFG simply improves speaker similarity without any degradation in text adherence, so our proposed strategy is unnecessary.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot TTS is a type of voice cloning restricted to only a single sample of the speaker&#8217;s voice, optionally also including a transcript of the sample. Zero-shot TTS is one of the most common forms of controllable TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and can be further combined with other functionalities such as emotion control or pitch control.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Two standard objective metrics for evaluating zero-shot TTS systems are similarity score (SIM) and word error rate (WER). SIM is measured by using a speaker verification system such as WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract speaker embeddings from both the reference audio and the generated audio, with the cosine similarity between the two embeddings reported as SIM. WER is calculated by comparing the input text with the results of an automatic speech recognition (ASR) model on the generated audio and calculating the percentage of words that are added, removed, or substituted. SIM and WER are useful metrics for zero-shot TTS systems as they individually measure adherence to the reference audio and the input text, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "reported",
                    "model",
                    "tts",
                    "zeroshot",
                    "sim",
                    "wer",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Classifier-free guidance (CFG)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, originally introduced in diffusion models, involve amplifying the difference between a conditioned and unconditioned prediction to increase the effect of the conditioning. Given </span>\n  <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#955;</mi>\n      <annotation encoding=\"application/x-tex\">\\lambda</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the constant CFG weight, it can be implemented as replacing the conditioned model prediction </span>\n  <math alttext=\"\\epsilon(x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#1013;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\epsilon(x_{t},c)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the following:</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Perp-Neg&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and CFG-Zero*&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> both calculate the perpendicular component between the two prediction vectors instead of just the difference between them. Perp-Neg uses the component of the negative prediction perpendicular to the positive prediction, while CFG-Zero* uses the component of the positive prediction perpendicular to the unconditioned prediction. (CFG-Zero* uses a different formalization, but the resulting algorithm is identical to what we describe.) CFG-Zero* also suggests the zero-init strategy, where ignoring the first few update steps during flow matching (i. e. beginning flow matching from </span>\n  <math alttext=\"t&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t&gt;0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> while still using pure noise as the starting point) may improve generation results, especially for underfitted models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We investigate two strong (SOTA or near-SOTA) open weight models, F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We selected these models primarily based on availability, as many other state-of-the-art zero-shot TTS models are not publicly available (most notably Seed-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). In addition, these models represent the two most popular flow-matching TTS paradigms: purely non-autoregressive flow matching and flow matching on autoregressively generated speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "models",
                    "tts",
                    "seedtts",
                    "zeroshot",
                    "f5tts",
                    "stateoftheart"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS is based on E2-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, primarily trained with the open YouTube-based dataset Emilia&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and utilizing an updated architecture and sampling strategy compared to E2-TTS. The model utilizes input audio and input text as conditioning, where the input audio is an audio clip of the voice to be cloned, and the input text is the transcript of the input audio concatenated with the desired speech text. The best-performing released checkpoint of this model has 336 million parameters. The version of the model included in the original paper has 0.66 SIM score and 0.024 WER on LibriSpeech, but later updates to the model improve it to 0.676 SIM and 0.020 WER. We consider the best iteration of this model as open-weight because the exact training or fine-tuning methodology of the updated model checkpoint is unpublished. The model uses a text embedder and a Diffusion Transformer backbone with 4.3 and 332 million parameters, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech",
                    "sim",
                    "f5tts",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the second generation of CosyVoice models. There is a newer CosyVoice 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by the same team, but as CosyVoice 3 is not public, we use the older CosyVoice 2 for our experiments. CosyVoice 2 utilizes a pre-trained LLM, Qwen2.5-0.5B, that has been fine-tuned to autoregressively generate the intermediate representations of an ASR model based on the input text. The intermediate representations of the ASR model, referred to as semantic tokens, capture the text information. The reference audio and transcript are processed into a speaker embedding. The semantic tokens and speaker embedding are used as input to a 71 million&#8211;parameter flow matching model.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This biases inference towards smaller updates at the start, when noise levels are high. F5-TTS defaults to 32 timesteps while CosyVoice 2 defaults to 10 timesteps. Both models perform flow matching with mel-spectrograms, and the generated mel-spectrogram is then decoded by a vocoder into audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "f5tts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The second condition we propose is def_text, which uses standard CFG for early timesteps below a certain threshold </span>\n  <math alttext=\"t_{threshold}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{threshold}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and uses the same strategy as input_text for all timesteps above </span>\n  <math alttext=\"t_{threshold}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{threshold}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This design is motivated by listening to an extrapolated generation at each timestep, where the extrapolated signal is defined as </span>\n  <math alttext=\"x_{t}+(1-t)\\epsilon(x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">&#1013;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">x</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">c</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">x_{t}+(1-t)\\epsilon(x_{t},c)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We observe that the words become audible very quickly, at around 6 timesteps with </span>\n  <math alttext=\"t\\approx 0.04\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8776;</mo>\n        <mn mathsize=\"0.900em\">0.04</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t\\approx 0.04</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We hypothesize that CFG for text adherence may not be necessary after the initial steps, since the conditioned model has already captured sufficient text information. The timestep-dependent nature of conditioning has also been reported in previous work on negative prompting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We evaluated candidate values of </span>\n  <math alttext=\"t_{threshold}\\in\\{0.02,0.04,0.06,0.08\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">e</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">l</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">0.02</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">0.04</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">0.06</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">0.08</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t_{threshold}\\in\\{0.02,0.04,0.06,0.08\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on F5-TTS and found that </span>\n  <math alttext=\"t_{threshold}=0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">e</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">l</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.08</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t_{threshold}=0.08</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves a good balance between improved SIM while minimizing WER increases. </span>\n  <math alttext=\"t_{threshold}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">h</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{threshold}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is between the 9th and 10th timesteps for F5-TTS and the 3rd and 4th timesteps for CosyVoice 2. The input_text and def_text conditions are evaluated using both F5-TTS and CosyVoice 2 on LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the English and Mandarin subsets of Seed-TTS-eval&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "reported",
                    "deftext",
                    "model",
                    "cosyvoice",
                    "librispeech",
                    "sim",
                    "f5tts",
                    "inputtext",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We define a third condition input_audio which, similarly to input_text, replaces the unconditioned prediction with a prediction partially conditioned on the input audio. This is only evaluated using F5-TTS on LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> due to its poor performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "inputtext",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Seed-TTS-eval&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use the provided English and Mandarin cross-sentence prompt lists.\nFor LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use the 1127-sample prompt list provided by F5-TTS. Experiments with F5-TTS follow the original evaluation protocol of taking the average of three seeded trials&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, but differences between seeds are insignificant. For experiments with CosyVoice 2 we perform only one trial.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "librispeech",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluate the weight schedules proposed by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;namely the linearly increasing schedule and the clamped-minimum linearly increasing schedule&#8212;as well as the perpendicular reweighting and zero-init strategies introduced in CFG-Zero*&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using F5-TTS on the LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For CosyVoice 2, </span>\n  <math alttext=\"c_{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p7.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">k</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{spk}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is treated as the speaker embedding and </span>\n  <math alttext=\"c_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p7.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the output semantic tokens of the Qwen2.5-0.5B model.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we find that input_audio does not have lower WER. However, input_text does achieve a trade-off of higher SIM at the cost of worse WER.</span>\n</p>\n\n",
                "matched_terms": [
                    "inputtext",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We find that def_text with a threshold </span>\n  <math alttext=\"t_{threshold}=0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">e</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">l</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.08</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t_{threshold}=0.08</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves a good balance of increasing SIM with minimal impact to WER, as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This approach also works for the English subset of Seed-TTS-eval&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, using the exact same model, neither input_text nor def_text improve SIM for the Mandarin dataset and only increases WER, as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F6\" style=\"font-size:90%;\" title=\"Figure 6 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">6</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "deftext",
                    "model",
                    "sim",
                    "inputtext",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">From Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F7\" style=\"font-size:90%;\" title=\"Figure 7 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">7</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we observe that with CosyVoice 2, the input_text condition does not degrade WER as it did with F5-TTS. We also did not observe significant language differences between English and Mandarin, unlike F5-TTS. Also we note that while CosyVoice 2 defaults to a CFG strength of 0.7, higher SIM scores appear to be achieved at CFG strength of around 1.0.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "sim",
                    "f5tts",
                    "inputtext",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We observed that the tested CFG strategies developed for image generation, namely weight schedules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, perpendicular reweighting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and zero-init&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, do not generalize well to zero-shot TTS with F5-TTS as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This could be due to differences in conditioning and output modality. We suggest that future research assess the contribution of these techniques across different target modalities, such as various audio codecs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Language differences have an impact on CFG strategy effectiveness for F5-TTS, but not for CosyVoice 2. This may be due to differences in text representation. CosyVoice 2 uses a 506 million&#8211;parameter LLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, so generated semantic tokens allow the model to achieve strong text adherence even without CFG as seen in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#S5.F7\" style=\"font-size:90%;\" title=\"Figure 7 &#8227; 5 Results &#8227; Selective Classifier-free Guidance for Zero-shot Text-to-speech\">\n    <span class=\"ltx_text ltx_ref_tag\">7</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. F5-TTS relies on a much smaller 4.3 million&#8211;parameter ConvNeXt 2 module to process text, which may result in English and Chinese text acting as different conditioning modalities. However, the language differences do not arise in WER but rather through a failure to improve SIM when using either input_text or def_text conditions, which is not where language differences may be expected to arise. Another possible cause of the language difference for F5-TTS is training methodology or dataset differences, but as the training procedure for the best-performing checkpoint is not published, this remains speculation.</span>\n</p>\n\n",
                "matched_terms": [
                    "deftext",
                    "model",
                    "cosyvoice",
                    "sim",
                    "f5tts",
                    "inputtext",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Here, we confirm prior results&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that a trade-off between speaker similarity and text adherence can be achieved by separately emphasizing the speaker conditioning with CFG. We show that by starting with regular CFG and switching to selective CFG after the several timesteps, as inspired by prior investigations into negative prompting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, may minimize WER increases while still improving SIM. However, we demonstrate that the efficacy of separated-condition CFG depends on both the language and the model being used.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), Discovery Grant RGPIN-2024-04966. The authors have no other relevant financial interests to disclose. This study was conducted using code and model checkpoints from F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, code and model checkpoints from CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Seed-TTS-eval&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19668v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> under public domain, share-alike, or non-commercial licenses.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "model",
                    "librispeech",
                    "cosyvoice"
                ]
            }
        ]
    }
}