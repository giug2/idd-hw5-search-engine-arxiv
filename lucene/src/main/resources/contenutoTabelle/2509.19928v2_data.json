{
    "S2.T1": {
        "caption": "Table 1: \nCorrelation matrix of average Pearson coefficients (r¯\\bar{r}) between human ratings and objective metrics, aggregated across groups via Fisher’s ZZ transformation.\nValues in brackets denote 95% confidence intervals, computed in the Fisher space and back-transformed.\nDarker shades of green indicate stronger correlations.\nAll correlations are statistically significant at p<0.001p<0.001.\nThe strongest correlation with human ratings is highlighted in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PMOS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">log </span><math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">F</mi><mn mathsize=\"0.900em\">0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PMOS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6FFE6;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6FFE6;\">0.30<span class=\"ltx_text\" style=\"font-size:56%;\">[0.19,&#8196;0.40]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#BFFFBF;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#BFFFBF;\">0.66<span class=\"ltx_text\" style=\"font-size:56%;\">[0.58,&#8196;0.73]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#8CFF8C;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#8CFF8C;\">0.77<span class=\"ltx_text ltx_font_medium\" style=\"font-size:56%;\">[0.73,&#8196;0.81]</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">log </span><math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">F</mi><mn mathsize=\"0.900em\">0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#E6FFE6;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6FFE6;\">0.30<span class=\"ltx_text\" style=\"font-size:56%;\">[0.19,&#8196;0.40]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#D9FFD9;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D9FFD9;\">0.35<span class=\"ltx_text\" style=\"font-size:56%;\">[0.25,&#8196;0.44]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#D9FFD9;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D9FFD9;\">0.36<span class=\"ltx_text\" style=\"font-size:56%;\">[0.26,&#8196;0.45]</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#BFFFBF;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#BFFFBF;\">0.66<span class=\"ltx_text\" style=\"font-size:56%;\">[0.58,&#8196;0.73]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#D9FFD9;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D9FFD9;\">0.35<span class=\"ltx_text\" style=\"font-size:56%;\">[0.25,&#8196;0.44]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"--ltx-bg-color:#80FF80;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#80FF80;\">0.82<span class=\"ltx_text\" style=\"font-size:56%;\">[0.74,&#8196;0.87]</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#8CFF8C;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#8CFF8C;\">0.77<span class=\"ltx_text ltx_font_medium\" style=\"font-size:56%;\">[0.73,&#8196;0.81]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#D9FFD9;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D9FFD9;\">0.36<span class=\"ltx_text\" style=\"font-size:56%;\">[0.26,&#8196;0.45]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#80FF80;padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#80FF80;\">0.82<span class=\"ltx_text\" style=\"font-size:56%;\">[0.74,&#8196;0.87]</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "pmos",
            "mcd",
            "significant",
            "r¯barr",
            "matrix",
            "pearson",
            "f0f0",
            "strongest",
            "darker",
            "shades",
            "transformation",
            "green",
            "stronger",
            "correlations",
            "coefficients",
            "statistically",
            "average",
            "log",
            "objective",
            "backtransformed",
            "computed",
            "between",
            "dswed",
            "across",
            "indicate",
            "metrics",
            "denote",
            "bold",
            "highlighted",
            "aggregated",
            "confidence",
            "groups",
            "values",
            "via",
            "correlation",
            "intervals",
            "fisher",
            "ratings",
            "p0001p0001",
            "fisher’s",
            "space",
            "all",
            "human",
            "brackets",
            "rmse"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PMOS exhibits the strongest correlation with DS-WED (</span>\n  <math alttext=\"\\bar{r}=0.77\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.77</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.77</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), followed by a substantial correlation with MCD (</span>\n  <math alttext=\"\\bar{r}=0.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.66</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe correlation with log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE is weaker but still significant (</span>\n  <math alttext=\"\\bar{r}=0.30\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.30</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.30</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), suggesting that pitch deviations contribute to perceived differences but capture only part of the variability.\nOverall, these results demonstrate that DS-WED aligns most closely with subjective human judgments, substantially outperforming widely used acoustic metrics.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS).\nHowever, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored.\nTo bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics.\nProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings.\nBuilding on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens.\nExperiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM.\nLeveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations.\nAudio samples are available at https://prosodyeval.github.io.</span>\n</p>\n\n",
                "matched_terms": [
                    "pmos",
                    "correlation",
                    "ratings",
                    "metrics",
                    "objective",
                    "human",
                    "dswed",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody plays a central role in spoken communication, conveying paralinguistic information such as emotion, attitude, and intent that shapes how listeners interpret meaning. Subtle variations in pitch, intensity, and duration can fundamentally alter the interpretation of an utterance even with identical text, making prosody crucial to naturalness and expressiveness in speech synthesis.\nWhile zero-shot TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced in intelligibility and speaker similarity, prosody diversity has received far less attention.\nExisting evaluation practices&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rely largely on log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> root mean squared error (RMSE), which correlates weakly with human judgments, captures pitch but neglects rhythm and intensity&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and requires costly dynamic time warping (DTW).\nConsequently, a clear gap remains between automatic metrics and human perception of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "f0f0",
                    "log",
                    "human",
                    "rmse",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent efforts have shown that both continuous&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and discrete&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> self-supervised (SSL) speech representations effectively encode prosodic information.\nConcurrently, NLP automatic evaluation metrics have been adapted to discrete speech representations for reference-free&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and reference-aware&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech quality assessment. In particular, SpeechTokenDistance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a simple approach but remains preliminary, as it focuses solely on measuring correlation with acoustic metrics without examining what the differences represent or further analyzing correlations with human perception.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "correlation",
                    "metrics",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With this perspective in mind, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ProsodyEval</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the first human-annotated dataset for prosody diversity assessment, and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">iscretized </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eighted </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">dit </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">istance), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic token sequences derived from speech SSL models such as HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-kmeans clustering.\nProsodyEval contains 1000 synthetic samples from 7 mainstream TTS systems, paired with 2000 human ratings of prosody diversity.\nExperiments show that DS-WED achieves substantially higher correlation with human judgments than frequently used acoustic metrics, including log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and Mel cepstral distortion (MCD), while remaining robust across models, layers, and cluster sizes.\nBuilding on DS-WED, we establish the first systematic benchmark of prosody diversity across state-of-the-art open-source TTS systems on LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur further explorations reveal the impact of modeling paradigms, duration control, and reinforcement learning (RL), and demonstrate that even advanced large audio language models (LALMs) like Gemini 2.5 Pro remain unreliable for prosody evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "mcd",
                    "ratings",
                    "metrics",
                    "f0f0",
                    "log",
                    "objective",
                    "human",
                    "rmse",
                    "dswed",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We propose DS-WED, a new objective prosody diversity metric based on semantic token weighted edit distance that better correlates with human ratings than existing acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "ratings",
                    "metrics",
                    "objective",
                    "human",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Each system generates a group of five samples per input with random seeds from 0 to 4, using prompt speech and text from LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo exclude synthesis errors, we filter all groups to retain only those in which every sample is subjectively perceived as word-by-word aligned with the text.\nIn total, ProsodyEval contains 1000 synthetic speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Listening Test Design</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;All MOS tests are conducted online under controlled conditions. After a short training session, all raters are instructed to perform the assessments in a quiet environment.\nIn each trial, a group of five audio samples generated by the same system and speaker is presented. Raters perform pairwise comparisons across all ten possible pairs within the group in random order. Each pair is rated on a five-point Likert scale according to the perceived prosodic difference.\nTo aid judgment, both audio playback and corresponding waveform visualization are provided.\nRaters are encouraged to replay samples as needed, enabling them to refine their judgments and capture both subtle and pronounced differences across pairs.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ProsodyEval incorporates two frequently used acoustic metrics, namely log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD.\nFor each pair of samples within a group, FastDTW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a linear-time approximation of DTW, is first applied to align their lengths, after which log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD are computed.\nTogether, these two metrics serve as complementary baseline indicators for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "metrics",
                    "f0f0",
                    "log",
                    "computed",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate correlations between human ratings and the three objective metrics DS-WED, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE, and MCD on the ProsodyEval dataset across all groups.\nFor DS-WED, speech is discretized by applying a 50-cluster </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means model trained on LibriSpeech 960h to the hidden embeddings from the 8th Transformer encoder layer.\nSince PMOS ratings are relative within groups and not comparable across groups, we compute Pearson correlation for each group and then aggregate the group-wise correlations using Fisher&#8217;s </span>\n  <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Z</mi>\n      <annotation encoding=\"application/x-tex\">Z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformation.\nStatistical significance is tested with a two-sided one-sample </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test against zero, and 95% confidence intervals are obtained in the Fisher space and back-transformed.</span>\n</p>\n\n",
                "matched_terms": [
                    "pmos",
                    "mcd",
                    "pearson",
                    "f0f0",
                    "transformation",
                    "correlations",
                    "log",
                    "objective",
                    "backtransformed",
                    "between",
                    "dswed",
                    "across",
                    "metrics",
                    "confidence",
                    "groups",
                    "correlation",
                    "intervals",
                    "fisher",
                    "ratings",
                    "fisher’s",
                    "space",
                    "all",
                    "human",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Computational efficiency is measured on ProsodyEval by Real-Time Factor (RTF), computed as processing time divided by average speech-pair duration, on NVIDIA A100 with batch size 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE reaches an RTF of 0.549, and MCD reaches an RTF of 0.203. Both rely on signal-processing front-ends and DTW alignment, which are CPU-bound and difficult to accelerate on GPUs. In addition, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE requires mel-cepstrum computation for DTW, adding extra overhead.\nIn contrast, DS-WED involves only a forward pass through a pretrained speech-SSL encoder followed by </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering and edit distance at the discrete level, achieving an RTF of 0.110. It is GPU-friendly and can be further accelerated by batching.\nOverall, DS-WED is scalable for large-scale evaluation and practical for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "f0f0",
                    "log",
                    "rmse",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We study the effect of the SSL backbone, the encoder layer, and the number of </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clusters on the correlation between DS-WED and human ratings, using the ProsodyEval dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "ratings",
                    "human",
                    "dswed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 4.3 Ablation Studies of DS-WED &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that DS-WED remains quite robust across different layers, models, and vocabulary sizes, with correlations consistently around 0.7.\nMiddle layers 6-9 achieve stronger correlations, consistent with the richer encoded prosody information.\nRelative smaller cluster sizes perform best, while larger ones reduce peak correlations, making the edit-distance calculation overly sensitive and misaligned with human perceptual sensitivity to prosody.\nOverall, WavLM-base provides more stable correlations, while HuBERT-base exhibits larger variance. The 8th layer of HuBERT-base using 50 clusters achieves the strongest correlations.\nWe also tried the large versions, which yield slightly higher correlations but at much greater cost, hence we use the base versions for all experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "across",
                    "all",
                    "human",
                    "dswed",
                    "strongest",
                    "stronger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Our evaluations are conducted on two widely used benchmarks: professionally read audiobooks LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and crowdsourced read speech Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo measure prosody diversity, we report two conventional acoustic metrics, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD, together with our proposed DS-WED. Each metric is computed in two ways: (1) micro average across all samples, and (2) rank-based Borda aggregation, where systems are ranked within each group and assigned scores from seven for the best system to one for the worst, and the average score across groups is reported, thereby eliminating the influence of absolute metric values.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "mcd",
                    "average",
                    "metrics",
                    "f0f0",
                    "log",
                    "all",
                    "computed",
                    "rmse",
                    "dswed",
                    "groups",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All NAR systems are trained on the same 100k-hour Emilia corpus, yet MaskGCT performs best in DS-WED while F5-TTS ranks lowest.\nMoreover, XTTS-v2, trained on less than one-third of the data, still shows superior prosody diversity, suggesting that modeling paradigm rather than data scale is the dominant factor.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Can large audio language models (LALMs) serve as reliable evaluators of prosodic differences?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe test Gemini 2.5 Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by prompting it to rate the relative prosodic difference between two samples within a group of five. It listens to all five samples to form a reference range of variation and then assigns a score from 1 to 5.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Gemini&#8217;s scores show a statistically significant but weak correlation with human ratings, while correlations with objective metrics fluctuate with wide confidence intervals.\nCombined with high prompt sensitivity, these findings suggest that Gemini 2.5 Pro is not a reliable evaluator of prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "correlation",
                    "intervals",
                    "statistically",
                    "significant",
                    "ratings",
                    "metrics",
                    "objective",
                    "all",
                    "human",
                    "between",
                    "confidence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we introduce ProsodyEval, the first human-annotated dataset for assessing prosody diversity in zero-shot TTS, together with DS-WED, a new objective diversity metric based on weighted edit distance over semantic tokens.\nDS-WED outperforms frequently used acoustic metrics in correlating with human judgments and remains robust in speech discretization across models, layers, and cluster sizes, enabling more reliable evaluation of prosodic variation.\nLeveraging DS-WED, we systematically benchmark and analyze mainstream zero-shot TTS systems.\nOur findings indicate that (1) AR systems outperform NAR flow-matching models but not MGM, (2) duration variation strongly shapes prosody diversity, yet NAR systems lack duration control, and flow-matching models with implicit alignment suffer from inherent architectural limitations, (3) RL via DPO improves intelligibility while reducing prosody diversity, and (4) even advanced LALMs like Gemini 2.5 Pro remain unreliable for prosody evaluation.\nOne limitation lies in the cross-lingual applicability of DS-WED, validated only on English.\nLooking ahead, ProsodyEval and DS-WED fill a gap in zero-shot TTS evaluation and open new avenues for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "indicate",
                    "metrics",
                    "objective",
                    "human",
                    "dswed",
                    "via"
                ]
            }
        ]
    },
    "S2.T2": {
        "caption": "Table 2: \nProsody diversity benchmark of zero-shot TTS systems from diverse generative paradigms on LibriSpeech test-clean and Seed-TTS test-en, using traditional acoustic metrics log F0F_{0} RMSE and MCD as well as our proposed DS-WED, each computed as micro-average (Avg.) and rank-based score (Borda Avg.). The best results are highlighted in bold, and second-best are underlined.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" rowspan=\"3\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"6\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">log <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> RMSE</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">log <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> RMSE</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m12\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Borda Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" colspan=\"13\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AR (next-token prediction)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">XTTS-v2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.31</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.14</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.18</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.70</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">127.84</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.89</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.28</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.73</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.16</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.15</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">5.50</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.27</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.46</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.08</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.41</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">120.59</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.59</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.22</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.49</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.29</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.85</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.74</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.85</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.56</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.07</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.47</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">134.34</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">5.38</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.45</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.35</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">88.04</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" colspan=\"13\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">NAR (flow matching)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">E2 TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.27</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.26</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.40</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.87</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.91</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.11</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.78</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.57</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.35</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.98</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.48</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.19</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.59</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.50</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.49</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.28</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.00</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.51</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ZipVoice</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.29</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.55</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.91</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.85</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">114.52</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.22</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.55</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.99</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.09</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.56</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.88</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" colspan=\"13\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">NAR (masked generative modeling)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MaskGCT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.28</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.04</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.76</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.51</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">139.75</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.61</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.78</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.13</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.70</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.36</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.30</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "paradigms",
            "masked",
            "mcd",
            "avg↑uparrow",
            "matching",
            "modeling",
            "librispeech",
            "seedtts",
            "nexttoken",
            "f0f0",
            "each",
            "nar",
            "our",
            "zeroshot",
            "tts",
            "acoustic",
            "systems",
            "zipvoice",
            "prediction",
            "log",
            "diversity",
            "from",
            "computed",
            "dswed",
            "system",
            "rankbased",
            "cosyvoice",
            "avg",
            "prosody",
            "microaverage",
            "flow",
            "metrics",
            "well",
            "traditional",
            "bold",
            "maskgct",
            "highlighted",
            "f5tts",
            "results",
            "underlined",
            "benchmark",
            "generative",
            "score",
            "secondbest",
            "xttsv2",
            "proposed",
            "borda",
            "best",
            "testen",
            "rmse",
            "diverse"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, AR systems consistently outperform NAR flow-matching systems in prosody diversity. However, when compared with NAR MGM systems, AR systems show comparable performance. MaskGCT even surpasses all AR systems on LibriSpeech and remains competitive on Seed-TTS.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS).\nHowever, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored.\nTo bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics.\nProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings.\nBuilding on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens.\nExperiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM.\nLeveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations.\nAudio samples are available at https://prosodyeval.github.io.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "generative",
                    "testclean",
                    "score",
                    "paradigms",
                    "zeroshot",
                    "modeling",
                    "tts",
                    "acoustic",
                    "systems",
                    "prosody",
                    "librispeech",
                    "seedtts",
                    "metrics",
                    "diversity",
                    "from",
                    "testen",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nZero-shot TTS, prosody diversity, prosody evaluation, prosody benchmark</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "prosody",
                    "tts",
                    "zeroshot",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody plays a central role in spoken communication, conveying paralinguistic information such as emotion, attitude, and intent that shapes how listeners interpret meaning. Subtle variations in pitch, intensity, and duration can fundamentally alter the interpretation of an utterance even with identical text, making prosody crucial to naturalness and expressiveness in speech synthesis.\nWhile zero-shot TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced in intelligibility and speaker similarity, prosody diversity has received far less attention.\nExisting evaluation practices&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rely largely on log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> root mean squared error (RMSE), which correlates weakly with human judgments, captures pitch but neglects rhythm and intensity&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and requires costly dynamic time warping (DTW).\nConsequently, a clear gap remains between automatic metrics and human perception of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "metrics",
                    "f0f0",
                    "log",
                    "diversity",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent efforts have shown that both continuous&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and discrete&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> self-supervised (SSL) speech representations effectively encode prosodic information.\nConcurrently, NLP automatic evaluation metrics have been adapted to discrete speech representations for reference-free&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and reference-aware&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech quality assessment. In particular, SpeechTokenDistance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a simple approach but remains preliminary, as it focuses solely on measuring correlation with acoustic metrics without examining what the differences represent or further analyzing correlations with human perception.</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With this perspective in mind, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ProsodyEval</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the first human-annotated dataset for prosody diversity assessment, and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">iscretized </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eighted </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">dit </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">istance), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic token sequences derived from speech SSL models such as HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-kmeans clustering.\nProsodyEval contains 1000 synthetic samples from 7 mainstream TTS systems, paired with 2000 human ratings of prosody diversity.\nExperiments show that DS-WED achieves substantially higher correlation with human judgments than frequently used acoustic metrics, including log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and Mel cepstral distortion (MCD), while remaining robust across models, layers, and cluster sizes.\nBuilding on DS-WED, we establish the first systematic benchmark of prosody diversity across state-of-the-art open-source TTS systems on LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur further explorations reveal the impact of modeling paradigms, duration control, and reinforcement learning (RL), and demonstrate that even advanced large audio language models (LALMs) like Gemini 2.5 Pro remain unreliable for prosody evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "paradigms",
                    "mcd",
                    "modeling",
                    "librispeech",
                    "seedtts",
                    "f0f0",
                    "our",
                    "tts",
                    "acoustic",
                    "systems",
                    "log",
                    "diversity",
                    "from",
                    "dswed",
                    "prosody",
                    "metrics",
                    "benchmark",
                    "testen",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We present ProsodyEval, the first human-annotated dataset for prosody diversity assessment in zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We propose DS-WED, a new objective prosody diversity metric based on semantic token weighted edit distance that better correlates with human ratings than existing acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "acoustic",
                    "metrics",
                    "diversity",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Benchmark.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We establish the first benchmark of prosody diversity across state-of-the-art open-source TTS systems, offering systematic comparisons and analyses of different generative paradigms from the perspective of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "generative",
                    "paradigms",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Exploration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We investigate key factors shaping prosody diversity, showing that (1) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">autoregressive (AR) systems outperform flow-matching based non-autoregressive (NAR) systems but not masked generative modeling (MGM)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (2) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">duration variation is critical, yet NAR systems lack explicit duration control, and flow-matching models with implicit alignment suffer from inherent constraints</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (3) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">RL via direct preference optimization (DPO) trades off diversity for intelligibility</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; and (4) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">current LALMs remain unreliable for prosody understanding</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generative",
                    "masked",
                    "prosody",
                    "modeling",
                    "systems",
                    "diversity",
                    "from",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct the ProsodyEval dataset by aggregating synthetic speech from diverse generative paradigms, spanning AR and NAR approaches, including next-token prediction&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MGM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Concretely, ProsodyEval comprises samples synthesized by seven recent open-source TTS systems, namely XTTS-v2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2 TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and ZipVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These paradigms reflect widely used approaches in modern TTS, rendering the dataset representative for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "generative",
                    "xttsv2",
                    "paradigms",
                    "tts",
                    "systems",
                    "matching",
                    "flow",
                    "nexttoken",
                    "zipvoice",
                    "prediction",
                    "maskgct",
                    "from",
                    "cosyvoice",
                    "f5tts",
                    "diverse",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Each system generates a group of five samples per input with random seeds from 0 to 4, using prompt speech and text from LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo exclude synthesis errors, we filter all groups to retain only those in which every sample is subjectively perceived as word-by-word aligned with the text.\nIn total, ProsodyEval contains 1000 synthetic speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "seedtts",
                    "each",
                    "from",
                    "testen",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dimension</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;The prosodic difference score measures the extent of variation in pitch, rhythm, and stress patterns between two audio samples generated with the same model, text, and speaker.\nA score of 1 indicates nearly identical prosody with imperceptible variation, while a score of 5 reflects clear and consistent prosodic differences, suggesting noticeably distinct speaking styles.</span>\n</p>\n\n",
                "matched_terms": [
                    "score",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Listening Test Design</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;All MOS tests are conducted online under controlled conditions. After a short training session, all raters are instructed to perform the assessments in a quiet environment.\nIn each trial, a group of five audio samples generated by the same system and speaker is presented. Raters perform pairwise comparisons across all ten possible pairs within the group in random order. Each pair is rated on a five-point Likert scale according to the perceived prosodic difference.\nTo aid judgment, both audio playback and corresponding waveform visualization are provided.\nRaters are encouraged to replay samples as needed, enabling them to refine their judgments and capture both subtle and pronounced differences across pairs.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ProsodyEval incorporates two frequently used acoustic metrics, namely log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD.\nFor each pair of samples within a group, FastDTW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a linear-time approximation of DTW, is first applied to align their lengths, after which log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD are computed.\nTogether, these two metrics serve as complementary baseline indicators for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "mcd",
                    "metrics",
                    "f0f0",
                    "log",
                    "computed",
                    "each",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a zero-shot TTS system, we aim to quantify prosody diversity between two generated speech samples conditioned on the same text and reference speech prompt, using distinct random seeds.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "zeroshot",
                    "tts",
                    "diversity",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Acoustic tokens from EnCodec&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are not considered, as they retain low-level signal details that are not relevant to prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "acoustic",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate correlations between human ratings and the three objective metrics DS-WED, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE, and MCD on the ProsodyEval dataset across all groups.\nFor DS-WED, speech is discretized by applying a 50-cluster </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means model trained on LibriSpeech 960h to the hidden embeddings from the 8th Transformer encoder layer.\nSince PMOS ratings are relative within groups and not comparable across groups, we compute Pearson correlation for each group and then aggregate the group-wise correlations using Fisher&#8217;s </span>\n  <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Z</mi>\n      <annotation encoding=\"application/x-tex\">Z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformation.\nStatistical significance is tested with a two-sided one-sample </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test against zero, and 95% confidence intervals are obtained in the Fisher space and back-transformed.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "librispeech",
                    "metrics",
                    "f0f0",
                    "log",
                    "each",
                    "from",
                    "rmse",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PMOS exhibits the strongest correlation with DS-WED (</span>\n  <math alttext=\"\\bar{r}=0.77\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.77</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.77</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), followed by a substantial correlation with MCD (</span>\n  <math alttext=\"\\bar{r}=0.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.66</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe correlation with log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE is weaker but still significant (</span>\n  <math alttext=\"\\bar{r}=0.30\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.30</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.30</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), suggesting that pitch deviations contribute to perceived differences but capture only part of the variability.\nOverall, these results demonstrate that DS-WED aligns most closely with subjective human judgments, substantially outperforming widely used acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "mcd",
                    "metrics",
                    "f0f0",
                    "log",
                    "results",
                    "dswed",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE reaches an RTF of 0.549, and MCD reaches an RTF of 0.203. Both rely on signal-processing front-ends and DTW alignment, which are CPU-bound and difficult to accelerate on GPUs. In addition, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE requires mel-cepstrum computation for DTW, adding extra overhead.\nIn contrast, DS-WED involves only a forward pass through a pretrained speech-SSL encoder followed by </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering and edit distance at the discrete level, achieving an RTF of 0.110. It is GPU-friendly and can be further accelerated by batching.\nOverall, DS-WED is scalable for large-scale evaluation and practical for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "f0f0",
                    "log",
                    "results",
                    "dswed",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 4.3 Ablation Studies of DS-WED &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that DS-WED remains quite robust across different layers, models, and vocabulary sizes, with correlations consistently around 0.7.\nMiddle layers 6-9 achieve stronger correlations, consistent with the richer encoded prosody information.\nRelative smaller cluster sizes perform best, while larger ones reduce peak correlations, making the edit-distance calculation overly sensitive and misaligned with human perceptual sensitivity to prosody.\nOverall, WavLM-base provides more stable correlations, while HuBERT-base exhibits larger variance. The 8th layer of HuBERT-base using 50 clusters achieves the strongest correlations.\nWe also tried the large versions, which yield slightly higher correlations but at much greater cost, hence we use the base versions for all experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "best",
                    "prosody",
                    "results",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Details</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate the following open-source zero-shot TTS systems across three representative paradigms: next-token prediction, flow matching, and MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigms",
                    "zeroshot",
                    "tts",
                    "systems",
                    "flow",
                    "matching",
                    "nexttoken",
                    "prediction",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice 2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Two-stage AR+flow-matching systems trained on 166.8k hours of multilingual data. We evaluate CosyVoice-300M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/iic/CosyVoice-300M\" title=\"\">https://www.modelscope.cn/iic/CosyVoice-300M</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and CosyVoice2-0.5B</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/iic/CosyVoice2-0.5B\" title=\"\">https://www.modelscope.cn/iic/CosyVoice2-0.5B</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MaskGCT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A two-stage NAR MGM system</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/amphion/MaskGCT\" title=\"\">https://huggingface.co/amphion/MaskGCT</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with 695M text-to-semantic and 353M semantic-to-acoustic models, trained on 100k hours of Chinese and English speech from Emilia&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "nar",
                    "maskgct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E2 TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Fully NAR flow-matching systems with 333M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">4</sup>\n        <span class=\"ltx_tag ltx_tag_note\">4</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/E2-TTS\" title=\"\">https://huggingface.co/SWivid/E2-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 336M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">5</sup>\n        <span class=\"ltx_tag ltx_tag_note\">5</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/F5-TTS\" title=\"\">https://huggingface.co/SWivid/F5-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> parameters, trained on Emilia 100k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "tts",
                    "nar",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Our evaluations are conducted on two widely used benchmarks: professionally read audiobooks LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and crowdsourced read speech Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo measure prosody diversity, we report two conventional acoustic metrics, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD, together with our proposed DS-WED. Each metric is computed in two ways: (1) micro average across all samples, and (2) rank-based Borda aggregation, where systems are ranked within each group and assigned scores from seven for the best system to one for the worst, and the average score across groups is reported, thereby eliminating the influence of absolute metric values.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "mcd",
                    "librispeech",
                    "seedtts",
                    "f0f0",
                    "each",
                    "our",
                    "acoustic",
                    "systems",
                    "log",
                    "diversity",
                    "from",
                    "computed",
                    "dswed",
                    "system",
                    "rankbased",
                    "prosody",
                    "metrics",
                    "score",
                    "proposed",
                    "borda",
                    "best",
                    "testen",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Do AR zero-shot TTS systems generate more prosody diversity than NAR systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAR systems indeed outperform flow-matching models in prosody diversity but offer no advantage over MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">AR models generate speech sequentially, providing explicit temporal modeling and natural variation in duration.\nIn contrast, NAR flow-matching systems such as E2-TTS, F5-TTS, and ZipVoice pursue architectural simplicity by adopting implicit alignment.\nWhen regression objectives are applied to these weakly aligned and entangled representations, the models collapse toward mean predictions of highly multimodal and diverse prosodic patterns, resulting in blurry and over-smoothed outputs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAverage upsampling-based alignment combined with a text encoder in ZipVoice partly alleviates this issue, but the problem remains non-trivial.\nBy comparison, MGM introduces stochasticity through iterative mask-and-prediction.</span>\n</p>\n\n",
                "matched_terms": [
                    "modeling",
                    "systems",
                    "zipvoice",
                    "f5tts",
                    "diverse",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All NAR systems are trained on the same 100k-hour Emilia corpus, yet MaskGCT performs best in DS-WED while F5-TTS ranks lowest.\nMoreover, XTTS-v2, trained on less than one-third of the data, still shows superior prosody diversity, suggesting that modeling paradigm rather than data scale is the dominant factor.</span>\n</p>\n\n",
                "matched_terms": [
                    "xttsv2",
                    "prosody",
                    "modeling",
                    "systems",
                    "maskgct",
                    "best",
                    "diversity",
                    "f5tts",
                    "dswed",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">To what extent does duration variation shape prosody diversity in NAR zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply duration perturbation (DP) with factors 0.8, 0.9, 1.0, 1.1, and 1.2 to audio samples within each group.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows duration variations in this range have little effect on intelligibility.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DP consistently increases prosody diversity for two NAR systems, indicating that duration variation during inference is a key factor shaping prosody.\nNotably, F5-TTS exhibits a relative increase of nearly 30% with DP, yet still lags behind AR and MGM systems without DP, suggesting that prosodic monotony in flow-matching systems with implicit alignment stems from inherent architectural limitations.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity",
                    "each",
                    "from",
                    "f5tts",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Does reinforcement learning (RL) reduce prosody diversity in zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate the prosody diversity of zero-shot TTS systems from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are aligned via Direct Preference Optimization (DPO) on the INTP dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for intelligibility, using vanilla DPO for AR and extended DPO&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for NAR MGM.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying DPO consistently reduces prosodic diversity for AR and NAR systems, reflecting the general tendency of RL to prune variation while amplifying reward-aligned behaviors.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity",
                    "from",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Can large audio language models (LALMs) serve as reliable evaluators of prosodic differences?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe test Gemini 2.5 Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by prompting it to rate the relative prosodic difference between two samples within a group of five. It listens to all five samples to form a reference range of variation and then assigns a score from 1 to 5.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Gemini&#8217;s scores show a statistically significant but weak correlation with human ratings, while correlations with objective metrics fluctuate with wide confidence intervals.\nCombined with high prompt sensitivity, these findings suggest that Gemini 2.5 Pro is not a reliable evaluator of prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "score",
                    "metrics",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we introduce ProsodyEval, the first human-annotated dataset for assessing prosody diversity in zero-shot TTS, together with DS-WED, a new objective diversity metric based on weighted edit distance over semantic tokens.\nDS-WED outperforms frequently used acoustic metrics in correlating with human judgments and remains robust in speech discretization across models, layers, and cluster sizes, enabling more reliable evaluation of prosodic variation.\nLeveraging DS-WED, we systematically benchmark and analyze mainstream zero-shot TTS systems.\nOur findings indicate that (1) AR systems outperform NAR flow-matching models but not MGM, (2) duration variation strongly shapes prosody diversity, yet NAR systems lack duration control, and flow-matching models with implicit alignment suffer from inherent architectural limitations, (3) RL via DPO improves intelligibility while reducing prosody diversity, and (4) even advanced LALMs like Gemini 2.5 Pro remain unreliable for prosody evaluation.\nOne limitation lies in the cross-lingual applicability of DS-WED, validated only on English.\nLooking ahead, ProsodyEval and DS-WED fill a gap in zero-shot TTS evaluation and open new avenues for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "zeroshot",
                    "acoustic",
                    "tts",
                    "systems",
                    "prosody",
                    "metrics",
                    "diversity",
                    "from",
                    "dswed",
                    "nar",
                    "our"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: \nEffect of duration perturbation (DP) on prosody diversity of NAR TTS systems on LibriSpeech test-clean and Seed-TTS test-en, using DS-WED, computed as micro-average (Avg.).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.59</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS w/ DP</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">100.88</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">+26.7%</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">62.95</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">+28.5%</span></sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MaskGCT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">139.75</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MaskGCT w/ DP</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">159.10</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">+13.8%</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">92.71</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">+15.4%</span></sub>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "avg↑uparrow",
            "librispeech",
            "seedtts",
            "nar",
            "tts",
            "systems",
            "effect",
            "diversity",
            "computed",
            "dswed",
            "system",
            "perturbation",
            "avg",
            "prosody",
            "microaverage",
            "maskgct",
            "f5tts",
            "duration",
            "testen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">To what extent does duration variation shape prosody diversity in NAR zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply duration perturbation (DP) with factors 0.8, 0.9, 1.0, 1.1, and 1.2 to audio samples within each group.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows duration variations in this range have little effect on intelligibility.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DP consistently increases prosody diversity for two NAR systems, indicating that duration variation during inference is a key factor shaping prosody.\nNotably, F5-TTS exhibits a relative increase of nearly 30% with DP, yet still lags behind AR and MGM systems without DP, suggesting that prosodic monotony in flow-matching systems with implicit alignment stems from inherent architectural limitations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS).\nHowever, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored.\nTo bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics.\nProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings.\nBuilding on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens.\nExperiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM.\nLeveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations.\nAudio samples are available at https://prosodyeval.github.io.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prosody",
                    "tts",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "duration",
                    "diversity",
                    "testen",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nZero-shot TTS, prosody diversity, prosody evaluation, prosody benchmark</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody plays a central role in spoken communication, conveying paralinguistic information such as emotion, attitude, and intent that shapes how listeners interpret meaning. Subtle variations in pitch, intensity, and duration can fundamentally alter the interpretation of an utterance even with identical text, making prosody crucial to naturalness and expressiveness in speech synthesis.\nWhile zero-shot TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced in intelligibility and speaker similarity, prosody diversity has received far less attention.\nExisting evaluation practices&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rely largely on log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> root mean squared error (RMSE), which correlates weakly with human judgments, captures pitch but neglects rhythm and intensity&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and requires costly dynamic time warping (DTW).\nConsequently, a clear gap remains between automatic metrics and human perception of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "duration",
                    "diversity",
                    "prosody",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With this perspective in mind, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ProsodyEval</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the first human-annotated dataset for prosody diversity assessment, and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">iscretized </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eighted </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">dit </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">istance), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic token sequences derived from speech SSL models such as HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-kmeans clustering.\nProsodyEval contains 1000 synthetic samples from 7 mainstream TTS systems, paired with 2000 human ratings of prosody diversity.\nExperiments show that DS-WED achieves substantially higher correlation with human judgments than frequently used acoustic metrics, including log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and Mel cepstral distortion (MCD), while remaining robust across models, layers, and cluster sizes.\nBuilding on DS-WED, we establish the first systematic benchmark of prosody diversity across state-of-the-art open-source TTS systems on LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur further explorations reveal the impact of modeling paradigms, duration control, and reinforcement learning (RL), and demonstrate that even advanced large audio language models (LALMs) like Gemini 2.5 Pro remain unreliable for prosody evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prosody",
                    "tts",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "duration",
                    "diversity",
                    "testen",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We present ProsodyEval, the first human-annotated dataset for prosody diversity assessment in zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We propose DS-WED, a new objective prosody diversity metric based on semantic token weighted edit distance that better correlates with human ratings than existing acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Benchmark.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We establish the first benchmark of prosody diversity across state-of-the-art open-source TTS systems, offering systematic comparisons and analyses of different generative paradigms from the perspective of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Exploration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We investigate key factors shaping prosody diversity, showing that (1) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">autoregressive (AR) systems outperform flow-matching based non-autoregressive (NAR) systems but not masked generative modeling (MGM)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (2) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">duration variation is critical, yet NAR systems lack explicit duration control, and flow-matching models with implicit alignment suffer from inherent constraints</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (3) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">RL via direct preference optimization (DPO) trades off diversity for intelligibility</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; and (4) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">current LALMs remain unreliable for prosody understanding</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "systems",
                    "duration",
                    "diversity",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct the ProsodyEval dataset by aggregating synthetic speech from diverse generative paradigms, spanning AR and NAR approaches, including next-token prediction&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MGM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Concretely, ProsodyEval comprises samples synthesized by seven recent open-source TTS systems, namely XTTS-v2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2 TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and ZipVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These paradigms reflect widely used approaches in modern TTS, rendering the dataset representative for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "maskgct",
                    "f5tts",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Each system generates a group of five samples per input with random seeds from 0 to 4, using prompt speech and text from LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo exclude synthesis errors, we filter all groups to retain only those in which every sample is subjectively perceived as word-by-word aligned with the text.\nIn total, ProsodyEval contains 1000 synthetic speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "seedtts",
                    "testen",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a zero-shot TTS system, we aim to quantify prosody diversity between two generated speech samples conditioned on the same text and reference speech prompt, using distinct random seeds.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate correlations between human ratings and the three objective metrics DS-WED, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE, and MCD on the ProsodyEval dataset across all groups.\nFor DS-WED, speech is discretized by applying a 50-cluster </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means model trained on LibriSpeech 960h to the hidden embeddings from the 8th Transformer encoder layer.\nSince PMOS ratings are relative within groups and not comparable across groups, we compute Pearson correlation for each group and then aggregate the group-wise correlations using Fisher&#8217;s </span>\n  <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Z</mi>\n      <annotation encoding=\"application/x-tex\">Z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformation.\nStatistical significance is tested with a two-sided one-sample </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test against zero, and 95% confidence intervals are obtained in the Fisher space and back-transformed.</span>\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Computational efficiency is measured on ProsodyEval by Real-Time Factor (RTF), computed as processing time divided by average speech-pair duration, on NVIDIA A100 with batch size 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "duration",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We study the effect of the SSL backbone, the encoder layer, and the number of </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clusters on the correlation between DS-WED and human ratings, using the ProsodyEval dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "effect",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 4.3 Ablation Studies of DS-WED &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that DS-WED remains quite robust across different layers, models, and vocabulary sizes, with correlations consistently around 0.7.\nMiddle layers 6-9 achieve stronger correlations, consistent with the richer encoded prosody information.\nRelative smaller cluster sizes perform best, while larger ones reduce peak correlations, making the edit-distance calculation overly sensitive and misaligned with human perceptual sensitivity to prosody.\nOverall, WavLM-base provides more stable correlations, while HuBERT-base exhibits larger variance. The 8th layer of HuBERT-base using 50 clusters achieves the strongest correlations.\nWe also tried the large versions, which yield slightly higher correlations but at much greater cost, hence we use the base versions for all experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Details</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate the following open-source zero-shot TTS systems across three representative paradigms: next-token prediction, flow matching, and MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MaskGCT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: A two-stage NAR MGM system</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/amphion/MaskGCT\" title=\"\">https://huggingface.co/amphion/MaskGCT</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with 695M text-to-semantic and 353M semantic-to-acoustic models, trained on 100k hours of Chinese and English speech from Emilia&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "nar",
                    "maskgct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E2 TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Fully NAR flow-matching systems with 333M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">4</sup>\n        <span class=\"ltx_tag ltx_tag_note\">4</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/E2-TTS\" title=\"\">https://huggingface.co/SWivid/E2-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 336M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">5</sup>\n        <span class=\"ltx_tag ltx_tag_note\">5</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/F5-TTS\" title=\"\">https://huggingface.co/SWivid/F5-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> parameters, trained on Emilia 100k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "tts",
                    "nar",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Our evaluations are conducted on two widely used benchmarks: professionally read audiobooks LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and crowdsourced read speech Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo measure prosody diversity, we report two conventional acoustic metrics, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD, together with our proposed DS-WED. Each metric is computed in two ways: (1) micro average across all samples, and (2) rank-based Borda aggregation, where systems are ranked within each group and assigned scores from seven for the best system to one for the worst, and the average score across groups is reported, thereby eliminating the influence of absolute metric values.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prosody",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "diversity",
                    "computed",
                    "testen",
                    "dswed",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, AR systems consistently outperform NAR flow-matching systems in prosody diversity. However, when compared with NAR MGM systems, AR systems show comparable performance. MaskGCT even surpasses all AR systems on LibriSpeech and remains competitive on Seed-TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "maskgct",
                    "diversity",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Do AR zero-shot TTS systems generate more prosody diversity than NAR systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAR systems indeed outperform flow-matching models in prosody diversity but offer no advantage over MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "tts",
                    "systems",
                    "diversity",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">AR models generate speech sequentially, providing explicit temporal modeling and natural variation in duration.\nIn contrast, NAR flow-matching systems such as E2-TTS, F5-TTS, and ZipVoice pursue architectural simplicity by adopting implicit alignment.\nWhen regression objectives are applied to these weakly aligned and entangled representations, the models collapse toward mean predictions of highly multimodal and diverse prosodic patterns, resulting in blurry and over-smoothed outputs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAverage upsampling-based alignment combined with a text encoder in ZipVoice partly alleviates this issue, but the problem remains non-trivial.\nBy comparison, MGM introduces stochasticity through iterative mask-and-prediction.</span>\n</p>\n\n",
                "matched_terms": [
                    "duration",
                    "f5tts",
                    "nar",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All NAR systems are trained on the same 100k-hour Emilia corpus, yet MaskGCT performs best in DS-WED while F5-TTS ranks lowest.\nMoreover, XTTS-v2, trained on less than one-third of the data, still shows superior prosody diversity, suggesting that modeling paradigm rather than data scale is the dominant factor.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "systems",
                    "maskgct",
                    "diversity",
                    "f5tts",
                    "dswed",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Does reinforcement learning (RL) reduce prosody diversity in zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate the prosody diversity of zero-shot TTS systems from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are aligned via Direct Preference Optimization (DPO) on the INTP dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for intelligibility, using vanilla DPO for AR and extended DPO&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for NAR MGM.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying DPO consistently reduces prosodic diversity for AR and NAR systems, reflecting the general tendency of RL to prune variation while amplifying reward-aligned behaviors.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "tts",
                    "systems",
                    "diversity",
                    "nar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we introduce ProsodyEval, the first human-annotated dataset for assessing prosody diversity in zero-shot TTS, together with DS-WED, a new objective diversity metric based on weighted edit distance over semantic tokens.\nDS-WED outperforms frequently used acoustic metrics in correlating with human judgments and remains robust in speech discretization across models, layers, and cluster sizes, enabling more reliable evaluation of prosodic variation.\nLeveraging DS-WED, we systematically benchmark and analyze mainstream zero-shot TTS systems.\nOur findings indicate that (1) AR systems outperform NAR flow-matching models but not MGM, (2) duration variation strongly shapes prosody diversity, yet NAR systems lack duration control, and flow-matching models with implicit alignment suffer from inherent architectural limitations, (3) RL via DPO improves intelligibility while reducing prosody diversity, and (4) even advanced LALMs like Gemini 2.5 Pro remain unreliable for prosody evaluation.\nOne limitation lies in the cross-lingual applicability of DS-WED, validated only on English.\nLooking ahead, ProsodyEval and DS-WED fill a gap in zero-shot TTS evaluation and open new avenues for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "tts",
                    "systems",
                    "duration",
                    "diversity",
                    "dswed",
                    "nar"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: \nEffect of DPO on prosody diversity of zero-shot TTS systems on LibriSpeech test-clean and Seed-TTS test-en, using DS-WED, computed as micro-average (Avg.).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Avg.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">134.34</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2 w/ DPO</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">109.09</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">-18.8%</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">71.64</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">-18.6%</span></sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MaskGCT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">139.75</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MaskGCT w/ DPO</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">135.75</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">-2.9%</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">77.80</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">-3.2%</span></sub>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "avg↑uparrow",
            "librispeech",
            "seedtts",
            "zeroshot",
            "tts",
            "systems",
            "effect",
            "diversity",
            "computed",
            "dswed",
            "system",
            "cosyvoice",
            "avg",
            "prosody",
            "microaverage",
            "maskgct",
            "dpo",
            "testen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Does reinforcement learning (RL) reduce prosody diversity in zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate the prosody diversity of zero-shot TTS systems from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are aligned via Direct Preference Optimization (DPO) on the INTP dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for intelligibility, using vanilla DPO for AR and extended DPO&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for NAR MGM.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying DPO consistently reduces prosodic diversity for AR and NAR systems, reflecting the general tendency of RL to prune variation while amplifying reward-aligned behaviors.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS).\nHowever, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored.\nTo bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics.\nProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings.\nBuilding on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens.\nExperiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM.\nLeveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations.\nAudio samples are available at https://prosodyeval.github.io.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "diversity",
                    "testen",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nZero-shot TTS, prosody diversity, prosody evaluation, prosody benchmark</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody plays a central role in spoken communication, conveying paralinguistic information such as emotion, attitude, and intent that shapes how listeners interpret meaning. Subtle variations in pitch, intensity, and duration can fundamentally alter the interpretation of an utterance even with identical text, making prosody crucial to naturalness and expressiveness in speech synthesis.\nWhile zero-shot TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced in intelligibility and speaker similarity, prosody diversity has received far less attention.\nExisting evaluation practices&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rely largely on log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> root mean squared error (RMSE), which correlates weakly with human judgments, captures pitch but neglects rhythm and intensity&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and requires costly dynamic time warping (DTW).\nConsequently, a clear gap remains between automatic metrics and human perception of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "zeroshot",
                    "tts",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With this perspective in mind, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ProsodyEval</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the first human-annotated dataset for prosody diversity assessment, and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">iscretized </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eighted </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">dit </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">istance), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic token sequences derived from speech SSL models such as HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-kmeans clustering.\nProsodyEval contains 1000 synthetic samples from 7 mainstream TTS systems, paired with 2000 human ratings of prosody diversity.\nExperiments show that DS-WED achieves substantially higher correlation with human judgments than frequently used acoustic metrics, including log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and Mel cepstral distortion (MCD), while remaining robust across models, layers, and cluster sizes.\nBuilding on DS-WED, we establish the first systematic benchmark of prosody diversity across state-of-the-art open-source TTS systems on LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur further explorations reveal the impact of modeling paradigms, duration control, and reinforcement learning (RL), and demonstrate that even advanced large audio language models (LALMs) like Gemini 2.5 Pro remain unreliable for prosody evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prosody",
                    "tts",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "diversity",
                    "testen",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Dataset.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We present ProsodyEval, the first human-annotated dataset for prosody diversity assessment in zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We propose DS-WED, a new objective prosody diversity metric based on semantic token weighted edit distance that better correlates with human ratings than existing acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Benchmark.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We establish the first benchmark of prosody diversity across state-of-the-art open-source TTS systems, offering systematic comparisons and analyses of different generative paradigms from the perspective of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Exploration.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We investigate key factors shaping prosody diversity, showing that (1) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">autoregressive (AR) systems outperform flow-matching based non-autoregressive (NAR) systems but not masked generative modeling (MGM)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (2) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">duration variation is critical, yet NAR systems lack explicit duration control, and flow-matching models with implicit alignment suffer from inherent constraints</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (3) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">RL via direct preference optimization (DPO) trades off diversity for intelligibility</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; and (4) </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">current LALMs remain unreliable for prosody understanding</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "prosody",
                    "dpo",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct the ProsodyEval dataset by aggregating synthetic speech from diverse generative paradigms, spanning AR and NAR approaches, including next-token prediction&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MGM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Concretely, ProsodyEval comprises samples synthesized by seven recent open-source TTS systems, namely XTTS-v2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2 TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and ZipVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These paradigms reflect widely used approaches in modern TTS, rendering the dataset representative for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "maskgct",
                    "tts",
                    "cosyvoice",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Each system generates a group of five samples per input with random seeds from 0 to 4, using prompt speech and text from LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo exclude synthesis errors, we filter all groups to retain only those in which every sample is subjectively perceived as word-by-word aligned with the text.\nIn total, ProsodyEval contains 1000 synthetic speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "seedtts",
                    "testen",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a zero-shot TTS system, we aim to quantify prosody diversity between two generated speech samples conditioned on the same text and reference speech prompt, using distinct random seeds.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "zeroshot",
                    "tts",
                    "diversity",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate correlations between human ratings and the three objective metrics DS-WED, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE, and MCD on the ProsodyEval dataset across all groups.\nFor DS-WED, speech is discretized by applying a 50-cluster </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means model trained on LibriSpeech 960h to the hidden embeddings from the 8th Transformer encoder layer.\nSince PMOS ratings are relative within groups and not comparable across groups, we compute Pearson correlation for each group and then aggregate the group-wise correlations using Fisher&#8217;s </span>\n  <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Z</mi>\n      <annotation encoding=\"application/x-tex\">Z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformation.\nStatistical significance is tested with a two-sided one-sample </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test against zero, and 95% confidence intervals are obtained in the Fisher space and back-transformed.</span>\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We study the effect of the SSL backbone, the encoder layer, and the number of </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clusters on the correlation between DS-WED and human ratings, using the ProsodyEval dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "effect",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 4.3 Ablation Studies of DS-WED &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that DS-WED remains quite robust across different layers, models, and vocabulary sizes, with correlations consistently around 0.7.\nMiddle layers 6-9 achieve stronger correlations, consistent with the richer encoded prosody information.\nRelative smaller cluster sizes perform best, while larger ones reduce peak correlations, making the edit-distance calculation overly sensitive and misaligned with human perceptual sensitivity to prosody.\nOverall, WavLM-base provides more stable correlations, while HuBERT-base exhibits larger variance. The 8th layer of HuBERT-base using 50 clusters achieves the strongest correlations.\nWe also tried the large versions, which yield slightly higher correlations but at much greater cost, hence we use the base versions for all experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Details</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate the following open-source zero-shot TTS systems across three representative paradigms: next-token prediction, flow matching, and MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "zeroshot",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice 2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Two-stage AR+flow-matching systems trained on 166.8k hours of multilingual data. We evaluate CosyVoice-300M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/iic/CosyVoice-300M\" title=\"\">https://www.modelscope.cn/iic/CosyVoice-300M</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and CosyVoice2-0.5B</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/iic/CosyVoice2-0.5B\" title=\"\">https://www.modelscope.cn/iic/CosyVoice2-0.5B</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E2 TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &amp; </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Fully NAR flow-matching systems with 333M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">4</sup>\n        <span class=\"ltx_tag ltx_tag_note\">4</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/E2-TTS\" title=\"\">https://huggingface.co/SWivid/E2-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 336M</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">5</sup>\n        <span class=\"ltx_tag ltx_tag_note\">5</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/F5-TTS\" title=\"\">https://huggingface.co/SWivid/F5-TTS</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> parameters, trained on Emilia 100k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Our evaluations are conducted on two widely used benchmarks: professionally read audiobooks LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and crowdsourced read speech Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo measure prosody diversity, we report two conventional acoustic metrics, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD, together with our proposed DS-WED. Each metric is computed in two ways: (1) micro average across all samples, and (2) rank-based Borda aggregation, where systems are ranked within each group and assigned scores from seven for the best system to one for the worst, and the average score across groups is reported, thereby eliminating the influence of absolute metric values.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prosody",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "diversity",
                    "computed",
                    "testen",
                    "dswed",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, AR systems consistently outperform NAR flow-matching systems in prosody diversity. However, when compared with NAR MGM systems, AR systems show comparable performance. MaskGCT even surpasses all AR systems on LibriSpeech and remains competitive on Seed-TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "systems",
                    "librispeech",
                    "seedtts",
                    "maskgct",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Do AR zero-shot TTS systems generate more prosody diversity than NAR systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAR systems indeed outperform flow-matching models in prosody diversity but offer no advantage over MGM.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All NAR systems are trained on the same 100k-hour Emilia corpus, yet MaskGCT performs best in DS-WED while F5-TTS ranks lowest.\nMoreover, XTTS-v2, trained on less than one-third of the data, still shows superior prosody diversity, suggesting that modeling paradigm rather than data scale is the dominant factor.</span>\n</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "systems",
                    "maskgct",
                    "diversity",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">To what extent does duration variation shape prosody diversity in NAR zero-shot TTS systems?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply duration perturbation (DP) with factors 0.8, 0.9, 1.0, 1.1, and 1.2 to audio samples within each group.\nPrior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows duration variations in this range have little effect on intelligibility.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DP consistently increases prosody diversity for two NAR systems, indicating that duration variation during inference is a key factor shaping prosody.\nNotably, F5-TTS exhibits a relative increase of nearly 30% with DP, yet still lags behind AR and MGM systems without DP, suggesting that prosodic monotony in flow-matching systems with implicit alignment stems from inherent architectural limitations.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "effect",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we introduce ProsodyEval, the first human-annotated dataset for assessing prosody diversity in zero-shot TTS, together with DS-WED, a new objective diversity metric based on weighted edit distance over semantic tokens.\nDS-WED outperforms frequently used acoustic metrics in correlating with human judgments and remains robust in speech discretization across models, layers, and cluster sizes, enabling more reliable evaluation of prosodic variation.\nLeveraging DS-WED, we systematically benchmark and analyze mainstream zero-shot TTS systems.\nOur findings indicate that (1) AR systems outperform NAR flow-matching models but not MGM, (2) duration variation strongly shapes prosody diversity, yet NAR systems lack duration control, and flow-matching models with implicit alignment suffer from inherent architectural limitations, (3) RL via DPO improves intelligibility while reducing prosody diversity, and (4) even advanced LALMs like Gemini 2.5 Pro remain unreliable for prosody evaluation.\nOne limitation lies in the cross-lingual applicability of DS-WED, validated only on English.\nLooking ahead, ProsodyEval and DS-WED fill a gap in zero-shot TTS evaluation and open new avenues for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "prosody",
                    "systems",
                    "diversity",
                    "dpo",
                    "dswed"
                ]
            }
        ]
    },
    "S4.T5": {
        "caption": "Table 5: \nAverage Pearson (r¯\\bar{r}) correlations between LALM-as-Judges and prosody-related metrics, aggregated across groups via Fisher’s ZZ transformation.\nValues in brackets denote 95% confidence intervals, computed in the Fisher space and back-transformed.\n* marks correlations statistical significance at p<0.05p<0.05.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PMOS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">log </span><math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">F</mi><mn mathsize=\"0.900em\">0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MCD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DS-WED</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemini 2.5 Pro</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.27*</span><span class=\"ltx_text\" style=\"font-size:50%;\">[0.16,&#8196;0.38]</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span><span class=\"ltx_text\" style=\"font-size:50%;\">[-0.08,&#8196;0.27]</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.16*</span><span class=\"ltx_text\" style=\"font-size:50%;\">[0.01,&#8196;0.30]</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.22*</span><span class=\"ltx_text\" style=\"font-size:50%;\">[0.05,&#8196;0.38]</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "pmos",
            "mcd",
            "r¯barr",
            "pearson",
            "f0f0",
            "marks",
            "transformation",
            "pro",
            "correlations",
            "average",
            "p005p005",
            "log",
            "backtransformed",
            "computed",
            "between",
            "dswed",
            "across",
            "metrics",
            "denote",
            "prosodyrelated",
            "aggregated",
            "confidence",
            "groups",
            "lalmasjudges",
            "values",
            "via",
            "intervals",
            "fisher",
            "gemini",
            "fisher’s",
            "space",
            "significance",
            "brackets",
            "rmse",
            "statistical"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Can large audio language models (LALMs) serve as reliable evaluators of prosodic differences?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe test Gemini 2.5 Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by prompting it to rate the relative prosodic difference between two samples within a group of five. It listens to all five samples to form a reference range of variation and then assigns a score from 1 to 5.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 4.4 Benchmarking Zero-shot TTS Systems &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Gemini&#8217;s scores show a statistically significant but weak correlation with human ratings, while correlations with objective metrics fluctuate with wide confidence intervals.\nCombined with high prompt sensitivity, these findings suggest that Gemini 2.5 Pro is not a reliable evaluator of prosodic variation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody diversity is essential for achieving naturalness and expressiveness in zero-shot text-to-speech (TTS).\nHowever, frequently used acoustic metrics capture only partial views of prosodic variation and correlate poorly with human perception, leaving the problem of reliably quantifying prosody diversity underexplored.\nTo bridge this gap, we introduce ProsodyEval, a prosody diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS) alongside conventional acoustic metrics.\nProsodyEval comprises 1000 speech samples derived from 7 mainstream TTS systems, with 2000 human ratings.\nBuilding on this, we propose the Discretized Speech Weighted Edit Distance (DS-WED), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic tokens.\nExperiments on ProsodyEval show that DS-WED achieves substantially higher correlation with human judgments than existing acoustic metrics, while remaining highly robust in speech tokenization from HuBERT and WavLM.\nLeveraging DS-WED, we benchmark state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS test-en, and further explorations uncover several factors that influence prosody diversity, including generative modeling paradigms, duration control, and reinforcement learning. Moreover, we find that current large audio language models (LALMs) remain limited in capturing prosodic variations.\nAudio samples are available at https://prosodyeval.github.io.</span>\n</p>\n\n",
                "matched_terms": [
                    "pmos",
                    "metrics",
                    "via",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prosody plays a central role in spoken communication, conveying paralinguistic information such as emotion, attitude, and intent that shapes how listeners interpret meaning. Subtle variations in pitch, intensity, and duration can fundamentally alter the interpretation of an utterance even with identical text, making prosody crucial to naturalness and expressiveness in speech synthesis.\nWhile zero-shot TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has advanced in intelligibility and speaker similarity, prosody diversity has received far less attention.\nExisting evaluation practices&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rely largely on log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> root mean squared error (RMSE), which correlates weakly with human judgments, captures pitch but neglects rhythm and intensity&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and requires costly dynamic time warping (DTW).\nConsequently, a clear gap remains between automatic metrics and human perception of prosody diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "f0f0",
                    "log",
                    "rmse",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent efforts have shown that both continuous&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and discrete&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> self-supervised (SSL) speech representations effectively encode prosodic information.\nConcurrently, NLP automatic evaluation metrics have been adapted to discrete speech representations for reference-free&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and reference-aware&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech quality assessment. In particular, SpeechTokenDistance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a simple approach but remains preliminary, as it focuses solely on measuring correlation with acoustic metrics without examining what the differences represent or further analyzing correlations with human perception.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With this perspective in mind, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ProsodyEval</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the first human-annotated dataset for prosody diversity assessment, and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DS-WED</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">iscretized </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eighted </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">dit </span>\n  <span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">D</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">istance), a new objective diversity metric that quantifies prosodic variation via weighted edit distance over semantic token sequences derived from speech SSL models such as HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> through </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-kmeans clustering.\nProsodyEval contains 1000 synthetic samples from 7 mainstream TTS systems, paired with 2000 human ratings of prosody diversity.\nExperiments show that DS-WED achieves substantially higher correlation with human judgments than frequently used acoustic metrics, including log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and Mel cepstral distortion (MCD), while remaining robust across models, layers, and cluster sizes.\nBuilding on DS-WED, we establish the first systematic benchmark of prosody diversity across state-of-the-art open-source TTS systems on LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOur further explorations reveal the impact of modeling paradigms, duration control, and reinforcement learning (RL), and demonstrate that even advanced large audio language models (LALMs) like Gemini 2.5 Pro remain unreliable for prosody evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "mcd",
                    "gemini",
                    "metrics",
                    "f0f0",
                    "log",
                    "rmse",
                    "dswed",
                    "pro",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We propose DS-WED, a new objective prosody diversity metric based on semantic token weighted edit distance that better correlates with human ratings than existing acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ProsodyEval incorporates two frequently used acoustic metrics, namely log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD.\nFor each pair of samples within a group, FastDTW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a linear-time approximation of DTW, is first applied to align their lengths, after which log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD are computed.\nTogether, these two metrics serve as complementary baseline indicators for evaluating prosodic variation.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "metrics",
                    "f0f0",
                    "log",
                    "computed",
                    "rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate correlations between human ratings and the three objective metrics DS-WED, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE, and MCD on the ProsodyEval dataset across all groups.\nFor DS-WED, speech is discretized by applying a 50-cluster </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means model trained on LibriSpeech 960h to the hidden embeddings from the 8th Transformer encoder layer.\nSince PMOS ratings are relative within groups and not comparable across groups, we compute Pearson correlation for each group and then aggregate the group-wise correlations using Fisher&#8217;s </span>\n  <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Z</mi>\n      <annotation encoding=\"application/x-tex\">Z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformation.\nStatistical significance is tested with a two-sided one-sample </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test against zero, and 95% confidence intervals are obtained in the Fisher space and back-transformed.</span>\n</p>\n\n",
                "matched_terms": [
                    "pmos",
                    "mcd",
                    "pearson",
                    "f0f0",
                    "transformation",
                    "correlations",
                    "log",
                    "backtransformed",
                    "between",
                    "dswed",
                    "across",
                    "metrics",
                    "confidence",
                    "groups",
                    "intervals",
                    "fisher",
                    "fisher’s",
                    "space",
                    "significance",
                    "rmse",
                    "statistical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.3 Acoustic Prosody Metrics &#8227; 2 ProsodyEval Dataset &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PMOS exhibits the strongest correlation with DS-WED (</span>\n  <math alttext=\"\\bar{r}=0.77\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.77</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.77</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), followed by a substantial correlation with MCD (</span>\n  <math alttext=\"\\bar{r}=0.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.66</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe correlation with log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE is weaker but still significant (</span>\n  <math alttext=\"\\bar{r}=0.30\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.30</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bar{r}=0.30</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), suggesting that pitch deviations contribute to perceived differences but capture only part of the variability.\nOverall, these results demonstrate that DS-WED aligns most closely with subjective human judgments, substantially outperforming widely used acoustic metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "pmos",
                    "mcd",
                    "metrics",
                    "f0f0",
                    "log",
                    "rmse",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Computational efficiency is measured on ProsodyEval by Real-Time Factor (RTF), computed as processing time divided by average speech-pair duration, on NVIDIA A100 with batch size 1.</span>\n</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE reaches an RTF of 0.549, and MCD reaches an RTF of 0.203. Both rely on signal-processing front-ends and DTW alignment, which are CPU-bound and difficult to accelerate on GPUs. In addition, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE requires mel-cepstrum computation for DTW, adding extra overhead.\nIn contrast, DS-WED involves only a forward pass through a pretrained speech-SSL encoder followed by </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering and edit distance at the discrete level, achieving an RTF of 0.110. It is GPU-friendly and can be further accelerated by batching.\nOverall, DS-WED is scalable for large-scale evaluation and practical for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "mcd",
                    "f0f0",
                    "log",
                    "rmse",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We study the effect of the SSL backbone, the encoder layer, and the number of </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clusters on the correlation between DS-WED and human ratings, using the ProsodyEval dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "dswed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Results</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19928v2#S4.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 4.3 Ablation Studies of DS-WED &#8227; 4 Experiments &#8227; Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that DS-WED remains quite robust across different layers, models, and vocabulary sizes, with correlations consistently around 0.7.\nMiddle layers 6-9 achieve stronger correlations, consistent with the richer encoded prosody information.\nRelative smaller cluster sizes perform best, while larger ones reduce peak correlations, making the edit-distance calculation overly sensitive and misaligned with human perceptual sensitivity to prosody.\nOverall, WavLM-base provides more stable correlations, while HuBERT-base exhibits larger variance. The 8th layer of HuBERT-base using 50 clusters achieves the strongest correlations.\nWe also tried the large versions, which yield slightly higher correlations but at much greater cost, hence we use the base versions for all experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "across",
                    "dswed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setup</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Our evaluations are conducted on two widely used benchmarks: professionally read audiobooks LibriSpeech </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and crowdsourced read speech Seed-TTS </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test-en</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo measure prosody diversity, we report two conventional acoustic metrics, log </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> RMSE and MCD, together with our proposed DS-WED. Each metric is computed in two ways: (1) micro average across all samples, and (2) rank-based Borda aggregation, where systems are ranked within each group and assigned scores from seven for the best system to one for the worst, and the average score across groups is reported, thereby eliminating the influence of absolute metric values.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "mcd",
                    "average",
                    "metrics",
                    "f0f0",
                    "log",
                    "computed",
                    "rmse",
                    "dswed",
                    "groups",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we introduce ProsodyEval, the first human-annotated dataset for assessing prosody diversity in zero-shot TTS, together with DS-WED, a new objective diversity metric based on weighted edit distance over semantic tokens.\nDS-WED outperforms frequently used acoustic metrics in correlating with human judgments and remains robust in speech discretization across models, layers, and cluster sizes, enabling more reliable evaluation of prosodic variation.\nLeveraging DS-WED, we systematically benchmark and analyze mainstream zero-shot TTS systems.\nOur findings indicate that (1) AR systems outperform NAR flow-matching models but not MGM, (2) duration variation strongly shapes prosody diversity, yet NAR systems lack duration control, and flow-matching models with implicit alignment suffer from inherent architectural limitations, (3) RL via DPO improves intelligibility while reducing prosody diversity, and (4) even advanced LALMs like Gemini 2.5 Pro remain unreliable for prosody evaluation.\nOne limitation lies in the cross-lingual applicability of DS-WED, validated only on English.\nLooking ahead, ProsodyEval and DS-WED fill a gap in zero-shot TTS evaluation and open new avenues for speech data engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "gemini",
                    "metrics",
                    "dswed",
                    "pro",
                    "via"
                ]
            }
        ]
    }
}