{
    "S2.T1": {
        "source_file": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
        "caption": "Table 1: Participant demographics in the MODMA dataset.\nM: Male, F: Female, HC: Healthy Control, and MDD: Major Depressive Disorder.",
        "body": "Modality\nTotal\nMDD\nHC\nAge\n\n\n\n\n(M/F)\n(M/F)\n(MDD/HC)\n\n\n128-ch EEG\n53\n24 (13/11)\n29 (20/9)\n16‚Äì56 / 18‚Äì55\n\n\nSpeech\n52\n23 (16/7)\n29 (20/9)\n16‚Äì56 / 18‚Äì55",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MDD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">HC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Age</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(M/F)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(M/F)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(MDD/HC)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">128-ch EEG</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">24 (13/11)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">29 (20/9)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">16&#8211;56 / 18&#8211;55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">23 (16/7)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">29 (20/9)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">16&#8211;56 / 18&#8211;55</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "demographics",
            "modma",
            "modality",
            "18‚Äì55",
            "male",
            "speech",
            "depressive",
            "mdd",
            "mddhc",
            "eeg",
            "age",
            "disorder",
            "dataset",
            "female",
            "healthy",
            "participant",
            "128ch",
            "total",
            "16‚Äì56",
            "control",
            "major"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study employs the Multi-modal Open Dataset for Mental-disorder Analysis (MODMA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which provides: (1) 5-minute resting-state EEG recorded with a 128-channel HydroCel Geodesic Sensor Net at 250&#8201;Hz, and (2) audio from structured clinical interviews. For each subject, the interview audio consists of </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> separate recordings (question&#8211;answer items) whose durations vary across and within subjects (the total interview time is approximately 25 minutes per subject). Since MODMA does not include text transcriptions from clinical interviews, we generate automatic transcriptions using speech-to-text models. The dataset comprises individuals diagnosed with Major Depressive Disorder (MDD), recruited from Lanzhou University Second Hospital, and healthy controls (HCC) obtained via public advertising; MDD diagnoses were confirmed by licensed psychiatrists. In this study, we retain only subjects who participated in both EEG and interview recordings, resulting in a filtered cohort of 38 subjects. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Data &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes demographic information across groups and protocols. Additional details are available in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "disorder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health condition predicted to become the second leading cause of disease burden by 2030 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with COVID-19 causing a 27.6 </span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rise in global cases </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn recent years, there has been growing interest in developing automatic depression detection systems to support clinical decision-making and enable telemedicine applications. More recently, multimodal approaches have gained particular attention, motivated by the fact that in clinical settings, such as diagnostic interviews, human expression is inherently multimodal, spanning speech, language, and neural activity. However, current studies often suffer from critical methodological gaps, including limited modality integration, inconsistent evaluation protocols, and potential data leakage, which hinder reproducibility and the fair assessment of model performance.\nModels that leverage two modalities dominate the field.\nNotable examples include&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who applied DenseNet121 to EEG and speech spectrograms from the MODMA dataset, and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who employed Vision Transformers on comparable EEG&#8211;speech data from MODMA. Other bimodal studies investigated EEG&#8211;speech integration with graph convolutional networks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech&#8211;text fusion on the E-DAIC dataset using CNN-LSTM attention&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and EEG&#8211;facial expression fusion&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an extensive speech&#8211;text comparative analysis with multiple fusion techniques was conducted, but EEG was entirely excluded. Overall, state-of-the-art performances in multimodal depression detection span roughly 85&#8211;97%, depending on the dataset and modality combinations.\nAll the aforementioned approaches only comprise two modalities, constraining their potential by overlooking trimodal approaches. Moreover, most of them exclude text modality and lack transparent data-splitting protocols.\nIn&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech, EEG, and text were integrated using GAT-CNN-MpNet architectures on MODMA, achieving about 90</span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> balanced performance through weighted late fusion, though without comparing handcrafted and pretrained features and with only basic fusion strategies explored. Moreover, the study did not clarify whether 5-fold cross-validation was performed at the segment or subject level.\nOur work addresses key limitations in multimodal depression detection by systematically exploring feature representations and modeling strategies across EEG, together with speech and text. We perform a complete comparative analysis of handcrafted features and pretrained embeddings, including, for the first time, brain-pretrained models, evaluate multiple deep learning architectures, and compare unimodal, bimodal, and trimodal configurations. We further investigate how different fusion strategies impact detection accuracy and robustness, with particular attention to the role of EEG. Using consistent subject-independent data splits to ensure reproducible benchmarking, we demonstrate that carefully designed trimodal models achieve state-of-the-art performance. Our study lays the groundwork for the future of multimodal depression detection, guiding the development of more accurate and robust systems. We make both the code and the model checkpoints available to foster transparency and reproducibility.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Link will be available upon acceptance.</span>\n    </span>\n  </span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "dataset",
                    "modma",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Many studies lack clarity in data splitting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where segment-level splits can leak information by placing recordings from the same subject in both training and test sets, yielding inflated performance. To avoid this, we use stratified 5-fold subject-level cross-validation with consistent splits across experiments. We also release these splits on our companion website to ensure reproducibility and fair comparison.\nTo address the lack of transcriptions in the MODMA dataset, we employed WhisperX&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate text for each subject&#8217;s 29 recordings, without further post-processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "modma",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a unified pipeline for multimodal depression detection with EEG, speech, and text.\nFor EEG, we adopt two processing branches: a 29-channel, 250 Hz, 10 s segmentation setup, consistent with prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a 19-channel, 200 Hz, 5 s segmentation setup replicating the preprocessing used in CBraMod&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the MUMTAZ depression dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n. For CBraMod, we evaluated both the original pre-trained version and the model fine-tuned on MUMTAZ, as described in the official documentation</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wjq-learning/CBraMod/blob/main/\" style=\"font-size:70%;\" title=\"\">https://github.com/wjq-learning/CBraMod/blob/main/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n, and found the latter consistently superior. Therefore, throughout this work we refer to CBraMod as the MUMTAZ-fine-tuned model.\nSpeech recordings are resampled to 16 kHz, denoised, and segmented into 5 s windows with 50% overlap, while text is used directly from raw Chinese transcriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Feature extraction combines handcrafted descriptors (EEG statistics, spectral power, entropy; speech MFCCs with/without prosody) with embeddings from large pre-trained models. For EEG, we employ both the Large Brain Model (LaBraM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">2,500 hours of EEG from 20 datasets, and CBraMod, a patch-based masked reconstruction model. For speech, we use XLSR-53&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multilingual wav2vec&#160;2.0 encoder, and Chinese HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 10k hours of WenetSpeech. For text, we use Chinese BERT Base&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MacBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, XLNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MPNet Multilingual&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Segment-level representations are encoded with a combination of CNNs, LSTMs, and/or GRUs (with/without attention) and fused using decision-level strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a recording of duration </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds (post-trimming), the number of segments is\n</span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8970;</mo>\n            <mrow>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mrow>\n                  <mi mathsize=\"0.900em\">L</mi>\n                  <mo mathsize=\"0.900em\">&#8722;</mo>\n                  <mi mathsize=\"0.900em\">w</mi>\n                </mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mi mathsize=\"0.900em\">h</mi>\n            </mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8971;</mo>\n          </mrow>\n          <mo mathsize=\"0.900em\">+</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"L\\geq w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mi mathsize=\"0.900em\">w</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">L\\geq w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while recordings shorter than </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are retained as a single segment. The segmented waveform is represented as </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where each row corresponds to one waveform segment. Each subject has </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> interview recordings; after windowing, recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> yields </span>\n  <math alttext=\"S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msubsup>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mi mathsize=\"0.900em\">r</mi>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n            </msubsup>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The subject-level speech representation is the concatenation along the segment axis:\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwith a total of </span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\" rspace=\"0.111em\">=</mo>\n        <mrow>\n          <msubsup>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">&#8721;</mo>\n            <mrow>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo mathsize=\"0.900em\">=</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n          </msubsup>\n          <msubsup>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We re-implement two multimodal baselines for depression detection that use standard image architectures on EEG and speech </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">2D-spectrograms (Spec2D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: DenseNet-121&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Vision Transformer (ViT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These studies are among the few that explore EEG&#8211;speech multimodality in this task and report promising results. In our experiments, we retain their model architectures but apply our own subject-level cross-validation splits for consistency, making results not directly comparable to the original works. Additional implementation details are provided on our companion website.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To keep notation light, we use </span>\n  <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119813;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to denote the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">generic feature matrix</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per modality, either handcrafted features or embeddings from pretrained models.\nConcretely, </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (EEG), </span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech, per recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (text, subject-level).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Unimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports the performance of baseline and unimodal models. Among EEG features, CBraMod embeddings combined with a GRU and attention achieved the best result, confirming the benefit of pre-training on a depression-related corpus. For speech, both XLSR-53 and HuBERT embeddings provided strong performance, with XLSR-53 coupled with a CNN+GRU slightly outperforming. Handcrafted MFCC and prosodic features yielded considerably lower scores, indicating that deep speech embeddings capture richer information. In the text modality, all transformer-based embeddings performed competitively, with Chinese MacBERT and XLNet reaching the top results. Overall, unimodal experiments highlight that text provided the most informative single modality, while speech embeddings also achieved strong performance, and EEG remained less predictive in isolation.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the baselines with different fusion strategies. Simple baselines such as ViT and DenseNet-121 reached F1-scores around 0.56. Fusion strategies, however, substantially outperformed unimodal and baseline models. Weighted averaging already boosted performance when fusing EEG and Text, and Bayesian fusion further improved results, with Speech+Text achieving the highest F1-score overall. Majority voting also proved effective, with the tri-modal configuration EEG+Speech+Text reaching </span>\n  <math alttext=\"F_{1}=0.874\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.874</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}=0.874</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These results confirm the complementarity of modalities: while text dominates in unimodal settings, integrating speech and EEG consistently improves robustness and yields the strongest overall performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Framework for Multimodal Depression Detection &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Building on this systematic exploration of feature extraction methods, neural architectures, and fusion strategies, we propose an experimental framework for multimodal depression detection, illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.3 Data Preprocessing &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The framework selects the best-performing predictors for each modality: </span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">CBraMod</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with a GRU+Attn for EEG, </span>\n  <math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">XLSR</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprocessed with a CNN+GRU for speech, and\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{MacBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">MacBERT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MacBERT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with an LSTM for text. These modality-specific pipelines are then combined through alternative fusion strategies. This design allows us to isolate the contribution of each fusion method while keeping the strongest unimodal configurations fixed. Our best-performing architecture employs majority voting across the three modalities, achieving an accuracy of 88.6% and an F1-score of 87.4%, to the best of our knowledge, establishing the state of the art in multimodal depression detection. The framework thus serves as a reference setup for future experiments, enabling systematic evaluation of new fusion strategies or additional modalities. To the best of our knowledge, our tri-modal configuration with majority voting fusion represents the current state of the art in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We addressed key limitations in multimodal depression detection by adopting subject-level stratified cross-validation and exploring EEG-based representations in combination with speech and text. Our experiments compared handcrafted features with deep representations from large pretrained models, consistently showing the superiority of the latter. In the unimodal setting, CNN+GRU proved effective for speech, while LSTM architectures yielded the best results for EEG and text. In the multimodal setting, late-fusion methods further improved performance, with Majority Voting across all three modalities achieving the strongest results, which to the best of our knowledge represents the current state of the art. Beyond the best-performing configuration, we introduce an experimental framework that fixes the optimal unimodal predictors and systematically evaluates alternative fusion strategies. This framework serves as a reference setup for future work, and by releasing all code and preprocessing scripts in a public repository, we ensure reproducibility and support further advances in multimodal depression detection research.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work uses data from the MODMA dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provided by the Gansu Provincial Key Laboratory of Wearable Computing, Lanzhou University, China. We gratefully acknowledge the data contributors for granting access to the data.</span>\n</p>\n\n",
                "matched_terms": [
                    "modma",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study was conducted retrospectively using human subject data from the MODMA dataset &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Access to the dataset is granted by the data owners upon request, and we do not redistribute any data. According to the terms of use specified by the dataset providers, separate ethical approval was not required for our analyses. All experiments were carried out in compliance with these conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "modma",
                    "dataset"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
        "caption": "Table 2: Shapes of speech feature matrices per recording rr and at the subject level.",
        "body": "Feature name\nPer-recording shape\nSubject-level shape\n\n\nùêóMFCC\\mathbf{X}_{\\text{MFCC}}\n‚ÑùSSPEECH(r)√ó40\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 40}\n‚ÑùSSPEECH√ó40\\mathbb{R}^{S_{\\text{SPEECH}}\\times 40}\n\n\nùêóPROSODY+MFCC\\mathbf{X}_{\\text{PROSODY+MFCC}}\n‚ÑùSSPEECH(r)√ó46\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 46}\n‚ÑùSSPEECH√ó46\\mathbb{R}^{S_{\\text{SPEECH}}\\times 46}\n\n\nùêóXLSR\\mathbf{X}_{\\text{XLSR}}\n‚ÑùSSPEECH(r)√ó1024\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 1024}\n‚ÑùSSPEECH√ó1024\\mathbb{R}^{S_{\\text{SPEECH}}\\times 1024}\n\n\nùêóHuBERT\\mathbf{X}_{\\text{HuBERT}}\n‚ÑùSSPEECH(r)√ó768\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 768}\n‚ÑùSSPEECH√ó768\\mathbb{R}^{S_{\\text{SPEECH}}\\times 768}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Feature name</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Per-recording shape</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subject-level shape</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\mathbf{X}_{\\text{MFCC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m3\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">&#119831;</mi><mtext mathsize=\"0.900em\">MFCC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MFCC}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 40}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m4\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msubsup><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">r</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">40</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 40}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}\\times 40}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m5\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msub><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">40</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}\\times 40}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><math alttext=\"\\mathbf{X}_{\\text{PROSODY+MFCC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m6\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">&#119831;</mi><mtext mathsize=\"0.900em\">PROSODY+MFCC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{PROSODY+MFCC}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 46}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m7\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msubsup><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">r</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">46</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 46}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}\\times 46}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m8\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msub><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">46</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}\\times 46}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m9\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">&#119831;</mi><mtext mathsize=\"0.900em\">XLSR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 1024}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m10\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msubsup><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">r</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">1024</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 1024}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}\\times 1024}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m11\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msub><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">1024</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}\\times 1024}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><math alttext=\"\\mathbf{X}_{\\text{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m12\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">&#119831;</mi><mtext mathsize=\"0.900em\">HuBERT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{HuBERT}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m13\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msubsup><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">r</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">768</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times 768}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><math alttext=\"\\mathbb{R}^{S_{\\text{SPEECH}}\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m14\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">&#8477;</mi><mrow><msub><mi mathsize=\"0.900em\">S</mi><mtext mathsize=\"0.900em\">SPEECH</mtext></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><mn mathsize=\"0.900em\">768</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{S_{\\text{SPEECH}}\\times 768}</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "‚Ñùsspeechr√ó40mathbbrstextspeechrtimes",
            "‚Ñùsspeech√ó768mathbbrstextspeechtimes",
            "perrecording",
            "shape",
            "name",
            "speech",
            "‚Ñùsspeechr√ó1024mathbbrstextspeechrtimes",
            "‚Ñùsspeech√ó40mathbbrstextspeechtimes",
            "ùêómfccmathbfxtextmfcc",
            "shapes",
            "ùêóxlsrmathbfxtextxlsr",
            "subject",
            "subjectlevel",
            "‚Ñùsspeech√ó1024mathbbrstextspeechtimes",
            "matrices",
            "‚Ñùsspeech√ó46mathbbrstextspeechtimes",
            "‚Ñùsspeechr√ó46mathbbrstextspeechrtimes",
            "ùêóhubertmathbfxtexthubert",
            "level",
            "feature",
            "ùêóprosodymfccmathbfxtextprosodymfcc",
            "‚Ñùsspeechr√ó768mathbbrstextspeechrtimes",
            "recording"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> From each waveform segment, we compute two handcrafted variant, namely MFCCs (40 coefficients) and Prosody </span>\n  <math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">+</mo>\n      <annotation encoding=\"application/x-tex\">+</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MFCCs (46 features: 40 MFCCs plus energy, </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, RMS energy, pause rate, phonation time, speech rate). We also extract segment embeddings with XLSR-53 and Chinese HuBERT Large.\nSegment-level features are stacked per recording and then concatenated across the 29 recordings of each subject to form the subject-level representation.\nThe exact tensor shapes (per recording and subject-level) are summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter feature extraction, the raw sample length is no longer present; each segment is represented by a fixed-size vector (40/46/768/1024 dimensions).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the subject-level text features (Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nA detection module (LSTM or CNN) transforms </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into </span>\n  <math alttext=\"\\mathbf{H}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an MLP head outputs </span>\n  <math alttext=\"y_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health condition predicted to become the second leading cause of disease burden by 2030 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with COVID-19 causing a 27.6 </span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rise in global cases </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn recent years, there has been growing interest in developing automatic depression detection systems to support clinical decision-making and enable telemedicine applications. More recently, multimodal approaches have gained particular attention, motivated by the fact that in clinical settings, such as diagnostic interviews, human expression is inherently multimodal, spanning speech, language, and neural activity. However, current studies often suffer from critical methodological gaps, including limited modality integration, inconsistent evaluation protocols, and potential data leakage, which hinder reproducibility and the fair assessment of model performance.\nModels that leverage two modalities dominate the field.\nNotable examples include&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who applied DenseNet121 to EEG and speech spectrograms from the MODMA dataset, and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who employed Vision Transformers on comparable EEG&#8211;speech data from MODMA. Other bimodal studies investigated EEG&#8211;speech integration with graph convolutional networks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech&#8211;text fusion on the E-DAIC dataset using CNN-LSTM attention&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and EEG&#8211;facial expression fusion&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an extensive speech&#8211;text comparative analysis with multiple fusion techniques was conducted, but EEG was entirely excluded. Overall, state-of-the-art performances in multimodal depression detection span roughly 85&#8211;97%, depending on the dataset and modality combinations.\nAll the aforementioned approaches only comprise two modalities, constraining their potential by overlooking trimodal approaches. Moreover, most of them exclude text modality and lack transparent data-splitting protocols.\nIn&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech, EEG, and text were integrated using GAT-CNN-MpNet architectures on MODMA, achieving about 90</span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> balanced performance through weighted late fusion, though without comparing handcrafted and pretrained features and with only basic fusion strategies explored. Moreover, the study did not clarify whether 5-fold cross-validation was performed at the segment or subject level.\nOur work addresses key limitations in multimodal depression detection by systematically exploring feature representations and modeling strategies across EEG, together with speech and text. We perform a complete comparative analysis of handcrafted features and pretrained embeddings, including, for the first time, brain-pretrained models, evaluate multiple deep learning architectures, and compare unimodal, bimodal, and trimodal configurations. We further investigate how different fusion strategies impact detection accuracy and robustness, with particular attention to the role of EEG. Using consistent subject-independent data splits to ensure reproducible benchmarking, we demonstrate that carefully designed trimodal models achieve state-of-the-art performance. Our study lays the groundwork for the future of multimodal depression detection, guiding the development of more accurate and robust systems. We make both the code and the model checkpoints available to foster transparency and reproducibility.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Link will be available upon acceptance.</span>\n    </span>\n  </span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature",
                    "subject",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Many studies lack clarity in data splitting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where segment-level splits can leak information by placing recordings from the same subject in both training and test sets, yielding inflated performance. To avoid this, we use stratified 5-fold subject-level cross-validation with consistent splits across experiments. We also release these splits on our companion website to ensure reproducibility and fair comparison.\nTo address the lack of transcriptions in the MODMA dataset, we employed WhisperX&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate text for each subject&#8217;s 29 recordings, without further post-processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "subjectlevel",
                    "subject"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Feature extraction combines handcrafted descriptors (EEG statistics, spectral power, entropy; speech MFCCs with/without prosody) with embeddings from large pre-trained models. For EEG, we employ both the Large Brain Model (LaBraM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">2,500 hours of EEG from 20 datasets, and CBraMod, a patch-based masked reconstruction model. For speech, we use XLSR-53&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multilingual wav2vec&#160;2.0 encoder, and Chinese HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 10k hours of WenetSpeech. For text, we use Chinese BERT Base&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MacBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, XLNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MPNet Multilingual&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Segment-level representations are encoded with a combination of CNNs, LSTMs, and/or GRUs (with/without attention) and fused using decision-level strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a recording of duration </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds (post-trimming), the number of segments is\n</span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8970;</mo>\n            <mrow>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mrow>\n                  <mi mathsize=\"0.900em\">L</mi>\n                  <mo mathsize=\"0.900em\">&#8722;</mo>\n                  <mi mathsize=\"0.900em\">w</mi>\n                </mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mi mathsize=\"0.900em\">h</mi>\n            </mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8971;</mo>\n          </mrow>\n          <mo mathsize=\"0.900em\">+</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"L\\geq w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mi mathsize=\"0.900em\">w</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">L\\geq w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while recordings shorter than </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are retained as a single segment. The segmented waveform is represented as </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where each row corresponds to one waveform segment. Each subject has </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> interview recordings; after windowing, recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> yields </span>\n  <math alttext=\"S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msubsup>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mi mathsize=\"0.900em\">r</mi>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n            </msubsup>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The subject-level speech representation is the concatenation along the segment axis:\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwith a total of </span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\" rspace=\"0.111em\">=</mo>\n        <mrow>\n          <msubsup>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">&#8721;</mo>\n            <mrow>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo mathsize=\"0.900em\">=</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n          </msubsup>\n          <msubsup>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjectlevel",
                    "recording",
                    "subject"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nEach recording has a single transcript. After tokenization, the subject-level text representation is the concatenation of all transcript representations,\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{TEXT}}=\\big[\\mathbf{X}_{\\text{TEXT}}^{(1)};\\dots;\\mathbf{X}_{\\text{TEXT}}^{(R)}\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">TEXT</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">TEXT</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">TEXT</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{TEXT}}=\\big[\\mathbf{X}_{\\text{TEXT}}^{(1)};\\dots;\\mathbf{X}_{\\text{TEXT}}^{(R)}\\big]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "subjectlevel",
                    "recording"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Remark.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAfter feature extraction, the raw temporal dimension (</span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <math alttext=\"T_{\\text{patch}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mtext mathsize=\"0.900em\">patch</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">T_{\\text{patch}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">)\nis no longer present, as each segment is reduced to a fixed-size representation\nof dimension </span>\n  <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">F</mi>\n      <annotation encoding=\"application/x-tex\">F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (handcrafted) or </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (embeddings).\nFor subject-level modeling, features from all recordings of the same subject\nare stacked to form the final subject representation.</span>\n</p>\n\n",
                "matched_terms": [
                    "subjectlevel",
                    "feature",
                    "subject"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n\nEach recording has a single transcript, which we encode with a pretrained language model (BERT, MacBERT, XLNet, or MPNet) to obtain one </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embedding per recording; for a subject with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> recordings, stacking these yields </span>\n  <math alttext=\"\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">BERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MacBERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">XLNet</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MPNet</mtext>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "recording",
                    "subject"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We re-implement two multimodal baselines for depression detection that use standard image architectures on EEG and speech </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">2D-spectrograms (Spec2D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: DenseNet-121&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Vision Transformer (ViT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These studies are among the few that explore EEG&#8211;speech multimodality in this task and report promising results. In our experiments, we retain their model architectures but apply our own subject-level cross-validation splits for consistency, making results not directly comparable to the original works. Additional implementation details are provided on our companion website.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjectlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To keep notation light, we use </span>\n  <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119813;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to denote the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">generic feature matrix</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per modality, either handcrafted features or embeddings from pretrained models.\nConcretely, </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (EEG), </span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech, per recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (text, subject-level).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjectlevel",
                    "recording",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nA shallow CNN extracts segment-level features from each recording.\nThese are reduced to a single fixed-size vector\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">d</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nusing one of three encoders:\n(i) max pooling,\n(ii) GRU with attention, or\n(iii) BiGRU with attention (the latter extending the GRU+Attn design with bidirectional recurrence).\nThe resulting </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vectors are stacked into the subject-level matrix\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis sequence is then processed by an LSTM to produce the subject-level representation\n</span>\n  <math alttext=\"\\mathbf{H}_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwhich is fed to an MLP head to obtain the final prediction\n</span>\n  <math alttext=\"y_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjectlevel",
                    "recording"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Framework for Multimodal Depression Detection &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Building on this systematic exploration of feature extraction methods, neural architectures, and fusion strategies, we propose an experimental framework for multimodal depression detection, illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.3 Data Preprocessing &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The framework selects the best-performing predictors for each modality: </span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">CBraMod</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with a GRU+Attn for EEG, </span>\n  <math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">XLSR</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprocessed with a CNN+GRU for speech, and\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{MacBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">MacBERT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MacBERT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with an LSTM for text. These modality-specific pipelines are then combined through alternative fusion strategies. This design allows us to isolate the contribution of each fusion method while keeping the strongest unimodal configurations fixed. Our best-performing architecture employs majority voting across the three modalities, achieving an accuracy of 88.6% and an F1-score of 87.4%, to the best of our knowledge, establishing the state of the art in multimodal depression detection. The framework thus serves as a reference setup for future experiments, enabling systematic evaluation of new fusion strategies or additional modalities. To the best of our knowledge, our tri-modal configuration with majority voting fusion represents the current state of the art in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature",
                    "ùêóxlsrmathbfxtextxlsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We addressed key limitations in multimodal depression detection by adopting subject-level stratified cross-validation and exploring EEG-based representations in combination with speech and text. Our experiments compared handcrafted features with deep representations from large pretrained models, consistently showing the superiority of the latter. In the unimodal setting, CNN+GRU proved effective for speech, while LSTM architectures yielded the best results for EEG and text. In the multimodal setting, late-fusion methods further improved performance, with Majority Voting across all three modalities achieving the strongest results, which to the best of our knowledge represents the current state of the art. Beyond the best-performing configuration, we introduce an experimental framework that fixes the optimal unimodal predictors and systematically evaluates alternative fusion strategies. This framework serves as a reference setup for future work, and by releasing all code and preprocessing scripts in a public repository, we ensure reproducibility and support further advances in multimodal depression detection research.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjectlevel"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
        "caption": "Table 3: Results of baselines and unimodal models (F1-score, mean ¬±\\pm std across 5 folds).\nIn bold, the best performing model‚Äìfeature pair per modality.",
        "body": "Category\n\n\n\n\nFeatures\n\n\n\n\nModel\n\n\n\n\nF1\n\n\n\n\n\n\n \n\n\nBaselines\n\n(Speech+EEG)\n\n\n\n\nùêóSpec2D\\mathbf{X}_{\\text{Spec2D}}\n\n\n\n\nViT\n\n\n\n\n0.560 ¬±\\pm 0.190\n\n\n\n\n\n\n\nùêóSpec2D\\mathbf{X}_{\\text{Spec2D}}\n\n\n\n\nDenseNet-121\n\n\n\n\n0.586 ¬±\\pm 0.240\n\n\n\n\n\n\n \n\n\nEEG\n\n\n\n\nùêóHAND\\mathbf{X}_{\\text{HAND}}\n\n\n\n\nCNN+LSTM\n\n\n\n\n0.585 ¬±\\pm 0.102\n\n\n\n\n\n\n\nùêóLaBraM\\mathbf{X}_{\\text{LaBraM}}\n\n\n\n\nGRU+Attn\n\n\n\n\n0.508 ¬±\\pm 0.075\n\n\n\n\n\n\n\nùêóCBraMod\\mathbf{X}_{\\text{CBraMod}}\n\n\n\n\nGRU+Attn\n\n\n\n\n0.600 ¬±\\pm 0.173\n\n\n\n\n\n\n \n\n\nSpeech\n\n\n\n\nùêóMFCC\\mathbf{X}_{\\text{MFCC}}\n\n\n\n\nCNN+MaxPool+LSTM\n\n\n\n\n0.554 ¬±\\pm 0.125\n\n\n\n\n\n\n\nùêóProsody+MFCC\\mathbf{X}_{\\text{Prosody+MFCC}}\n\n\n\n\nCNN+BiGRU+Attn+LSTM\n\n\n\n\n0.673 ¬±\\pm 0.152\n\n\n\n\n\n\n\nùêóHuBERT\\mathbf{X}_{\\text{HuBERT}}\n\n\n\n\nCNN+BiGRU+Attn+LSTM\n\n\n\n\n0.809 ¬±\\pm 0.073\n\n\n\n\n\n\n\nùêóXLSR\\mathbf{X}_{\\text{XLSR}}\n\n\n\n\nCNN+GRU+LSTM\n\n\n\n\n0.814 ¬±\\pm 0.052\n\n\n\n\n\n\n \n\n\nText\n\n\n\n\nùêóMPNet\\mathbf{X}_{\\text{MPNet}}\n\n\n\n\nCNN\n\n\n\n\n0.865 ¬±\\pm 0.085\n\n\n\n\n\n\n\nùêóBERT\\mathbf{X}_{\\text{BERT}}\n\n\n\n\nCNN\n\n\n\n\n0.839 ¬±\\pm 0.123\n\n\n\n\n\n\n\nùêóXLNet\\mathbf{X}_{\\text{XLNet}}\n\n\n\n\nLSTM\n\n\n\n\n0.671 ¬±\\pm 0.099\n\n\n\n\n\n\n\nùêóMacBERT\\mathbf{X}_{\\text{MacBERT}}\n\n\n\n\nLSTM\n\n\n\n\n0.868 ¬±\\pm 0.119",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Category</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Features</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">F1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T3.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 0.5pt;\">Baselines</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 0.5pt;\">(Speech+EEG)</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{Spec2D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">Spec2D</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{Spec2D}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">ViT</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.560 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.190</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{Spec2D}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">Spec2D</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{Spec2D}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">DenseNet-121</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.586 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.240</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T3.p2\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 0.5pt;\">EEG</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{HAND}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">HAND</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{HAND}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN+LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.585 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.102</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{LaBraM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">LaBraM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{LaBraM}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">GRU+Attn</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.508 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.075</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">CBraMod</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">GRU+Attn</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.600 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.173</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T3.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 0.5pt;\">Speech</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{MFCC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">MFCC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MFCC}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN+MaxPool+LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.554 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.125</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{Prosody+MFCC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m15\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">Prosody+MFCC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{Prosody+MFCC}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN+BiGRU+Attn+LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.673 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.152</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m17\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">HuBERT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{HuBERT}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN+BiGRU+Attn+LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.809 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.073</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m19\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">XLSR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">CNN+GRU+LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.814 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.052</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T3.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 0.5pt;\">Text</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{MPNet}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m21\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">MPNet</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MPNet}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.865 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.085</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{BERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m23\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">BERT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{BERT}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">CNN</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.839 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.123</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{XLNet}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m25\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">XLNet</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLNet}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.671 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.099</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:82.5pt;padding:0.6pt 0.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:42.7pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><math alttext=\"\\mathbf{X}_{\\text{MacBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m27\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.700em\">&#119831;</mi><mtext mathsize=\"0.700em\">MacBERT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MacBERT}}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:76.8pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">LSTM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:56.9pt;padding:0.6pt 0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.868 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.119</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ùêómpnetmathbfxtextmpnet",
            "cnn",
            "features",
            "ùêólabrammathbfxtextlabram",
            "text",
            "ùêómacbertmathbfxtextmacbert",
            "f1score",
            "std",
            "folds",
            "modality",
            "cnnmaxpoollstm",
            "¬±pm",
            "speech",
            "cnnbigruattnlstm",
            "unimodal",
            "ùêóspec2dmathbfxtextspec2d",
            "across",
            "ùêóxlnetmathbfxtextxlnet",
            "ùêómfccmathbfxtextmfcc",
            "mean",
            "ùêóxlsrmathbfxtextxlsr",
            "results",
            "performing",
            "model‚Äìfeature",
            "model",
            "eeg",
            "cnnlstm",
            "pair",
            "lstm",
            "densenet121",
            "vit",
            "gruattn",
            "cnngrulstm",
            "ùêócbramodmathbfxtextcbramod",
            "bold",
            "ùêóhubertmathbfxtexthubert",
            "category",
            "models",
            "ùêóhandmathbfxtexthand",
            "ùêóbertmathbfxtextbert",
            "ùêóprosodymfccmathbfxtextprosodymfcc",
            "best",
            "speecheeg",
            "baselines"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we report the performance of all experimental categories: baseline re-implementations, unimodal models, and our proposed multimodal architectures. F1-scores are reported as mean &#177; standard deviation across folds. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the baselines and unimodal models, including the best-performing model for each set of features per modality. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports the performance of baseline models and multimodal fusion strategies, highlighting the best configuration within each category and the overall best-performing model. Further results are available on our companion website.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Unimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports the performance of baseline and unimodal models. Among EEG features, CBraMod embeddings combined with a GRU and attention achieved the best result, confirming the benefit of pre-training on a depression-related corpus. For speech, both XLSR-53 and HuBERT embeddings provided strong performance, with XLSR-53 coupled with a CNN+GRU slightly outperforming. Handcrafted MFCC and prosodic features yielded considerably lower scores, indicating that deep speech embeddings capture richer information. In the text modality, all transformer-based embeddings performed competitively, with Chinese MacBERT and XLNet reaching the top results. Overall, unimodal experiments highlight that text provided the most informative single modality, while speech embeddings also achieved strong performance, and EEG remained less predictive in isolation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health condition predicted to become the second leading cause of disease burden by 2030 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with COVID-19 causing a 27.6 </span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rise in global cases </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn recent years, there has been growing interest in developing automatic depression detection systems to support clinical decision-making and enable telemedicine applications. More recently, multimodal approaches have gained particular attention, motivated by the fact that in clinical settings, such as diagnostic interviews, human expression is inherently multimodal, spanning speech, language, and neural activity. However, current studies often suffer from critical methodological gaps, including limited modality integration, inconsistent evaluation protocols, and potential data leakage, which hinder reproducibility and the fair assessment of model performance.\nModels that leverage two modalities dominate the field.\nNotable examples include&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who applied DenseNet121 to EEG and speech spectrograms from the MODMA dataset, and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who employed Vision Transformers on comparable EEG&#8211;speech data from MODMA. Other bimodal studies investigated EEG&#8211;speech integration with graph convolutional networks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech&#8211;text fusion on the E-DAIC dataset using CNN-LSTM attention&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and EEG&#8211;facial expression fusion&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an extensive speech&#8211;text comparative analysis with multiple fusion techniques was conducted, but EEG was entirely excluded. Overall, state-of-the-art performances in multimodal depression detection span roughly 85&#8211;97%, depending on the dataset and modality combinations.\nAll the aforementioned approaches only comprise two modalities, constraining their potential by overlooking trimodal approaches. Moreover, most of them exclude text modality and lack transparent data-splitting protocols.\nIn&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech, EEG, and text were integrated using GAT-CNN-MpNet architectures on MODMA, achieving about 90</span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> balanced performance through weighted late fusion, though without comparing handcrafted and pretrained features and with only basic fusion strategies explored. Moreover, the study did not clarify whether 5-fold cross-validation was performed at the segment or subject level.\nOur work addresses key limitations in multimodal depression detection by systematically exploring feature representations and modeling strategies across EEG, together with speech and text. We perform a complete comparative analysis of handcrafted features and pretrained embeddings, including, for the first time, brain-pretrained models, evaluate multiple deep learning architectures, and compare unimodal, bimodal, and trimodal configurations. We further investigate how different fusion strategies impact detection accuracy and robustness, with particular attention to the role of EEG. Using consistent subject-independent data splits to ensure reproducible benchmarking, we demonstrate that carefully designed trimodal models achieve state-of-the-art performance. Our study lays the groundwork for the future of multimodal depression detection, guiding the development of more accurate and robust systems. We make both the code and the model checkpoints available to foster transparency and reproducibility.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Link will be available upon acceptance.</span>\n    </span>\n  </span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "model",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "cnnlstm",
                    "models",
                    "densenet121",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study employs the Multi-modal Open Dataset for Mental-disorder Analysis (MODMA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which provides: (1) 5-minute resting-state EEG recorded with a 128-channel HydroCel Geodesic Sensor Net at 250&#8201;Hz, and (2) audio from structured clinical interviews. For each subject, the interview audio consists of </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> separate recordings (question&#8211;answer items) whose durations vary across and within subjects (the total interview time is approximately 25 minutes per subject). Since MODMA does not include text transcriptions from clinical interviews, we generate automatic transcriptions using speech-to-text models. The dataset comprises individuals diagnosed with Major Depressive Disorder (MDD), recruited from Lanzhou University Second Hospital, and healthy controls (HCC) obtained via public advertising; MDD diagnoses were confirmed by licensed psychiatrists. In this study, we retain only subjects who participated in both EEG and interview recordings, resulting in a filtered cohort of 38 subjects. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Data &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes demographic information across groups and protocols. Additional details are available in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "eeg",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Many studies lack clarity in data splitting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where segment-level splits can leak information by placing recordings from the same subject in both training and test sets, yielding inflated performance. To avoid this, we use stratified 5-fold subject-level cross-validation with consistent splits across experiments. We also release these splits on our companion website to ensure reproducibility and fair comparison.\nTo address the lack of transcriptions in the MODMA dataset, we employed WhisperX&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate text for each subject&#8217;s 29 recordings, without further post-processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a unified pipeline for multimodal depression detection with EEG, speech, and text.\nFor EEG, we adopt two processing branches: a 29-channel, 250 Hz, 10 s segmentation setup, consistent with prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a 19-channel, 200 Hz, 5 s segmentation setup replicating the preprocessing used in CBraMod&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the MUMTAZ depression dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n. For CBraMod, we evaluated both the original pre-trained version and the model fine-tuned on MUMTAZ, as described in the official documentation</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wjq-learning/CBraMod/blob/main/\" style=\"font-size:70%;\" title=\"\">https://github.com/wjq-learning/CBraMod/blob/main/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n, and found the latter consistently superior. Therefore, throughout this work we refer to CBraMod as the MUMTAZ-fine-tuned model.\nSpeech recordings are resampled to 16 kHz, denoised, and segmented into 5 s windows with 50% overlap, while text is used directly from raw Chinese transcriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "eeg",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Feature extraction combines handcrafted descriptors (EEG statistics, spectral power, entropy; speech MFCCs with/without prosody) with embeddings from large pre-trained models. For EEG, we employ both the Large Brain Model (LaBraM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">2,500 hours of EEG from 20 datasets, and CBraMod, a patch-based masked reconstruction model. For speech, we use XLSR-53&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multilingual wav2vec&#160;2.0 encoder, and Chinese HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 10k hours of WenetSpeech. For text, we use Chinese BERT Base&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MacBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, XLNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MPNet Multilingual&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Segment-level representations are encoded with a combination of CNNs, LSTMs, and/or GRUs (with/without attention) and fused using decision-level strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "model",
                    "text",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The preprocessing stage serves multiple objectives, including cleaning and structuring the raw data, as well as preparing it for multimodal analysis.\nOne key objective is the segmentation of the input into smaller units that can be more effectively processed by the models.\nWe denote with </span>\n  <math alttext=\"\\mathbf{S}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{S}_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\mathbf{S}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the number of segments obtained after preprocessing for each input modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor handcrafted features and LaBraM, we follow prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which comprises retaining </span>\n  <math alttext=\"C=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">C</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">C=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> channels</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Full list available on our companion website. Link will be available upon acceptance.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying a 0.5&#8211;50&#8201;Hz bandpass filter with a 50&#8201;Hz notch, and average re-referencing. Recordings are segmented into 10&#8201;s windows; at 250&#8201;Hz, each window contains </span>\n  <math alttext=\"T=250\\times 10=2500\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">250</mn>\n          <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">2500</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T=250\\times 10=2500</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> samples. Thus, a recording of length </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds produces </span>\n  <math alttext=\"S_{\\text{EEG}}=L/10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{EEG}}=L/10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> windows (e.g., </span>\n  <math alttext=\"S_{\\text{EEG}}=30\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">30</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{EEG}}=30</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for a 5-min recording), represented as </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{S_{\\text{EEG}}\\times C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">EEG</mtext>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{S_{\\text{EEG}}\\times C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "eeg",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Handcrafted features.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor each segment </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwe extract </span>\n  <math alttext=\"F=10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F=10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> hancrafted descriptors per channel\n(statistical, spectral, entropy), yielding\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{HAND}}\\in\\mathbb{R}^{S\\times C\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">HAND</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{HAND}}\\in\\mathbb{R}^{S\\times C\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "eeg",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-trained models.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe further extract embeddings from LaBraM and\nCBraMod.\nLaBraM operates on </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and maps each segment to a\n</span>\n  <math alttext=\"D=200\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embedding, producing\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{LaBraM}}\\in\\mathbb{R}^{S\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">LaBraM</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{LaBraM}}\\in\\mathbb{R}^{S\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nCBraMod operates on </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(2)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where each 5&#8201;s segment is\npatch-encoded and then averaged across channels and patches to form\n</span>\n  <math alttext=\"D=200\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embeddings, resulting in\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\\in\\mathbb{R}^{S\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">CBraMod</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}\\in\\mathbb{R}^{S\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> From each waveform segment, we compute two handcrafted variant, namely MFCCs (40 coefficients) and Prosody </span>\n  <math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">+</mo>\n      <annotation encoding=\"application/x-tex\">+</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MFCCs (46 features: 40 MFCCs plus energy, </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, RMS energy, pause rate, phonation time, speech rate). We also extract segment embeddings with XLSR-53 and Chinese HuBERT Large.\nSegment-level features are stacked per recording and then concatenated across the 29 recordings of each subject to form the subject-level representation.\nThe exact tensor shapes (per recording and subject-level) are summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter feature extraction, the raw sample length is no longer present; each segment is represented by a fixed-size vector (40/46/768/1024 dimensions).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n\nEach recording has a single transcript, which we encode with a pretrained language model (BERT, MacBERT, XLNet, or MPNet) to obtain one </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embedding per recording; for a subject with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> recordings, stacking these yields </span>\n  <math alttext=\"\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">BERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MacBERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">XLNet</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MPNet</mtext>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We re-implement two multimodal baselines for depression detection that use standard image architectures on EEG and speech </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">2D-spectrograms (Spec2D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: DenseNet-121&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Vision Transformer (ViT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These studies are among the few that explore EEG&#8211;speech multimodality in this task and report promising results. In our experiments, we retain their model architectures but apply our own subject-level cross-validation splits for consistency, making results not directly comparable to the original works. Additional implementation details are provided on our companion website.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "eeg",
                    "densenet121",
                    "vit",
                    "baselines",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We assess several modality-tailored architectures. We also experiment with multimodality, combining predictions from the best-performing feature&#8211;model pair in each modality in a late fusion fashion.</span>\n</p>\n\n",
                "matched_terms": [
                    "pair",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To keep notation light, we use </span>\n  <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119813;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to denote the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">generic feature matrix</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per modality, either handcrafted features or embeddings from pretrained models.\nConcretely, </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (EEG), </span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech, per recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (text, subject-level).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "features",
                    "text",
                    "models",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe consider two sequence encoders: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CNN+LSTM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GRU+Attention</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe CNN branch uses two 1D convolutions (kernel size 3, padding 1) with dropout to capture local temporal patterns, followed by a 2-layer LSTM for sequence modeling.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GRU+Attention</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses a 2-layer GRU with an attention mechanism that weights hidden states to form a subject-level summary.\nBoth encoders consume </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and produce a latent representation </span>\n  <math alttext=\"\\mathbf{H}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; an MLP head outputs </span>\n  <math alttext=\"y_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "eeg",
                    "cnn",
                    "lstm",
                    "cnnlstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nA shallow CNN extracts segment-level features from each recording.\nThese are reduced to a single fixed-size vector\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">d</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nusing one of three encoders:\n(i) max pooling,\n(ii) GRU with attention, or\n(iii) BiGRU with attention (the latter extending the GRU+Attn design with bidirectional recurrence).\nThe resulting </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vectors are stacked into the subject-level matrix\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis sequence is then processed by an LSTM to produce the subject-level representation\n</span>\n  <math alttext=\"\\mathbf{H}_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwhich is fed to an MLP head to obtain the final prediction\n</span>\n  <math alttext=\"y_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "cnn",
                    "features",
                    "lstm",
                    "gruattn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the subject-level text features (Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nA detection module (LSTM or CNN) transforms </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into </span>\n  <math alttext=\"\\mathbf{H}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an MLP head outputs </span>\n  <math alttext=\"y_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "cnn",
                    "lstm",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multimodal Fusion &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We select, for each modality, the best-performing feature&#8211;model pair and fuse their predictions via late fusion. This design choice ensures that our multimodal architectures are built upon the strongest unimodal predictors, allowing us to attribute performance gains directly to the fusion strategy rather than suboptimal single-modality components. We consider three schemes: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Bayesian fusion</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; convert modality-specific posteriors to likelihood ratios, combine them with predefined weights, and map back to a posterior; </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">soft voting (mean)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; average class probabilities across modalities and predict the class with highest average probability with ties resolved at 0.5; </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">weighted averaging</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; compute weighted combination of modality probabilities where weights sum to one, then predict the class with highest weighted probability.</span>\n</p>\n\n",
                "matched_terms": [
                    "unimodal",
                    "across",
                    "pair",
                    "mean",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the baselines with different fusion strategies. Simple baselines such as ViT and DenseNet-121 reached F1-scores around 0.56. Fusion strategies, however, substantially outperformed unimodal and baseline models. Weighted averaging already boosted performance when fusing EEG and Text, and Bayesian fusion further improved results, with Speech+Text achieving the highest F1-score overall. Majority voting also proved effective, with the tri-modal configuration EEG+Speech+Text reaching </span>\n  <math alttext=\"F_{1}=0.874\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.874</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}=0.874</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These results confirm the complementarity of modalities: while text dominates in unimodal settings, integrating speech and EEG consistently improves robustness and yields the strongest overall performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "eeg",
                    "text",
                    "models",
                    "f1score",
                    "densenet121",
                    "vit",
                    "baselines",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Framework for Multimodal Depression Detection &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Building on this systematic exploration of feature extraction methods, neural architectures, and fusion strategies, we propose an experimental framework for multimodal depression detection, illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.3 Data Preprocessing &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The framework selects the best-performing predictors for each modality: </span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">CBraMod</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with a GRU+Attn for EEG, </span>\n  <math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">XLSR</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprocessed with a CNN+GRU for speech, and\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{MacBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">MacBERT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MacBERT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with an LSTM for text. These modality-specific pipelines are then combined through alternative fusion strategies. This design allows us to isolate the contribution of each fusion method while keeping the strongest unimodal configurations fixed. Our best-performing architecture employs majority voting across the three modalities, achieving an accuracy of 88.6% and an F1-score of 87.4%, to the best of our knowledge, establishing the state of the art in multimodal depression detection. The framework thus serves as a reference setup for future experiments, enabling systematic evaluation of new fusion strategies or additional modalities. To the best of our knowledge, our tri-modal configuration with majority voting fusion represents the current state of the art in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "eeg",
                    "across",
                    "text",
                    "ùêómacbertmathbfxtextmacbert",
                    "f1score",
                    "lstm",
                    "best",
                    "ùêóxlsrmathbfxtextxlsr",
                    "modality",
                    "gruattn",
                    "ùêócbramodmathbfxtextcbramod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We addressed key limitations in multimodal depression detection by adopting subject-level stratified cross-validation and exploring EEG-based representations in combination with speech and text. Our experiments compared handcrafted features with deep representations from large pretrained models, consistently showing the superiority of the latter. In the unimodal setting, CNN+GRU proved effective for speech, while LSTM architectures yielded the best results for EEG and text. In the multimodal setting, late-fusion methods further improved performance, with Majority Voting across all three modalities achieving the strongest results, which to the best of our knowledge represents the current state of the art. Beyond the best-performing configuration, we introduce an experimental framework that fixes the optimal unimodal predictors and systematically evaluates alternative fusion strategies. This framework serves as a reference setup for future work, and by releasing all code and preprocessing scripts in a public repository, we ensure reproducibility and support further advances in multimodal depression detection research.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimodal",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "models",
                    "lstm",
                    "best",
                    "results"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
        "caption": "Table 4: Baseline and multimodal models (F1-score, mean ¬±\\pm std across 5 folds).\nIn bold, the best performing configuration per category (baselines or fusion strategy).\nThe overall best across all models and features configurations is additionally underlined.\nFor fusion methods, the numbers in parentheses (e.g., 0.4, 0.6) indicate the weights assigned to each modality.",
        "body": "Category\n\n\n\n\nConfiguration\n\n\n\n\nF1-score\n\n\n\n\n\n\n \n\n\nBaselines\n\n\n\n\nViT\n\n\n\n\n0.560 ¬±\\pm 0.190\n\n\n\n\n\n\n\nDenseNet-121\n\n\n\n\n0.586 ¬±\\pm 0.240\n\n\n\n\n\n\n \n\n\nWeighted Averaging\n\n\n\n\nEEG + Speech + Text (0.2 : 0.4 : 0.4)\n\n\n\n\n0.603 ¬±\\pm 0.306\n\n\n\n\n\n\n\nEEG + Speech (0.4 : 0.6)\n\n\n\n\n0.510 ¬±\\pm 0.425\n\n\n\n\n\n\n\nEEG + Text (0.4 : 0.6)\n\n\n\n\n0.783 ¬±\\pm 0.203\n\n\n\n\n\n\n\nSpeech + Text (0.4 : 0.6)\n\n\n\n\n0.470 ¬±\\pm 0.384\n\n\n\n\n\n\n \n\n\nBayesian Fusion\n\n\n\n\nEEG + Speech + Text (0.2 : 0.4 : 0.4)\n\n\n\n\n0.855 ¬±\\pm 0.133\n\n\n\n\n\n\n\nEEG + Speech (0.4 : 0.6)\n\n\n\n\n0.676 ¬±\\pm 0.168\n\n\n\n\n\n\n\nEEG + Text (0.4 : 0.6)\n\n\n\n\n0.824 ¬±\\pm 0.178\n\n\n\n\n\n\n\nSpeech + Text (0.4 : 0.6)\n\n\n\n\n0.875 ¬±\\pm 0.132\n\n\n\n\n\n\n \n\n\nMajority Voting\n\n\n\n\nEEG + Speech\n\n\n\n\n0.643 ¬±\\pm 0.340\n\n\n\n\n\n\n\nEEG + Text\n\n\n\n\n0.510 ¬±\\pm 0.425\n\n\n\n\n\n\n\nSpeech + Text\n\n\n\n\n0.783 ¬±\\pm 0.203\n\n\n\n\n\n\n\nEEG + Speech + Text\n\n\n\n\n0.874 ¬±\\pm 0.067",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Category</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Configuration</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">F1-score</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T4.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 1.0pt;\">Baselines</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">ViT</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.560 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.190</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">DenseNet-121</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.586 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.240</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T4.p2\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 1.0pt;\">Weighted Averaging</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Speech + Text (0.2 : 0.4 : 0.4)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.603 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.306</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Speech (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.510 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.425</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EEG + Text (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.783 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.203</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">Speech + Text (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.470 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.384</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T4.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 1.0pt;\">Bayesian Fusion</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Speech + Text (0.2 : 0.4 : 0.4)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.855 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.133</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Speech (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.676 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.168</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Text (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.824 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.178</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Speech + Text (0.4 : 0.6)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.875 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.132</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent ltx_align_center\" id=\"S4.T4.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><span class=\"ltx_text\" style=\"font-size:70%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:0.6pt 1.0pt;\">Majority Voting</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:70%;\"/></span>\n</span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Speech</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.643 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.340</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">EEG + Text</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.510 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.425</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:70%;\">Speech + Text</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.783 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.203</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:91.0pt;padding:0.6pt 1.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:105.3pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">EEG + Speech + Text</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" style=\"width:71.1pt;padding:0.6pt 1.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">0.874 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.067</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "strategy",
            "features",
            "text",
            "f1score",
            "std",
            "additionally",
            "folds",
            "modality",
            "¬±pm",
            "weights",
            "assigned",
            "numbers",
            "speech",
            "each",
            "voting",
            "across",
            "all",
            "mean",
            "baseline",
            "methods",
            "majority",
            "indicate",
            "averaging",
            "performing",
            "eeg",
            "bayesian",
            "densenet121",
            "vit",
            "configuration",
            "fusion",
            "bold",
            "underlined",
            "category",
            "models",
            "configurations",
            "weighted",
            "best",
            "baselines",
            "parentheses",
            "multimodal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we report the performance of all experimental categories: baseline re-implementations, unimodal models, and our proposed multimodal architectures. F1-scores are reported as mean &#177; standard deviation across folds. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the baselines and unimodal models, including the best-performing model for each set of features per modality. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports the performance of baseline models and multimodal fusion strategies, highlighting the best configuration within each category and the overall best-performing model. Further results are available on our companion website.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the baselines with different fusion strategies. Simple baselines such as ViT and DenseNet-121 reached F1-scores around 0.56. Fusion strategies, however, substantially outperformed unimodal and baseline models. Weighted averaging already boosted performance when fusing EEG and Text, and Bayesian fusion further improved results, with Speech+Text achieving the highest F1-score overall. Majority voting also proved effective, with the tri-modal configuration EEG+Speech+Text reaching </span>\n  <math alttext=\"F_{1}=0.874\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.874</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}=0.874</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These results confirm the complementarity of modalities: while text dominates in unimodal settings, integrating speech and EEG consistently improves robustness and yields the strongest overall performance.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "models",
                    "configurations",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Depression is a widespread mental health condition predicted to become the second leading cause of disease burden by 2030 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with COVID-19 causing a 27.6 </span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rise in global cases </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn recent years, there has been growing interest in developing automatic depression detection systems to support clinical decision-making and enable telemedicine applications. More recently, multimodal approaches have gained particular attention, motivated by the fact that in clinical settings, such as diagnostic interviews, human expression is inherently multimodal, spanning speech, language, and neural activity. However, current studies often suffer from critical methodological gaps, including limited modality integration, inconsistent evaluation protocols, and potential data leakage, which hinder reproducibility and the fair assessment of model performance.\nModels that leverage two modalities dominate the field.\nNotable examples include&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who applied DenseNet121 to EEG and speech spectrograms from the MODMA dataset, and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who employed Vision Transformers on comparable EEG&#8211;speech data from MODMA. Other bimodal studies investigated EEG&#8211;speech integration with graph convolutional networks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech&#8211;text fusion on the E-DAIC dataset using CNN-LSTM attention&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and EEG&#8211;facial expression fusion&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an extensive speech&#8211;text comparative analysis with multiple fusion techniques was conducted, but EEG was entirely excluded. Overall, state-of-the-art performances in multimodal depression detection span roughly 85&#8211;97%, depending on the dataset and modality combinations.\nAll the aforementioned approaches only comprise two modalities, constraining their potential by overlooking trimodal approaches. Moreover, most of them exclude text modality and lack transparent data-splitting protocols.\nIn&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech, EEG, and text were integrated using GAT-CNN-MpNet architectures on MODMA, achieving about 90</span>\n  <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">%</mo>\n      <annotation encoding=\"application/x-tex\">\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> balanced performance through weighted late fusion, though without comparing handcrafted and pretrained features and with only basic fusion strategies explored. Moreover, the study did not clarify whether 5-fold cross-validation was performed at the segment or subject level.\nOur work addresses key limitations in multimodal depression detection by systematically exploring feature representations and modeling strategies across EEG, together with speech and text. We perform a complete comparative analysis of handcrafted features and pretrained embeddings, including, for the first time, brain-pretrained models, evaluate multiple deep learning architectures, and compare unimodal, bimodal, and trimodal configurations. We further investigate how different fusion strategies impact detection accuracy and robustness, with particular attention to the role of EEG. Using consistent subject-independent data splits to ensure reproducible benchmarking, we demonstrate that carefully designed trimodal models achieve state-of-the-art performance. Our study lays the groundwork for the future of multimodal depression detection, guiding the development of more accurate and robust systems. We make both the code and the model checkpoints available to foster transparency and reproducibility.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Link will be available upon acceptance.</span>\n    </span>\n  </span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "models",
                    "all",
                    "densenet121",
                    "configurations",
                    "weighted",
                    "modality",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study employs the Multi-modal Open Dataset for Mental-disorder Analysis (MODMA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which provides: (1) 5-minute resting-state EEG recorded with a 128-channel HydroCel Geodesic Sensor Net at 250&#8201;Hz, and (2) audio from structured clinical interviews. For each subject, the interview audio consists of </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> separate recordings (question&#8211;answer items) whose durations vary across and within subjects (the total interview time is approximately 25 minutes per subject). Since MODMA does not include text transcriptions from clinical interviews, we generate automatic transcriptions using speech-to-text models. The dataset comprises individuals diagnosed with Major Depressive Disorder (MDD), recruited from Lanzhou University Second Hospital, and healthy controls (HCC) obtained via public advertising; MDD diagnoses were confirmed by licensed psychiatrists. In this study, we retain only subjects who participated in both EEG and interview recordings, resulting in a filtered cohort of 38 subjects. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Data &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes demographic information across groups and protocols. Additional details are available in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "eeg",
                    "across",
                    "text",
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Many studies lack clarity in data splitting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where segment-level splits can leak information by placing recordings from the same subject in both training and test sets, yielding inflated performance. To avoid this, we use stratified 5-fold subject-level cross-validation with consistent splits across experiments. We also release these splits on our companion website to ensure reproducibility and fair comparison.\nTo address the lack of transcriptions in the MODMA dataset, we employed WhisperX&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate text for each subject&#8217;s 29 recordings, without further post-processing.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a unified pipeline for multimodal depression detection with EEG, speech, and text.\nFor EEG, we adopt two processing branches: a 29-channel, 250 Hz, 10 s segmentation setup, consistent with prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a 19-channel, 200 Hz, 5 s segmentation setup replicating the preprocessing used in CBraMod&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the MUMTAZ depression dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n. For CBraMod, we evaluated both the original pre-trained version and the model fine-tuned on MUMTAZ, as described in the official documentation</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wjq-learning/CBraMod/blob/main/\" style=\"font-size:70%;\" title=\"\">https://github.com/wjq-learning/CBraMod/blob/main/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n, and found the latter consistently superior. Therefore, throughout this work we refer to CBraMod as the MUMTAZ-fine-tuned model.\nSpeech recordings are resampled to 16 kHz, denoised, and segmented into 5 s windows with 50% overlap, while text is used directly from raw Chinese transcriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "eeg",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Feature extraction combines handcrafted descriptors (EEG statistics, spectral power, entropy; speech MFCCs with/without prosody) with embeddings from large pre-trained models. For EEG, we employ both the Large Brain Model (LaBraM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">2,500 hours of EEG from 20 datasets, and CBraMod, a patch-based masked reconstruction model. For speech, we use XLSR-53&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multilingual wav2vec&#160;2.0 encoder, and Chinese HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 10k hours of WenetSpeech. For text, we use Chinese BERT Base&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, MacBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, XLNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MPNet Multilingual&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Segment-level representations are encoded with a combination of CNNs, LSTMs, and/or GRUs (with/without attention) and fused using decision-level strategies.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The preprocessing stage serves multiple objectives, including cleaning and structuring the raw data, as well as preparing it for multimodal analysis.\nOne key objective is the segmentation of the input into smaller units that can be more effectively processed by the models.\nWe denote with </span>\n  <math alttext=\"\\mathbf{S}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{S}_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\mathbf{S}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the number of segments obtained after preprocessing for each input modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "each",
                    "modality",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor handcrafted features and LaBraM, we follow prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which comprises retaining </span>\n  <math alttext=\"C=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">C</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">C=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> channels</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Full list available on our companion website. Link will be available upon acceptance.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applying a 0.5&#8211;50&#8201;Hz bandpass filter with a 50&#8201;Hz notch, and average re-referencing. Recordings are segmented into 10&#8201;s windows; at 250&#8201;Hz, each window contains </span>\n  <math alttext=\"T=250\\times 10=2500\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">250</mn>\n          <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">2500</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T=250\\times 10=2500</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> samples. Thus, a recording of length </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds produces </span>\n  <math alttext=\"S_{\\text{EEG}}=L/10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{EEG}}=L/10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> windows (e.g., </span>\n  <math alttext=\"S_{\\text{EEG}}=30\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">30</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{EEG}}=30</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for a 5-min recording), represented as </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{S_{\\text{EEG}}\\times C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">EEG</mtext>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{S_{\\text{EEG}}\\times C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "eeg",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Audio recordings are resampled from 44&#8201;kHz to 16&#8201;kHz, converted to mono PCM, amplitude-normalized to </span>\n  <math alttext=\"[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[-1,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, silence-trimmed, and denoised with a median filter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Each signal is segmented into overlapping windows of length </span>\n  <math alttext=\"w=5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">5</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w=5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8201;s with hop size </span>\n  <math alttext=\"h=2.5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">2.5</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">h=2.5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8201;s (50% overlap). At a sampling rate of 16&#8201;kHz, each segment contains </span>\n  <math alttext=\"T_{\\text{seg}}=80{,}000\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">T</mi>\n          <mtext mathsize=\"0.900em\">seg</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">80</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">000</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T_{\\text{seg}}=80{,}000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> samples and each hop </span>\n  <math alttext=\"T_{\\text{hop}}=40{,}000\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">T</mi>\n          <mtext mathsize=\"0.900em\">hop</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">40</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">000</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T_{\\text{hop}}=40{,}000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a recording of duration </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds (post-trimming), the number of segments is\n</span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8970;</mo>\n            <mrow>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mrow>\n                  <mi mathsize=\"0.900em\">L</mi>\n                  <mo mathsize=\"0.900em\">&#8722;</mo>\n                  <mi mathsize=\"0.900em\">w</mi>\n                </mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mi mathsize=\"0.900em\">h</mi>\n            </mrow>\n            <mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">&#8971;</mo>\n          </mrow>\n          <mo mathsize=\"0.900em\">+</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\big\\lfloor(L-w)/h\\big\\rfloor+1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"L\\geq w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mi mathsize=\"0.900em\">w</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">L\\geq w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while recordings shorter than </span>\n  <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">w</mi>\n      <annotation encoding=\"application/x-tex\">w</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are retained as a single segment. The segmented waveform is represented as </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}\\in\\mathbb{R}^{S_{\\text{SPEECH}}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where each row corresponds to one waveform segment. Each subject has </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> interview recordings; after windowing, recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> yields </span>\n  <math alttext=\"S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments </span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msubsup>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mtext mathsize=\"0.900em\">SPEECH</mtext>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mi mathsize=\"0.900em\">r</mi>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n            </msubsup>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mtext mathsize=\"0.900em\">seg</mtext>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{S_{\\text{SPEECH}}^{(r)}\\times T_{\\text{seg}}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The subject-level speech representation is the concatenation along the segment axis:\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{SPEECH}}=\\big[\\mathbf{X}_{\\text{SPEECH}}^{(1)};\\dots;\\mathbf{X}_{\\text{SPEECH}}^{(R)}\\big]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwith a total of </span>\n  <math alttext=\"S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\" rspace=\"0.111em\">=</mo>\n        <mrow>\n          <msubsup>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">&#8721;</mo>\n            <mrow>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo mathsize=\"0.900em\">=</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n          </msubsup>\n          <msubsup>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mtext mathsize=\"0.900em\">SPEECH</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">r</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">S_{\\text{SPEECH}}=\\sum_{r=1}^{R}S_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> segments.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nEach recording has a single transcript. After tokenization, the subject-level text representation is the concatenation of all transcript representations,\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{TEXT}}=\\big[\\mathbf{X}_{\\text{TEXT}}^{(1)};\\dots;\\mathbf{X}_{\\text{TEXT}}^{(R)}\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">TEXT</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">TEXT</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">TEXT</mtext>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </msubsup>\n          <mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{TEXT}}=\\big[\\mathbf{X}_{\\text{TEXT}}^{(1)};\\dots;\\mathbf{X}_{\\text{TEXT}}^{(R)}\\big]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Handcrafted features.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor each segment </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{C\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">EEG</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">T</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}\\in\\mathbb{R}^{C\\times T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwe extract </span>\n  <math alttext=\"F=10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F=10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> hancrafted descriptors per channel\n(statistical, spectral, entropy), yielding\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{HAND}}\\in\\mathbb{R}^{S\\times C\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">HAND</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">C</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">F</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{HAND}}\\in\\mathbb{R}^{S\\times C\\times F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "eeg",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-trained models.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe further extract embeddings from LaBraM and\nCBraMod.\nLaBraM operates on </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(1)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and maps each segment to a\n</span>\n  <math alttext=\"D=200\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embedding, producing\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{LaBraM}}\\in\\mathbb{R}^{S\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">LaBraM</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{LaBraM}}\\in\\mathbb{R}^{S\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nCBraMod operates on </span>\n  <math alttext=\"\\mathbf{X}_{\\text{EEG}}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{EEG}}^{(2)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where each 5&#8201;s segment is\npatch-encoded and then averaged across channels and patches to form\n</span>\n  <math alttext=\"D=200\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">200</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embeddings, resulting in\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\\in\\mathbb{R}^{S\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mtext mathsize=\"0.900em\">CBraMod</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}\\in\\mathbb{R}^{S\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "each",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Remark.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAfter feature extraction, the raw temporal dimension (</span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <math alttext=\"T_{\\text{patch}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mtext mathsize=\"0.900em\">patch</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">T_{\\text{patch}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">)\nis no longer present, as each segment is reduced to a fixed-size representation\nof dimension </span>\n  <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">F</mi>\n      <annotation encoding=\"application/x-tex\">F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (handcrafted) or </span>\n  <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">D</mi>\n      <annotation encoding=\"application/x-tex\">D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (embeddings).\nFor subject-level modeling, features from all recordings of the same subject\nare stacked to form the final subject representation.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "all",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> From each waveform segment, we compute two handcrafted variant, namely MFCCs (40 coefficients) and Prosody </span>\n  <math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">+</mo>\n      <annotation encoding=\"application/x-tex\">+</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> MFCCs (46 features: 40 MFCCs plus energy, </span>\n  <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, RMS energy, pause rate, phonation time, speech rate). We also extract segment embeddings with XLSR-53 and Chinese HuBERT Large.\nSegment-level features are stacked per recording and then concatenated across the 29 recordings of each subject to form the subject-level representation.\nThe exact tensor shapes (per recording and subject-level) are summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter feature extraction, the raw sample length is no longer present; each segment is represented by a fixed-size vector (40/46/768/1024 dimensions).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "across",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n\nEach recording has a single transcript, which we encode with a pretrained language model (BERT, MacBERT, XLNet, or MPNet) to obtain one </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional embedding per recording; for a subject with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> recordings, stacking these yields </span>\n  <math alttext=\"\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">BERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MacBERT</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">XLNet</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mtext mathsize=\"0.900em\">MPNet</mtext>\n          </msub>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">D</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{BERT}},\\mathbf{X}_{\\text{MacBERT}},\\mathbf{X}_{\\text{XLNet}},\\mathbf{X}_{\\text{MPNet}}\\in\\mathbb{R}^{R\\times D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (with </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"D=768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">D=768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We re-implement two multimodal baselines for depression detection that use standard image architectures on EEG and speech </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">2D-spectrograms (Spec2D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: DenseNet-121&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Vision Transformer (ViT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These studies are among the few that explore EEG&#8211;speech multimodality in this task and report promising results. In our experiments, we retain their model architectures but apply our own subject-level cross-validation splits for consistency, making results not directly comparable to the original works. Additional implementation details are provided on our companion website.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "densenet121",
                    "vit",
                    "baselines",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We assess several modality-tailored architectures. We also experiment with multimodality, combining predictions from the best-performing feature&#8211;model pair in each modality in a late fusion fashion.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "modality",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To keep notation light, we use </span>\n  <math alttext=\"\\mathbf{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119813;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to denote the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">generic feature matrix</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per modality, either handcrafted features or embeddings from pretrained models.\nConcretely, </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (EEG), </span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (speech, per recording </span>\n  <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">r</mi>\n      <annotation encoding=\"application/x-tex\">r</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (text, subject-level).</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "eeg",
                    "features",
                    "text",
                    "models",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EEG &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe consider two sequence encoders: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CNN+LSTM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GRU+Attention</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe CNN branch uses two 1D convolutions (kernel size 3, padding 1) with dropout to capture local temporal patterns, followed by a 2-layer LSTM for sequence modeling.\n</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GRU+Attention</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses a 2-layer GRU with an attention mechanism that weights hidden states to form a subject-level summary.\nBoth encoders consume </span>\n  <math alttext=\"\\mathbf{F}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and produce a latent representation </span>\n  <math alttext=\"\\mathbf{H}_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; an MLP head outputs </span>\n  <math alttext=\"y_{\\text{EEG}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">EEG</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{EEG}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "weights",
                    "eeg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nA shallow CNN extracts segment-level features from each recording.\nThese are reduced to a single fixed-size vector\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">r</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </msubsup>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">d</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}^{(r)}\\in\\mathbb{R}^{d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nusing one of three encoders:\n(i) max pooling,\n(ii) GRU with attention, or\n(iii) BiGRU with attention (the latter extending the GRU+Attn design with bidirectional recurrence).\nThe resulting </span>\n  <math alttext=\"R=29\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">29</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R=29</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vectors are stacked into the subject-level matrix\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119813;</mi>\n          <mtext mathsize=\"0.900em\">SPEECH</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{SPEECH}}\\in\\mathbb{R}^{R\\times d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis sequence is then processed by an LSTM to produce the subject-level representation\n</span>\n  <math alttext=\"\\mathbf{H}_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwhich is fed to an MLP head to obtain the final prediction\n</span>\n  <math alttext=\"y_{\\text{SPEECH}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">SPEECH</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{SPEECH}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the subject-level text features (Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.4 Feature Extraction &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nA detection module (LSTM or CNN) transforms </span>\n  <math alttext=\"\\mathbf{F}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119813;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{F}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into </span>\n  <math alttext=\"\\mathbf{H}_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119815;</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{H}_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an MLP head outputs </span>\n  <math alttext=\"y_{\\text{TEXT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS6.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mtext mathsize=\"0.900em\">TEXT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{\\text{TEXT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multimodal Fusion &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We select, for each modality, the best-performing feature&#8211;model pair and fuse their predictions via late fusion. This design choice ensures that our multimodal architectures are built upon the strongest unimodal predictors, allowing us to attribute performance gains directly to the fusion strategy rather than suboptimal single-modality components. We consider three schemes: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Bayesian fusion</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; convert modality-specific posteriors to likelihood ratios, combine them with predefined weights, and map back to a posterior; </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">soft voting (mean)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; average class probabilities across modalities and predict the class with highest average probability with ties resolved at 0.5; </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">weighted averaging</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#8211; compute weighted combination of modality probabilities where weights sum to one, then predict the class with highest weighted probability.</span>\n</p>\n\n",
                "matched_terms": [
                    "voting",
                    "each",
                    "bayesian",
                    "strategy",
                    "across",
                    "mean",
                    "weighted",
                    "modality",
                    "fusion",
                    "averaging",
                    "weights",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopt stratified 5-fold cross-validation with fixed subject splits to ensure balanced and comparable experiments, and prevent data leakage. Models are trained with cross-entropy loss and softmax output, with hyperparameters tuned manually. All implementation details are provided in our companion materials.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Unimodal &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Results &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports the performance of baseline and unimodal models. Among EEG features, CBraMod embeddings combined with a GRU and attention achieved the best result, confirming the benefit of pre-training on a depression-related corpus. For speech, both XLSR-53 and HuBERT embeddings provided strong performance, with XLSR-53 coupled with a CNN+GRU slightly outperforming. Handcrafted MFCC and prosodic features yielded considerably lower scores, indicating that deep speech embeddings capture richer information. In the text modality, all transformer-based embeddings performed competitively, with Chinese MacBERT and XLNet reaching the top results. Overall, unimodal experiments highlight that text provided the most informative single modality, while speech embeddings also achieved strong performance, and EEG remained less predictive in isolation.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall",
                    "eeg",
                    "features",
                    "text",
                    "models",
                    "all",
                    "best",
                    "modality",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Framework for Multimodal Depression Detection &#8212;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Building on this systematic exploration of feature extraction methods, neural architectures, and fusion strategies, we propose an experimental framework for multimodal depression detection, illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14922v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.3 Data Preprocessing &#8227; 2 Methodology &#8227; TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The framework selects the best-performing predictors for each modality: </span>\n  <math alttext=\"\\mathbf{X}_{\\text{CBraMod}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">CBraMod</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{CBraMod}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with a GRU+Attn for EEG, </span>\n  <math alttext=\"\\mathbf{X}_{\\text{XLSR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">XLSR</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{XLSR}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprocessed with a CNN+GRU for speech, and\n</span>\n  <math alttext=\"\\mathbf{X}_{\\text{MacBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">MacBERT</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{MacBERT}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> processed with an LSTM for text. These modality-specific pipelines are then combined through alternative fusion strategies. This design allows us to isolate the contribution of each fusion method while keeping the strongest unimodal configurations fixed. Our best-performing architecture employs majority voting across the three modalities, achieving an accuracy of 88.6% and an F1-score of 87.4%, to the best of our knowledge, establishing the state of the art in multimodal depression detection. The framework thus serves as a reference setup for future experiments, enabling systematic evaluation of new fusion strategies or additional modalities. To the best of our knowledge, our tri-modal configuration with majority voting fusion represents the current state of the art in multimodal depression detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "voting",
                    "eeg",
                    "across",
                    "text",
                    "f1score",
                    "configurations",
                    "best",
                    "configuration",
                    "modality",
                    "methods",
                    "majority",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We addressed key limitations in multimodal depression detection by adopting subject-level stratified cross-validation and exploring EEG-based representations in combination with speech and text. Our experiments compared handcrafted features with deep representations from large pretrained models, consistently showing the superiority of the latter. In the unimodal setting, CNN+GRU proved effective for speech, while LSTM architectures yielded the best results for EEG and text. In the multimodal setting, late-fusion methods further improved performance, with Majority Voting across all three modalities achieving the strongest results, which to the best of our knowledge represents the current state of the art. Beyond the best-performing configuration, we introduce an experimental framework that fixes the optimal unimodal predictors and systematically evaluates alternative fusion strategies. This framework serves as a reference setup for future work, and by releasing all code and preprocessing scripts in a public repository, we ensure reproducibility and support further advances in multimodal depression detection research.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voting",
                    "eeg",
                    "across",
                    "features",
                    "text",
                    "models",
                    "all",
                    "best",
                    "configuration",
                    "methods",
                    "majority",
                    "fusion",
                    "multimodal"
                ]
            }
        ]
    }
}