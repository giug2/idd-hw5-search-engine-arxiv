{
    "S3.T1": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 1: Results on Zarma POS tagging (1000-sentence test set), averaged over 5 seeds.",
        "body": "Model\nPRON\nNOUN\nVERB\nADJ\nAUX\nPART\nDET\nPUNCT\nMacro F1\nWord Acc. (%)\n\n\n\n\nBiLSTM-CRF\n0.99±.01\n0.98±.01\n0.97±.01\n0.96±.02\n0.99±.01\n0.96±.02\n0.95±.03\n1.00±.00\n0.975±.01\n98.8±.1\n\n\nR2T-BiLSTM\n0.99±.01\n0.97±.01\n0.96±.02\n0.94±.03\n0.98±.01\n0.95±.02\n0.94±.03\n1.00±.00\n0.968±.01\n98.2±.2\n\n\nAfriBERTa (SFT-300)\n0.98±.02\n0.95±.02\n0.94±.03\n0.88±.04\n0.97±.01\n0.92±.03\n0.89±.05\n1.00±.00\n0.941±.02\n96.8±.3\n\n\nR2T-Trans. SFT-50\n0.98±.02\n0.94±.02\n0.93±.03\n0.89±.04\n0.96±.02\n0.91±.03\n0.90±.04\n1.00±.00\n0.935±.02\n96.3±.4\n\n\nR2T-Transformer\n0.96±.03\n0.87±.04\n0.86±.04\n0.74±.06\n0.92±.03\n0.84±.05\n0.80±.06\n0.98±.01\n0.852±.04\n89.8±.8\n\n\nXLM-RoBERTa (SFT-300)\n0.40±.08\n0.45±.07\n0.38±.09\n0.27±.11\n0.41±.08\n0.39±.08\n0.30±.12\n0.70±.05\n0.413±.08\n49.1±.2.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PRON</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NOUN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">VERB</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ADJ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">AUX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PART</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">DET</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PUNCT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Macro F1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Word Acc. (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BiLSTM-CRF</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.98&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.97&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.96&#177;.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.96&#177;.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95&#177;.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.00&#177;.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.975&#177;.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.8&#177;.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">R2T-BiLSTM</th>\n<td class=\"ltx_td ltx_align_center\">0.99&#177;.01</td>\n<td class=\"ltx_td ltx_align_center\">0.97&#177;.01</td>\n<td class=\"ltx_td ltx_align_center\">0.96&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">0.94&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">0.98&#177;.01</td>\n<td class=\"ltx_td ltx_align_center\">0.95&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">0.94&#177;.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00&#177;.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.968&#177;.01</span></td>\n<td class=\"ltx_td ltx_align_center\">98.2&#177;.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">AfriBERTa (SFT-300)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.98&#177;.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95&#177;.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.94&#177;.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.88&#177;.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.97&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92&#177;.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.89&#177;.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.00&#177;.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.941&#177;.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.8&#177;.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">R2T-Trans. SFT-50</th>\n<td class=\"ltx_td ltx_align_center\">0.98&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">0.94&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">0.93&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">0.89&#177;.04</td>\n<td class=\"ltx_td ltx_align_center\">0.96&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">0.91&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">0.90&#177;.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00&#177;.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.935&#177;.02</td>\n<td class=\"ltx_td ltx_align_center\">96.3&#177;.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">R2T-Transformer</th>\n<td class=\"ltx_td ltx_align_center\">0.96&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">0.87&#177;.04</td>\n<td class=\"ltx_td ltx_align_center\">0.86&#177;.04</td>\n<td class=\"ltx_td ltx_align_center\">0.74&#177;.06</td>\n<td class=\"ltx_td ltx_align_center\">0.92&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">0.84&#177;.05</td>\n<td class=\"ltx_td ltx_align_center\">0.80&#177;.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.98&#177;.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.852&#177;.04</td>\n<td class=\"ltx_td ltx_align_center\">89.8&#177;.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">XLM-RoBERTa (SFT-300)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.40&#177;.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.45&#177;.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.38&#177;.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.27&#177;.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.41&#177;.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.39&#177;.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.30&#177;.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.70&#177;.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.413&#177;.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">49.1&#177;.2.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "094±03",
            "0935±02",
            "0941±02",
            "over",
            "098±02",
            "070±05",
            "968±3",
            "aux",
            "seeds",
            "088±04",
            "084±05",
            "noun",
            "r2tbilstm",
            "verb",
            "092±03",
            "089±04",
            "096±03",
            "afriberta",
            "041±08",
            "098±01",
            "macro",
            "095±02",
            "080±06",
            "100±00",
            "039±08",
            "test",
            "sft50",
            "038±09",
            "adj",
            "sft300",
            "0852±04",
            "090±04",
            "963±4",
            "pos",
            "r2ttransformer",
            "086±04",
            "results",
            "0975±01",
            "491±21",
            "r2ttrans",
            "0413±08",
            "acc",
            "averaged",
            "model",
            "tagging",
            "045±07",
            "word",
            "095±03",
            "0968±01",
            "099±01",
            "093±03",
            "087±04",
            "part",
            "set",
            "punct",
            "040±08",
            "982±2",
            "898±8",
            "xlmroberta",
            "pron",
            "091±03",
            "096±02",
            "030±12",
            "zarma",
            "089±05",
            "det",
            "988±1",
            "094±02",
            "027±11",
            "1000sentence",
            "bilstmcrf",
            "097±01",
            "074±06"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Results &#8227; 3 Experiments &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results for Zarma POS tagging. We report per-tag F1-scores and macro averages as the primary evaluation metric, following standard practice in sequence tagging. Overall accuracy is included for completeness, but our focus is on F1, which better captures performance under class imbalance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "zarma",
                    "r2tbilstm",
                    "pos",
                    "afriberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Part-of-speech (POS) tagging is a foundational task in Natural Language Processing (NLP), serving as a prerequisite for complex downstream applications such as machine translation, syntactic parsing, and information extraction. For high-resource languages, deep learning models achieve near-perfect accuracy in POS tasks. However, that is not case for low-resource languages, where there is a lack of large manually annotated dataset these data-hungry models require. This data scarcity limits the development of robust linguistic tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of R2T through a comprehensive case study on Zarma, a language for which no large-scale POS corpus previously existed. Our work is guided by the following research questions:</p>\n\n",
                "matched_terms": [
                    "pos",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Can a model trained with linguistic rules and unlabeled text outperform a large pre-trained model fine-tuned on a small set of labeled data?</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench &amp; ZarmaNER-600:</span> We release the first POS-tagged and NER-annotated corpora for Zarma. This includes a large silver-standard and 300 gold-standard datasets for POS, and a 600-sentence gold-standard NER dataset.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model release:</span> We release the pre-trained Zarma FastText embeddings and our best models for both POS and NER tasks&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/27Group\" title=\"\">https://huggingface.co/27Group</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "model",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of POS tagging in low-resource settings, we introduce <span class=\"ltx_text ltx_font_bold\">R2T</span>. R2T is a hybrid framework that combines the contextual learning ability of neural networks with a structured, multi-tiered system of linguistic knowledge. Instead of treating rules as a rigid post-processing step, we integrate them directly into the model&#8217;s learning objective through a novel, adaptive loss function. This method forces the model to adhere to known linguistic facts while teaching it to handle uncertainty gracefully when encountering unknown words.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "word",
                    "tagging",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sequence of characters is fed into a separate character-level sequential neural model (transformer or bidirectional long short-term memory (BiLSTM)), and the final hidden states are concatenated. This technique allows the model to infer representations for unseen words based on their sub-word structure, a method proven effective in numerous tagging tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tagging",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "word",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "pos",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 4: Syntactic rules.</span> This tier models local grammatical structure by defining valid and invalid transitions between adjacent POS tags. These rules are represented as a matrix of bigram probabilities or constraints&#8212;e.g., a &#8217;DETERMINER&#8217; is very likely to be followed by a &#8217;NOUN&#8217; but not by a &#8217;VERB&#8217;. This helps the model produce more coherent and grammatically plausible tag sequences.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span> serves as an architectural ablation study. It replaces the BiLSTM core with a Transformer encoder&#8212;10 layers, 6 attention heads, 768 hidden units and 3072 feed-forward&#8212;but uses the exact same rule system and training objective as the R2T-BiLSTM.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> is the R2T-Transformer model after it has been further fine-tuned for 20 epochs on the first 50 sentences of our annotated dataset using a standard cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "model",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span> is an African-centric baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span> is a widely-used multilingual baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the detailed hyperparameters for all our models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2\" title=\"Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. For evaluation, we use a comprehensive set of metrics. We report overall <span class=\"ltx_text ltx_font_bold\">Word-Level Accuracy</span> and the <span class=\"ltx_text ltx_font_bold\">Macro F1-Score</span>, which is the unweighted average of the F1-score for each tag.</p>\n\n",
                "matched_terms": [
                    "set",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "bilstmcrf",
                    "model",
                    "zarma",
                    "r2tbilstm",
                    "afriberta",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results provide several key insights into the challenges and opportunities of low-resource POS tagging.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "zarma",
                    "r2tbilstm",
                    "verb",
                    "afriberta",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zarma",
                    "r2tbilstm",
                    "r2ttransformer",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study focuses on POS tagging, the R2T design is not task-specific: any task with declarative linguistic or structural rules&#8212;e.g., morphological analysis, shallow parsing, phonotactic constraints&#8212;can be mapped into loss components. We therefore view POS tagging in Zarma as a case study of PrL.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary obstacle in low-resource NLP research is the lack absence of large-scale annotated dataset for tasks like POS tagging&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib14\" title=\"\">2022</a>)</cite>. To address this gap and to stimulate further research&#8212;for Zarma&#8212;we introduce <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span>, the first POS-tagged benchmark dataset for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a high-quality, reliable test set for evaluating any new Zarma POS tagger.</p>\n\n",
                "matched_terms": [
                    "zarma",
                    "pos",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a SFT set to further improve models trained on ZarmaPOS-Bench, adjusting the silver-standard model&#8217;s systematic errors, as shown in our SFT-50 experiment.</p>\n\n",
                "matched_terms": [
                    "set",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "zarma",
                    "pos",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our R2T framework demonstrates significant promise and achieves high results for Zarma, we acknowledge several limitations that define the scope of this work and offer avenues for future investigation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "1000sentence",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge in low-resource POS tagging is the lack of annotated data. A common strategy is cross-lingual projection, which transfers supervision from high-resource languages via parallel data or word alignments <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>; T&#228;ckstr&#246;m et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib24\" title=\"\">2013</a>)</cite>. Other approaches rely on classic probabilistic models like HMMs or TnT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brants, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib2\" title=\"\">2000</a>)</cite>, which can be effective but often lack the contextual power of neural models. More recent work has shown that small, targeted amounts of annotation, when combined with morphological information and type-level constraints, can be highly effective <cite class=\"ltx_cite ltx_citemacro_citep\">(Garrette and Baldridge, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our R2T framework builds on this insight by formalizing the injection of such constraints directly into a neural model&#8217;s training objective, removing the need for any initial labeled data.</p>\n\n",
                "matched_terms": [
                    "word",
                    "pos",
                    "tagging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work employs standard neural architectures for sequence tagging, such as BiLSTMs with character-level embeddings, which are known to be effective for handling OOV words and morphology <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>. While a conditional random field (CRF) layer is often used for structured prediction <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib11\" title=\"\">2015</a>)</cite>, our approach replaces this with a soft, differentiable syntactic loss. We also compare our approach to large multilingual models like XLM-RoBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite> and African-centric models like AfriBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. While powerful, these models can suffer from tokenizer mismatches in low-resource languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib22\" title=\"\">2021</a>)</cite>, a finding our experiments confirm. Finally, our adaptive OOV loss is related to confidence regularization techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Pereyra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib19\" title=\"\">2017</a>)</cite>, but it is applied selectively which encourages principled uncertainty only when the model has no rule-based guidance.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "tagging",
                    "model",
                    "xlmroberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "word",
                    "tagging",
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer.</span> Our attention-based model, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, replaces the recurrent core with a Transformer encoder. The initial 350-dimensional input vector is first projected to match the Transformer&#8217;s hidden dimension of 768 using a linear layer. We then add sinusoidal positional encodings to this vector to provide the model with sequence order information. This final 768-dimensional vector is processed by a 10-layer Transformer encoder. Each layer contains 6 self-attention heads and a feed-forward network with 3072 hidden units. The 768-dimensional output vector from the final layer is then passed through a linear layer to produce the tag logits.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This result is insightful. It confirms that the R2T framework can be successfully adapted to a new language, and also reinforces our central claim: a modest investment in encoding linguistic knowledge can be more effective than fine-tuning on a small, potentially noisy, annotated dataset. The +0.13 absolute improvement in Macro F1 demonstrates the power of providing a model with explicit grammatical principles.</p>\n\n",
                "matched_terms": [
                    "model",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "aux",
                    "zarma",
                    "noun",
                    "verb",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tagging",
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "zarma",
                    "tagging",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a similar set of models as in our POS experiments:</p>\n\n",
                "matched_terms": [
                    "pos",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "set",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span>, which takes the unsupervised R2T-Transformer and fine-tunes it on 50 gold sentences.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sft50",
                    "r2ttransformer",
                    "afriberta",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides supplementary figures that offer further insight into our experimental results and model behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Data Efficiency in Zarma NER &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides a visual representation of the data efficiency demonstrated in our Zarma NER experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5\" title=\"Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). The plot clearly shows that the R2T-Transformer starts from a much higher baseline accuracy (67.4%) than a standard fine-tuning approach. This strong foundation allows it to surpass the performance of the AfriBERTa baseline after being fine-tuned on only 50 labeled examples.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "afriberta",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "r2tbilstm",
                    "1000sentence",
                    "set"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 2: Qualitative error analysis across different models.",
        "body": "Model\n\n\n\n\nError Category\n\n\n\n\nExample Sentence & Prediction\n\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\nXLM-RoBERTa\n\n\n\n\nCatastrophic Tokenization Mismatch\n\n\n\n\nNi neera moo.\nTokens: ’[’Ni’, ’neera’, ’moo.’]’ \nTags: ’[’PRON’, ’VERB’, ’VERB’]’\n\n\n\n\nThe tokenizer fails to separate punctuation, treating \"moo.\" as one token. This guarantees an error on every sentence and confuses the model, causing it to misclassify the word itself.\n\n\n\n\n\n\nAfriBERTa\n\n\n\n\nLexical Ambiguity\n\n\n\n\nAy no a se moo.\nPred: ’no’ →\\rightarrow ’AUX’ \nCorrect: ’no’ →\\rightarrow ’VERB’\n\n\n\n\nThe model incorrectly defaults to the more frequent auxiliary sense of \"no\", failing to use the syntactic context (Subject _ Object) to identify it as the main verb \"to give\".\n\n\n\n\n\n\n\nWord Class Confusion\n\n\n\n\nNi ya boro hanno no.\nPred: ’hanno’ →\\rightarrow ’NOUN’ \nCorrect: ’hanno’ →\\rightarrow ’ADJ’\n\n\n\n\nWithout enough labeled examples of the ’NOUN + ADJ’ pattern, the model fails to generalize and misclassifies the adjective \"hanno\" (beautiful) as a noun.\n\n\n\n\n\n\nBiLSTM-CRF\n\n\n\n\nOut-of-Vocabulary (OOV) Word\n\n\n\n\n…care fassaro te.\nPred: ’fassaro’ →\\rightarrow ’NOUN’ \nCorrect: ’fassaro’ →\\rightarrow ’VERB’\n\n\n\n\nHaving never seen \"fassaro\" (to explain) in the 300 training sentences, the model makes a plausible but incorrect guess based on context and morphology, highlighting the limits of a small supervised dataset.\n\n\n\n\n\n\nR2T-Transformer (Normal)\n\n\n\n\nSystemic Verb Misclassification\n\n\n\n\nIri ga wani.\nPred: ’wani’ →\\rightarrow ’NOUN’ \nCorrect: ’wani’ →\\rightarrow ’VERB’\n\n\n\n\nThe Transformer’s global attention appears to dilute the strong token-level signal from the lexical rule for \"wani\" (to play), leading it to favor a contextually plausible but incorrect tag.\n\n\n\n\n\n\n\nFailure to Disambiguate\n\n\n\n\nAy no a se moo.\nPred: ’no’ →\\rightarrow ’AUX’ \nCorrect: ’no’ →\\rightarrow ’VERB’\n\n\n\n\nSimilar to AfriBERTa, the model defaults to the ’AUX’ tag for \"no\". This shows that the ambiguous rule alone was not enough to guide the Transformer architecture without supervised examples.\n\n\n\n\n\n\nR2T-Transformer SFT-50\n\n\n\n\nResidual Ambiguity\n\n\n\n\nAy no a se moo.\nPred: ’no’ →\\rightarrow ’AUX’ \nCorrect: ’no’ →\\rightarrow ’VERB’\n\n\n\n\nWhile SFT fixed most errors, the 50 sentences did not provide enough diverse examples for the model to fully learn the contextual cues for disambiguating \"no\" as a verb. This remains its primary weakness.\n\n\n\n\n\n\n\nResidual Word Class Confusion\n\n\n\n\nWayboro hanno na ay no gaasi.\nPred: ’hanno’ →\\rightarrow ’NOUN’ \nCorrect: ’hanno’ →\\rightarrow ’ADJ’\n\n\n\n\nSimilar to the ambiguity issue, the fine-tuning set likely lacked sufficient examples of this specific adjective to correct the model’s pre-existing bias.\n\n\n\n\n\n\nR2T-BiLSTM\n\n\n\n\nMinor Syntactic Ambiguity\n\n\n\n\nIri ya boro yaaje no.\nPred: ’yaaje’ →\\rightarrow ’NOUN’ \nCorrect: ’yaaje’ →\\rightarrow ’ADJ’\n\n\n\n\nThe model makes a rare error on a complex adjective. While the rules handle most cases, this specific pattern (’PRON AUX PRON ADJ AUX’) proved challenging for the model without explicit labels.",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Error Category</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Example Sentence &amp; Prediction</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Analysis</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">XLM-RoBERTa</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Catastrophic Tokenization Mismatch</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ni neera moo.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Tokens:</span> &#8217;[&#8217;Ni&#8217;, &#8217;neera&#8217;, &#8217;moo.&#8217;]&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Tags:</span> &#8217;[&#8217;PRON&#8217;, &#8217;VERB&#8217;, &#8217;VERB&#8217;]&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">The tokenizer fails to separate punctuation, treating \"moo.\" as one token. This guarantees an error on every sentence and confuses the model, causing it to misclassify the word itself.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">AfriBERTa</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Lexical Ambiguity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ay no a se moo.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;AUX&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">The model incorrectly defaults to the more frequent auxiliary sense of \"no\", failing to use the syntactic context (Subject _ Object) to identify it as the main verb \"to give\".</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Word Class Confusion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ni ya boro hanno no.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;hanno&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;hanno&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;ADJ&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Without enough labeled examples of the &#8217;NOUN + ADJ&#8217; pattern, the model fails to generalize and misclassifies the adjective \"hanno\" (beautiful) as a noun.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">BiLSTM-CRF</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Out-of-Vocabulary (OOV) Word</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">&#8230;care fassaro te.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;fassaro&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;fassaro&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Having never seen \"fassaro\" (to explain) in the 300 training sentences, the model makes a plausible but incorrect guess based on context and morphology, highlighting the limits of a small supervised dataset.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">R2T-Transformer (Normal)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Systemic Verb Misclassification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Iri ga wani.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;wani&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;wani&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">The Transformer&#8217;s global attention appears to dilute the strong token-level signal from the lexical rule for \"wani\" (to play), leading it to favor a contextually plausible but incorrect tag.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Failure to Disambiguate</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ay no a se moo.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;AUX&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Similar to AfriBERTa, the model defaults to the &#8217;AUX&#8217; tag for \"no\". This shows that the ambiguous rule alone was not enough to guide the Transformer architecture without supervised examples.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">R2T-Transformer SFT-50</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Residual Ambiguity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ay no a se moo.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;AUX&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;no&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">While SFT fixed most errors, the 50 sentences did not provide enough diverse examples for the model to fully learn the contextual cues for disambiguating \"no\" as a verb. This remains its primary weakness.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Residual Word Class Confusion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Wayboro hanno na ay no gaasi.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;hanno&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;hanno&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;ADJ&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">Similar to the ambiguity issue, the fine-tuning set likely lacked sufficient examples of this specific adjective to correct the model&#8217;s pre-existing bias.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">R2T-BiLSTM</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Minor Syntactic Ambiguity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Iri ya boro yaaje no.</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;yaaje&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;yaaje&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;ADJ&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">The model makes a rare error on a complex adjective. While the rules handle most cases, this specific pattern (&#8217;PRON AUX PRON ADJ AUX&#8217;) proved challenging for the model without explicit labels.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "punctuation",
            "sense",
            "global",
            "rule",
            "guess",
            "architecture",
            "enough",
            "r2tbilstm",
            "verb",
            "guide",
            "class",
            "iri",
            "never",
            "cues",
            "yaaje",
            "dilute",
            "qualitative",
            "object",
            "’noun",
            "tags",
            "’’ni’",
            "plausible",
            "adj",
            "challenging",
            "analysis",
            "’moo’’",
            "itself",
            "highlighting",
            "treating",
            "r2ttransformer",
            "sentences",
            "remains",
            "handle",
            "disambiguate",
            "seen",
            "’’pron’",
            "word",
            "bias",
            "hanno",
            "learn",
            "set",
            "’no’",
            "syntactic",
            "’wani’",
            "its",
            "failing",
            "moo",
            "incorrect",
            "weakness",
            "based",
            "cases",
            "causing",
            "tag",
            "wayboro",
            "confuses",
            "likely",
            "→rightarrow",
            "’noun’",
            "’adj’",
            "errors",
            "across",
            "misclassify",
            "wani",
            "adj’",
            "frequent",
            "adjective",
            "did",
            "explicit",
            "strong",
            "dataset",
            "examples",
            "specific",
            "primary",
            "model’s",
            "guarantees",
            "having",
            "favor",
            "mismatch",
            "…care",
            "appears",
            "aux’",
            "identify",
            "lacked",
            "separate",
            "sft",
            "alone",
            "tokens",
            "complex",
            "oov",
            "training",
            "fixed",
            "sentence",
            "systemic",
            "rare",
            "tokenlevel",
            "more",
            "transformer",
            "afriberta",
            "finetuning",
            "minor",
            "prediction",
            "one",
            "’aux’",
            "misclassification",
            "most",
            "from",
            "every",
            "subject",
            "’neera’",
            "not",
            "gaasi",
            "provide",
            "fails",
            "preexisting",
            "’hanno’",
            "issue",
            "pred",
            "contextually",
            "confusion",
            "residual",
            "ambiguity",
            "ambiguous",
            "context",
            "token",
            "outofvocabulary",
            "catastrophic",
            "defaults",
            "transformer’s",
            "play",
            "’verb’’",
            "pron",
            "models",
            "different",
            "neera",
            "explain",
            "small",
            "disambiguating",
            "contextual",
            "labeled",
            "error",
            "pattern",
            "morphology",
            "’yaaje’",
            "lexical",
            "normal",
            "aux",
            "supervised",
            "noun",
            "signal",
            "auxiliary",
            "leading",
            "shows",
            "example",
            "tokenization",
            "give",
            "generalize",
            "sft50",
            "rules",
            "sufficient",
            "fully",
            "main",
            "’verb’",
            "tokenizer",
            "beautiful",
            "boro",
            "model",
            "misclassifies",
            "correct",
            "fassaro",
            "limits",
            "’pron",
            "proved",
            "labels",
            "without",
            "’fassaro’",
            "makes",
            "incorrectly",
            "xlmroberta",
            "category",
            "similar",
            "failure",
            "attention",
            "use",
            "while",
            "bilstmcrf",
            "diverse"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "training",
                    "rules",
                    "models",
                    "afriberta",
                    "examples",
                    "its",
                    "alone",
                    "outofvocabulary",
                    "r2tbilstm",
                    "more",
                    "labeled",
                    "sentences",
                    "complex",
                    "handle",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Part-of-speech (POS) tagging is a foundational task in Natural Language Processing (NLP), serving as a prerequisite for complex downstream applications such as machine translation, syntactic parsing, and information extraction. For high-resource languages, deep learning models achieve near-perfect accuracy in POS tasks. However, that is not case for low-resource languages, where there is a lack of large manually annotated dataset these data-hungry models require. This data scarcity limits the development of robust linguistic tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "limits",
                    "syntactic",
                    "dataset",
                    "not",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Researchers often attempt to bridge this gap using two primary strategies: transfer learning or traditional rule-based systems.\nTransfer learning needs parallel data and careful alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>)</cite>. Multilingual transformers help in many languages, but they still depend on large-scale pretraining pipelines, tokenizers that match the target script, and computing resources that many communities do not have <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib6\" title=\"\">2020</a>)</cite>. Conversely, purely rule-based taggers do not scale either: they work on easy cases and then break on ambiguity.</p>\n\n",
                "matched_terms": [
                    "ambiguity",
                    "cases",
                    "primary",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find an effective solution to these challenge, we propose the <span class=\"ltx_text ltx_font_bold\">rule-to-tag (R2T)</span> framework, a novel hybrid approach that <em class=\"ltx_emph ltx_font_italic\">integrates explicit linguistic rules directly into the neural network&#8217;s training objective</em>. This method creates a powerful linguistic scaffold, guiding the model&#8217;s learning process even when labeled data is unavailable. Additionally, R2T incorporates an adaptive out-of-vocabulary (OOV) loss term. This term teaches the model to express principled uncertainty when it encounters unknown words, preventing confident but incorrect guesses. This is especially important in underresourced languages, where code-switching and borrowed words are common.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "training",
                    "rules",
                    "outofvocabulary",
                    "model’s",
                    "labeled",
                    "incorrect",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, our work contributes to a paradigm we call <span class=\"ltx_text ltx_font_bold\">principled learning (PrL)</span>: training models not only from labeled examples, but by embedding explicit task-based principles directly into the learning objective&#8212;to our knowledge, the first to operate as such. We show this approach can work as a complete unsupervised method for simpler tasks, and as a powerful pre-training stage for more complex ones.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "training",
                    "models",
                    "examples",
                    "from",
                    "not",
                    "more",
                    "labeled",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Can a model trained with linguistic rules and unlabeled text outperform a large pre-trained model fine-tuned on a small set of labeled data?</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "small",
                    "labeled",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How effectively can a model pre-trained with the R2T framework be improved with a minimal amount of supervised fine-tuning, especially for more complex tasks?</p>\n\n",
                "matched_terms": [
                    "model",
                    "supervised",
                    "more",
                    "finetuning",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The R2T framework:</span> We introduce a novel hybrid architecture that leverages a multi-tiered linguistic rule system integrated directly into the training objective.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "training",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV regularization:</span> We propose and implement a novel loss term that regularizes the model&#8217;s confidence on out-of-vocabulary tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "outofvocabulary",
                    "model’s",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strong",
                    "supervised",
                    "without",
                    "analysis",
                    "r2tbilstm",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Principled pre-training for complex tasks:</span> We show that for a sparser task like NER, R2T serves as a highly data-efficient pre-training method which enables a model to be fine-tuned on just 50 sentences and surpass a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "sentences",
                    "model",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model release:</span> We release the pre-trained Zarma FastText embeddings and our best models for both POS and NER tasks&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/27Group\" title=\"\">https://huggingface.co/27Group</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of POS tagging in low-resource settings, we introduce <span class=\"ltx_text ltx_font_bold\">R2T</span>. R2T is a hybrid framework that combines the contextual learning ability of neural networks with a structured, multi-tiered system of linguistic knowledge. Instead of treating rules as a rigid post-processing step, we integrate them directly into the model&#8217;s learning objective through a novel, adaptive loss function. This method forces the model to adhere to known linguistic facts while teaching it to handle uncertainty gracefully when encountering unknown words.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "model’s",
                    "while",
                    "treating",
                    "contextual",
                    "handle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At its core, the R2T framework consists of three main components. First, a foundational neural architecture captures contextual patterns from text. Second, a multi-tiered rule system provides explicit linguistic constraints. Finally, a rule-informed adaptive loss function orchestrates the interaction between the two, guiding the model towards grammatically sound and robust predictions. We detail each of these components in the following subsections.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "rule",
                    "architecture",
                    "main",
                    "from",
                    "its",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sentence",
                    "provide",
                    "architecture",
                    "dataset",
                    "word",
                    "from",
                    "small",
                    "token",
                    "alone",
                    "use",
                    "learn",
                    "handle",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sequence of characters is fed into a separate character-level sequential neural model (transformer or bidirectional long short-term memory (BiLSTM)), and the final hidden states are concatenated. This technique allows the model to infer representations for unseen words based on their sub-word structure, a method proven effective in numerous tagging tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "separate",
                    "based",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "word",
                    "tokenlevel",
                    "main",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary innovation of R2T lies not just in using rules, but in structuring them into a multi-tiered system that provides a scaffold for the neural model&#8217;s learning process. This system organizes linguistic knowledge from high-confidence facts to general heuristics, allowing for a more nuanced form of guidance. We define four tiers of rules.</p>\n\n",
                "matched_terms": [
                    "rules",
                    "from",
                    "primary",
                    "not",
                    "more",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 1: Unambiguous lexical rules.</span> This tier forms the bedrock of our knowledge base. It contains a lexicon of words that map to a single, unambiguous POS tag. This typically includes high-frequency function words&#8212;e.g., pronouns, determiners, prepositions&#8212;and core vocabulary whose tags are constant across contexts.</p>\n\n",
                "matched_terms": [
                    "rules",
                    "lexical",
                    "tag",
                    "across",
                    "tags"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "lexical",
                    "tag",
                    "provide",
                    "architecture",
                    "ambiguous",
                    "ambiguity",
                    "word",
                    "context",
                    "set",
                    "not",
                    "’verb’",
                    "’noun’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 3: Morphological rules.</span> To improve generalization to unseen words, this tier captures common morphological patterns. These rules are typically suffix- or prefix-based and suggest a likely tag. For example, a rule might specify that words ending in a particular suffix are likely to be nouns. This provides a heuristic when no lexical entry exists for a word.</p>\n\n",
                "matched_terms": [
                    "example",
                    "rule",
                    "lexical",
                    "rules",
                    "tag",
                    "likely",
                    "word"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 4: Syntactic rules.</span> This tier models local grammatical structure by defining valid and invalid transitions between adjacent POS tags. These rules are represented as a matrix of bigram probabilities or constraints&#8212;e.g., a &#8217;DETERMINER&#8217; is very likely to be followed by a &#8217;NOUN&#8217; but not by a &#8217;VERB&#8217;. This helps the model produce more coherent and grammatically plausible tag sequences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "tag",
                    "models",
                    "syntactic",
                    "tags",
                    "likely",
                    "plausible",
                    "not",
                    "more",
                    "’verb’",
                    "’noun’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The R2T framework&#8217;s components are unified through a carefully designed multi-part loss function. This function translates the multi-tiered rule system into a set of training objectives that guide the model&#8217;s training. The total loss <math alttext=\"\\mathcal{L}_{\\text{R2T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>R2T</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{R2T}}</annotation></semantics></math> is a weighted sum of four distinct components:</p>\n\n",
                "matched_terms": [
                    "training",
                    "rule",
                    "model’s",
                    "guide",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical loss (<math alttext=\"\\mathcal{L}_{\\text{lex}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">lex</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{lex}}</annotation></semantics></math>).</span> This term enforces the high-confidence lexical and morphological rules (Tiers 1-3). For a token <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> with an unambiguous tag <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> defined in the rule set, the loss is the standard negative log-likelihood:</p>\n\n",
                "matched_terms": [
                    "rule",
                    "rules",
                    "lexical",
                    "tag",
                    "token",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "model",
                    "tags",
                    "without",
                    "its",
                    "token",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{p}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}</annotation></semantics></math> is the vector of tag probabilities for the token at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. This term effectively discourages the model from outputting grammatically invalid tag sequences.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "from",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distributional loss (<math alttext=\"\\mathcal{L}_{\\text{dist}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">dist</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dist}}</annotation></semantics></math>).</span> This is a simple regularization term, calculated as the Kullback-Leibler (KL) Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shlens, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib23\" title=\"\">2014</a>)</cite> between the model&#8217;s average predicted tag distribution and a uniform distribution. It encourages the model to utilize the entire tagset, preventing it from skewing towards only a few high-frequency tags.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tag",
                    "tags",
                    "from",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "prediction",
                    "likely",
                    "word",
                    "from",
                    "its",
                    "not",
                    "incorrect",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|T|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>T</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|T|</annotation></semantics></math> is the number of tags in the tagset. This loss term acts as a regularizer for uncertainty. By minimizing it, the model learns a form of principled humility: it produces confident, peaked distributions for words it knows and flatter, more uncertain distributions for words it does not. This adaptive behavior helps to make the tagger robust to the diverse and unseen vocabulary inherent in low-resource language texts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tags",
                    "not",
                    "more",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these components make R2T an end-to-end differentiable system, where rules are not heuristics or constraints applied after the fact but are part of the training objective. This specific design is what distinguishes our paradigm from earlier constraint-based approaches that operate outside the model&#8217;s gradient update.</p>\n\n",
                "matched_terms": [
                    "training",
                    "rules",
                    "from",
                    "specific",
                    "not",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of experiments to evaluate the effectiveness of our approach. Our goal is twofold. First, we aim to demonstrate that the R2T framework, which leverages only linguistic rules and unlabeled text, can outperform strong pre-trained language models fine-tuned on a small annotated dataset. Second, we analyze the impact of the underlying neural architecture (BiLSTM vs. Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib25\" title=\"\">2023</a>)</cite>) and the effect of supervised fine-tuning (SFT) on the R2T model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rules",
                    "strong",
                    "models",
                    "supervised",
                    "architecture",
                    "dataset",
                    "sft",
                    "small",
                    "transformer",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments focus on the Zarma language, a member of the Songhay language family spoken primarily in Niger. Zarma is a low-resource language, with very limited publicly available annotated corpora suitable for training standard NLP models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unsupervised pre-training, we used 25,000 sentences from the Zarma GEC dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Keita et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib13\" title=\"\">2025</a>)</cite>. We trained FastText embeddings on the full dataset.</p>\n\n",
                "matched_terms": [
                    "from",
                    "sentences",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we created a gold-standard dataset of 1,300 sentences, annotated by three experts (IAA: <math alttext=\"\\alpha=0.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.93</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.93</annotation></semantics></math> for POS, <math alttext=\"\\alpha=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.97</annotation></semantics></math> for NER). We use four disjoint splits: (i) Unlabeled training (25k sents), (ii) Rule-Dev (100 sents) for rule refinement, (iii) Gold-Train (300 sents) for baselines and SFT, and (iv) Gold-Test (1,000 sents) for final evaluation. These splits are released with of the ZarmaPOS-Bench dataset&#8212;built from Feriji&#8212;and detailed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5\" title=\"5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Rules are described below.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "training",
                    "rules",
                    "dataset",
                    "sft",
                    "from",
                    "use",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "rule",
                    "rules",
                    "model",
                    "syntactic",
                    "ambiguous",
                    "from",
                    "auxiliary",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recap, We use four disjoint splits:\n(i) <span class=\"ltx_text ltx_font_bold\">Unlabeled training</span> (25k sents) for unsupervised R2T;\n(ii) <span class=\"ltx_text ltx_font_bold\">Rule-Dev</span> (100 sents), sampled from the same source as the unlabeled corpus, used <em class=\"ltx_emph ltx_font_italic\">only</em> for error inspection during iterative rule refinement;\n(iii) <span class=\"ltx_text ltx_font_bold\">Gold-Train</span> (300 sents) used for supervised baselines and SFT;\n(iv) <span class=\"ltx_text ltx_font_bold\">Gold-Test</span> (1,000 sents) held out and <em class=\"ltx_emph ltx_font_italic\">never inspected</em> until the final evaluation.\nNo sentence appears in more than one split. All rules and hyperparameters were frozen on Rule-Dev before evaluating on Gold-Test.</p>\n\n",
                "matched_terms": [
                    "appears",
                    "rule",
                    "training",
                    "rules",
                    "one",
                    "sentence",
                    "supervised",
                    "sft",
                    "from",
                    "error",
                    "use",
                    "more",
                    "never"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the performance of six different models to provide a comprehensive evaluation. We consider an array of transformer models, which is the state-of-the-art architecture for language models and embeddings, and BiLSTMs, which has demonstrated strong performance in capturing long-range features in text <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochreiter and Schmidhuber, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib10\" title=\"\">1997</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "models",
                    "different",
                    "architecture",
                    "transformer",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BiLSTM-CRF</span> is a classic and strong supervised baseline. It uses the architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a> with a final CRF layer for structured prediction It is trained from scratch on our full 300-sentence annotated dataset.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "prediction",
                    "supervised",
                    "architecture",
                    "dataset",
                    "from",
                    "bilstmcrf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "primary",
                    "r2tbilstm",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span> serves as an architectural ablation study. It replaces the BiLSTM core with a Transformer encoder&#8212;10 layers, 6 attention heads, 768 hidden units and 3072 feed-forward&#8212;but uses the exact same rule system and training objective as the R2T-BiLSTM.</p>\n\n",
                "matched_terms": [
                    "training",
                    "rule",
                    "attention",
                    "r2tbilstm",
                    "transformer",
                    "r2ttransformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> is the R2T-Transformer model after it has been further fine-tuned for 20 epochs on the first 50 sentences of our annotated dataset using a standard cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sft50",
                    "dataset",
                    "r2ttransformer",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span> is an African-centric baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span> is a widely-used multilingual baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the detailed hyperparameters for all our models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2\" title=\"Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. For evaluation, we use a comprehensive set of metrics. We report overall <span class=\"ltx_text ltx_font_bold\">Word-Level Accuracy</span> and the <span class=\"ltx_text ltx_font_bold\">Macro F1-Score</span>, which is the unweighted average of the F1-score for each tag.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "models",
                    "set",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all baselines we apply the same <span class=\"ltx_text ltx_font_typewriter\">wordpunct</span> tokenization.\nThis removes tokenizer mismatches and ensures fair comparison.</p>\n\n",
                "matched_terms": [
                    "tokenizer",
                    "tokenization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Results &#8227; 3 Experiments &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results for Zarma POS tagging. We report per-tag F1-scores and macro averages as the primary evaluation metric, following standard practice in sequence tagging. Overall accuracy is included for completeness, but our focus is on F1, which better captures performance under class imbalance.</p>\n\n",
                "matched_terms": [
                    "class",
                    "main",
                    "primary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "tokenization",
                    "xlmroberta",
                    "sentences",
                    "model",
                    "across",
                    "strong",
                    "bilstmcrf",
                    "supervised",
                    "tags",
                    "rare",
                    "fully",
                    "r2tbilstm",
                    "transformer",
                    "frequent",
                    "afriberta",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "rule",
                    "lexical",
                    "give",
                    "most",
                    "rules",
                    "sentences",
                    "ambiguous",
                    "its",
                    "r2tbilstm",
                    "verb",
                    "more",
                    "afriberta",
                    "auxiliary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "global",
                    "appears",
                    "errors",
                    "model",
                    "rules",
                    "dilute",
                    "normal",
                    "play",
                    "architecture",
                    "tokenlevel",
                    "signal",
                    "wani",
                    "r2tbilstm",
                    "more",
                    "token",
                    "r2ttransformer",
                    "leading",
                    "transformer’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "strong",
                    "rules",
                    "correct",
                    "supervised",
                    "afriberta",
                    "small",
                    "its",
                    "specific",
                    "from",
                    "more",
                    "labeled",
                    "sentences",
                    "finetuning",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study focuses on POS tagging, the R2T design is not task-specific: any task with declarative linguistic or structural rules&#8212;e.g., morphological analysis, shallow parsing, phonotactic constraints&#8212;can be mapped into loss components. We therefore view POS tagging in Zarma as a case study of PrL.</p>\n\n",
                "matched_terms": [
                    "while",
                    "analysis",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important Note:</span> Because R2T&#8217;s loss terms co-define the training dynamics, removing any term constitutes a different algorithm rather than an informative probe of the same method. We therefore evaluate architecture sensitivity (BiLSTM vs. Transformer) and data-regime sensitivity (unsupervised vs. SFT-50), keeping the objective intact and testing whether the combined design transfers across inductive biases.</p>\n\n",
                "matched_terms": [
                    "training",
                    "across",
                    "sft50",
                    "different",
                    "architecture",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary obstacle in low-resource NLP research is the lack absence of large-scale annotated dataset for tasks like POS tagging&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib14\" title=\"\">2022</a>)</cite>. To address this gap and to stimulate further research&#8212;for Zarma&#8212;we introduce <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span>, the first POS-tagged benchmark dataset for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "primary",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "sufficient",
                    "without",
                    "not",
                    "r2tbilstm",
                    "labeled",
                    "while",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "example",
                    "sentences",
                    "model",
                    "sentence",
                    "dataset",
                    "from",
                    "r2tbilstm",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench is a comprehensive resource containing over <span class=\"ltx_text ltx_font_bold\">55,000 sentences</span> and more than <span class=\"ltx_text ltx_font_bold\">1,000,000 tokens</span>. The dataset is provided in the JSONL format, where each line represents a single sentence and contains three fields:</p>\n\n",
                "matched_terms": [
                    "sentences",
                    "sentence",
                    "dataset",
                    "more",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">tokens</span>: A list of strings representing the tokenized sentence.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">tags</span>: A parallel list of strings representing the predicted POS tag for each token.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "tags",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An example entry from the dataset is shown below:</p>\n\n",
                "matched_terms": [
                    "example",
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The distribution of the POS tags across the entire dataset is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.T3\" title=\"Table 3 &#8227; 5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. As expected, nouns, verbs, pronouns, and auxiliaries are the most frequent categories, reflecting typical linguistic patterns.</p>\n\n",
                "matched_terms": [
                    "across",
                    "most",
                    "tags",
                    "dataset",
                    "frequent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As ZarmaPOS-Bench was generated automatically, it is a silver-standard dataset and inevitably contains errors. Based on our analysis in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S4\" title=\"4 Analysis and Discussion &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, these errors are likely to be minor and concentrated around subtle ambiguities&#8212;e.g., distinguishing between &#8217;ADJ&#8217; and &#8217;NOUN&#8217; in complex phrases&#8212;or very rare, out-of-domain words. The overall quality, however, is exceptionally high for a synthetically generated corpus.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "’adj’",
                    "likely",
                    "dataset",
                    "rare",
                    "analysis",
                    "minor",
                    "complex",
                    "’noun’",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this limitation and to encourage a cycle of continuous improvement, we are releasing ZarmaPOS-Bench alongside our <span class=\"ltx_text ltx_font_bold\">300-sentence gold dataset</span>. This smaller, manually verified set is an important companion resource that can be used in several ways:</p>\n\n",
                "matched_terms": [
                    "set",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a SFT set to further improve models trained on ZarmaPOS-Bench, adjusting the silver-standard model&#8217;s systematic errors, as shown in our SFT-50 experiment.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "sft50",
                    "models",
                    "sft",
                    "model’s",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "most",
                    "labels",
                    "examples",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "rules",
                    "models",
                    "supervised",
                    "proved",
                    "without",
                    "labeled",
                    "sentences",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "training",
                    "rules",
                    "correct",
                    "provide",
                    "seen",
                    "without",
                    "alone",
                    "specific",
                    "from",
                    "model’s",
                    "auxiliary",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "small",
                    "itself",
                    "not",
                    "more",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the R2T framework introduces several hyperparameters, the weights (<math alttext=\"\\alpha,\\beta,\\gamma,\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>,</mo><mi>&#946;</mi><mo>,</mo><mi>&#947;</mi><mo>,</mo><mi>&#948;</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha,\\beta,\\gamma,\\delta</annotation></semantics></math>) that balance the components of our adaptive loss function. Finding the optimal balance for these weights, along with the ideal neural architecture, requires a degree of empirical exploration. Although individual training runs are computationally efficient compared to pre-training large language models from scratch, this search process can still be resource-intensive for researchers with limited computational budgets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "architecture",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "rules",
                    "one",
                    "similar",
                    "not",
                    "sentences",
                    "complex",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, our experiments deliberately exclude state-of-the-art large language models (except for the embedding-based models). While powerful, these models do not align with the conditions and principles of our low-resource setting. Our focus is on developing accessible, reproducible, and computationally efficient methods that can be trained and deployed by researchers and communities with limited resources. Therefore, we restricted our comparisons to publicly available, open-source models that can be run and fine-tuned on consumer-grade hardware.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our case study is on Zarma, a language of the Songhay familiy. The framework&#8217;s performance on languages on different language family remains an open question&#8212;although we carried an experiment with Bambara&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3\" title=\"Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>). Such languages might require more complex morphological or syntactic rule tiers to be effective.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "syntactic",
                    "different",
                    "more",
                    "complex",
                    "remains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge in low-resource POS tagging is the lack of annotated data. A common strategy is cross-lingual projection, which transfers supervision from high-resource languages via parallel data or word alignments <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>; T&#228;ckstr&#246;m et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib24\" title=\"\">2013</a>)</cite>. Other approaches rely on classic probabilistic models like HMMs or TnT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brants, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib2\" title=\"\">2000</a>)</cite>, which can be effective but often lack the contextual power of neural models. More recent work has shown that small, targeted amounts of annotation, when combined with morphological information and type-level constraints, can be highly effective <cite class=\"ltx_cite ltx_citemacro_citep\">(Garrette and Baldridge, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our R2T framework builds on this insight by formalizing the injection of such constraints directly into a neural model&#8217;s training objective, removing the need for any initial labeled data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "models",
                    "word",
                    "from",
                    "small",
                    "primary",
                    "more",
                    "labeled",
                    "model’s",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The idea of embedding prior knowledge into machine learning models has a rich history. Methods like posterior regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(Ganchev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib8\" title=\"\">2010</a>)</cite> and generalized expectation criteria <cite class=\"ltx_cite ltx_citemacro_citep\">(Mann and McCallum, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib16\" title=\"\">2010</a>)</cite> use constraints to guide model posteriors, often through an auxiliary optimization process. Similarly, constrained conditional models shape the inference process to ensure outputs adhere to pre-defined rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib4\" title=\"\">2012</a>)</cite>. More recently, weak supervision frameworks like Snorkel and data programming have enabled the aggregation of noisy, heuristic labeling functions into a unified training signal <cite class=\"ltx_cite ltx_citemacro_citep\">(Ratner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "rules",
                    "models",
                    "signal",
                    "use",
                    "more",
                    "guide",
                    "auxiliary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our PrL paradigm is distinct from these prior works in an interesting way. Instead of using rules to constrain inference, regularize posteriors, or generate pseudo-labels, R2T integrates them as direct, differentiable components of the end-to-end training loss. In our unsupervised setup, these rule-based losses are the <em class=\"ltx_emph ltx_font_italic\">primary</em> learning signal, entirely replacing the need for labeled examples.</p>\n\n",
                "matched_terms": [
                    "training",
                    "rules",
                    "examples",
                    "from",
                    "signal",
                    "primary",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work employs standard neural architectures for sequence tagging, such as BiLSTMs with character-level embeddings, which are known to be effective for handling OOV words and morphology <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>. While a conditional random field (CRF) layer is often used for structured prediction <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib11\" title=\"\">2015</a>)</cite>, our approach replaces this with a soft, differentiable syntactic loss. We also compare our approach to large multilingual models like XLM-RoBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite> and African-centric models like AfriBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. While powerful, these models can suffer from tokenizer mismatches in low-resource languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib22\" title=\"\">2021</a>)</cite>, a finding our experiments confirm. Finally, our adaptive OOV loss is related to confidence regularization techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Pereyra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib19\" title=\"\">2017</a>)</cite>, but it is applied selectively which encourages principled uncertainty only when the model has no rule-based guidance.</p>\n\n",
                "matched_terms": [
                    "tokenizer",
                    "xlmroberta",
                    "model",
                    "prediction",
                    "models",
                    "syntactic",
                    "from",
                    "while",
                    "afriberta",
                    "morphology",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the specific architectural details and training hyperparameters used in our experiments, ensuring full reproducibility of our results.</p>\n\n",
                "matched_terms": [
                    "specific",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While both of our R2T models share the same input representation&#8212;concatenated FastText and character embeddings&#8212;and the same rule-informed loss function, their core sequence processing architectures differ significantly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "one",
                    "tokenlevel",
                    "main",
                    "word",
                    "token",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer.</span> Our attention-based model, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, replaces the recurrent core with a Transformer encoder. The initial 350-dimensional input vector is first projected to match the Transformer&#8217;s hidden dimension of 768 using a linear layer. We then add sinusoidal positional encodings to this vector to provide the model with sequence order information. This final 768-dimensional vector is processed by a 10-layer Transformer encoder. Each layer contains 6 self-attention heads and a feed-forward network with 3072 hidden units. The 768-dimensional output vector from the final layer is then passed through a linear layer to produce the tag logits.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tag",
                    "from",
                    "transformer",
                    "r2ttransformer",
                    "provide",
                    "transformer’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.T4\" title=\"Table 4 &#8227; B.2 Training Hyperparameters &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the list of the hyperparameters used for training and fine-tuning all models evaluated in our experiments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "complex",
                    "different",
                    "set",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Rules.</span> We drafted a new multi-tiered rule system specifically for Bambara&#8212;mainly drafted from Daba morphemic rules&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maslinsky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib17\" title=\"\">2014</a>)</cite>. This included a lexicon of approximately 100 unambiguous words, rules for ambiguous function words (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span>, <span class=\"ltx_text ltx_font_italic\">ka</span>, <span class=\"ltx_text ltx_font_italic\">ma</span>), common morphological suffixes (e.g., plural &#8217;-w&#8217;), and a set of core syntactic constraints. This rule set was intentionally drafted in a few hours to simulate a rapid development scenario for a new language.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "rules",
                    "syntactic",
                    "ambiguous",
                    "from",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> For the unsupervised training phase, we used a monolingual Bambara corpus of approximately 864 sentences sourced from the SMOL dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Caswell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib3\" title=\"\">2025</a>)</cite>. For evaluation, we used Bambara 1000 sentences.</p>\n\n",
                "matched_terms": [
                    "from",
                    "sentences",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model.</span> For this experiment, we used a hybrid architecture combining a pre-trained T5 encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib20\" title=\"\">2020</a>)</cite> with our BiLSTM tagger head. The T5 encoder&#8212;<span class=\"ltx_text ltx_font_bold\">t5-small</span>&#8212;was used to generate contextual embeddings, which were then fed into the BiLSTM. The entire model was trained from scratch using only our Bambara rule system and the unlabeled corpus.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rule",
                    "architecture",
                    "from",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline.</span> We compare our model against the <span class=\"ltx_text ltx_font_bold\">Masakhane AfroXLMR</span> model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>on huggingface:(<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"masakhane/bambara-pos-tagger-afroxlmr\" title=\"\">masakhane/bambara-pos-tagger-afroxlmr</a>)</span></span></span>, which was fine-tuned on a manually annotated Bambara dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n",
                "matched_terms": [
                    "supervised",
                    "model",
                    "without",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This result is insightful. It confirms that the R2T framework can be successfully adapted to a new language, and also reinforces our central claim: a modest investment in encoding linguistic knowledge can be more effective than fine-tuning on a small, potentially noisy, annotated dataset. The +0.13 absolute improvement in Macro F1 demonstrates the power of providing a model with explicit grammatical principles.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "dataset",
                    "small",
                    "more",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A qualitative analysis of the errors made by the Masakhane model reveals why our R2T approach is effective. The baseline model&#8217;s errors are systematic and arise from the exact issues R2T is designed to solve, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T6\" title=\"Table 6 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "errors",
                    "qualitative",
                    "analysis",
                    "from",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "training",
                    "rule",
                    "lexical",
                    "rules",
                    "aux",
                    "model",
                    "noun",
                    "specific",
                    "wani",
                    "verb",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "model",
                    "identify",
                    "across",
                    "generalize",
                    "most",
                    "prediction",
                    "limits",
                    "its",
                    "not",
                    "more",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "training",
                    "supervised",
                    "dataset",
                    "sft",
                    "from",
                    "use",
                    "sentences",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a similar set of models as in our POS experiments:</p>\n\n",
                "matched_terms": [
                    "models",
                    "set",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "r2tbilstm",
                    "set",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span>, which takes the unsupervised R2T-Transformer and fine-tunes it on 50 gold sentences.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "sentences",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span>, fine-tuned on the 300 gold sentences.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "sentences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5.T7\" title=\"Table 7 &#8227; E.2 Results and Analysis &#8227; Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports span-level F1-scores as the primary evaluation measure for Zarma NER. This provides a fairer evaluation than token-level accuracy, as it requires both correct entity type and correct span boundaries.</p>\n\n",
                "matched_terms": [
                    "tokenlevel",
                    "primary",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "sentences",
                    "model",
                    "sft50",
                    "rules",
                    "models",
                    "r2ttransformer",
                    "afriberta",
                    "complex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Data Efficiency in Zarma NER &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides a visual representation of the data efficiency demonstrated in our Zarma NER experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5\" title=\"Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). The plot clearly shows that the R2T-Transformer starts from a much higher baseline accuracy (67.4%) than a standard fine-tuning approach. This strong foundation allows it to surpass the performance of the AfriBERTa baseline after being fine-tuned on only 50 labeled examples.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "afriberta",
                    "examples",
                    "from",
                    "labeled",
                    "r2ttransformer",
                    "finetuning",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "errors",
                    "model",
                    "across",
                    "’aux’",
                    "strong",
                    "provide",
                    "tags",
                    "confusion",
                    "set",
                    "r2tbilstm",
                    "more",
                    "not",
                    "model’s",
                    "minor",
                    "’verb’",
                    "’noun’"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 3: Estimated tag distribution in the ZarmaPOS-Bench dataset. Counts are rounded for clarity. \"OTHER\" tag is used for very low-confidence tokens",
        "body": "POS Tag\nCount\nFrequency (%)\n\n\n\n\nNOUN\n241,274\n24.0\n\n\nPRON\n168,153\n16.7\n\n\nAUX\n162,423\n16.2\n\n\nPUNCT\n156,019\n15.5\n\n\nVERB\n146,118\n14.5\n\n\nPART\n81,387\n8.1\n\n\nADJ\n26,777\n2.7\n\n\nDET\n21,340\n2.1\n\n\nOTHER\n1,804\n0.2\n\n\nTotal\n1,005,295\n100.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">POS Tag</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Count</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Frequency (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">NOUN</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">241,274</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">24.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PRON</th>\n<td class=\"ltx_td ltx_align_right\">168,153</td>\n<td class=\"ltx_td ltx_align_right\">16.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AUX</th>\n<td class=\"ltx_td ltx_align_right\">162,423</td>\n<td class=\"ltx_td ltx_align_right\">16.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PUNCT</th>\n<td class=\"ltx_td ltx_align_right\">156,019</td>\n<td class=\"ltx_td ltx_align_right\">15.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VERB</th>\n<td class=\"ltx_td ltx_align_right\">146,118</td>\n<td class=\"ltx_td ltx_align_right\">14.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PART</th>\n<td class=\"ltx_td ltx_align_right\">81,387</td>\n<td class=\"ltx_td ltx_align_right\">8.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ADJ</th>\n<td class=\"ltx_td ltx_align_right\">26,777</td>\n<td class=\"ltx_td ltx_align_right\">2.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DET</th>\n<td class=\"ltx_td ltx_align_right\">21,340</td>\n<td class=\"ltx_td ltx_align_right\">2.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OTHER</th>\n<td class=\"ltx_td ltx_align_right\">1,804</td>\n<td class=\"ltx_td ltx_align_right\">0.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">Total</th>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\">1,005,295</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\">100.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "very",
            "tag",
            "counts",
            "aux",
            "rounded",
            "clarity",
            "noun",
            "verb",
            "used",
            "zarmaposbench",
            "adj",
            "frequency",
            "pos",
            "distribution",
            "dataset",
            "estimated",
            "count",
            "part",
            "punct",
            "lowconfidence",
            "pron",
            "total",
            "det",
            "tokens"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The distribution of the POS tags across the entire dataset is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.T3\" title=\"Table 3 &#8227; 5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. As expected, nouns, verbs, pronouns, and auxiliaries are the most frequent categories, reflecting typical linguistic patterns.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Part-of-speech (POS) tagging is a foundational task in Natural Language Processing (NLP), serving as a prerequisite for complex downstream applications such as machine translation, syntactic parsing, and information extraction. For high-resource languages, deep learning models achieve near-perfect accuracy in POS tasks. However, that is not case for low-resource languages, where there is a lack of large manually annotated dataset these data-hungry models require. This data scarcity limits the development of robust linguistic tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench &amp; ZarmaNER-600:</span> We release the first POS-tagged and NER-annotated corpora for Zarma. This includes a large silver-standard and 300 gold-standard datasets for POS, and a 600-sentence gold-standard NER dataset.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "other",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 1: Unambiguous lexical rules.</span> This tier forms the bedrock of our knowledge base. It contains a lexicon of words that map to a single, unambiguous POS tag. This typically includes high-frequency function words&#8212;e.g., pronouns, determiners, prepositions&#8212;and core vocabulary whose tags are constant across contexts.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 4: Syntactic rules.</span> This tier models local grammatical structure by defining valid and invalid transitions between adjacent POS tags. These rules are represented as a matrix of bigram probabilities or constraints&#8212;e.g., a &#8217;DETERMINER&#8217; is very likely to be followed by a &#8217;NOUN&#8217; but not by a &#8217;VERB&#8217;. This helps the model produce more coherent and grammatically plausible tag sequences.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "pos",
                    "very"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distributional loss (<math alttext=\"\\mathcal{L}_{\\text{dist}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">dist</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dist}}</annotation></semantics></math>).</span> This is a simple regularization term, calculated as the Kullback-Leibler (KL) Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shlens, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib23\" title=\"\">2014</a>)</cite> between the model&#8217;s average predicted tag distribution and a uniform distribution. It encourages the model to utilize the entire tagset, preventing it from skewing towards only a few high-frequency tags.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For unsupervised pre-training, we used 25,000 sentences from the Zarma GEC dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Keita et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib13\" title=\"\">2025</a>)</cite>. We trained FastText embeddings on the full dataset.</p>\n\n",
                "matched_terms": [
                    "used",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we created a gold-standard dataset of 1,300 sentences, annotated by three experts (IAA: <math alttext=\"\\alpha=0.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.93</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.93</annotation></semantics></math> for POS, <math alttext=\"\\alpha=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.97</annotation></semantics></math> for NER). We use four disjoint splits: (i) Unlabeled training (25k sents), (ii) Rule-Dev (100 sents) for rule refinement, (iii) Gold-Train (300 sents) for baselines and SFT, and (iv) Gold-Test (1,000 sents) for final evaluation. These splits are released with of the ZarmaPOS-Bench dataset&#8212;built from Feriji&#8212;and detailed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5\" title=\"5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Rules are described below.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary obstacle in low-resource NLP research is the lack absence of large-scale annotated dataset for tasks like POS tagging&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib14\" title=\"\">2022</a>)</cite>. To address this gap and to stimulate further research&#8212;for Zarma&#8212;we introduce <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span>, the first POS-tagged benchmark dataset for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench is a comprehensive resource containing over <span class=\"ltx_text ltx_font_bold\">55,000 sentences</span> and more than <span class=\"ltx_text ltx_font_bold\">1,000,000 tokens</span>. The dataset is provided in the JSONL format, where each line represents a single sentence and contains three fields:</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">tags</span>: A parallel list of strings representing the predicted POS tag for each token.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As ZarmaPOS-Bench was generated automatically, it is a silver-standard dataset and inevitably contains errors. Based on our analysis in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S4\" title=\"4 Analysis and Discussion &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, these errors are likely to be minor and concentrated around subtle ambiguities&#8212;e.g., distinguishing between &#8217;ADJ&#8217; and &#8217;NOUN&#8217; in complex phrases&#8212;or very rare, out-of-domain words. The overall quality, however, is exceptionally high for a synthetically generated corpus.</p>\n\n",
                "matched_terms": [
                    "very",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this limitation and to encourage a cycle of continuous improvement, we are releasing ZarmaPOS-Bench alongside our <span class=\"ltx_text ltx_font_bold\">300-sentence gold dataset</span>. This smaller, manually verified set is an important companion resource that can be used in several ways:</p>\n\n",
                "matched_terms": [
                    "used",
                    "zarmaposbench",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "part",
                    "zarmaposbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge in low-resource POS tagging is the lack of annotated data. A common strategy is cross-lingual projection, which transfers supervision from high-resource languages via parallel data or word alignments <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>; T&#228;ckstr&#246;m et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib24\" title=\"\">2013</a>)</cite>. Other approaches rely on classic probabilistic models like HMMs or TnT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brants, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib2\" title=\"\">2000</a>)</cite>, which can be effective but often lack the contextual power of neural models. More recent work has shown that small, targeted amounts of annotation, when combined with morphological information and type-level constraints, can be highly effective <cite class=\"ltx_cite ltx_citemacro_citep\">(Garrette and Baldridge, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our R2T framework builds on this insight by formalizing the injection of such constraints directly into a neural model&#8217;s training objective, removing the need for any initial labeled data.</p>\n\n",
                "matched_terms": [
                    "other",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> For the unsupervised training phase, we used a monolingual Bambara corpus of approximately 864 sentences sourced from the SMOL dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Caswell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib3\" title=\"\">2025</a>)</cite>. For evaluation, we used Bambara 1000 sentences.</p>\n\n",
                "matched_terms": [
                    "used",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "aux",
                    "noun",
                    "verb"
                ]
            }
        ]
    },
    "A2.T4": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 4: Training and architectural hyperparameters for all models in our experiments.",
        "body": "Hyperparameter\nR2T-BiLSTM\nR2T-Transformer\nAfriBERTa\nXLM-RoBERTa\nBiLSTM-CRF\n\n\nModel Architecture\n\n\nWord Embedding Dim\n300 (FastText)\n300 (FastText)\n768\n768\n100 (Learned)\n\n\nChar Embedding Dim\n50\n50\nN/A\nN/A\n25\n\n\nHidden Dim\n256 (x2)\n768\n768\n768\n128 (x2)\n\n\nNum. Layers\n1\n10\n12\n12\n1\n\n\nNum. Heads\nN/A\n6\n12\n12\nN/A\n\n\nFeed-Forward Dim\nN/A\n3072\n3072\n3072\nN/A\n\n\nDropout\n0.3\n0.1\n0.1\n0.1\n0.5\n\n\nTraining & Fine-Tuning\n\n\nOptimizer\nAdam\nAdam\nAdamW\nAdamW\nAdam\n\n\nLearning Rate\n1e-3\n5e-5\n2e-5\n2e-5\n1e-3\n\n\nBatch Size\n256\n64\n16\n16\n16\n\n\nEpochs\n30\n30 (unsup.) / 20 (SFT)\n10\n10\n50\n\n\nWeight Decay\n1e-5\n1e-5\n0.01\n0.01\n1e-4\n\n\nMax Grad Norm\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nR2T Loss Weights\n\n\n\nα\\alpha (Lexical)\n0.85\n0.85\nN/A\nN/A\nN/A\n\n\n\nβ\\beta (Syntactic)\n0.08\n0.08\nN/A\nN/A\nN/A\n\n\n\nγ\\gamma (Distributional)\n0.02\n0.02\nN/A\nN/A\nN/A\n\n\n\nδ\\delta (OOV)\n0.05\n0.05\nN/A\nN/A\nN/A",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BiLSTM-CRF</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Model Architecture</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Word Embedding Dim</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">300 (FastText)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">300 (FastText)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100 (Learned)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Char Embedding Dim</th>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Hidden Dim</th>\n<td class=\"ltx_td ltx_align_center\">256 (x2)</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">128 (x2)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Num. Layers</th>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Num. Heads</th>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">12</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Feed-Forward Dim</th>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">3072</td>\n<td class=\"ltx_td ltx_align_center\">3072</td>\n<td class=\"ltx_td ltx_align_center\">3072</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Dropout</th>\n<td class=\"ltx_td ltx_align_center\">0.3</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">Training &amp; Fine-Tuning</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Adam</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Adam</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Adam</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-3</td>\n<td class=\"ltx_td ltx_align_center\">5e-5</td>\n<td class=\"ltx_td ltx_align_center\">2e-5</td>\n<td class=\"ltx_td ltx_align_center\">2e-5</td>\n<td class=\"ltx_td ltx_align_center\">1e-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Batch Size</th>\n<td class=\"ltx_td ltx_align_center\">256</td>\n<td class=\"ltx_td ltx_align_center\">64</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Epochs</th>\n<td class=\"ltx_td ltx_align_center\">30</td>\n<td class=\"ltx_td ltx_align_center\">30 (unsup.) / 20 (SFT)</td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Weight Decay</th>\n<td class=\"ltx_td ltx_align_center\">1e-5</td>\n<td class=\"ltx_td ltx_align_center\">1e-5</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Max Grad Norm</th>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_italic\">R2T Loss Weights</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (Lexical)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (Syntactic)</th>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m3\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> (Distributional)</th>\n<td class=\"ltx_td ltx_align_center\">0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.02</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">\n<math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m4\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> (OOV)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">N/A</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "char",
            "training",
            "size",
            "lexical",
            "hyperparameter",
            "learned",
            "rate",
            "architecture",
            "max",
            "r2tbilstm",
            "architectural",
            "loss",
            "weights",
            "afriberta",
            "finetuning",
            "embedding",
            "our",
            "hyperparameters",
            "adam",
            "1e3",
            "all",
            "decay",
            "learning",
            "feedforward",
            "1e4",
            "unsup",
            "αalpha",
            "r2ttransformer",
            "distributional",
            "dim",
            "norm",
            "layers",
            "model",
            "1e5",
            "weight",
            "adamw",
            "2e5",
            "word",
            "hidden",
            "r2t",
            "dropout",
            "5e5",
            "epochs",
            "grad",
            "xlmroberta",
            "num",
            "γgamma",
            "models",
            "βbeta",
            "experiments",
            "optimizer",
            "syntactic",
            "batch",
            "sft",
            "heads",
            "bilstmcrf",
            "fasttext",
            "δdelta",
            "oov"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.T4\" title=\"Table 4 &#8227; B.2 Training Hyperparameters &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> provides the list of the hyperparameters used for training and fine-tuning all models evaluated in our experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "models",
                    "experiments",
                    "learning",
                    "r2t",
                    "r2tbilstm",
                    "loss",
                    "afriberta",
                    "our",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging</span>\n</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Part-of-speech (POS) tagging is a foundational task in Natural Language Processing (NLP), serving as a prerequisite for complex downstream applications such as machine translation, syntactic parsing, and information extraction. For high-resource languages, deep learning models achieve near-perfect accuracy in POS tasks. However, that is not case for low-resource languages, where there is a lack of large manually annotated dataset these data-hungry models require. This data scarcity limits the development of robust linguistic tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "syntactic",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find an effective solution to these challenge, we propose the <span class=\"ltx_text ltx_font_bold\">rule-to-tag (R2T)</span> framework, a novel hybrid approach that <em class=\"ltx_emph ltx_font_italic\">integrates explicit linguistic rules directly into the neural network&#8217;s training objective</em>. This method creates a powerful linguistic scaffold, guiding the model&#8217;s learning process even when labeled data is unavailable. Additionally, R2T incorporates an adaptive out-of-vocabulary (OOV) loss term. This term teaches the model to express principled uncertainty when it encounters unknown words, preventing confident but incorrect guesses. This is especially important in underresourced languages, where code-switching and borrowed words are common.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "learning",
                    "r2t",
                    "loss",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, our work contributes to a paradigm we call <span class=\"ltx_text ltx_font_bold\">principled learning (PrL)</span>: training models not only from labeled examples, but by embedding explicit task-based principles directly into the learning objective&#8212;to our knowledge, the first to operate as such. We show this approach can work as a complete unsupervised method for simpler tasks, and as a powerful pre-training stage for more complex ones.</p>\n\n",
                "matched_terms": [
                    "training",
                    "models",
                    "learning",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of R2T through a comprehensive case study on Zarma, a language for which no large-scale POS corpus previously existed. Our work is guided by the following research questions:</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How does the choice of neural architecture&#8212;recurrent vs. attention-based&#8212;interact with our rule-centric training objective?</p>\n\n",
                "matched_terms": [
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How effectively can a model pre-trained with the R2T framework be improved with a minimal amount of supervised fine-tuning, especially for more complex tasks?</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "r2t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The R2T framework:</span> We introduce a novel hybrid architecture that leverages a multi-tiered linguistic rule system integrated directly into the training objective.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "r2t",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV regularization:</span> We propose and implement a novel loss term that regularizes the model&#8217;s confidence on out-of-vocabulary tokens.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Principled pre-training for complex tasks:</span> We show that for a sparser task like NER, R2T serves as a highly data-efficient pre-training method which enables a model to be fine-tuned on just 50 sentences and surpass a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model release:</span> We release the pre-trained Zarma FastText embeddings and our best models for both POS and NER tasks&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/27Group\" title=\"\">https://huggingface.co/27Group</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "fasttext",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of POS tagging in low-resource settings, we introduce <span class=\"ltx_text ltx_font_bold\">R2T</span>. R2T is a hybrid framework that combines the contextual learning ability of neural networks with a structured, multi-tiered system of linguistic knowledge. Instead of treating rules as a rigid post-processing step, we integrate them directly into the model&#8217;s learning objective through a novel, adaptive loss function. This method forces the model to adhere to known linguistic facts while teaching it to handle uncertainty gracefully when encountering unknown words.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "learning",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At its core, the R2T framework consists of three main components. First, a foundational neural architecture captures contextual patterns from text. Second, a multi-tiered rule system provides explicit linguistic constraints. Finally, a rule-informed adaptive loss function orchestrates the interaction between the two, guiding the model towards grammatically sound and robust predictions. We detail each of these components in the following subsections.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "r2t",
                    "model",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "word",
                    "r2t",
                    "distributional",
                    "fasttext",
                    "embedding",
                    "our",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sequence of characters is fed into a separate character-level sequential neural model (transformer or bidirectional long short-term memory (BiLSTM)), and the final hidden states are concatenated. This technique allows the model to infer representations for unseen words based on their sub-word structure, a method proven effective in numerous tagging tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "word",
                    "embedding",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary innovation of R2T lies not just in using rules, but in structuring them into a multi-tiered system that provides a scaffold for the neural model&#8217;s learning process. This system organizes linguistic knowledge from high-confidence facts to general heuristics, allowing for a more nuanced form of guidance. We define four tiers of rules.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 1: Unambiguous lexical rules.</span> This tier forms the bedrock of our knowledge base. It contains a lexicon of words that map to a single, unambiguous POS tag. This typically includes high-frequency function words&#8212;e.g., pronouns, determiners, prepositions&#8212;and core vocabulary whose tags are constant across contexts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "architecture",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 3: Morphological rules.</span> To improve generalization to unseen words, this tier captures common morphological patterns. These rules are typically suffix- or prefix-based and suggest a likely tag. For example, a rule might specify that words ending in a particular suffix are likely to be nouns. This provides a heuristic when no lexical entry exists for a word.</p>\n\n",
                "matched_terms": [
                    "word",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 4: Syntactic rules.</span> This tier models local grammatical structure by defining valid and invalid transitions between adjacent POS tags. These rules are represented as a matrix of bigram probabilities or constraints&#8212;e.g., a &#8217;DETERMINER&#8217; is very likely to be followed by a &#8217;NOUN&#8217; but not by a &#8217;VERB&#8217;. This helps the model produce more coherent and grammatically plausible tag sequences.</p>\n\n",
                "matched_terms": [
                    "models",
                    "syntactic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The R2T framework&#8217;s components are unified through a carefully designed multi-part loss function. This function translates the multi-tiered rule system into a set of training objectives that guide the model&#8217;s training. The total loss <math alttext=\"\\mathcal{L}_{\\text{R2T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>R2T</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{R2T}}</annotation></semantics></math> is a weighted sum of four distinct components:</p>\n\n",
                "matched_terms": [
                    "loss",
                    "r2t",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha,\\beta,\\gamma,\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>&#945;</mi><mo>,</mo><mi>&#946;</mi><mo>,</mo><mi>&#947;</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\alpha,\\beta,\\gamma,</annotation></semantics></math> and <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> are hyperparameters that balance the contribution of each term.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "δdelta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical loss (<math alttext=\"\\mathcal{L}_{\\text{lex}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">lex</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{lex}}</annotation></semantics></math>).</span> This term enforces the high-confidence lexical and morphological rules (Tiers 1-3). For a token <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> with an unambiguous tag <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> defined in the rule set, the loss is the standard negative log-likelihood:</p>\n\n",
                "matched_terms": [
                    "loss",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syntactic loss (<math alttext=\"\\mathcal{L}_{\\text{syn}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">syn</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{syn}}</annotation></semantics></math>).</span> This term enforces the Tier 4 syntactic constraints. We define a transition invalidity matrix <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>, where <math alttext=\"M_{jk}=1-\\text{validity}(tag_{j}\\to tag_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mtext>validity</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M_{jk}=1-\\text{validity}(tag_{j}\\to tag_{k})</annotation></semantics></math>. The loss for a sequence is calculated by summing the penalty for each adjacent pair of predictions:</p>\n\n",
                "matched_terms": [
                    "syntactic",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distributional loss (<math alttext=\"\\mathcal{L}_{\\text{dist}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">dist</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dist}}</annotation></semantics></math>).</span> This is a simple regularization term, calculated as the Kullback-Leibler (KL) Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shlens, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib23\" title=\"\">2014</a>)</cite> between the model&#8217;s average predicted tag distribution and a uniform distribution. It encourages the model to utilize the entire tagset, preventing it from skewing towards only a few high-frequency tags.</p>\n\n",
                "matched_terms": [
                    "distributional",
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "model",
                    "word",
                    "loss",
                    "our",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|T|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>T</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|T|</annotation></semantics></math> is the number of tags in the tagset. This loss term acts as a regularizer for uncertainty. By minimizing it, the model learns a form of principled humility: it produces confident, peaked distributions for words it knows and flatter, more uncertain distributions for words it does not. This adaptive behavior helps to make the tagger robust to the diverse and unseen vocabulary inherent in low-resource language texts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these components make R2T an end-to-end differentiable system, where rules are not heuristics or constraints applied after the fact but are part of the training objective. This specific design is what distinguishes our paradigm from earlier constraint-based approaches that operate outside the model&#8217;s gradient update.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of experiments to evaluate the effectiveness of our approach. Our goal is twofold. First, we aim to demonstrate that the R2T framework, which leverages only linguistic rules and unlabeled text, can outperform strong pre-trained language models fine-tuned on a small annotated dataset. Second, we analyze the impact of the underlying neural architecture (BiLSTM vs. Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib25\" title=\"\">2023</a>)</cite>) and the effect of supervised fine-tuning (SFT) on the R2T model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "experiments",
                    "architecture",
                    "sft",
                    "r2t",
                    "finetuning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments focus on the Zarma language, a member of the Songhay language family spoken primarily in Niger. Zarma is a low-resource language, with very limited publicly available annotated corpora suitable for training standard NLP models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "experiments",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we created a gold-standard dataset of 1,300 sentences, annotated by three experts (IAA: <math alttext=\"\\alpha=0.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.93</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.93</annotation></semantics></math> for POS, <math alttext=\"\\alpha=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.97</annotation></semantics></math> for NER). We use four disjoint splits: (i) Unlabeled training (25k sents), (ii) Rule-Dev (100 sents) for rule refinement, (iii) Gold-Train (300 sents) for baselines and SFT, and (iv) Gold-Test (1,000 sents) for final evaluation. These splits are released with of the ZarmaPOS-Bench dataset&#8212;built from Feriji&#8212;and detailed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5\" title=\"5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Rules are described below.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "syntactic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recap, We use four disjoint splits:\n(i) <span class=\"ltx_text ltx_font_bold\">Unlabeled training</span> (25k sents) for unsupervised R2T;\n(ii) <span class=\"ltx_text ltx_font_bold\">Rule-Dev</span> (100 sents), sampled from the same source as the unlabeled corpus, used <em class=\"ltx_emph ltx_font_italic\">only</em> for error inspection during iterative rule refinement;\n(iii) <span class=\"ltx_text ltx_font_bold\">Gold-Train</span> (300 sents) used for supervised baselines and SFT;\n(iv) <span class=\"ltx_text ltx_font_bold\">Gold-Test</span> (1,000 sents) held out and <em class=\"ltx_emph ltx_font_italic\">never inspected</em> until the final evaluation.\nNo sentence appears in more than one split. All rules and hyperparameters were frozen on Rule-Dev before evaluating on Gold-Test.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "training",
                    "all",
                    "sft",
                    "r2t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the performance of six different models to provide a comprehensive evaluation. We consider an array of transformer models, which is the state-of-the-art architecture for language models and embeddings, and BiLSTMs, which has demonstrated strong performance in capturing long-range features in text <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochreiter and Schmidhuber, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib10\" title=\"\">1997</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BiLSTM-CRF</span> is a classic and strong supervised baseline. It uses the architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a> with a final CRF layer for structured prediction It is trained from scratch on our full 300-sentence annotated dataset.</p>\n\n",
                "matched_terms": [
                    "bilstmcrf",
                    "architecture",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "r2tbilstm",
                    "loss",
                    "epochs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span> serves as an architectural ablation study. It replaces the BiLSTM core with a Transformer encoder&#8212;10 layers, 6 attention heads, 768 hidden units and 3072 feed-forward&#8212;but uses the exact same rule system and training objective as the R2T-BiLSTM.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "training",
                    "hidden",
                    "heads",
                    "r2tbilstm",
                    "architectural",
                    "r2ttransformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> is the R2T-Transformer model after it has been further fine-tuned for 20 epochs on the first 50 sentences of our annotated dataset using a standard cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "model",
                    "loss",
                    "r2ttransformer",
                    "epochs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span> is an African-centric baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "epochs",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span> is a widely-used multilingual baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "epochs",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the detailed hyperparameters for all our models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2\" title=\"Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. For evaluation, we use a comprehensive set of metrics. We report overall <span class=\"ltx_text ltx_font_bold\">Word-Level Accuracy</span> and the <span class=\"ltx_text ltx_font_bold\">Macro F1-Score</span>, which is the unweighted average of the F1-score for each tag.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hyperparameters",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "bilstmcrf",
                    "model",
                    "r2tbilstm",
                    "afriberta",
                    "finetuning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "our",
                    "r2tbilstm",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "r2tbilstm",
                    "loss",
                    "r2ttransformer",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "learning",
                    "afriberta",
                    "finetuning",
                    "epochs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study focuses on POS tagging, the R2T design is not task-specific: any task with declarative linguistic or structural rules&#8212;e.g., morphological analysis, shallow parsing, phonotactic constraints&#8212;can be mapped into loss components. We therefore view POS tagging in Zarma as a case study of PrL.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "loss",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important Note:</span> Because R2T&#8217;s loss terms co-define the training dynamics, removing any term constitutes a different algorithm rather than an informative probe of the same method. We therefore evaluate architecture sensitivity (BiLSTM vs. Transformer) and data-regime sensitivity (unsupervised vs. SFT-50), keeping the objective intact and testing whether the combined design transfers across inductive biases.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "loss",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a SFT set to further improve models trained on ZarmaPOS-Bench, adjusting the silver-standard model&#8217;s systematic errors, as shown in our SFT-50 experiment.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "models",
                    "experiments",
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "learning",
                    "r2t",
                    "loss",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our R2T framework demonstrates significant promise and achieves high results for Zarma, we acknowledge several limitations that define the scope of this work and offer avenues for future investigation.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the R2T framework introduces several hyperparameters, the weights (<math alttext=\"\\alpha,\\beta,\\gamma,\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>,</mo><mi>&#946;</mi><mo>,</mo><mi>&#947;</mi><mo>,</mo><mi>&#948;</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha,\\beta,\\gamma,\\delta</annotation></semantics></math>) that balance the components of our adaptive loss function. Finding the optimal balance for these weights, along with the ideal neural architecture, requires a degree of empirical exploration. Although individual training runs are computationally efficient compared to pre-training large language models from scratch, this search process can still be resource-intensive for researchers with limited computational budgets.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "training",
                    "models",
                    "architecture",
                    "r2t",
                    "loss",
                    "weights",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, our experiments deliberately exclude state-of-the-art large language models (except for the embedding-based models). While powerful, these models do not align with the conditions and principles of our low-resource setting. Our focus is on developing accessible, reproducible, and computationally efficient methods that can be trained and deployed by researchers and communities with limited resources. Therefore, we restricted our comparisons to publicly available, open-source models that can be run and fine-tuned on consumer-grade hardware.</p>\n\n",
                "matched_terms": [
                    "models",
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our case study is on Zarma, a language of the Songhay familiy. The framework&#8217;s performance on languages on different language family remains an open question&#8212;although we carried an experiment with Bambara&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3\" title=\"Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>). Such languages might require more complex morphological or syntactic rule tiers to be effective.</p>\n\n",
                "matched_terms": [
                    "syntactic",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge in low-resource POS tagging is the lack of annotated data. A common strategy is cross-lingual projection, which transfers supervision from high-resource languages via parallel data or word alignments <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>; T&#228;ckstr&#246;m et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib24\" title=\"\">2013</a>)</cite>. Other approaches rely on classic probabilistic models like HMMs or TnT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brants, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib2\" title=\"\">2000</a>)</cite>, which can be effective but often lack the contextual power of neural models. More recent work has shown that small, targeted amounts of annotation, when combined with morphological information and type-level constraints, can be highly effective <cite class=\"ltx_cite ltx_citemacro_citep\">(Garrette and Baldridge, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our R2T framework builds on this insight by formalizing the injection of such constraints directly into a neural model&#8217;s training objective, removing the need for any initial labeled data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "models",
                    "word",
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The idea of embedding prior knowledge into machine learning models has a rich history. Methods like posterior regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(Ganchev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib8\" title=\"\">2010</a>)</cite> and generalized expectation criteria <cite class=\"ltx_cite ltx_citemacro_citep\">(Mann and McCallum, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib16\" title=\"\">2010</a>)</cite> use constraints to guide model posteriors, often through an auxiliary optimization process. Similarly, constrained conditional models shape the inference process to ensure outputs adhere to pre-defined rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib4\" title=\"\">2012</a>)</cite>. More recently, weak supervision frameworks like Snorkel and data programming have enabled the aggregation of noisy, heuristic labeling functions into a unified training signal <cite class=\"ltx_cite ltx_citemacro_citep\">(Ratner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "models",
                    "learning",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our PrL paradigm is distinct from these prior works in an interesting way. Instead of using rules to constrain inference, regularize posteriors, or generate pseudo-labels, R2T integrates them as direct, differentiable components of the end-to-end training loss. In our unsupervised setup, these rule-based losses are the <em class=\"ltx_emph ltx_font_italic\">primary</em> learning signal, entirely replacing the need for labeled examples.</p>\n\n",
                "matched_terms": [
                    "training",
                    "learning",
                    "r2t",
                    "loss",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work employs standard neural architectures for sequence tagging, such as BiLSTMs with character-level embeddings, which are known to be effective for handling OOV words and morphology <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>. While a conditional random field (CRF) layer is often used for structured prediction <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib11\" title=\"\">2015</a>)</cite>, our approach replaces this with a soft, differentiable syntactic loss. We also compare our approach to large multilingual models like XLM-RoBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite> and African-centric models like AfriBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. While powerful, these models can suffer from tokenizer mismatches in low-resource languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib22\" title=\"\">2021</a>)</cite>, a finding our experiments confirm. Finally, our adaptive OOV loss is related to confidence regularization techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Pereyra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib19\" title=\"\">2017</a>)</cite>, but it is applied selectively which encourages principled uncertainty only when the model has no rule-based guidance.</p>\n\n",
                "matched_terms": [
                    "xlmroberta",
                    "model",
                    "models",
                    "experiments",
                    "syntactic",
                    "loss",
                    "afriberta",
                    "our",
                    "oov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the specific architectural details and training hyperparameters used in our experiments, ensuring full reproducibility of our results.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "training",
                    "experiments",
                    "architectural",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While both of our R2T models share the same input representation&#8212;concatenated FastText and character embeddings&#8212;and the same rule-informed loss function, their core sequence processing architectures differ significantly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "r2t",
                    "loss",
                    "fasttext",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "word",
                    "hidden",
                    "r2tbilstm",
                    "fasttext",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer.</span> Our attention-based model, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, replaces the recurrent core with a Transformer encoder. The initial 350-dimensional input vector is first projected to match the Transformer&#8217;s hidden dimension of 768 using a linear layer. We then add sinusoidal positional encodings to this vector to provide the model with sequence order information. This final 768-dimensional vector is processed by a 10-layer Transformer encoder. Each layer contains 6 self-attention heads and a feed-forward network with 3072 hidden units. The 768-dimensional output vector from the final layer is then passed through a linear layer to produce the tag logits.</p>\n\n",
                "matched_terms": [
                    "model",
                    "feedforward",
                    "hidden",
                    "heads",
                    "r2ttransformer",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model.</span> For this experiment, we used a hybrid architecture combining a pre-trained T5 encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib20\" title=\"\">2020</a>)</cite> with our BiLSTM tagger head. The T5 encoder&#8212;<span class=\"ltx_text ltx_font_bold\">t5-small</span>&#8212;was used to generate contextual embeddings, which were then fed into the BiLSTM. The entire model was trained from scratch using only our Bambara rule system and the unlabeled corpus.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline.</span> We compare our model against the <span class=\"ltx_text ltx_font_bold\">Masakhane AfroXLMR</span> model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>on huggingface:(<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"masakhane/bambara-pos-tagger-afroxlmr\" title=\"\">masakhane/bambara-pos-tagger-afroxlmr</a>)</span></span></span>, which was fine-tuned on a manually annotated Bambara dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This result is insightful. It confirms that the R2T framework can be successfully adapted to a new language, and also reinforces our central claim: a modest investment in encoding linguistic knowledge can be more effective than fine-tuning on a small, potentially noisy, annotated dataset. The +0.13 absolute improvement in Macro F1 demonstrates the power of providing a model with explicit grammatical principles.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "r2t",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A qualitative analysis of the errors made by the Masakhane model reveals why our R2T approach is effective. The baseline model&#8217;s errors are systematic and arise from the exact issues R2T is designed to solve, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T6\" title=\"Table 6 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "training",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "experiments",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a similar set of models as in our POS experiments:</p>\n\n",
                "matched_terms": [
                    "models",
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "r2t",
                    "r2ttransformer",
                    "afriberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides supplementary figures that offer further insight into our experimental results and model behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Data Efficiency in Zarma NER &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides a visual representation of the data efficiency demonstrated in our Zarma NER experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5\" title=\"Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). The plot clearly shows that the R2T-Transformer starts from a much higher baseline accuracy (67.4%) than a standard fine-tuning approach. This strong foundation allows it to surpass the performance of the AfriBERTa baseline after being fine-tuned on only 50 labeled examples.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "experiments",
                    "r2ttransformer",
                    "finetuning",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "r2tbilstm",
                    "our"
                ]
            }
        ]
    },
    "A3.T5": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 5: Results on the 100-sentence Bambara test set, averaged over 5 seeds.",
        "body": "Model\n\n\n\n\nMacro F1\n\n\n\n\nWord Acc. (%)\n\n\n\n\n\n\n\n\nR2T-BiLSTM + T5\n\n\n\n\n0.91±.02\n\n\n\n\n92.7±.4\n\n\n\n\n\n\nMasakhane AfroXLMR\n\n\n\n\n0.78±.03\n\n\n\n\n82.5±.7",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold\">Macro F1</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\"><span class=\"ltx_text ltx_font_bold\">Word Acc. (%)</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM + T5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold\">0.91&#177;.02</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\">92.7&#177;.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Masakhane AfroXLMR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\">0.78&#177;.03</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:37.0pt;\">82.5&#177;.7</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "over",
            "afroxlmr",
            "seeds",
            "masakhane",
            "927±4",
            "100sentence",
            "r2tbilstm",
            "macro",
            "test",
            "091±02",
            "results",
            "acc",
            "averaged",
            "model",
            "078±03",
            "825±7",
            "word",
            "set",
            "bambara"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Can a model trained with linguistic rules and unlabeled text outperform a large pre-trained model fine-tuned on a small set of labeled data?</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "word",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the detailed hyperparameters for all our models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2\" title=\"Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. For evaluation, we use a comprehensive set of metrics. We report overall <span class=\"ltx_text ltx_font_bold\">Word-Level Accuracy</span> and the <span class=\"ltx_text ltx_font_bold\">Macro F1-Score</span>, which is the unweighted average of the F1-score for each tag.</p>\n\n",
                "matched_terms": [
                    "set",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Results &#8227; 3 Experiments &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results for Zarma POS tagging. We report per-tag F1-scores and macro averages as the primary evaluation metric, following standard practice in sequence tagging. Overall accuracy is included for completeness, but our focus is on F1, which better captures performance under class imbalance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "r2tbilstm",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a high-quality, reliable test set for evaluating any new Zarma POS tagger.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model.</span> For this experiment, we used a hybrid architecture combining a pre-trained T5 encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib20\" title=\"\">2020</a>)</cite> with our BiLSTM tagger head. The T5 encoder&#8212;<span class=\"ltx_text ltx_font_bold\">t5-small</span>&#8212;was used to generate contextual embeddings, which were then fed into the BiLSTM. The entire model was trained from scratch using only our Bambara rule system and the unlabeled corpus.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline.</span> We compare our model against the <span class=\"ltx_text ltx_font_bold\">Masakhane AfroXLMR</span> model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>on huggingface:(<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"masakhane/bambara-pos-tagger-afroxlmr\" title=\"\">masakhane/bambara-pos-tagger-afroxlmr</a>)</span></span></span>, which was fine-tuned on a manually annotated Bambara dataset.</p>\n\n",
                "matched_terms": [
                    "masakhane",
                    "model",
                    "bambara",
                    "afroxlmr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This result is insightful. It confirms that the R2T framework can be successfully adapted to a new language, and also reinforces our central claim: a modest investment in encoding linguistic knowledge can be more effective than fine-tuning on a small, potentially noisy, annotated dataset. The +0.13 absolute improvement in Macro F1 demonstrates the power of providing a model with explicit grammatical principles.</p>\n\n",
                "matched_terms": [
                    "model",
                    "macro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A qualitative analysis of the errors made by the Masakhane model reveals why our R2T approach is effective. The baseline model&#8217;s errors are systematic and arise from the exact issues R2T is designed to solve, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T6\" title=\"Table 6 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "masakhane",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides supplementary figures that offer further insight into our experimental results and model behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "test",
                    "set"
                ]
            }
        ]
    },
    "A3.T6": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 6: Qualitative error analysis of the Masakhane baseline on the Bambara test set.",
        "body": "Error Category\n\n\n\n\nExample Sentence & Prediction\n\n\n\n\nAnalysis & R2T Advantage\n\n\n\n\n\n\n\n\nPervasive Ambiguity of Function Words\n\n\n\n\nI ye wulu ye. (You saw a dog.) \nPred: ’ye’ →\\rightarrow ’PART’, ’ye’ →\\rightarrow ’PART’ \nCorrect: ’ye’ →\\rightarrow ’AUX’, ’ye’ →\\rightarrow ’VERB’\n\n\n\n\nThe baseline model incorrectly assigns the same tag to both instances of \"ye\". The R2T framework’s ambiguous rule ”ye’: [’AUX’, ’VERB’, ’PART’]’ combined with syntactic constraints allows our model to correctly disambiguate them based on their position in the sentence.\n\n\n\n\n\n\nWord Class Confusion (ADJ/NOUN)\n\n\n\n\nCεsurun bεtaa. (The short man is going.) \nPred: ’surun’ →\\rightarrow ’NOUN’ \nCorrect: ’surun’ →\\rightarrow ’ADJ’\n\n\n\n\nThe baseline fails to learn the ’NOUN + ADJ’ pattern from its limited data. Our R2T model is guided by the explicit syntactic rule ’(’NOUN’, ’ADJ’): 1.0’, which strongly encourages the correct prediction and helps it generalize this pattern.\n\n\n\n\n\n\nInconsistent Tagging of Core Vocabulary\n\n\n\n\nJi bεmin. (Water is being drunk.) \nPred: ’min’ →\\rightarrow ’PRON’ \nCorrect: ’min’ →\\rightarrow ’VERB’\n\n\n\n\nThe baseline makes a surprising error on a common verb. Our R2T model has \"min\" explicitly defined as a ’VERB’ in its Tier 1 lexicon, making this error impossible and ensuring consistent, reliable tagging for core vocabulary.",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Error Category</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Example Sentence &amp; Prediction</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Analysis &amp; R2T Advantage</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Pervasive Ambiguity of Function Words</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">I ye wulu ye.</span> (You saw a dog.) \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;ye&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;PART&#8217;, &#8217;ye&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;PART&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;ye&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;AUX&#8217;, &#8217;ye&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">The baseline model incorrectly assigns the same tag to both instances of \"ye\". The R2T framework&#8217;s ambiguous rule &#8221;ye&#8217;: [&#8217;AUX&#8217;, &#8217;VERB&#8217;, &#8217;PART&#8217;]&#8217; combined with syntactic constraints allows our model to correctly disambiguate them based on their position in the sentence.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Word Class Confusion (ADJ/NOUN)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">C&#949;surun b&#949;taa.</span> (The short man is going.) \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;surun&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;NOUN&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;surun&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;ADJ&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">The baseline fails to learn the &#8217;NOUN + ADJ&#8217; pattern from its limited data. Our R2T model is guided by the explicit syntactic rule &#8217;(&#8217;NOUN&#8217;, &#8217;ADJ&#8217;): 1.0&#8217;, which strongly encourages the correct prediction and helps it generalize this pattern.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\">Inconsistent Tagging of Core Vocabulary</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_italic\">Ji b&#949;min.</span> (Water is being drunk.) \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Pred:</span> &#8217;min&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;PRON&#8217; \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Correct:</span> &#8217;min&#8217; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> &#8217;VERB&#8217;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">The baseline makes a surprising error on a common verb. Our R2T model has \"min\" explicitly defined as a &#8217;VERB&#8217; in its Tier 1 lexicon, making this error impossible and ensuring consistent, reliable tagging for core vocabulary.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "reliable",
            "rule",
            "helps",
            "”ye’",
            "verb",
            "them",
            "same",
            "class",
            "inconsistent",
            "saw",
            "man",
            "pervasive",
            "qualitative",
            "wulu",
            "’noun",
            "vocabulary",
            "analysis",
            "’part’",
            "disambiguate",
            "tagging",
            "word",
            "learn",
            "’’noun’",
            "being",
            "set",
            "impossible",
            "strongly",
            "syntactic",
            "its",
            "’pron’",
            "short",
            "adjnoun",
            "based",
            "bεmin",
            "tag",
            "min",
            "core",
            "→rightarrow",
            "assigns",
            "water",
            "guided",
            "’noun’",
            "surprising",
            "has",
            "combined",
            "’adj’",
            "position",
            "adj’",
            "going",
            "function",
            "explicit",
            "lexicon",
            "ensuring",
            "’part’’",
            "10’",
            "bεtaa",
            "making",
            "sentence",
            "instances",
            "’surun’",
            "allows",
            "constraints",
            "’ye’",
            "which",
            "test",
            "prediction",
            "’aux’",
            "you",
            "from",
            "baseline",
            "defined",
            "fails",
            "limited",
            "pred",
            "confusion",
            "ambiguity",
            "ambiguous",
            "r2t",
            "correctly",
            "cεsurun",
            "explicitly",
            "’min’",
            "bambara",
            "their",
            "error",
            "pattern",
            "masakhane",
            "dog",
            "words",
            "our",
            "example",
            "consistent",
            "generalize",
            "drunk",
            "both",
            "’verb’",
            "common",
            "model",
            "correct",
            "makes",
            "incorrectly",
            "category",
            "advantage",
            "encourages",
            "tier",
            "data",
            "framework’s"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A qualitative analysis of the errors made by the Masakhane model reveals why our R2T approach is effective. The baseline model&#8217;s errors are systematic and arise from the exact issues R2T is designed to solve, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T6\" title=\"Table 6 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "which",
                    "tagging",
                    "words",
                    "its",
                    "r2t",
                    "baseline",
                    "constraints",
                    "function",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging</span>\n</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "tagging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Part-of-speech (POS) tagging is a foundational task in Natural Language Processing (NLP), serving as a prerequisite for complex downstream applications such as machine translation, syntactic parsing, and information extraction. For high-resource languages, deep learning models achieve near-perfect accuracy in POS tasks. However, that is not case for low-resource languages, where there is a lack of large manually annotated dataset these data-hungry models require. This data scarcity limits the development of robust linguistic tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "data",
                    "syntactic",
                    "tagging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Researchers often attempt to bridge this gap using two primary strategies: transfer learning or traditional rule-based systems.\nTransfer learning needs parallel data and careful alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>)</cite>. Multilingual transformers help in many languages, but they still depend on large-scale pretraining pipelines, tokenizers that match the target script, and computing resources that many communities do not have <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib6\" title=\"\">2020</a>)</cite>. Conversely, purely rule-based taggers do not scale either: they work on easy cases and then break on ambiguity.</p>\n\n",
                "matched_terms": [
                    "ambiguity",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find an effective solution to these challenge, we propose the <span class=\"ltx_text ltx_font_bold\">rule-to-tag (R2T)</span> framework, a novel hybrid approach that <em class=\"ltx_emph ltx_font_italic\">integrates explicit linguistic rules directly into the neural network&#8217;s training objective</em>. This method creates a powerful linguistic scaffold, guiding the model&#8217;s learning process even when labeled data is unavailable. Additionally, R2T incorporates an adaptive out-of-vocabulary (OOV) loss term. This term teaches the model to express principled uncertainty when it encounters unknown words, preventing confident but incorrect guesses. This is especially important in underresourced languages, where code-switching and borrowed words are common.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "r2t",
                    "words",
                    "data",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, our work contributes to a paradigm we call <span class=\"ltx_text ltx_font_bold\">principled learning (PrL)</span>: training models not only from labeled examples, but by embedding explicit task-based principles directly into the learning objective&#8212;to our knowledge, the first to operate as such. We show this approach can work as a complete unsupervised method for simpler tasks, and as a powerful pre-training stage for more complex ones.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of R2T through a comprehensive case study on Zarma, a language for which no large-scale POS corpus previously existed. Our work is guided by the following research questions:</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "guided",
                    "which",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Can a model trained with linguistic rules and unlabeled text outperform a large pre-trained model fine-tuned on a small set of labeled data?</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How effectively can a model pre-trained with the R2T framework be improved with a minimal amount of supervised fine-tuning, especially for more complex tasks?</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The R2T framework:</span> We introduce a novel hybrid architecture that leverages a multi-tiered linguistic rule system integrated directly into the training objective.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "analysis",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Principled pre-training for complex tasks:</span> We show that for a sparser task like NER, R2T serves as a highly data-efficient pre-training method which enables a model to be fine-tuned on just 50 sentences and surpass a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "baseline",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model release:</span> We release the pre-trained Zarma FastText embeddings and our best models for both POS and NER tasks&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/27Group\" title=\"\">https://huggingface.co/27Group</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of POS tagging in low-resource settings, we introduce <span class=\"ltx_text ltx_font_bold\">R2T</span>. R2T is a hybrid framework that combines the contextual learning ability of neural networks with a structured, multi-tiered system of linguistic knowledge. Instead of treating rules as a rigid post-processing step, we integrate them directly into the model&#8217;s learning objective through a novel, adaptive loss function. This method forces the model to adhere to known linguistic facts while teaching it to handle uncertainty gracefully when encountering unknown words.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "r2t",
                    "them",
                    "words",
                    "function"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At its core, the R2T framework consists of three main components. First, a foundational neural architecture captures contextual patterns from text. Second, a multi-tiered rule system provides explicit linguistic constraints. Finally, a rule-informed adaptive loss function orchestrates the interaction between the two, guiding the model towards grammatically sound and robust predictions. We detail each of these components in the following subsections.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "model",
                    "rule",
                    "core",
                    "from",
                    "r2t",
                    "its",
                    "constraints",
                    "function"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "sentence",
                    "tagging",
                    "core",
                    "from",
                    "word",
                    "r2t",
                    "words",
                    "learn",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sequence of characters is fed into a separate character-level sequential neural model (transformer or bidirectional long short-term memory (BiLSTM)), and the final hidden states are concatenated. This technique allows the model to infer representations for unseen words based on their sub-word structure, a method proven effective in numerous tagging tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "allows",
                    "words",
                    "their",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "word",
                    "function",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary innovation of R2T lies not just in using rules, but in structuring them into a multi-tiered system that provides a scaffold for the neural model&#8217;s learning process. This system organizes linguistic knowledge from high-confidence facts to general heuristics, allowing for a more nuanced form of guidance. We define four tiers of rules.</p>\n\n",
                "matched_terms": [
                    "from",
                    "r2t",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 1: Unambiguous lexical rules.</span> This tier forms the bedrock of our knowledge base. It contains a lexicon of words that map to a single, unambiguous POS tag. This typically includes high-frequency function words&#8212;e.g., pronouns, determiners, prepositions&#8212;and core vocabulary whose tags are constant across contexts.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "lexicon",
                    "core",
                    "vocabulary",
                    "words",
                    "tier",
                    "function",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "explicitly",
                    "defined",
                    "tag",
                    "ambiguous",
                    "ambiguity",
                    "word",
                    "set",
                    "words",
                    "tier",
                    "’verb’",
                    "’noun’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 3: Morphological rules.</span> To improve generalization to unseen words, this tier captures common morphological patterns. These rules are typically suffix- or prefix-based and suggest a likely tag. For example, a rule might specify that words ending in a particular suffix are likely to be nouns. This provides a heuristic when no lexical entry exists for a word.</p>\n\n",
                "matched_terms": [
                    "example",
                    "rule",
                    "tag",
                    "word",
                    "words",
                    "tier",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 4: Syntactic rules.</span> This tier models local grammatical structure by defining valid and invalid transitions between adjacent POS tags. These rules are represented as a matrix of bigram probabilities or constraints&#8212;e.g., a &#8217;DETERMINER&#8217; is very likely to be followed by a &#8217;NOUN&#8217; but not by a &#8217;VERB&#8217;. This helps the model produce more coherent and grammatically plausible tag sequences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "helps",
                    "tag",
                    "syntactic",
                    "tier",
                    "’verb’",
                    "’noun’"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The R2T framework&#8217;s components are unified through a carefully designed multi-part loss function. This function translates the multi-tiered rule system into a set of training objectives that guide the model&#8217;s training. The total loss <math alttext=\"\\mathcal{L}_{\\text{R2T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>R2T</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{R2T}}</annotation></semantics></math> is a weighted sum of four distinct components:</p>\n\n",
                "matched_terms": [
                    "rule",
                    "r2t",
                    "framework’s",
                    "function",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical loss (<math alttext=\"\\mathcal{L}_{\\text{lex}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">lex</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{lex}}</annotation></semantics></math>).</span> This term enforces the high-confidence lexical and morphological rules (Tiers 1-3). For a token <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> with an unambiguous tag <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> defined in the rule set, the loss is the standard negative log-likelihood:</p>\n\n",
                "matched_terms": [
                    "tag",
                    "defined",
                    "rule",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "its",
                    "encourages",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syntactic loss (<math alttext=\"\\mathcal{L}_{\\text{syn}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">syn</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{syn}}</annotation></semantics></math>).</span> This term enforces the Tier 4 syntactic constraints. We define a transition invalidity matrix <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m2\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>, where <math alttext=\"M_{jk}=1-\\text{validity}(tag_{j}\\to tag_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mtext>validity</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">M_{jk}=1-\\text{validity}(tag_{j}\\to tag_{k})</annotation></semantics></math>. The loss for a sequence is calculated by summing the penalty for each adjacent pair of predictions:</p>\n\n",
                "matched_terms": [
                    "tier",
                    "syntactic",
                    "constraints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{p}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}</annotation></semantics></math> is the vector of tag probabilities for the token at position <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. This term effectively discourages the model from outputting grammatically invalid tag sequences.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "from",
                    "model",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distributional loss (<math alttext=\"\\mathcal{L}_{\\text{dist}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">dist</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dist}}</annotation></semantics></math>).</span> This is a simple regularization term, calculated as the Kullback-Leibler (KL) Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shlens, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib23\" title=\"\">2014</a>)</cite> between the model&#8217;s average predicted tag distribution and a uniform distribution. It encourages the model to utilize the entire tagset, preventing it from skewing towards only a few high-frequency tags.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "from",
                    "encourages",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "model",
                    "prediction",
                    "word",
                    "from",
                    "its",
                    "words",
                    "tier",
                    "function",
                    "our",
                    "making"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|T|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>T</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|T|</annotation></semantics></math> is the number of tags in the tagset. This loss term acts as a regularizer for uncertainty. By minimizing it, the model learns a form of principled humility: it produces confident, peaked distributions for words it knows and flatter, more uncertain distributions for words it does not. This adaptive behavior helps to make the tagger robust to the diverse and unseen vocabulary inherent in low-resource language texts.</p>\n\n",
                "matched_terms": [
                    "vocabulary",
                    "model",
                    "helps",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these components make R2T an end-to-end differentiable system, where rules are not heuristics or constraints applied after the fact but are part of the training objective. This specific design is what distinguishes our paradigm from earlier constraint-based approaches that operate outside the model&#8217;s gradient update.</p>\n\n",
                "matched_terms": [
                    "from",
                    "r2t",
                    "our",
                    "constraints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of experiments to evaluate the effectiveness of our approach. Our goal is twofold. First, we aim to demonstrate that the R2T framework, which leverages only linguistic rules and unlabeled text, can outperform strong pre-trained language models fine-tuned on a small annotated dataset. Second, we analyze the impact of the underlying neural architecture (BiLSTM vs. Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib25\" title=\"\">2023</a>)</cite>) and the effect of supervised fine-tuning (SFT) on the R2T model.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "which",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments focus on the Zarma language, a member of the Songhay language family spoken primarily in Niger. Zarma is a low-resource language, with very limited publicly available annotated corpora suitable for training standard NLP models.</p>\n\n",
                "matched_terms": [
                    "limited",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we created a gold-standard dataset of 1,300 sentences, annotated by three experts (IAA: <math alttext=\"\\alpha=0.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.93</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.93</annotation></semantics></math> for POS, <math alttext=\"\\alpha=0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.97</annotation></semantics></math> for NER). We use four disjoint splits: (i) Unlabeled training (25k sents), (ii) Rule-Dev (100 sents) for rule refinement, (iii) Gold-Train (300 sents) for baselines and SFT, and (iv) Gold-Test (1,000 sents) for final evaluation. These splits are released with of the ZarmaPOS-Bench dataset&#8212;built from Feriji&#8212;and detailed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5\" title=\"5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Rules are described below.</p>\n\n",
                "matched_terms": [
                    "from",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rule",
                    "explicitly",
                    "lexicon",
                    "syntactic",
                    "words",
                    "ambiguous",
                    "from",
                    "constraints",
                    "tier",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recap, We use four disjoint splits:\n(i) <span class=\"ltx_text ltx_font_bold\">Unlabeled training</span> (25k sents) for unsupervised R2T;\n(ii) <span class=\"ltx_text ltx_font_bold\">Rule-Dev</span> (100 sents), sampled from the same source as the unlabeled corpus, used <em class=\"ltx_emph ltx_font_italic\">only</em> for error inspection during iterative rule refinement;\n(iii) <span class=\"ltx_text ltx_font_bold\">Gold-Train</span> (300 sents) used for supervised baselines and SFT;\n(iv) <span class=\"ltx_text ltx_font_bold\">Gold-Test</span> (1,000 sents) held out and <em class=\"ltx_emph ltx_font_italic\">never inspected</em> until the final evaluation.\nNo sentence appears in more than one split. All rules and hyperparameters were frozen on Rule-Dev before evaluating on Gold-Test.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "sentence",
                    "from",
                    "r2t",
                    "same",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the performance of six different models to provide a comprehensive evaluation. We consider an array of transformer models, which is the state-of-the-art architecture for language models and embeddings, and BiLSTMs, which has demonstrated strong performance in capturing long-range features in text <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochreiter and Schmidhuber, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib10\" title=\"\">1997</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "which",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BiLSTM-CRF</span> is a classic and strong supervised baseline. It uses the architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a> with a final CRF layer for structured prediction It is trained from scratch on our full 300-sentence annotated dataset.</p>\n\n",
                "matched_terms": [
                    "from",
                    "baseline",
                    "our",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "function",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span> serves as an architectural ablation study. It replaces the BiLSTM core with a Transformer encoder&#8212;10 layers, 6 attention heads, 768 hidden units and 3072 feed-forward&#8212;but uses the exact same rule system and training objective as the R2T-BiLSTM.</p>\n\n",
                "matched_terms": [
                    "same",
                    "core",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> is the R2T-Transformer model after it has been further fine-tuned for 20 epochs on the first 50 sentences of our annotated dataset using a standard cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "has",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span> is an African-centric baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span> is a widely-used multilingual baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report the detailed hyperparameters for all our models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2\" title=\"Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. For evaluation, we use a comprehensive set of metrics. We report overall <span class=\"ltx_text ltx_font_bold\">Word-Level Accuracy</span> and the <span class=\"ltx_text ltx_font_bold\">Macro F1-Score</span>, which is the unweighted average of the F1-score for each tag.</p>\n\n",
                "matched_terms": [
                    "tag",
                    "set",
                    "which",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Results &#8227; 3 Experiments &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results for Zarma POS tagging. We report per-tag F1-scores and macro averages as the primary evaluation metric, following standard practice in sequence tagging. Overall accuracy is included for completeness, but our focus is on F1, which better captures performance under class imbalance.</p>\n\n",
                "matched_terms": [
                    "class",
                    "tagging",
                    "which",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strongly",
                    "both",
                    "same",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "ambiguous",
                    "its",
                    "verb",
                    "tier",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "model",
                    "function",
                    "common",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "correct",
                    "based",
                    "its",
                    "from",
                    "baseline",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study focuses on POS tagging, the R2T design is not task-specific: any task with declarative linguistic or structural rules&#8212;e.g., morphological analysis, shallow parsing, phonotactic constraints&#8212;can be mapped into loss components. We therefore view POS tagging in Zarma as a case study of PrL.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "analysis",
                    "tagging",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important Note:</span> Because R2T&#8217;s loss terms co-define the training dynamics, removing any term constitutes a different algorithm rather than an informative probe of the same method. We therefore evaluate architecture sensitivity (BiLSTM vs. Transformer) and data-regime sensitivity (unsupervised vs. SFT-50), keeping the objective intact and testing whether the combined design transfers across inductive biases.</p>\n\n",
                "matched_terms": [
                    "same",
                    "combined"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "example",
                    "model",
                    "sentence",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An example entry from the dataset is shown below:</p>\n\n",
                "matched_terms": [
                    "example",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As ZarmaPOS-Bench was generated automatically, it is a silver-standard dataset and inevitably contains errors. Based on our analysis in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S4\" title=\"4 Analysis and Discussion &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, these errors are likely to be minor and concentrated around subtle ambiguities&#8212;e.g., distinguishing between &#8217;ADJ&#8217; and &#8217;NOUN&#8217; in complex phrases&#8212;or very rare, out-of-domain words. The overall quality, however, is exceptionally high for a synthetically generated corpus.</p>\n\n",
                "matched_terms": [
                    "’adj’",
                    "based",
                    "analysis",
                    "words",
                    "’noun’",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this limitation and to encourage a cycle of continuous improvement, we are releasing ZarmaPOS-Bench alongside our <span class=\"ltx_text ltx_font_bold\">300-sentence gold dataset</span>. This smaller, manually verified set is an important companion resource that can be used in several ways:</p>\n\n",
                "matched_terms": [
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a high-quality, reliable test set for evaluating any new Zarma POS tagger.</p>\n\n",
                "matched_terms": [
                    "reliable",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a SFT set to further improve models trained on ZarmaPOS-Bench, adjusting the silver-standard model&#8217;s systematic errors, as shown in our SFT-50 experiment.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tagging",
                    "r2t",
                    "constraints",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "our",
                    "model",
                    "correct",
                    "from",
                    "r2t",
                    "constraints",
                    "data",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our R2T framework demonstrates significant promise and achieves high results for Zarma, we acknowledge several limitations that define the scope of this work and offer avenues for future investigation.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "test",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the R2T framework introduces several hyperparameters, the weights (<math alttext=\"\\alpha,\\beta,\\gamma,\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>,</mo><mi>&#946;</mi><mo>,</mo><mi>&#947;</mi><mo>,</mo><mi>&#948;</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha,\\beta,\\gamma,\\delta</annotation></semantics></math>) that balance the components of our adaptive loss function. Finding the optimal balance for these weights, along with the ideal neural architecture, requires a degree of empirical exploration. Although individual training runs are computationally efficient compared to pre-training large language models from scratch, this search process can still be resource-intensive for researchers with limited computational budgets.</p>\n\n",
                "matched_terms": [
                    "limited",
                    "from",
                    "r2t",
                    "function",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "bambara",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, our experiments deliberately exclude state-of-the-art large language models (except for the embedding-based models). While powerful, these models do not align with the conditions and principles of our low-resource setting. Our focus is on developing accessible, reproducible, and computationally efficient methods that can be trained and deployed by researchers and communities with limited resources. Therefore, we restricted our comparisons to publicly available, open-source models that can be run and fine-tuned on consumer-grade hardware.</p>\n\n",
                "matched_terms": [
                    "limited",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our case study is on Zarma, a language of the Songhay familiy. The framework&#8217;s performance on languages on different language family remains an open question&#8212;although we carried an experiment with Bambara&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3\" title=\"Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>). Such languages might require more complex morphological or syntactic rule tiers to be effective.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "syntactic",
                    "bambara",
                    "framework’s",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A primary challenge in low-resource POS tagging is the lack of annotated data. A common strategy is cross-lingual projection, which transfers supervision from high-resource languages via parallel data or word alignments <cite class=\"ltx_cite ltx_citemacro_citep\">(Das and Petrov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib7\" title=\"\">2011</a>; T&#228;ckstr&#246;m et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib24\" title=\"\">2013</a>)</cite>. Other approaches rely on classic probabilistic models like HMMs or TnT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brants, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib2\" title=\"\">2000</a>)</cite>, which can be effective but often lack the contextual power of neural models. More recent work has shown that small, targeted amounts of annotation, when combined with morphological information and type-level constraints, can be highly effective <cite class=\"ltx_cite ltx_citemacro_citep\">(Garrette and Baldridge, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our R2T framework builds on this insight by formalizing the injection of such constraints directly into a neural model&#8217;s training objective, removing the need for any initial labeled data.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "which",
                    "tagging",
                    "word",
                    "from",
                    "r2t",
                    "constraints",
                    "data",
                    "has",
                    "common",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The idea of embedding prior knowledge into machine learning models has a rich history. Methods like posterior regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(Ganchev et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib8\" title=\"\">2010</a>)</cite> and generalized expectation criteria <cite class=\"ltx_cite ltx_citemacro_citep\">(Mann and McCallum, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib16\" title=\"\">2010</a>)</cite> use constraints to guide model posteriors, often through an auxiliary optimization process. Similarly, constrained conditional models shape the inference process to ensure outputs adhere to pre-defined rules <cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib4\" title=\"\">2012</a>)</cite>. More recently, weak supervision frameworks like Snorkel and data programming have enabled the aggregation of noisy, heuristic labeling functions into a unified training signal <cite class=\"ltx_cite ltx_citemacro_citep\">(Ratner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "has",
                    "constraints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our PrL paradigm is distinct from these prior works in an interesting way. Instead of using rules to constrain inference, regularize posteriors, or generate pseudo-labels, R2T integrates them as direct, differentiable components of the end-to-end training loss. In our unsupervised setup, these rule-based losses are the <em class=\"ltx_emph ltx_font_italic\">primary</em> learning signal, entirely replacing the need for labeled examples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "r2t",
                    "our",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work employs standard neural architectures for sequence tagging, such as BiLSTMs with character-level embeddings, which are known to be effective for handling OOV words and morphology <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>. While a conditional random field (CRF) layer is often used for structured prediction <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib11\" title=\"\">2015</a>)</cite>, our approach replaces this with a soft, differentiable syntactic loss. We also compare our approach to large multilingual models like XLM-RoBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite> and African-centric models like AfriBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. While powerful, these models can suffer from tokenizer mismatches in low-resource languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib22\" title=\"\">2021</a>)</cite>, a finding our experiments confirm. Finally, our adaptive OOV loss is related to confidence regularization techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Pereyra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib19\" title=\"\">2017</a>)</cite>, but it is applied selectively which encourages principled uncertainty only when the model has no rule-based guidance.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "which",
                    "prediction",
                    "syntactic",
                    "tagging",
                    "encourages",
                    "from",
                    "words",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the specific architectural details and training hyperparameters used in our experiments, ensuring full reproducibility of our results.</p>\n\n",
                "matched_terms": [
                    "ensuring",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While both of our R2T models share the same input representation&#8212;concatenated FastText and character embeddings&#8212;and the same rule-informed loss function, their core sequence processing architectures differ significantly.</p>\n\n",
                "matched_terms": [
                    "their",
                    "core",
                    "r2t",
                    "both",
                    "same",
                    "function",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "our",
                    "model",
                    "which",
                    "tagging",
                    "word",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer.</span> Our attention-based model, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, replaces the recurrent core with a Transformer encoder. The initial 350-dimensional input vector is first projected to match the Transformer&#8217;s hidden dimension of 768 using a linear layer. We then add sinusoidal positional encodings to this vector to provide the model with sequence order information. This final 768-dimensional vector is processed by a 10-layer Transformer encoder. Each layer contains 6 self-attention heads and a feed-forward network with 3072 hidden units. The 768-dimensional output vector from the final layer is then passed through a linear layer to produce the tag logits.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tag",
                    "core",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "bambara",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We maintained the core R2T methodology while adapting the language-specific components.</p>\n\n",
                "matched_terms": [
                    "core",
                    "r2t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Rules.</span> We drafted a new multi-tiered rule system specifically for Bambara&#8212;mainly drafted from Daba morphemic rules&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Maslinsky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib17\" title=\"\">2014</a>)</cite>. This included a lexicon of approximately 100 unambiguous words, rules for ambiguous function words (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span>, <span class=\"ltx_text ltx_font_italic\">ka</span>, <span class=\"ltx_text ltx_font_italic\">ma</span>), common morphological suffixes (e.g., plural &#8217;-w&#8217;), and a set of core syntactic constraints. This rule set was intentionally drafted in a few hours to simulate a rapid development scenario for a new language.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "lexicon",
                    "syntactic",
                    "words",
                    "core",
                    "from",
                    "ambiguous",
                    "set",
                    "constraints",
                    "function",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> For the unsupervised training phase, we used a monolingual Bambara corpus of approximately 864 sentences sourced from the SMOL dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Caswell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib3\" title=\"\">2025</a>)</cite>. For evaluation, we used Bambara 1000 sentences.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "bambara"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model.</span> For this experiment, we used a hybrid architecture combining a pre-trained T5 encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib20\" title=\"\">2020</a>)</cite> with our BiLSTM tagger head. The T5 encoder&#8212;<span class=\"ltx_text ltx_font_bold\">t5-small</span>&#8212;was used to generate contextual embeddings, which were then fed into the BiLSTM. The entire model was trained from scratch using only our Bambara rule system and the unlabeled corpus.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "rule",
                    "from",
                    "bambara",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline.</span> We compare our model against the <span class=\"ltx_text ltx_font_bold\">Masakhane AfroXLMR</span> model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>on huggingface:(<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"masakhane/bambara-pos-tagger-afroxlmr\" title=\"\">masakhane/bambara-pos-tagger-afroxlmr</a>)</span></span></span>, which was fine-tuned on a manually annotated Bambara dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "masakhane",
                    "baseline",
                    "bambara",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n",
                "matched_terms": [
                    "model",
                    "masakhane",
                    "r2t",
                    "both",
                    "baseline",
                    "bambara",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This result is insightful. It confirms that the R2T framework can be successfully adapted to a new language, and also reinforces our central claim: a modest investment in encoding linguistic knowledge can be more effective than fine-tuning on a small, potentially noisy, annotated dataset. The +0.13 absolute improvement in Macro F1 demonstrates the power of providing a model with explicit grammatical principles.</p>\n\n",
                "matched_terms": [
                    "explicit",
                    "r2t",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rule",
                    "lexicon",
                    "r2t",
                    "bambara",
                    "verb",
                    "tier",
                    "set",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test",
                    "generalize",
                    "prediction",
                    "tagging",
                    "its",
                    "r2t",
                    "words",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "which",
                    "test",
                    "tagging",
                    "from",
                    "data",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a similar set of models as in our POS experiments:</p>\n\n",
                "matched_terms": [
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "rule"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5.T7\" title=\"Table 7 &#8227; E.2 Results and Analysis &#8227; Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports span-level F1-scores as the primary evaluation measure for Zarma NER. This provides a fairer evaluation than token-level accuracy, as it requires both correct entity type and correct span boundaries.</p>\n\n",
                "matched_terms": [
                    "both",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "r2t",
                    "model",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides supplementary figures that offer further insight into our experimental results and model behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Data Efficiency in Zarma NER &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides a visual representation of the data efficiency demonstrated in our Zarma NER experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5\" title=\"Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). The plot clearly shows that the R2T-Transformer starts from a much higher baseline accuracy (67.4%) than a standard fine-tuning approach. This strong foundation allows it to surpass the performance of the AfriBERTa baseline after being fine-tuned on only 50 labeled examples.</p>\n\n",
                "matched_terms": [
                    "allows",
                    "from",
                    "baseline",
                    "data",
                    "being",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "test",
                    "’aux’",
                    "’part’",
                    "confusion",
                    "set",
                    "’verb’",
                    "’noun’",
                    "our"
                ]
            }
        ]
    },
    "A5.T7": {
        "source_file": "R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging",
        "caption": "Table 7: Zarma NER results on the 100-sentence test set, averaged over 5 seeds.",
        "body": "Model\nSpan F1\nWord Acc. (%)\n\n\n\n\nR2T-Trans. SFT-50\n0.83±.02\n89.9±.5\n\n\nAfriBERTa (SFT-300)\n0.79±.03\n88.9±.6\n\n\nR2T-BiLSTM\n0.61±.04\n75.4±.9\n\n\nR2T-Transformer\n0.53±.05\n67.4±.1.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Span F1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Word Acc. (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">R2T-Trans. SFT-50</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.83&#177;.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">89.9&#177;.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">AfriBERTa (SFT-300)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.79&#177;.03</td>\n<td class=\"ltx_td ltx_align_center\">88.9&#177;.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">R2T-BiLSTM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.61&#177;.04</td>\n<td class=\"ltx_td ltx_align_center\">75.4&#177;.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">R2T-Transformer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.53&#177;.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">67.4&#177;.1.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "061±04",
            "674±12",
            "over",
            "seeds",
            "100sentence",
            "r2tbilstm",
            "899±5",
            "afriberta",
            "test",
            "sft50",
            "sft300",
            "r2ttransformer",
            "ner",
            "results",
            "r2ttrans",
            "083±02",
            "acc",
            "averaged",
            "model",
            "053±05",
            "word",
            "set",
            "754±9",
            "span",
            "889±6",
            "zarma",
            "079±03"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5.T7\" title=\"Table 7 &#8227; E.2 Results and Analysis &#8227; Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> reports span-level F1-scores as the primary evaluation measure for Zarma NER. This provides a fairer evaluation than token-level accuracy, as it requires both correct entity type and correct span boundaries.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. R2T&#8217;s novelty lies in its adaptive loss function, which includes a regularization term that teaches the model to handle out-of-vocabulary (OOV) words with principled uncertainty. We frame this work as a case study in a paradigm we call principled learning (PrL), where models are trained with explicit task constraints rather than on labeled examples alone. Our experiments on Zarma part-of-speech (POS) tagging show that the R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy, outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We further show that for more complex tasks like named entity recognition (NER), R2T serves as a powerful pre-training step; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zarma",
                    "r2tbilstm",
                    "afriberta",
                    "ner"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Can a model trained with linguistic rules and unlabeled text outperform a large pre-trained model fine-tuned on a small set of labeled data?</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance analysis:</span> We demonstrate that for POS tagging, our R2T-BiLSTM model achieves 98.2% accuracy without labeled data, and outperform strong supervised baselines.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Principled pre-training for complex tasks:</span> We show that for a sparser task like NER, R2T serves as a highly data-efficient pre-training method which enables a model to be fine-tuned on just 50 sentences and surpass a baseline trained on 300.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ner"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench &amp; ZarmaNER-600:</span> We release the first POS-tagged and NER-annotated corpora for Zarma. This includes a large silver-standard and 300 gold-standard datasets for POS, and a 600-sentence gold-standard NER dataset.</p>\n\n",
                "matched_terms": [
                    "ner",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model release:</span> We release the pre-trained Zarma FastText embeddings and our best models for both POS and NER tasks&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/27Group\" title=\"\">https://huggingface.co/27Group</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zarma",
                    "ner"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core of our R2T model is a standard yet effective neural architecture designed for sequence tagging tasks.\nFor each token in an input sentence, we generate a rich representation by combining two sources of information. First, we use pre-trained word embedding&#8212;e.g., from FastText <cite class=\"ltx_cite ltx_citemacro_citep\">(Bojanowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib1\" title=\"\">2017</a>)</cite> or any other embedding model. These embeddings provide valuable distributional semantics, which is important in low-resource scenarios where a model cannot learn such representations from a small annotated dataset alone. Second, to handle morphological variations and OOV words, we generate a character-level representation for each token.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained word embedding and the generated character-level embedding are then concatenated. This combined vector serves as the input to the main token-level BiLSTM. By processing the sequence in both forward and backward directions, this layer produces a context-aware representation for each token. Finally, a linear layer followed by a softmax function projects this representation into a probability distribution over the entire tagset. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates this foundational architecture.</p>\n\n",
                "matched_terms": [
                    "word",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tier 2: Ambiguous lexical rules.</span> A key challenge in many languages&#8212;specially low-resourced ones&#8212;is lexical ambiguity. This tier explicitly defines words that can belong to multiple POS categories. For instance, a word might be defined as a potential &#8217;NOUN&#8217; or &#8217;VERB&#8217;. By acknowledging this ambiguity, we do not force a single tag but instead provide the model with a constrained set of valid options, tasking the neural architecture with using context to perform the final disambiguation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a token with a set of multiple valid tags <math alttext=\"Y_{\\text{ambig}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mtext>ambig</mtext></msub><annotation encoding=\"application/x-tex\">Y_{\\text{ambig}}</annotation></semantics></math>, we modify the objective to sum the probabilities of all valid options. This encourages the model to place its predictive mass within the valid set without prematurely forcing a single choice:</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive OOV loss (<math alttext=\"\\mathcal{L}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_bold\">oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{oov}}</annotation></semantics></math>).</span> The final component of our loss function addresses the problem of OOV words. For any word <math alttext=\"x_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">x_{\\text{oov}}</annotation></semantics></math> that is not covered by our Tier 1-3 rules, we want the model to express uncertainty rather than making a confident and likely incorrect prediction. We achieve this by penalizing the model if its output distribution <math alttext=\"\\mathbf{p}_{\\text{oov}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#119849;</mi><mtext>oov</mtext></msub><annotation encoding=\"application/x-tex\">\\mathbf{p}_{\\text{oov}}</annotation></semantics></math> for an unknown word deviates significantly from a uniform distribution <math alttext=\"\\mathcal{U}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><annotation encoding=\"application/x-tex\">\\mathcal{U}</annotation></semantics></math>. We measure this deviation using the KL Divergence:</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the rules, we developed a multi-tiered rule system for Zarma, inclding 20 grammar rules derived from existing documents and native speaker feedback. The rules were created following three principles: (1) prioritizing high-frequency, low-ambiguity words; (2) explicitly codifying ambiguous words and (3) iteratively refining rules based on model errors on the Rule-Dev set. The workflow involved: (i) compiling a Tier 1 lexicon of unambiguous words, (ii) defining a Tier 2 lexicon for ambiguous words, (iii) encoding morphological patterns (e.g., definite article suffixes &#8217;-a&#8217;, &#8217;-o&#8217;), and (iv) specifying syntactic constraints (e.g., pronoun followed by auxiliary). The rules are available in machine-readable JSON format on HuggingFace: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/27Group/ZarmaLanguageRules\" title=\"\">https://huggingface.co/datasets/27Group/ZarmaLanguageRules</a>. Further details on iterative refinement are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A4\" title=\"Appendix D More Details about Rules Creation &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> is our primary model, using the BiLSTM architecture described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. It is trained for 30 epochs using only the 25,000 unlabeled sentences and our rule-informed adaptive loss function.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span> serves as an architectural ablation study. It replaces the BiLSTM core with a Transformer encoder&#8212;10 layers, 6 attention heads, 768 hidden units and 3072 feed-forward&#8212;but uses the exact same rule system and training objective as the R2T-BiLSTM.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> is the R2T-Transformer model after it has been further fine-tuned for 20 epochs on the first 50 sentences of our annotated dataset using a standard cross-entropy loss.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "model",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AfriBERTa</span> is an African-centric baseline <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. We fine-tune the model on our full 300-sentence annotated dataset for 10 epochs.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Results &#8227; 3 Experiments &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results for Zarma POS tagging. We report per-tag F1-scores and macro averages as the primary evaluation metric, following standard practice in sequence tagging. Overall accuracy is included for completeness, but our focus is on F1, which better captures performance under class imbalance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our R2T-BiLSTM model achieves strong performance across both frequent and rare tags, reaching a macro F1 of 0.968. Notably, this unsupervised model is performant with the fully supervised BiLSTM-CRF trained on 300 sentences (0.975), and surpasses AfriBERTa fine-tuned on the same data (0.941). The Transformer variant underperforms in the unsupervised setting but recovers strongly after fine-tuning on just 50 sentences, demonstrating the benefit of principled pre-training. XLM-RoBERTa, by contrast, performs poorly and confirms the mismatch between multilingual tokenization and Zarma text.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model",
                    "r2tbilstm",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Knowledge as a Data-Efficient Alternative.</span> The most impressive result is the success of the R2T-BiLSTM. It surpasses AfriBERTa fine-tuned on 300 expert-annotated sentences, with a higher Macro F1 (0.968 vs. 0.941), despite using only unlabeled text and a curated rule system. This suggests that for low-resource languages and settings, a modest <span class=\"ltx_text ltx_font_bold\">investment in encoding linguistic knowledge</span> can be more <span class=\"ltx_text ltx_font_bold\">data-efficient</span> and effective than the costly process of manual annotation. The errors made by AfriBERTa, such as confusing the verb \"no\" (\"give\" in Zarma) with its auxiliary counterpart, are precisely the kinds of ambiguities that our Tier 2 ambiguous lexical rules are designed to resolve.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "r2tbilstm",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture and Rule-Based Guidance.</span> Comparing the R2T-BiLSTM (Macro F1 = 0.968) with the normal R2T-Transformer (Macro F1 = 0.852) reveals a fascinating interaction. The BiLSTM&#8217;s sequential recurrent nature appears to adhere more effectively with our token-level loss function. We hypothesize that the recurrent state provides a stronger local signal, forcing the model to adhere more strictly to the rules for each token. In contrast, the Transformer&#8217;s global self-attention mechanism may dilute the impact of these token-specific rules, leading it to make more context-based errors, such as misclassifying common verbs like \"wani\" (\"to play\" in Zarma) as nouns.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "model",
                    "r2tbilstm",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-SFT.</span> The R2T-Transformer&#8217;s performance jump from Macro F1 = 0.852 (89.8% accuracy) to Macro F1 = 0.935 (96.3% accuracy) after fine-tuning on just 50 labeled sentences is strong evidence of our hybrid approach&#8217;s efficiency. The initial rule-informed training phase successfully imbued the model with a robust understanding of Zarma&#8217;s general grammatical structure. This created an excellent foundation, allowing a very small amount of supervised data to correct its specific weaknesses and enhance its performance to a high level with the AfriBERTa baseline. By projection and based on the observe learning trend during the training, <span class=\"ltx_text ltx_font_bold\">we can anticipate this method will outperform the BiLSTM if given more annotated data and/or training epochs</span>. This two-stage&#8212;learning from rules, followed by specialized learning from labels&#8212;represents a promising path for developing NLP tools in low-resource settings.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While manually creating a large, perfectly annotated \"gold-standard\" corpus is ideal, it is an extremely time-consuming and expensive process, often infeasible in low-resource contexts. An effective alternative may be to create a high-quality \"silver-standard\" dataset by leveraging a good model for automatic annotation. Given the high performance of our R2T-BiLSTM model&#8212;which demonstrated 98.2% accuracy without seeing any labeled data&#8212;it serves as an ideal candidate for creating such a corpus. The goal of ZarmaPOS-Bench is therefore to provide the research community with a large-scale, readily-available resource that, while not perfect, is of sufficient quality to enable a wide range of new research and applications for the Zarma language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ZarmaPOS-Bench was curated from the Feriji dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Keita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib12\" title=\"\">2024</a>)</cite>. We processed 46064 rows, segmenting multi-sentence entries and tokenizing with <span class=\"ltx_text ltx_font_typewriter\">wordpunct_tokenize</span>. Each sentence was tagged using our R2T-BiLSTM model, producing a silver-standard dataset of 55000 sentences and 1,005,295 tokens in JSONL format (example in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#S5.SS3\" title=\"5.3 Dataset Statistics &#8227; 5 ZarmaPOS-Bench &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a high-quality, reliable test set for evaluating any new Zarma POS tagger.</p>\n\n",
                "matched_terms": [
                    "zarma",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a SFT set to further improve models trained on ZarmaPOS-Bench, adjusting the silver-standard model&#8217;s systematic errors, as shown in our SFT-50 experiment.</p>\n\n",
                "matched_terms": [
                    "set",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a seed set for active learning or semi-supervised learning pipelines, where a model trained on the silver data can query a human for labels on the most uncertain examples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we addressed the challenge of sequence tagging for low-resource languages under resource constraints. We introduced the Rule-to-Tag (R2T) framework, a hybrid approach that integrates a multi-tiered system of linguistic rules directly into a neural network&#8217;s training objective. Our experiments on Zarma language demonstrated two major strengths of this approach. For a grammatically dense task like POS tagging, the R2T-BiLSTM&#8212;trained without any labeled data&#8212;achieved high performance, exceeding good supervised baselines. For a sparser&#8212;more complex task like NER&#8212;R2T proved to be a effective principled pre-training method; a model pre-trained with R2T and fine-tuned on just 50 labeled sentences outperformed a large language model fine-tuned on 300. As part of this work, we release <span class=\"ltx_text ltx_font_bold\">ZarmaPOS-Bench</span> and <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>, the first large-scale tagged corpora for Zarma, alongside our models and gold-standard data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the specific contributions of R2T, our work points towards a broader paradigm for machine learning in low-resource and knowledge-intensive domains. We propose the term <span class=\"ltx_text ltx_font_bold\">principled Learning (PrL)</span> to describe this paradigm. By PrL, we mean <span class=\"ltx_text ltx_font_bold ltx_font_italic\">learning within explicit task principles that are integrated directly into the training objective, rather than from example-based supervision alone</span>. Instead of primarily showing a model what the correct answer is, we provide it with unlabeled data and a set of constraints that encode the principles of the task. The model&#8217;s objective is then to discover valid solutions that satisfy these principles. What is new in our framing is the direct embedding of rules into the loss of a neural tagger, without requiring auxiliary optimization or pre-labeled data. Based on these, R2T can be seen as a pilot implementation of PrL that connects the gap between symbolic rules and gradient-based training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our R2T framework demonstrates significant promise and achieves high results for Zarma, we acknowledge several limitations that define the scope of this work and offer avenues for future investigation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our evaluation is conducted on a 1000-sentence test set. This choice was deliberate. We aim to simulate a realistic low-resource scenario where obtaining even a small, high-quality evaluation set is a significant challenge in itself. Using a larger test set would not align with the conditions our method is designed for and would begin to approximate a medium-resource setting. However, we acknowledge that a larger test set could potentially reveal more subtle performance differences between the top-performing models.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, R2T relies on human-made rules. In our setting, Zarma rules required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>4 hours for creating and refining by a trained native speaker plus one NLP researcher; Bambara required <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.75 hours&#8212;we leveraged on the rules made by Daba. By contrast, obtaining 300 gold POS sentences took <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p4.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9&#8211;12 annotator-hours&#8212;three annotators, 1,300 sentences with overlap, adjudication not counted. Thus, R2T&#8217;s knowledge engineering cost is smaller than creating a similar gold set, but does presuppose access to expertise and may grow for morphologically complex languages.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work employs standard neural architectures for sequence tagging, such as BiLSTMs with character-level embeddings, which are known to be effective for handling OOV words and morphology <cite class=\"ltx_cite ltx_citemacro_citep\">(Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib15\" title=\"\">2016</a>)</cite>. While a conditional random field (CRF) layer is often used for structured prediction <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib11\" title=\"\">2015</a>)</cite>, our approach replaces this with a soft, differentiable syntactic loss. We also compare our approach to large multilingual models like XLM-RoBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib5\" title=\"\">2019</a>)</cite> and African-centric models like AfriBERTa <cite class=\"ltx_cite ltx_citemacro_citep\">(Ogueji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib18\" title=\"\">2021</a>)</cite>. While powerful, these models can suffer from tokenizer mismatches in low-resource languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib22\" title=\"\">2021</a>)</cite>, a finding our experiments confirm. Finally, our adaptive OOV loss is related to confidence regularization techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Pereyra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#bib.bib19\" title=\"\">2017</a>)</cite>, but it is applied selectively which encourages principled uncertainty only when the model has no rule-based guidance.</p>\n\n",
                "matched_terms": [
                    "afriberta",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM.</span> Our recurrent model, illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F3\" title=\"Figure 3 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, follows a standard and effective design for sequence tagging. The input to the model for each token is a 350-dimensional vector, created by concatenating a 300-dimensional FastText word embedding with a 50-dimensional character-level embedding. The character embedding is generated by a single-layer character-level BiLSTM with 25 hidden units in each direction. This combined 350-dimensional vector is then fed into the main token-level BiLSTM, which has one layer with 256 hidden units in each direction. The resulting 512-dimensional context-aware representation is finally passed through a linear layer to produce logits for our tagset.</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer.</span> Our attention-based model, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.F4\" title=\"Figure 4 &#8227; B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, replaces the recurrent core with a Transformer encoder. The initial 350-dimensional input vector is first projected to match the Transformer&#8217;s hidden dimension of 768 using a linear layer. We then add sinusoidal positional encodings to this vector to provide the model with sequence order information. This final 768-dimensional vector is processed by a 10-layer Transformer encoder. Each layer contains 6 self-attention heads and a feed-forward network with 3072 hidden units. The 768-dimensional output vector from the final layer is then passed through a linear layer to produce the tag logits.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate that our R2T framework is a language-agnostic and adaptable methodology, we conducted a second series of experiments on Bambara&#8212;a Manding language spoken&#8212;in West Africa. Like Zarma, Bambara is a low-resource language, but it presents a different set of grammatical challenges, including a greater reliance on tone and more complex verb-auxiliary constructions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A3.T5\" title=\"Table 5 &#8227; C.2 Bambara Results and Analysis &#8227; Appendix C Generalization to Bambara &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of our Bambara experiment. Our R2T model, trained without any labeled data, outperforms the supervised Masakhane Bambara baseline both in Macro F1 (0.91 vs. 0.78) and in word-level accuracy (92.7% vs. 82.5%).</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rule creation process for Zarma and Bambara involved iterative refinement based on errors observed on the Rule-Dev set. For Zarma, initial rules misclassified certain verbs (e.g., \"wani\" as a noun), prompting the addition of specific lexical entries to Tier 1. For Bambara, tone-related ambiguities (e.g., <span class=\"ltx_text ltx_font_italic\">ye</span> as AUX or VERB) required expanding the Tier 2 lexicon. Each iteration involved training an initial R2T model, analyzing errors, and updating rules, typically requiring 2&#8211;3 cycles before freezing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "set",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test the versatility and limits of our PrL paradigm, we conducted a second series of experiments applying the R2T framework to a more complex structured prediction task: Named Entity Recognition (NER). Unlike POS tagging, where most words have a clear grammatical patterns, NER is a sparser task and requires the model to identify not just the type of an entity but also its exact boundaries&#8212;spans&#8212;often across multiple words. This experiment serves as a stress test of our approach&#8217;s ability to generalize beyond its initial application.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ner",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We created a new gold-standard dataset for Zarma NER, which we call <span class=\"ltx_text ltx_font_bold\">ZarmaNER-600</span>. It contains 600 manually annotated sentences with entities for Persons (&#8217;PER&#8217;), Locations (&#8217;LOC&#8217;), Organizations (&#8217;ORG&#8217;), and Dates (&#8217;DATE&#8217;), following the standard BIO tagging scheme. For our experiments, we use the first 300 sentences for training the supervised baselines, the next 100 for our held-out test set, and 50 sentences from the end of the training set for our SFT experiment.</p>\n\n",
                "matched_terms": [
                    "zarma",
                    "ner",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-BiLSTM</span> and <span class=\"ltx_text ltx_font_bold\">R2T-Transformer</span>, trained unsupervised using a new NER-specific rule set.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "set",
                    "r2tbilstm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span>, which takes the unsupervised R2T-Transformer and fine-tunes it on 50 gold sentences.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "sft50"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model architectures are identical to those described in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A2.SS1\" title=\"B.1 Model Architectures &#8227; Appendix B Technical Details &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>, with the final layer adjusted for the NER tagset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ner"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results show that the unsupervised R2T models achieve modest F1 (0.61&#8211;0.74) which highlights the difficulty of applying rules directly to a sparse task. However, the <span class=\"ltx_text ltx_font_bold\">R2T-Transformer SFT-50</span> model, pre-trained with rules and fine-tuned on just 50 gold sentences, reaches an F1 of 0.83. This surpasses AfriBERTa fine-tuned on 300 sentences (0.79), demonstrating the effectiveness of principled pre-training for complex tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sft50",
                    "r2ttransformer",
                    "afriberta",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides supplementary figures that offer further insight into our experimental results and model behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Data Efficiency in Zarma NER &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> provides a visual representation of the data efficiency demonstrated in our Zarma NER experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A5\" title=\"Appendix E Extending PrL to Named Entity Recognition &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). The plot clearly shows that the R2T-Transformer starts from a much higher baseline accuracy (67.4%) than a standard fine-tuning approach. This strong foundation allows it to surpass the performance of the AfriBERTa baseline after being fine-tuned on only 50 labeled examples.</p>\n\n",
                "matched_terms": [
                    "r2ttransformer",
                    "afriberta",
                    "ner",
                    "zarma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more detailed view of the performance of our best model, the R2T-BiLSTM, we present a confusion matrix in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.13854v1#A6.F6\" title=\"Figure 6 &#8227; F.2 Confusion Matrix for Zarma POS Tagging &#8227; Appendix F Additional Figures &#8227; R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The matrix visualizes the model&#8217;s predictions on the 1000-sentence gold test set. The strong diagonal indicates high accuracy across all tags. The few off-diagonal marks reveal the model&#8217;s minor confusions. For instance, there are slight confusions between &#8217;NOUN&#8217; and &#8217;VERB&#8217;, and between &#8217;PART&#8217; and &#8217;AUX&#8217;, which are grammatical errors. This visualization suggests that the model&#8217;s few mistakes are not random but rule-centric.</p>\n\n",
                "matched_terms": [
                    "model",
                    "r2tbilstm",
                    "test",
                    "set"
                ]
            }
        ]
    }
}