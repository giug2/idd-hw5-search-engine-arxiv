{
    "S5.T1": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 1: Comparison between ELLSA and full-duplex speech interaction LLMs. For Llama Q., Web Q. and TriviaQA, Acc.%\\% is used as the evaluation metric, while AlpacaEval is assessed with GPTScore. S2T and S2S denotes speech-to-text and speech-to-speech performance, respectively.",
        "body": "Model\nLlama Q.\nWeb Q.\nTriviaQA\nAlpacaEval\n\n\nS2T\nS2S\nS2T\nS2S\nS2T\nS2S\nS2T\nS2S\n\n\n\n\nMoshi (Défossez et al., 2024)\n\n60.8\n54.5\n23.4\n22.1\n25.6\n16.7\n1.84\n1.76\n\n\nFreeze-Omni (Wang et al., 2025a)\n\n74.2\n56.2\n40.8\n27.9\n45.1\n28.5\n3.90\n2.46\n\n\nELLSA\n74.7\n70.0\n39.5\n36.5\n45.2\n41.7\n3.09\n2.80",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Llama Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Web Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">TriviaQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">AlpacaEval</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">S2S</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">74.2</td>\n<td class=\"ltx_td ltx_align_center\">56.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center\">27.9</td>\n<td class=\"ltx_td ltx_align_center\">45.1</td>\n<td class=\"ltx_td ltx_align_center\">28.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.90</span></td>\n<td class=\"ltx_td ltx_align_center\">2.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">ELLSA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">74.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">70.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">45.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">41.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.80</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "respectively",
            "défossez",
            "speechtotext",
            "speech",
            "freezeomni",
            "metric",
            "used",
            "denotes",
            "evaluation",
            "llms",
            "llama",
            "s2s",
            "assessed",
            "gptscore",
            "while",
            "2025a",
            "alpacaeval",
            "s2t",
            "speechtospeech",
            "wang",
            "performance",
            "triviaqa",
            "web",
            "comparison",
            "acc",
            "interaction",
            "fullduplex",
            "ellsa",
            "moshi",
            "between",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for artificial general intelligence (AGI) has pivoted from purely computational intelligence toward embodied agents that can perceive, understand and act within interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib15\" title=\"\">2022</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib51\" title=\"\">2024</a>)</cite>. A defining feature of human intelligence is our capacity for full-duplex multimodal interaction, we seamlessly process multiple input streams (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, vision, hearing, touch) while producing multiple outputs (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, speech, facial expressions, body movements). We listen as we observe, speak while acting, and continuously adapt our behavior in real time to complex conversational dynamics such as turn-taking and interruptions. This fluid interplay forms the essence of natural interaction, yet it remains a gap in the capabilities of current AI models.</p>\n\n",
                "matched_terms": [
                    "fullduplex",
                    "speech",
                    "while",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite remarkable progress, prevailing paradigms address only isolated aspects of this holistic challenge, producing either disembodied &#8220;talkers&#8221; or non-conversant &#8220;doers&#8221;. On the one hand, full-duplex conversational speech LLMs have been developed to enable seamlessly more natural interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>. These models can engage in low-latency speech-to-speech interaction, capturing not only semantic content but also paralinguistic cues such as speaker identity and emotion. Vision information can also be incorporated to support video-based conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib34\" title=\"\">2024</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>)</cite>. While they can see, listen, and speak, they remain disembodied observers, fundamentally incapable of translating their understandings into physical actions to interact with the environment. On the other hand, Vision-Language-Action (VLA) models have achieved notable success in grounding language in manipulation tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>. However, these models are metaphorically &#8220;deaf&#8221; and &#8220;mute&#8221;. They typically operate on textual instructions within a rigid, turn-based framework and lack the ability to process raw auditory signals or generate spoken responses. This half-duplex, turn-based paradigm fundamentally limits their interactivity, making them unable to handle natural conversational behaviors like turn-taking and barge-ins.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "speechtospeech",
                    "wang",
                    "défossez",
                    "speech",
                    "llms",
                    "interaction",
                    "fullduplex",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SA-MoE, a novel and data-efficient architecture to integrate experts for different modalities to fuse concurrent multimodal input and output streams, leveraging the pretrained ability of each expert and mitigating modality interference. Experimental results demonstrate that SA-MoE exhibits significantly superior performance compared to one single dense model with less training cost.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "fullduplex",
                    "ellsa",
                    "while",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "wang",
                    "défossez",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we detail how we build ELLSA, whose overview is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (a). We begin by explaining how streaming duplex MIMO is achieved through interleaved sequences. Then the core architecture design SA-MoE is introduced, which equips the model with strong multimodal perception and generation capabilities. Finally, the training strategy is discussed from building separate experts to connecting experts through SA-MoE.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When developing multimodal LLMs, a major challenge is that combining multimodal perception and generation often degrades text performance <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite>, particularly in multimodal generation. Handling multiple modalities&#8212;speech, vision, text, and action&#8212;further complicates the problem. While it is possible to train a single dense model for all tasks, doing so makes it extremely difficult to balance the modalities and requires vast amounts of data. To address this issue, we propose Self-Attention Mixture-of-Experts (SA-MoE), a new paradigm for multimodal processing. Its mechanism is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Streaming Full-duplex MIMO &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this architecture, different experts are responsible for different modalities, while a unified attention mechanism integrates them. The design of SA-MoE draws inspiration from <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>, where the VLM backbone and the action expert are connected through attention. We extend this idea to interleaved multimodal sequences and cross-expert interaction.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "performance",
                    "défossez",
                    "llms",
                    "interaction",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the perspective of modality processing, each modality is handled by a designated expert: the speech expert processes both speech and text, while the action expert handles vision and action. This clear division of labor ensures that each expert focuses on its domain, effectively assigning the &#8220;mouth&#8221; and the &#8220;hand&#8221; to different modules. Such specialization reduces the complexity of multimodal modeling, mitigates modality interference and enhances controllability and interpretability. From the perspective of sequence processing, the entire MoE model functions as a transformer. Empowered by attention mechanism, SA-MoE efficiently fuses and integrates multimodal inputs as a unified system, enabling each expert to understand previously unfamiliar modalities. Looking from the whole sequence, its information flow is equivalent to that of a vanilla transformer. Looking into one single step, it behaves like a standard transformer except that the previous KV values may be derived from different experts. Thus, at any moment, only one expert&#8217;s weights are activated.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE integrates experts through attention, preserving the strengths of individual modules to reduce modality interference while enabling efficient multimodal fusion. Importantly, it offers a flexible and scalable framework for multimodal processing. In ELLSA, we employ two experts, a speech expert and an action expert. This, however, is definitely not the only possible configuration. An alternative is to introduce four separate experts for speech, vision, text, and action. We merge speech with text and vision with action to better leverage pretrained knowledge. Looking ahead, as we aim toward more human-like AGI, additional modalities such as smell or touch could also be easily incorporated by introducing dedicated experts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "speech",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "fullduplex",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Connecting speech synthesizer.</span> In the final stage, we integrate a streaming speech synthesizer with ELLSA in an end-to-end manner. The last hidden states of the speech expert are fed into the trainable synthesizer after being transformed by a randomly initialized connector. During Stage 3, ELLSA gains the ability to speak, completing its multimodal interaction loop.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ellsa",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "ellsa",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "between",
                    "wang",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "s2t",
                    "speechtospeech",
                    "performance",
                    "evaluation",
                    "speechtotext",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "s2s",
                    "gptscore",
                    "used",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Dialogue turn-taking success rate on speech interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "freezeomni",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "between",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "while",
                    "speech",
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "while",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "triviaqa",
                    "speech",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "ellsa",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "interaction",
                    "fullduplex",
                    "ellsa",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "fullduplex",
                    "performance",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "used",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "ellsa",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system prompts also differ by mode. In the default mode, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer by action or text following the speech instruction</span>&#8221;. For speech-only mode, prompts vary by task. For ASR, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Generate a transcript of the speech</span>&#8221;, while for QA it is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer the question</span>&#8221;. System prompts are enclosed within <span class=\"ltx_text ltx_font_typewriter\">&lt;bop&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eop&gt;</span>, processed by the speech expert and inserted at the beginning of the whole multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "défossez",
                    "speech",
                    "interaction",
                    "ellsa",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "used",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "used",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "alpacaeval",
                    "performance",
                    "triviaqa",
                    "web",
                    "speech",
                    "llama",
                    "interaction",
                    "ellsa",
                    "s2s",
                    "gptscore",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "evaluation",
                    "speech",
                    "comparison",
                    "fullduplex",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select the speech expert and the action expert as the foundation of ELLSA primarily because individual experts are readily accessible. However, this is not the only viable approach. SA-MoE provides a generalizable framework capable of integrating multimodal processing from any set of experts. Here, we would like to highlight an alternative solution that we consider more elegant and efficient: a universal backbone capable of processing all input modalities as the &#8220;brain&#8221;, supported by specialized encoders such as the speech encoder as the &#8220;ear&#8221;, and the vision encoder as the &#8220;eye&#8221;; text expert as the &#8220;mouth&#8221;, and the action expert as the &#8220;hand&#8221;. Such a &#8220;brain-hand-mouth&#8221; architecture more closely mirrors the natural information flow of humans.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies have explored multimodal processing with MoE to enhance performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib30\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib29\" title=\"\">2025b</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib21\" title=\"\">2025</a>)</cite>. However, SA-MoE differs from these works not only in the expert interaction mechanism but also the scope. Prior work focuses primarily on vision understanding, using MoE to alleviate domain conflicts, whereas SA-MoE integrates pretrained experts across diverse modalities to address modality interference. Moreover, from the perspective of training, existing approaches primarily serve as superior pretraining architectures, while SA-MoE is a high-performance and data-efficient architecture for post-training.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "while",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "interaction",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction S2T performance.</p>\n\n",
                "matched_terms": [
                    "s2t",
                    "speech",
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction S2T performance.</p>\n\n",
                "matched_terms": [
                    "s2t",
                    "speech",
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA still faces several limitations. First, although ELLSA successfully predicts duplex dynamics such as dialogue/action turn-taking and action barge-ins, it currently handles only a limited range of scenarios. Many aspects of natural communication, such as user and assistant backchanneling, remain unaddressed. Expanding support for these dynamics will be crucial for achieving more human-like interactions. Second, although ELLSA has shown promising results in handling full-duplex multimodal joint perception and concurrent generation within simulated environments, ELLSA has yet to be validated in real-world settings. Future work will focus on real-world deployment, and we believe our framework can be effectively adapted through targeted finetuning.</p>\n\n",
                "matched_terms": [
                    "fullduplex",
                    "ellsa"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 2: Comparison of ELLSA and text-conditioned VLA models on the LIBERO benchmark.",
        "body": "Model\nSPATIAL\nOBJECT\nGOAL\nLONG\nAverage\n\n\n\n\nDP* (Chi et al., 2023)\n\n78.3%\n92.5%\n68.3%\n50.5%\n72.4%\n\n\nOcto (Ghosh et al., 2024)\n\n78.9%\n85.7%\n84.6%\n51.1%\n75.1%\n\n\nOpenVLA (Kim et al., 2024)\n\n84.9%\n88.4%\n79.2%\n53.7%\n76.5%\n\n\nSpatialVLA (Qu et al., 2025)\n\n88.2%\n89.9%\n78.6%\n55.5%\n78.1%\n\n\nCoT-VLA (Zhao et al., 2025a)\n\n87.5%\n91.6%\n87.6%\n69.0%\n81.1%\n\n\n\nπ0\\pi_{0}-FAST (Pertsch et al., 2025)\n\n96.4%\n96.8%\n88.6%\n60.2%\n85.5%\n\n\nELLSA\n90.8%\n95.8%\n86.4%\n84.4%\n89.4%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SPATIAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OBJECT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GOAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LONG</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Average</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">DP* <cite class=\"ltx_cite ltx_citemacro_citep\">(Chi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib11\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.4%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Octo <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">78.9%</td>\n<td class=\"ltx_td ltx_align_center\">85.7%</td>\n<td class=\"ltx_td ltx_align_center\">84.6%</td>\n<td class=\"ltx_td ltx_align_center\">51.1%</td>\n<td class=\"ltx_td ltx_align_center\">75.1%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">OpenVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">84.9%</td>\n<td class=\"ltx_td ltx_align_center\">88.4%</td>\n<td class=\"ltx_td ltx_align_center\">79.2%</td>\n<td class=\"ltx_td ltx_align_center\">53.7%</td>\n<td class=\"ltx_td ltx_align_center\">76.5%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">SpatialVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib38\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">88.2%</td>\n<td class=\"ltx_td ltx_align_center\">89.9%</td>\n<td class=\"ltx_td ltx_align_center\">78.6%</td>\n<td class=\"ltx_td ltx_align_center\">55.5%</td>\n<td class=\"ltx_td ltx_align_center\">78.1%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">CoT-VLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib55\" title=\"\">2025a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">87.5%</td>\n<td class=\"ltx_td ltx_align_center\">91.6%</td>\n<td class=\"ltx_td ltx_align_center\">87.6%</td>\n<td class=\"ltx_td ltx_align_center\">69.0%</td>\n<td class=\"ltx_td ltx_align_center\">81.1%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math>-FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.4%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.8%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.6%</span></td>\n<td class=\"ltx_td ltx_align_center\">60.2%</td>\n<td class=\"ltx_td ltx_align_center\">85.5%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">ELLSA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">95.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">84.4%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">89.4%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ghosh",
            "octo",
            "zhao",
            "goal",
            "long",
            "benchmark",
            "pertsch",
            "chi",
            "kim",
            "openvla",
            "vla",
            "π0pi0fast",
            "2025a",
            "object",
            "cotvla",
            "comparison",
            "textconditioned",
            "spatial",
            "average",
            "libero",
            "models",
            "ellsa",
            "spatialvla",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite remarkable progress, prevailing paradigms address only isolated aspects of this holistic challenge, producing either disembodied &#8220;talkers&#8221; or non-conversant &#8220;doers&#8221;. On the one hand, full-duplex conversational speech LLMs have been developed to enable seamlessly more natural interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>. These models can engage in low-latency speech-to-speech interaction, capturing not only semantic content but also paralinguistic cues such as speaker identity and emotion. Vision information can also be incorporated to support video-based conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib34\" title=\"\">2024</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>)</cite>. While they can see, listen, and speak, they remain disembodied observers, fundamentally incapable of translating their understandings into physical actions to interact with the environment. On the other hand, Vision-Language-Action (VLA) models have achieved notable success in grounding language in manipulation tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>. However, these models are metaphorically &#8220;deaf&#8221; and &#8220;mute&#8221;. They typically operate on textual instructions within a rigid, turn-based framework and lack the ability to process raw auditory signals or generate spoken responses. This half-duplex, turn-based paradigm fundamentally limits their interactivity, making them unable to handle natural conversational behaviors like turn-taking and barge-ins.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "models",
                    "vla",
                    "kim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "models",
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "kim",
                    "models",
                    "zhao",
                    "ellsa",
                    "vla",
                    "pertsch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we detail how we build ELLSA, whose overview is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (a). We begin by explaining how streaming duplex MIMO is achieved through interleaved sequences. Then the core architecture design SA-MoE is introduced, which equips the model with strong multimodal perception and generation capabilities. Finally, the training strategy is discussed from building separate experts to connecting experts through SA-MoE.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "textconditioned",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "pertsch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "models",
                    "ellsa",
                    "long",
                    "libero",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "ellsa",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "textconditioned",
                    "average",
                    "ellsa",
                    "vla",
                    "benchmark",
                    "libero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "models",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "libero",
                    "ellsa",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "average",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "goal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "libero",
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "ghosh",
                    "models",
                    "ellsa",
                    "vla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "model",
                    "average",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "average",
                    "benchmark",
                    "libero",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 3: Performance of ELLSA’s duplex abilities across various turn-taking and barge-in scenarios.",
        "body": "General question\nInterruptive command\nSilence\n\n\n100.0%\n94.3%\n100.0%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">General question</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Interruptive command</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Silence</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">100.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">94.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">100.0%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "turntaking",
            "interruptive",
            "across",
            "performance",
            "various",
            "question",
            "command",
            "silence",
            "ellsa’s",
            "general",
            "bargein",
            "duplex",
            "scenarios",
            "abilities"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
            "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "question",
                    "general",
                    "across",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for artificial general intelligence (AGI) has pivoted from purely computational intelligence toward embodied agents that can perceive, understand and act within interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib15\" title=\"\">2022</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib51\" title=\"\">2024</a>)</cite>. A defining feature of human intelligence is our capacity for full-duplex multimodal interaction, we seamlessly process multiple input streams (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, vision, hearing, touch) while producing multiple outputs (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, speech, facial expressions, body movements). We listen as we observe, speak while acting, and continuously adapt our behavior in real time to complex conversational dynamics such as turn-taking and interruptions. This fluid interplay forms the essence of natural interaction, yet it remains a gap in the capabilities of current AI models.</p>\n\n",
                "matched_terms": [
                    "general",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "question",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "bargein",
                    "question",
                    "turntaking",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "general",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "question",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "question",
                    "turntaking",
                    "abilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "general",
                    "silence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "command",
                    "ellsa’s",
                    "general",
                    "bargein",
                    "abilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "ellsa’s",
                    "abilities",
                    "general",
                    "duplex",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "ellsa’s",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "ellsa’s",
                    "across",
                    "abilities",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "ellsa’s",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "general",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "ellsa’s",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "across",
                    "silence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "question",
                    "command",
                    "bargein"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "across",
                    "performance",
                    "question",
                    "command",
                    "general",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies have explored multimodal processing with MoE to enhance performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib30\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib29\" title=\"\">2025b</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib21\" title=\"\">2025</a>)</cite>. However, SA-MoE differs from these works not only in the expert interaction mechanism but also the scope. Prior work focuses primarily on vision understanding, using MoE to alleviate domain conflicts, whereas SA-MoE integrates pretrained experts across diverse modalities to address modality interference. Moreover, from the perspective of training, existing approaches primarily serve as superior pretraining architectures, while SA-MoE is a high-performance and data-efficient architecture for post-training.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA still faces several limitations. First, although ELLSA successfully predicts duplex dynamics such as dialogue/action turn-taking and action barge-ins, it currently handles only a limited range of scenarios. Many aspects of natural communication, such as user and assistant backchanneling, remain unaddressed. Expanding support for these dynamics will be crucial for achieving more human-like interactions. Second, although ELLSA has shown promising results in handling full-duplex multimodal joint perception and concurrent generation within simulated environments, ELLSA has yet to be validated in real-world settings. Future work will focus on real-world deployment, and we believe our framework can be effectively adapted through targeted finetuning.</p>\n\n",
                "matched_terms": [
                    "duplex",
                    "scenarios",
                    "turntaking"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 4: Performance of both speaking and acting on the speaking-while-acting task.",
        "body": "SPATIAL\nOBJECT\nGOAL\nLONG\n\n\n93.3%\n96.6%\n86.1%\n73.2%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SPATIAL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OBJECT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GOAL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LONG</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">93.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">96.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">86.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">73.2%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "both",
            "object",
            "performance",
            "speakingwhileacting",
            "task",
            "speaking",
            "spatial",
            "acting",
            "goal",
            "long"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "both",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "performance",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "both",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "both",
                    "speakingwhileacting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "long",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(c) Success rate across different types of speech input during action execution (i.e., the speaking-while-acting task). The expected behavior varies depending on the specific input.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "both",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "speakingwhileacting",
                    "acting",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting",
                    "goal",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "performance",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Semantic</span>: A task referencing an object that does not exist in the scene.</p>\n\n",
                "matched_terms": [
                    "object",
                    "task"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 5: The performance of ELLSA on context-grounded VQA task",
        "body": "Manual Acc.%\\%\nGemini Acc.%\\%\n\n\n82.5\n83.3",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Manual Acc.<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Gemini Acc.<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">82.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">83.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gemini",
            "performance",
            "manual",
            "task",
            "vqa",
            "acc",
            "ellsa",
            "contextgrounded"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "contextgrounded",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "task",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "ellsa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "task",
                    "ellsa",
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "task",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "task",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "task",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "task",
                    "ellsa",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "contextgrounded",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For context-grounded VQA, Gemini may occasionally misjudge accuracy because it processes videos at only 1 fps and therefore cannot take all frames into account, potentially missing essential contextual information.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded",
                    "gemini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For experiments, we use LLM to generate training data (Llama-3-Instruct and Gemini-2.5-Pro) and evaluate results for AlpacaEval (GPT-4.1-2025-04-14) and context-grounded VQA (Gemini-2.5-Pro), with prompts provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A5\" title=\"Appendix E Prompts &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            }
        ]
    },
    "A2.T6": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 6: Training dataset details",
        "body": "Task\nDataset\n#Samples\n\n\nASR\nLibriSpeech (Panayotov et al., 2015)\n\n281k\n\n\nGigaSpeech (Chen et al., 2021)\n\n200k\n\n\nQA\nAlpaca-52k (Taori et al., 2023)\n\n39k\n\n\nWeb Questions (Berant et al., 2013)\n\n4k\n\n\nTriviaQA (Joshi et al., 2017)\n\n58k\n\n\nSQuAD (Rajpurkar et al., 2016)\n\n127k\n\n\nNatural Questions (Kwiatkowski et al., 2019)\n\n301k\n\n\nVoiceAssistant-400k (Xie & Wu, 2024)\n\n79k\n\n\nUltraChat (Chen et al., 2025c)\n\n120k\n\n\nrobot manipulation\nLIBERO (Liu et al., 2023)\n\n3386\n\n\ndefective instruction rejection\n1693",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">#Samples</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib36\" title=\"\">2015</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">281k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">200k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"7\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Alpaca-52k <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib43\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">4k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">58k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib40\" title=\"\">2016</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">127k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Natural Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwiatkowski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib27\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">301k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VoiceAssistant-400k <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">79k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib9\" title=\"\">2025c</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">120k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">robot manipulation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\">LIBERO <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3386</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">defective instruction rejection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1693</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "natural",
            "joshi",
            "281k",
            "rajpurkar",
            "instruction",
            "panayotov",
            "voiceassistant400k",
            "samples",
            "taori",
            "ultrachat",
            "asr",
            "details",
            "gigaspeech",
            "2025c",
            "127k",
            "79k",
            "defective",
            "manipulation",
            "triviaqa",
            "robot",
            "web",
            "39k",
            "58k",
            "kwiatkowski",
            "alpaca52k",
            "301k",
            "berant",
            "training",
            "libero",
            "questions",
            "dataset",
            "120k",
            "xie",
            "200k",
            "librispeech",
            "task",
            "rejection",
            "squad",
            "liu",
            "chen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "rejection",
                    "instruction",
                    "defective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite remarkable progress, prevailing paradigms address only isolated aspects of this holistic challenge, producing either disembodied &#8220;talkers&#8221; or non-conversant &#8220;doers&#8221;. On the one hand, full-duplex conversational speech LLMs have been developed to enable seamlessly more natural interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>. These models can engage in low-latency speech-to-speech interaction, capturing not only semantic content but also paralinguistic cues such as speaker identity and emotion. Vision information can also be incorporated to support video-based conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib34\" title=\"\">2024</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>)</cite>. While they can see, listen, and speak, they remain disembodied observers, fundamentally incapable of translating their understandings into physical actions to interact with the environment. On the other hand, Vision-Language-Action (VLA) models have achieved notable success in grounding language in manipulation tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>. However, these models are metaphorically &#8220;deaf&#8221; and &#8220;mute&#8221;. They typically operate on textual instructions within a rigid, turn-based framework and lack the ability to process raw auditory signals or generate spoken responses. This half-duplex, turn-based paradigm fundamentally limits their interactivity, making them unable to handle natural conversational behaviors like turn-taking and barge-ins.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "natural",
                    "task",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "rejection",
                    "defective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "xie",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "rejection",
                    "instruction",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "manipulation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "robot",
                    "asr",
                    "instruction",
                    "rejection",
                    "training",
                    "defective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "robot",
                    "asr",
                    "details",
                    "instruction",
                    "rejection",
                    "training",
                    "defective",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "natural",
                    "manipulation",
                    "task",
                    "details",
                    "instruction",
                    "rejection",
                    "liu",
                    "libero",
                    "defective",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "manipulation",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "joshi",
                    "triviaqa",
                    "web",
                    "berant",
                    "questions",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "manipulation",
                    "robot",
                    "task",
                    "libero"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Action turn-taking success rate and defective instruction\n<br class=\"ltx_break\"/>rejection rate on speech-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "defective",
                    "rejection",
                    "instruction",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "defective",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "triviaqa",
                    "task",
                    "manipulation",
                    "libero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "details",
                    "instruction",
                    "rejection",
                    "training",
                    "defective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system prompts also differ by mode. In the default mode, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer by action or text following the speech instruction</span>&#8221;. For speech-only mode, prompts vary by task. For ASR, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Generate a transcript of the speech</span>&#8221;, while for QA it is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer the question</span>&#8221;. System prompts are enclosed within <span class=\"ltx_text ltx_font_typewriter\">&lt;bop&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eop&gt;</span>, processed by the speech expert and inserted at the beginning of the whole multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "robot",
                    "asr",
                    "task",
                    "gigaspeech",
                    "instruction",
                    "rejection",
                    "training",
                    "defective",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "joshi",
                    "manipulation",
                    "triviaqa",
                    "robot",
                    "web",
                    "task",
                    "chen",
                    "berant",
                    "questions",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "task",
                    "instruction",
                    "libero",
                    "defective",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation"
                ]
            }
        ]
    },
    "A6.T7": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 7: Comparison between SA-MoE and using one dense model",
        "body": "Model\nSPATIAL\nOBJECT\nGOAL\nLONG\n\n\n\n\nDense (from speech expert)\n0.2%\n3.2%\n5.2%\n0.0%\n\n\nDense (from action expert)\n76.8%\n76.4%\n67.8%\n60.6%\n\n\nSA-MoE\n90.8%\n95.8%\n86.4%\n84.4%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SPATIAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OBJECT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GOAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LONG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Dense (from speech expert)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Dense (from action expert)</td>\n<td class=\"ltx_td ltx_align_center\">76.8%</td>\n<td class=\"ltx_td ltx_align_center\">76.4%</td>\n<td class=\"ltx_td ltx_align_center\">67.8%</td>\n<td class=\"ltx_td ltx_align_center\">60.6%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">SA-MoE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">95.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">84.4%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "object",
            "expert",
            "dense",
            "one",
            "model",
            "comparison",
            "speech",
            "spatial",
            "goal",
            "long",
            "action",
            "between",
            "from",
            "samoe"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
            "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samoe",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for artificial general intelligence (AGI) has pivoted from purely computational intelligence toward embodied agents that can perceive, understand and act within interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib15\" title=\"\">2022</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib51\" title=\"\">2024</a>)</cite>. A defining feature of human intelligence is our capacity for full-duplex multimodal interaction, we seamlessly process multiple input streams (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, vision, hearing, touch) while producing multiple outputs (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, speech, facial expressions, body movements). We listen as we observe, speak while acting, and continuously adapt our behavior in real time to complex conversational dynamics such as turn-taking and interruptions. This fluid interplay forms the essence of natural interaction, yet it remains a gap in the capabilities of current AI models.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite remarkable progress, prevailing paradigms address only isolated aspects of this holistic challenge, producing either disembodied &#8220;talkers&#8221; or non-conversant &#8220;doers&#8221;. On the one hand, full-duplex conversational speech LLMs have been developed to enable seamlessly more natural interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>. These models can engage in low-latency speech-to-speech interaction, capturing not only semantic content but also paralinguistic cues such as speaker identity and emotion. Vision information can also be incorporated to support video-based conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib34\" title=\"\">2024</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>)</cite>. While they can see, listen, and speak, they remain disembodied observers, fundamentally incapable of translating their understandings into physical actions to interact with the environment. On the other hand, Vision-Language-Action (VLA) models have achieved notable success in grounding language in manipulation tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>. However, these models are metaphorically &#8220;deaf&#8221; and &#8220;mute&#8221;. They typically operate on textual instructions within a rigid, turn-based framework and lack the ability to process raw auditory signals or generate spoken responses. This half-duplex, turn-based paradigm fundamentally limits their interactivity, making them unable to handle natural conversational behaviors like turn-taking and barge-ins.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "one"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "speech",
                    "action",
                    "from",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SA-MoE, a novel and data-efficient architecture to integrate experts for different modalities to fuse concurrent multimodal input and output streams, leveraging the pretrained ability of each expert and mitigating modality interference. Experimental results demonstrate that SA-MoE exhibits significantly superior performance compared to one single dense model with less training cost.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "dense",
                    "one",
                    "model",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we detail how we build ELLSA, whose overview is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (a). We begin by explaining how streaming duplex MIMO is achieved through interleaved sequences. Then the core architecture design SA-MoE is introduced, which equips the model with strong multimodal perception and generation capabilities. Finally, the training strategy is discussed from building separate experts to connecting experts through SA-MoE.</p>\n\n",
                "matched_terms": [
                    "samoe",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When developing multimodal LLMs, a major challenge is that combining multimodal perception and generation often degrades text performance <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite>, particularly in multimodal generation. Handling multiple modalities&#8212;speech, vision, text, and action&#8212;further complicates the problem. While it is possible to train a single dense model for all tasks, doing so makes it extremely difficult to balance the modalities and requires vast amounts of data. To address this issue, we propose Self-Attention Mixture-of-Experts (SA-MoE), a new paradigm for multimodal processing. Its mechanism is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Streaming Full-duplex MIMO &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this architecture, different experts are responsible for different modalities, while a unified attention mechanism integrates them. The design of SA-MoE draws inspiration from <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>, where the VLM backbone and the action expert are connected through attention. We extend this idea to interleaved multimodal sequences and cross-expert interaction.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "dense",
                    "samoe",
                    "action",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the perspective of modality processing, each modality is handled by a designated expert: the speech expert processes both speech and text, while the action expert handles vision and action. This clear division of labor ensures that each expert focuses on its domain, effectively assigning the &#8220;mouth&#8221; and the &#8220;hand&#8221; to different modules. Such specialization reduces the complexity of multimodal modeling, mitigates modality interference and enhances controllability and interpretability. From the perspective of sequence processing, the entire MoE model functions as a transformer. Empowered by attention mechanism, SA-MoE efficiently fuses and integrates multimodal inputs as a unified system, enabling each expert to understand previously unfamiliar modalities. Looking from the whole sequence, its information flow is equivalent to that of a vanilla transformer. Looking into one single step, it behaves like a standard transformer except that the previous KV values may be derived from different experts. Thus, at any moment, only one expert&#8217;s weights are activated.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "samoe",
                    "one",
                    "speech",
                    "action",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE integrates experts through attention, preserving the strengths of individual modules to reduce modality interference while enabling efficient multimodal fusion. Importantly, it offers a flexible and scalable framework for multimodal processing. In ELLSA, we employ two experts, a speech expert and an action expert. This, however, is definitely not the only possible configuration. An alternative is to introduce four separate experts for speech, vision, text, and action. We merge speech with text and vision with action to better leverage pretrained knowledge. Looking ahead, as we aim toward more human-like AGI, additional modalities such as smell or touch could also be easily incorporated by introducing dedicated experts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samoe",
                    "expert",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "speech",
                    "action",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "action",
                    "from",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Connecting speech synthesizer.</span> In the final stage, we integrate a streaming speech synthesizer with ELLSA in an end-to-end manner. The last hidden states of the speech expert are fed into the trainable synthesizer after being transformed by a randomly initialized connector. During Stage 3, ELLSA gains the ability to speak, completing its multimodal interaction loop.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "expert",
                    "one",
                    "speech",
                    "action",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "speech",
                    "action",
                    "between",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "long",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "speech",
                    "action",
                    "from",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(c) Success rate across different types of speech input during action execution (i.e., the speaking-while-acting task). The expected behavior varies depending on the specific input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samoe",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samoe",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "goal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "one",
                    "action",
                    "between",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt;</span> {5 speech embeddings} <span class=\"ltx_text ltx_font_typewriter\">&lt;eos&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;bot&gt;</span> {8 text tokens / <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>} <span class=\"ltx_text ltx_font_typewriter\">&lt;eot&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boa&gt;</span> {dummy action tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoa&gt;</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt;</span> {5 speech embeddings} <span class=\"ltx_text ltx_font_typewriter\">&lt;eos&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> {front view image tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> {gripper view image tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;bot&gt;</span> {8 text tokens / <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>} <span class=\"ltx_text ltx_font_typewriter\">&lt;eot&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boa&gt;</span> {action tokens / dummy action tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoa&gt;</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system prompts also differ by mode. In the default mode, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer by action or text following the speech instruction</span>&#8221;. For speech-only mode, prompts vary by task. For ASR, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Generate a transcript of the speech</span>&#8221;, while for QA it is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer the question</span>&#8221;. System prompts are enclosed within <span class=\"ltx_text ltx_font_typewriter\">&lt;bop&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eop&gt;</span>, processed by the speech expert and inserted at the beginning of the whole multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "expert",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "one",
                    "speech",
                    "action",
                    "between",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samoe",
                    "expert",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "one",
                    "comparison",
                    "action",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select the speech expert and the action expert as the foundation of ELLSA primarily because individual experts are readily accessible. However, this is not the only viable approach. SA-MoE provides a generalizable framework capable of integrating multimodal processing from any set of experts. Here, we would like to highlight an alternative solution that we consider more elegant and efficient: a universal backbone capable of processing all input modalities as the &#8220;brain&#8221;, supported by specialized encoders such as the speech encoder as the &#8220;ear&#8221;, and the vision encoder as the &#8220;eye&#8221;; text expert as the &#8220;mouth&#8221;, and the action expert as the &#8220;hand&#8221;. Such a &#8220;brain-hand-mouth&#8221; architecture more closely mirrors the natural information flow of humans.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "speech",
                    "action",
                    "from",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies have explored multimodal processing with MoE to enhance performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib30\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib29\" title=\"\">2025b</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib21\" title=\"\">2025</a>)</cite>. However, SA-MoE differs from these works not only in the expert interaction mechanism but also the scope. Prior work focuses primarily on vision understanding, using MoE to alleviate domain conflicts, whereas SA-MoE integrates pretrained experts across diverse modalities to address modality interference. Moreover, from the perspective of training, existing approaches primarily serve as superior pretraining architectures, while SA-MoE is a high-performance and data-efficient architecture for post-training.</p>\n\n",
                "matched_terms": [
                    "samoe",
                    "from",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE proves to be an efficient and effective strategy for integrating experts. It not only achieves robust modality integration and mitigates modality interference, but also leverages and retains much of the pretrained capability of each individual expert.</p>\n\n",
                "matched_terms": [
                    "samoe",
                    "expert"
                ]
            }
        ]
    },
    "A6.T8": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 8: Comparison between ELLSA and individual experts",
        "body": "Model\nSPATIAL\nOBJECT\nGOAL\nLONG\n\n\n\n\nAction expert\n95.4%\n98.8%\n93.6%\n94.0%\n\n\nSA-MoE\n90.8%\n95.8%\n86.4%\n84.4%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SPATIAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OBJECT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GOAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LONG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Action expert</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">98.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">93.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">94.0%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">SA-MoE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">95.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">84.4%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "object",
            "expert",
            "comparison",
            "experts",
            "spatial",
            "individual",
            "ellsa",
            "goal",
            "long",
            "action",
            "between",
            "samoe",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "ellsa",
                    "action",
                    "samoe",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "ellsa",
                    "action",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SA-MoE, a novel and data-efficient architecture to integrate experts for different modalities to fuse concurrent multimodal input and output streams, leveraging the pretrained ability of each expert and mitigating modality interference. Experimental results demonstrate that SA-MoE exhibits significantly superior performance compared to one single dense model with less training cost.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "samoe",
                    "expert",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we detail how we build ELLSA, whose overview is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (a). We begin by explaining how streaming duplex MIMO is achieved through interleaved sequences. Then the core architecture design SA-MoE is introduced, which equips the model with strong multimodal perception and generation capabilities. Finally, the training strategy is discussed from building separate experts to connecting experts through SA-MoE.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "model",
                    "samoe",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When developing multimodal LLMs, a major challenge is that combining multimodal perception and generation often degrades text performance <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite>, particularly in multimodal generation. Handling multiple modalities&#8212;speech, vision, text, and action&#8212;further complicates the problem. While it is possible to train a single dense model for all tasks, doing so makes it extremely difficult to balance the modalities and requires vast amounts of data. To address this issue, we propose Self-Attention Mixture-of-Experts (SA-MoE), a new paradigm for multimodal processing. Its mechanism is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Streaming Full-duplex MIMO &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this architecture, different experts are responsible for different modalities, while a unified attention mechanism integrates them. The design of SA-MoE draws inspiration from <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>, where the VLM backbone and the action expert are connected through attention. We extend this idea to interleaved multimodal sequences and cross-expert interaction.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "model",
                    "experts",
                    "action",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the perspective of modality processing, each modality is handled by a designated expert: the speech expert processes both speech and text, while the action expert handles vision and action. This clear division of labor ensures that each expert focuses on its domain, effectively assigning the &#8220;mouth&#8221; and the &#8220;hand&#8221; to different modules. Such specialization reduces the complexity of multimodal modeling, mitigates modality interference and enhances controllability and interpretability. From the perspective of sequence processing, the entire MoE model functions as a transformer. Empowered by attention mechanism, SA-MoE efficiently fuses and integrates multimodal inputs as a unified system, enabling each expert to understand previously unfamiliar modalities. Looking from the whole sequence, its information flow is equivalent to that of a vanilla transformer. Looking into one single step, it behaves like a standard transformer except that the previous KV values may be derived from different experts. Thus, at any moment, only one expert&#8217;s weights are activated.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "model",
                    "experts",
                    "action",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE integrates experts through attention, preserving the strengths of individual modules to reduce modality interference while enabling efficient multimodal fusion. Importantly, it offers a flexible and scalable framework for multimodal processing. In ELLSA, we employ two experts, a speech expert and an action expert. This, however, is definitely not the only possible configuration. An alternative is to introduce four separate experts for speech, vision, text, and action. We merge speech with text and vision with action to better leverage pretrained knowledge. Looking ahead, as we aim toward more human-like AGI, additional modalities such as smell or touch could also be easily incorporated by introducing dedicated experts.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "individual",
                    "ellsa",
                    "action",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "individual",
                    "action",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "samoe",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Connecting speech synthesizer.</span> In the final stage, we integrate a streaming speech synthesizer with ELLSA in an end-to-end manner. The last hidden states of the speech expert are fed into the trainable synthesizer after being transformed by a randomly initialized connector. During Stage 3, ELLSA gains the ability to speak, completing its multimodal interaction loop.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "ellsa",
                    "action",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "between",
                    "expert",
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa",
                    "action",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "individual",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "samoe",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "samoe",
                    "expert",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "between",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "samoe",
                    "expert",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "ellsa",
                    "action",
                    "samoe",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "goal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "between",
                    "model",
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system prompts also differ by mode. In the default mode, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer by action or text following the speech instruction</span>&#8221;. For speech-only mode, prompts vary by task. For ASR, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Generate a transcript of the speech</span>&#8221;, while for QA it is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer the question</span>&#8221;. System prompts are enclosed within <span class=\"ltx_text ltx_font_typewriter\">&lt;bop&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eop&gt;</span>, processed by the speech expert and inserted at the beginning of the whole multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "action",
                    "between",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "samoe",
                    "expert",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "model",
                    "ellsa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select the speech expert and the action expert as the foundation of ELLSA primarily because individual experts are readily accessible. However, this is not the only viable approach. SA-MoE provides a generalizable framework capable of integrating multimodal processing from any set of experts. Here, we would like to highlight an alternative solution that we consider more elegant and efficient: a universal backbone capable of processing all input modalities as the &#8220;brain&#8221;, supported by specialized encoders such as the speech encoder as the &#8220;ear&#8221;, and the vision encoder as the &#8220;eye&#8221;; text expert as the &#8220;mouth&#8221;, and the action expert as the &#8220;hand&#8221;. Such a &#8220;brain-hand-mouth&#8221; architecture more closely mirrors the natural information flow of humans.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "experts",
                    "individual",
                    "ellsa",
                    "action",
                    "samoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies have explored multimodal processing with MoE to enhance performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib30\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib29\" title=\"\">2025b</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib21\" title=\"\">2025</a>)</cite>. However, SA-MoE differs from these works not only in the expert interaction mechanism but also the scope. Prior work focuses primarily on vision understanding, using MoE to alleviate domain conflicts, whereas SA-MoE integrates pretrained experts across diverse modalities to address modality interference. Moreover, from the perspective of training, existing approaches primarily serve as superior pretraining architectures, while SA-MoE is a high-performance and data-efficient architecture for post-training.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "samoe",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE proves to be an efficient and effective strategy for integrating experts. It not only achieves robust modality integration and mitigates modality interference, but also leverages and retains much of the pretrained capability of each individual expert.</p>\n\n",
                "matched_terms": [
                    "experts",
                    "individual",
                    "samoe",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA still faces several limitations. First, although ELLSA successfully predicts duplex dynamics such as dialogue/action turn-taking and action barge-ins, it currently handles only a limited range of scenarios. Many aspects of natural communication, such as user and assistant backchanneling, remain unaddressed. Expanding support for these dynamics will be crucial for achieving more human-like interactions. Second, although ELLSA has shown promising results in handling full-duplex multimodal joint perception and concurrent generation within simulated environments, ELLSA has yet to be validated in real-world settings. Future work will focus on real-world deployment, and we believe our framework can be effectively adapted through targeted finetuning.</p>\n\n",
                "matched_terms": [
                    "ellsa",
                    "action"
                ]
            }
        ]
    },
    "A7.T9": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 9: Detailed results when dealing with different types of speech input during action execution (i.e., the speaking-while-acting task).",
        "body": "Dataset\nGeneral question\nInterruptive command\nSilence\n\n\n\n\nSPATIAL\n100%\n96%\n100%\n\n\nOBJECT\n100%\n96%\n100%\n\n\nGOAL\n100%\n89%\n100%\n\n\nLONG\n100%\n96%\n100%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">General question</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Interruptive command</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Silence</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">SPATIAL</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">OBJECT</th>\n<td class=\"ltx_td ltx_align_center\">100%</td>\n<td class=\"ltx_td ltx_align_center\">96%</td>\n<td class=\"ltx_td ltx_align_center\">100%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">GOAL</th>\n<td class=\"ltx_td ltx_align_center\">100%</td>\n<td class=\"ltx_td ltx_align_center\">89%</td>\n<td class=\"ltx_td ltx_align_center\">100%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">LONG</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">96%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detailed",
            "speech",
            "speakingwhileacting",
            "silence",
            "goal",
            "long",
            "types",
            "question",
            "different",
            "results",
            "general",
            "action",
            "interruptive",
            "object",
            "command",
            "during",
            "spatial",
            "dataset",
            "input",
            "dealing",
            "task",
            "execution",
            "when"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "question",
                    "speakingwhileacting",
                    "speech",
                    "general",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for artificial general intelligence (AGI) has pivoted from purely computational intelligence toward embodied agents that can perceive, understand and act within interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib15\" title=\"\">2022</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib51\" title=\"\">2024</a>)</cite>. A defining feature of human intelligence is our capacity for full-duplex multimodal interaction, we seamlessly process multiple input streams (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, vision, hearing, touch) while producing multiple outputs (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, speech, facial expressions, body movements). We listen as we observe, speak while acting, and continuously adapt our behavior in real time to complex conversational dynamics such as turn-taking and interruptions. This fluid interplay forms the essence of natural interaction, yet it remains a gap in the capabilities of current AI models.</p>\n\n",
                "matched_terms": [
                    "general",
                    "speech",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "question",
                    "speech",
                    "command",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "types",
                    "question",
                    "speakingwhileacting",
                    "speech",
                    "task",
                    "different",
                    "results",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SA-MoE, a novel and data-efficient architecture to integrate experts for different modalities to fuse concurrent multimodal input and output streams, leveraging the pretrained ability of each expert and mitigating modality interference. Experimental results demonstrate that SA-MoE exhibits significantly superior performance compared to one single dense model with less training cost.</p>\n\n",
                "matched_terms": [
                    "results",
                    "input",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "results",
                    "general",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "execution",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "question",
                    "speech",
                    "during",
                    "execution",
                    "action",
                    "when",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "action",
                    "when",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When developing multimodal LLMs, a major challenge is that combining multimodal perception and generation often degrades text performance <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite>, particularly in multimodal generation. Handling multiple modalities&#8212;speech, vision, text, and action&#8212;further complicates the problem. While it is possible to train a single dense model for all tasks, doing so makes it extremely difficult to balance the modalities and requires vast amounts of data. To address this issue, we propose Self-Attention Mixture-of-Experts (SA-MoE), a new paradigm for multimodal processing. Its mechanism is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Streaming Full-duplex MIMO &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this architecture, different experts are responsible for different modalities, while a unified attention mechanism integrates them. The design of SA-MoE draws inspiration from <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>, where the VLM backbone and the action expert are connected through attention. We extend this idea to interleaved multimodal sequences and cross-expert interaction.</p>\n\n",
                "matched_terms": [
                    "when",
                    "action",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the perspective of modality processing, each modality is handled by a designated expert: the speech expert processes both speech and text, while the action expert handles vision and action. This clear division of labor ensures that each expert focuses on its domain, effectively assigning the &#8220;mouth&#8221; and the &#8220;hand&#8221; to different modules. Such specialization reduces the complexity of multimodal modeling, mitigates modality interference and enhances controllability and interpretability. From the perspective of sequence processing, the entire MoE model functions as a transformer. Empowered by attention mechanism, SA-MoE efficiently fuses and integrates multimodal inputs as a unified system, enabling each expert to understand previously unfamiliar modalities. Looking from the whole sequence, its information flow is equivalent to that of a vanilla transformer. Looking into one single step, it behaves like a standard transformer except that the previous KV values may be derived from different experts. Thus, at any moment, only one expert&#8217;s weights are activated.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, SA-MoE integrates experts through attention, preserving the strengths of individual modules to reduce modality interference while enabling efficient multimodal fusion. Importantly, it offers a flexible and scalable framework for multimodal processing. In ELLSA, we employ two experts, a speech expert and an action expert. This, however, is definitely not the only possible configuration. An alternative is to introduce four separate experts for speech, vision, text, and action. We merge speech with text and vision with action to better leverage pretrained knowledge. Looking ahead, as we aim toward more human-like AGI, additional modalities such as smell or touch could also be easily incorporated by introducing dedicated experts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "during",
                    "question",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Connecting speech synthesizer.</span> In the final stage, we integrate a streaming speech synthesizer with ELLSA in an end-to-end manner. The last hidden states of the speech expert are fed into the trainable synthesizer after being transformed by a randomly initialized connector. During Stage 3, ELLSA gains the ability to speak, completing its multimodal interaction loop.</p>\n\n",
                "matched_terms": [
                    "during",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "silence",
                    "general",
                    "action",
                    "when",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "during",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "action",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "speakingwhileacting",
                    "speech",
                    "command",
                    "task",
                    "during",
                    "execution",
                    "general",
                    "action",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "general",
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task",
                    "results",
                    "action",
                    "when",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(c) Success rate across different types of speech input during action execution (i.e., the speaking-while-acting task). The expected behavior varies depending on the specific input.</p>\n\n",
                "matched_terms": [
                    "types",
                    "speakingwhileacting",
                    "speech",
                    "task",
                    "different",
                    "during",
                    "execution",
                    "action",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "types",
                    "speech",
                    "results",
                    "execution",
                    "action",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "types",
                    "question",
                    "speakingwhileacting",
                    "speech",
                    "silence",
                    "command",
                    "task",
                    "during",
                    "execution",
                    "general",
                    "action",
                    "when",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "when",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "speakingwhileacting",
                    "speech",
                    "task",
                    "results",
                    "long",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "when",
                    "general",
                    "goal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt;</span> {5 speech embeddings} <span class=\"ltx_text ltx_font_typewriter\">&lt;eos&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;bot&gt;</span> {8 text tokens / <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>} <span class=\"ltx_text ltx_font_typewriter\">&lt;eot&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boa&gt;</span> {dummy action tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoa&gt;</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "silence",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt;</span> {5 speech embeddings} <span class=\"ltx_text ltx_font_typewriter\">&lt;eos&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> {front view image tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> {gripper view image tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;bot&gt;</span> {8 text tokens / <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>} <span class=\"ltx_text ltx_font_typewriter\">&lt;eot&gt;</span> <span class=\"ltx_text ltx_font_typewriter\">&lt;boa&gt;</span> {action tokens / dummy action tokens} <span class=\"ltx_text ltx_font_typewriter\">&lt;eoa&gt;</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "silence",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system prompts also differ by mode. In the default mode, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer by action or text following the speech instruction</span>&#8221;. For speech-only mode, prompts vary by task. For ASR, the system prompt is &#8220;<span class=\"ltx_text ltx_font_italic\">Generate a transcript of the speech</span>&#8221;, while for QA it is &#8220;<span class=\"ltx_text ltx_font_italic\">Please answer the question</span>&#8221;. System prompts are enclosed within <span class=\"ltx_text ltx_font_typewriter\">&lt;bop&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eop&gt;</span>, processed by the speech expert and inserted at the beginning of the whole multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "speech",
                    "silence",
                    "task",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "question",
                    "speech",
                    "command",
                    "action",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "interruptive",
                    "question",
                    "speakingwhileacting",
                    "speech",
                    "command",
                    "task",
                    "general",
                    "action",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question:</span> What object are you currently holding?</p>\n\n",
                "matched_terms": [
                    "object",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Semantic</span>: A task referencing an object that does not exist in the scene.</p>\n\n",
                "matched_terms": [
                    "object",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select the speech expert and the action expert as the foundation of ELLSA primarily because individual experts are readily accessible. However, this is not the only viable approach. SA-MoE provides a generalizable framework capable of integrating multimodal processing from any set of experts. Here, we would like to highlight an alternative solution that we consider more elegant and efficient: a universal backbone capable of processing all input modalities as the &#8220;brain&#8221;, supported by specialized encoders such as the speech encoder as the &#8220;ear&#8221;, and the vision encoder as the &#8220;eye&#8221;; text expert as the &#8220;mouth&#8221;, and the action expert as the &#8220;hand&#8221;. Such a &#8220;brain-hand-mouth&#8221; architecture more closely mirrors the natural information flow of humans.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "action",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA still faces several limitations. First, although ELLSA successfully predicts duplex dynamics such as dialogue/action turn-taking and action barge-ins, it currently handles only a limited range of scenarios. Many aspects of natural communication, such as user and assistant backchanneling, remain unaddressed. Expanding support for these dynamics will be crucial for achieving more human-like interactions. Second, although ELLSA has shown promising results in handling full-duplex multimodal joint perception and concurrent generation within simulated environments, ELLSA has yet to be validated in real-world settings. Future work will focus on real-world deployment, and we believe our framework can be effectively adapted through targeted finetuning.</p>\n\n",
                "matched_terms": [
                    "results",
                    "action"
                ]
            }
        ]
    },
    "A7.T10": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 10: Detailed results for both speaking and acting on speaking-while-acting. The results are robot manipulation success rate %, S2T speech interaction performance and S2S speech interaction performance, respectively.",
        "body": "Llama Q.\nWeb Q.\nTriviaQA\nAlpacaEval\n\n\n\n\nSPATIAL\n94.6%/71.0/65.0\n92.2%/33.8/28.7\n93.8%/34.5/30.2\n92.4%/2.65/2.19\n\n\nOBJECT\n96.4%/69.3/65.7\n97.2%/34.0/28.7\n96.6%/36.0/31.8\n96.2%/2.63/2.10\n\n\nGOAL\n84.8%/69.3/62.7\n86.4%/32.2/26.9\n90.2%/36.7/31.7\n82.8%/2.69/2.06\n\n\nLONG\n74.2%/66.0/57.3\n73.6%/31.1/26.1\n73.6%/33.3/29.2\n71.2%/2.68/2.14",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Llama Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Web Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TriviaQA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AlpacaEval</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SPATIAL</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">94.6%/71.0/65.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.2%/33.8/28.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">93.8%/34.5/30.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.4%/2.65/2.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">OBJECT</span></th>\n<td class=\"ltx_td ltx_align_center\">96.4%/69.3/65.7</td>\n<td class=\"ltx_td ltx_align_center\">97.2%/34.0/28.7</td>\n<td class=\"ltx_td ltx_align_center\">96.6%/36.0/31.8</td>\n<td class=\"ltx_td ltx_align_center\">96.2%/2.63/2.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GOAL</span></th>\n<td class=\"ltx_td ltx_align_center\">84.8%/69.3/62.7</td>\n<td class=\"ltx_td ltx_align_center\">86.4%/32.2/26.9</td>\n<td class=\"ltx_td ltx_align_center\">90.2%/36.7/31.7</td>\n<td class=\"ltx_td ltx_align_center\">82.8%/2.69/2.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">LONG</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">74.2%/66.0/57.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">73.6%/31.1/26.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">73.6%/33.3/29.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.2%/2.68/2.14</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "both",
            "detailed",
            "respectively",
            "speakingwhileacting",
            "speech",
            "speaking",
            "goal",
            "long",
            "rate",
            "llama",
            "results",
            "acting",
            "s2s",
            "success",
            "alpacaeval",
            "s2t",
            "object",
            "manipulation",
            "performance",
            "robot",
            "triviaqa",
            "web",
            "spatial",
            "interaction"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speakingwhileacting",
                    "acting",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for artificial general intelligence (AGI) has pivoted from purely computational intelligence toward embodied agents that can perceive, understand and act within interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib15\" title=\"\">2022</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib51\" title=\"\">2024</a>)</cite>. A defining feature of human intelligence is our capacity for full-duplex multimodal interaction, we seamlessly process multiple input streams (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, vision, hearing, touch) while producing multiple outputs (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, speech, facial expressions, body movements). We listen as we observe, speak while acting, and continuously adapt our behavior in real time to complex conversational dynamics such as turn-taking and interruptions. This fluid interplay forms the essence of natural interaction, yet it remains a gap in the capabilities of current AI models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "acting",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite remarkable progress, prevailing paradigms address only isolated aspects of this holistic challenge, producing either disembodied &#8220;talkers&#8221; or non-conversant &#8220;doers&#8221;. On the one hand, full-duplex conversational speech LLMs have been developed to enable seamlessly more natural interaction <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>. These models can engage in low-latency speech-to-speech interaction, capturing not only semantic content but also paralinguistic cues such as speaker identity and emotion. Vision information can also be incorporated to support video-based conversations <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib34\" title=\"\">2024</a>; Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>)</cite>. While they can see, listen, and speak, they remain disembodied observers, fundamentally incapable of translating their understandings into physical actions to interact with the environment. On the other hand, Vision-Language-Action (VLA) models have achieved notable success in grounding language in manipulation tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>. However, these models are metaphorically &#8220;deaf&#8221; and &#8220;mute&#8221;. They typically operate on textual instructions within a rigid, turn-based framework and lack the ability to process raw auditory signals or generate spoken responses. This half-duplex, turn-based paradigm fundamentally limits their interactivity, making them unable to handle natural conversational behaviors like turn-taking and barge-ins.</p>\n\n",
                "matched_terms": [
                    "success",
                    "speech",
                    "manipulation",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "both",
                    "speech",
                    "interaction",
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "performance",
                    "robot",
                    "speech",
                    "speakingwhileacting",
                    "interaction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SA-MoE, a novel and data-efficient architecture to integrate experts for different modalities to fuse concurrent multimodal input and output streams, leveraging the pretrained ability of each expert and mitigating modality interference. Experimental results demonstrate that SA-MoE exhibits significantly superior performance compared to one single dense model with less training cost.</p>\n\n",
                "matched_terms": [
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce ELLSA, the first end-to-end model that unifies vision, speech, text and action in a streaming full-duplex framework, enabling joint multimodal perception and concurrent generation. ELLSA achieves performance on par with specialized models across both speech interaction and robotic manipulation benchmarks.</p>\n\n",
                "matched_terms": [
                    "both",
                    "manipulation",
                    "performance",
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically demonstrate that ELLSA can accomplish tasks previously unattainable, such as dialogue and action turn-taking prediction, rejection of defective instructions, speaking while acting and responding to action barge-ins. These results highlight the feasibility and significance of full-duplex multimodal interaction as a foundation for more natural and general multimodal interactive intelligence.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "results",
                    "acting",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large end-to-end speech dialogue models have lowered response latency and enabled more natural human&#8211;machine communication. Half-duplex models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib49\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib53\" title=\"\">2024</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib13\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib47\" title=\"\">2025</a>)</cite> process speech inputs and generate spoken or textual responses in an end-to-end manner, however, their interaction style is inherently sequential, meaning they can only &#8220;listen-then-speak&#8221;. As a result, they cannot capture the intricate full-duplex dynamics of natural conversations without auxiliary modules. Full-duplex systems address this challenge by leveraging dual-model frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib45\" title=\"\">2025a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib8\" title=\"\">2025b</a>)</cite> or state transition mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib54\" title=\"\">2024</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib52\" title=\"\">2025</a>)</cite>, allowing seamless management of turn-taking, backchanneling and interruptions. Beyond processing speech inputs, recent efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib16\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib7\" title=\"\">2025a</a>; OpenBMB, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib35\" title=\"\">2025</a>)</cite> have also sought to integrate visual perception capabilities, but they remain largely &#8220;all talk and no action&#8221;, lacking the ability to interact with the physical environment. ELLSA advances beyond these limitations. It engages in spoken dialogue while simultaneously executing actions, unifying auditory processing, visual perception, speech generation, and action execution within a single end-to-end framework. To our knowledge, it is the first end-to-end large model to support simultaneously listening, looking, speaking, and acting, marking a significant milestone toward AGI.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "speech",
                    "acting",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large VLA models have achieved impressive progress across diverse robotic tasks by leveraging the perception and reasoning capabilities of vision&#8211;language models (VLMs) trained on Internet-scale data <cite class=\"ltx_cite ltx_citemacro_citep\">(Zitkovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib57\" title=\"\">2023</a>; Belkhale et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib1\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib26\" title=\"\">2024</a>; Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>; Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. Some approaches adopt world-model pretraining on large-scale egocentric videos to enhance generalization and action precision <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib48\" title=\"\">2024</a>; Cheang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib4\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib5\" title=\"\">2025</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib20\" title=\"\">2024</a>)</cite>. While these models effectively process multimodal inputs, their outputs are largely restricted to action sequences, limiting their ability to engage in natural language interaction. Extensions such as VLASCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib42\" title=\"\">2024</a>)</cite>, RationalVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, and IVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsieh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib22\" title=\"\">2025</a>)</cite> introduce question answering and instruction rejection, yet they remain constrained by a half-duplex design. ELLSA addresses this limitation by operating in full-duplex: it can decide when to answer questions, execute actions, or be interrupted during execution. Moreover, it supports end-to-end speech input and output, enabling more natural human&#8211;AI interaction. Although VLAS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib56\" title=\"\">2025b</a>)</cite> also accepts speech input, it is still half-duplex and limited to action outputs. A contemporary technical report, RoboEgo <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib50\" title=\"\">2025</a>)</cite>, pursues a similar vision but only generates simple action commands like &#8220;raise hand&#8221; requiring further downstream interpretation, whereas ELLSA provides precise, end-to-end action prediction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Streaming full-duplex interaction is a defining characteristic of natural human communication. Unlike turn-based models, streaming full-duplex model needs to determine when to start/stop speaking/acting by itself. ELLSA achieves this capability through its streaming full-duplex MIMO design, enabled by simply arranging multimodal sequences in an interleaved temporal order, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Within each time block, inputs and outputs from different modalities are organized in a fixed sequence: speech input, image input, text output, and action output. Speech output is derived directly from the embeddings of text output and is therefore excluded from the main sequence. To clearly delimit modality boundaries, each segment is wrapped with modality-specific tokens, <span class=\"ltx_text ltx_font_typewriter\">&lt;bo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eo<math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>&gt;</span>, where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denotes the modality type.\nELLSA operates in two modes: a default mode and a speech-only mode. In the default setting, all four modalities, speech, vision, text, and action, are active. By contrast, the speech-only mode restricts interaction to the speech and text modalities. In this configuration, ELLSA functions as a pure speech interaction model, producing dummy actions with placeholder visual inputs. More implementation details are shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When developing multimodal LLMs, a major challenge is that combining multimodal perception and generation often degrades text performance <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite>, particularly in multimodal generation. Handling multiple modalities&#8212;speech, vision, text, and action&#8212;further complicates the problem. While it is possible to train a single dense model for all tasks, doing so makes it extremely difficult to balance the modalities and requires vast amounts of data. To address this issue, we propose Self-Attention Mixture-of-Experts (SA-MoE), a new paradigm for multimodal processing. Its mechanism is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Streaming Full-duplex MIMO &#8227; 3 Methodology &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. In this architecture, different experts are responsible for different modalities, while a unified attention mechanism integrates them. The design of SA-MoE draws inspiration from <math alttext=\"\\pi_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\pi_{0}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Black et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib3\" title=\"\">2024</a>)</cite>, where the VLM backbone and the action expert are connected through attention. We extend this idea to interleaved multimodal sequences and cross-expert interaction.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the perspective of modality processing, each modality is handled by a designated expert: the speech expert processes both speech and text, while the action expert handles vision and action. This clear division of labor ensures that each expert focuses on its domain, effectively assigning the &#8220;mouth&#8221; and the &#8220;hand&#8221; to different modules. Such specialization reduces the complexity of multimodal modeling, mitigates modality interference and enhances controllability and interpretability. From the perspective of sequence processing, the entire MoE model functions as a transformer. Empowered by attention mechanism, SA-MoE efficiently fuses and integrates multimodal inputs as a unified system, enabling each expert to understand previously unfamiliar modalities. Looking from the whole sequence, its information flow is equivalent to that of a vanilla transformer. Looking into one single step, it behaves like a standard transformer except that the previous KV values may be derived from different experts. Thus, at any moment, only one expert&#8217;s weights are activated.</p>\n\n",
                "matched_terms": [
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Training individual experts.</span> In stage 1, we construct the speech expert and the action expert. The speech expert is built by connecting a streaming speech encoder with a text LLM and is trained on automatic speech recognition (ASR) and speech question answering (QA) tasks. During this stage, only the connector and the LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib23\" title=\"\">2022</a>)</cite> on LLM backbone are trained, while the encoder and the LLM remain frozen. For action expert, we directly use the pretrained UniVLA <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib46\" title=\"\">2025b</a>)</cite>. UniVLA is initialized from multimodal LLM Emu3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and full-finetuned with world model post-training and policy learning finetuning. By the end of Stage 1, the speech expert acquires fundamental speech understanding and turn-taking abilities, while the action expert develops skills for text-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Training SA-MoE.</span> In stage 2, we integrate the two experts within the SA-MoE framework. Training spans a diverse set of tasks, ranging from basic capabilities such as ASR, spoken QA, and speech-conditioned robot manipulation to advanced interactive skills including speaking-while-acting, defective instruction rejection, action barge-ins, and contextual VQA. Both the speech and action experts are further fine-tuned with LoRA. This stage yields a unified and versatile model capable of handling streaming, full-duplex multimodal MIMO with efficient modality fusion.</p>\n\n",
                "matched_terms": [
                    "both",
                    "manipulation",
                    "robot",
                    "speech",
                    "speakingwhileacting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Connecting speech synthesizer.</span> In the final stage, we integrate a streaming speech synthesizer with ELLSA in an end-to-end manner. The last hidden states of the speech expert are fed into the trainable synthesizer after being transformed by a randomly initialized connector. During Stage 3, ELLSA gains the ability to speak, completing its multimodal interaction loop.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we outline the basic configurations of ELLSA. Full specifications are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. In general, ELLSA operates on a one-second time block, within which it processes one second of speech input and a single video frame, generates eight tokens of text output (or a single <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span> token when no verbal response is required), and produces one second of speech and action output. The speech expert and action expert share the same number of layers, enabling attention-based interaction between experts at each layer. Below are specifications for components of ELLSA:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Expert</span> The speech encoder is a streaming Mamba encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu &amp; Dao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib19\" title=\"\">2024</a>)</cite>, paired with a two-layer MLP adapter that aligns the dimension between the outputs of the encoder and the hidden states of the LLM. The LLM backbone is LLama-3.1-8B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib18\" title=\"\">2024</a>)</cite>, with LoRA applied at rank 256 and scale 1.0 in both Stage 1 and Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action Expert</span> The image tokenizer is Emu3-VisionTokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib44\" title=\"\">2024</a>)</cite> and the action tokenizer is FAST <cite class=\"ltx_cite ltx_citemacro_citep\">(Pertsch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib37\" title=\"\">2025</a>)</cite>. The backbone is Emu3-Base, the final 1,024 token IDs of which are replaced with FAST tokens to enable action prediction. LoRA is applied with rank 256 and scale 1.0 during Stage 2. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speech Synthesizer</span> The streaming speech synthesizer is built upon CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>.\nOnly the language model component of the synthesizer is fine-tuned, which produces 25 speech codecs for every 8 textual embeddings from the LLM.\nA two-layer MLP adapter bridges the embeddings between the speech expert and the speech synthesizer.</p>\n\n",
                "matched_terms": [
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "speakingwhileacting",
                    "manipulation",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "speech",
                    "speakingwhileacting",
                    "interaction",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "s2t",
                    "manipulation",
                    "performance",
                    "rate",
                    "speech",
                    "interaction",
                    "s2s",
                    "success"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA&#8217;s speech interaction capabilities on knowledge QA benchmarks Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite> and TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, as well as open-ended oral conversation benchmark AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Speech Interaction &#8227; 5.1 Basic Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that ELLSA delivers performance comparable to current open-source full-duplex interaction models. In particular, ELLSA achieves the highest S2S performance, underscoring its strength in end-to-end speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "alpacaeval",
                    "performance",
                    "triviaqa",
                    "web",
                    "speech",
                    "llama",
                    "results",
                    "interaction",
                    "s2s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "performance",
                    "robot",
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Dialogue turn-taking success rate on speech interaction.</p>\n\n",
                "matched_terms": [
                    "success",
                    "rate",
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Action turn-taking success rate and defective instruction\n<br class=\"ltx_break\"/>rejection rate on speech-conditioned robotic manipulation.</p>\n\n",
                "matched_terms": [
                    "success",
                    "rate",
                    "manipulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(c) Success rate across different types of speech input during action execution (i.e., the speaking-while-acting task). The expected behavior varies depending on the specific input.</p>\n\n",
                "matched_terms": [
                    "speakingwhileacting",
                    "success",
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results of ELLSA&#8217;s advanced duplex abilities. For basic speech interaction and speech-conditioned robot manipulation tasks, ELLSA needs to determine the appropriate moment to begin speaking or acting, referred to as dialogue turn-taking and action turn-taking. Results show that ELLSA can always successfully predict both types of turn-taking. Notably, in dialogue turn-taking, ELLSA even consistently outperforms speech-only interaction models. We hypothesize that this advantage may be attributed to ELLSA&#8217;s longer time block for full-duplex modeling (1 second) compared to other models (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, 0.16 seconds for Freeze-Omni), which simplify the learning of full-duplex dynamics. When presented with all four types of defective commands, ELLSA consistently identifies and rejects them with high accuracy, while ensuring that the execution of valid instructions remains largely unaffected.</p>\n\n",
                "matched_terms": [
                    "both",
                    "manipulation",
                    "robot",
                    "speech",
                    "interaction",
                    "speaking",
                    "results",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speakingwhileacting",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "interaction",
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance when speaking while acting.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "performance",
                    "robot",
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "both",
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "speakingwhileacting",
                    "interaction",
                    "speaking",
                    "acting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces an end-to-end full-duplex framework capable of simultaneously listening, looking, speaking, and acting, thereby advancing the frontier of real-time multimodal interactive artificial general intelligence. While enhancing the performance and naturalness of human-like AGI is an important goal, we place equal emphasis on ensuring AI safety. This involves safeguarding against harmful, discriminatory, or biased outputs, as well as developing reliable detection models to identify AI-generated content. Moreover, we stress the importance of transparency: users should always be made aware when they are interacting with an AI model.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "acting",
                    "goal",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA operates in two modes: a default mode and a speech-only mode. Both modes follow the same modality order, speech &#8594; image &#8594; text &#8594; action. In speech-only mode, the visual input between <span class=\"ltx_text ltx_font_typewriter\">&lt;boi&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;eoi&gt;</span> is absent, and the model consistently produces dummy actions without actual movement. The default mode incorporates all modalities. Within the LIBERO simulation environment, the observation includes not only a front-view frame but also a gripper image, so each time block contains two visual inputs. Typical patterns in one time block of the two modes are illustrated below:</p>\n\n",
                "matched_terms": [
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For historical context handling, ELLSA retains the complete history of speech input and text output, while preserving only a limited window of vision input and action output. This design is inspired by prior speech interaction models <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib12\" title=\"\">2024</a>)</cite> and VLA systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib17\" title=\"\">2024</a>)</cite>. All speech and text history are preserved to ensure coherent context. Prior studies have shown that incorporating historical context benefits VLA tasks; however, extending the context beyond a certain length generally provides no further advantage. Given that our focus is restricted to VLA and VQA tasks based on current observations, we adopt the conventional design in which only limited vision and action history (within the last two seconds) are retained, thereby substantially reducing the sequence length required for modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder in ELLSA&#8217;s speech expert consists of 32 Mamba LM blocks, each with a hidden state dimension of 2048. It produces embeddings at a frame rate of 25 Hz, which are then downsampled to 5 Hz by concatenating every five consecutive embeddings into a single vector before being passed to the speech expert&#8217;s LLM backbone. The LLM backbone of the speech expert (LLaMA-3.1-Instruct) and that of the action expert (Emu3-Base) share identical configurations: 32 transformer layers, a hidden size of 4096, 32 attention heads, and 8 key-value heads. As a result, no additional parameters are required to construct SA-MoE, and attention-based fusion between experts naturally occurs at every corresponding layer. One distinction lies in the RoPE specifications, which differ across experts. Each expert retains its own RoPE settings, while the token index is shared across the entire multimodal sequence.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "robot",
                    "rate",
                    "speech",
                    "speakingwhileacting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech interaction, ELLSA is evaluated on four widely used datasets in speech-only mode: Llama Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib33\" title=\"\">2024</a>)</cite>, Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib2\" title=\"\">2013</a>)</cite>, TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib24\" title=\"\">2017</a>)</cite>, and AlpacaEval from VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib10\" title=\"\">2024</a>)</cite>. For Web Questions, the queries are converted into speech using a commercial TTS model from Volcano Engine <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.volcengine.com/\" title=\"\">https://www.volcengine.com/</a></span></span></span>. For TriviaQA, we adopt the 1,000-sample subset from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib28\" title=\"\">2025a</a>)</cite>. For the oral conversation dataset AlpacaEval, responses are evaluated using GPTScore <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The scores are obtained using gpt-4.1-2025-04-14.</span></span></span>, which rates the appropriateness and reasonableness of answers on a 1&#8211;5 scale, with 1 indicating the worst and 5 the best. In the S2S setting, answer speech is first transcribed into text using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite>. Except for speech QA, all other tasks are tested using the default mode. For robot manipulation, performance is measured by the average success rate across 500 episodes (50 per task).</p>\n\n",
                "matched_terms": [
                    "alpacaeval",
                    "manipulation",
                    "performance",
                    "robot",
                    "triviaqa",
                    "rate",
                    "speech",
                    "web",
                    "llama",
                    "interaction",
                    "s2s",
                    "success"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "rate",
                    "speech",
                    "speakingwhileacting",
                    "success"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies have explored multimodal processing with MoE to enhance performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib30\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib29\" title=\"\">2025b</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib21\" title=\"\">2025</a>)</cite>. However, SA-MoE differs from these works not only in the expert interaction mechanism but also the scope. Prior work focuses primarily on vision understanding, using MoE to alleviate domain conflicts, whereas SA-MoE integrates pretrained experts across diverse modalities to address modality interference. Moreover, from the perspective of training, existing approaches primarily serve as superior pretraining architectures, while SA-MoE is a high-performance and data-efficient architecture for post-training.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SA-MoE against a single dense model. SA-MoE is finetuned with LoRA for 500 steps with all tasks, while the dense model is fully finetuned for 3k steps with only basic tasks, speech interaction and robot manipulation. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> show that SA-MoE significantly outperforms the dense model. Although the dense model can partially inherit capabilities from its initialized components, it struggles to learn effectively from unfamiliar modalities given the limited scale of our training data, far smaller than typical pretraining corpora. Moreover, SA-MoE surpasses the dense model in modalities corresponding to the initialized model, demonstrating that SA-MoE effectively addresses the modality interference problem that hampers dense models. We further compare SA-MoE with its individual experts, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T8\" title=\"Table 8 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Results indicate that SA-MoE largely preserves expert-level performance, with a relative decline of 10.3% for the speech expert and 6.4% for the action expert. We assume that the greater drop in the speech expert&#8217;s performance may be attributed to sequence length differences: a single image typically generates around 300 tokens, while a 10-second speech sample yields only about 50 tokens, making alignment more challenging.</p>\n\n",
                "matched_terms": [
                    "manipulation",
                    "performance",
                    "robot",
                    "speech",
                    "interaction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction S2T performance.</p>\n\n",
                "matched_terms": [
                    "s2t",
                    "speech",
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(a) Speech interaction S2T performance.</p>\n\n",
                "matched_terms": [
                    "s2t",
                    "speech",
                    "performance",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">(b) Robot manipulation performance.</p>\n\n",
                "matched_terms": [
                    "robot",
                    "manipulation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For experiments, we use LLM to generate training data (Llama-3-Instruct and Gemini-2.5-Pro) and evaluate results for AlpacaEval (GPT-4.1-2025-04-14) and context-grounded VQA (Gemini-2.5-Pro), with prompts provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A5\" title=\"Appendix E Prompts &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "alpacaeval"
                ]
            }
        ]
    },
    "A7.T11": {
        "source_file": "End-to-end Listen, Look, Speak and Act",
        "caption": "Table 11: Detailed results on context-grounded VQA task",
        "body": "Question Num\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\nManual Acc.%\\%\n100\n70\n100\n100\n90\n60\n100\n70\n60\n100\n60\n90\n\n\nGemini Acc.%\\%\n100\n80\n70\n100\n100\n100\n90\n70\n40\n70\n90\n100",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Question Num</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">4</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">6</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">7</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">11</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">12</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Manual Acc.<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T11.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Gemini Acc.<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T11.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detailed",
            "gemini",
            "manual",
            "question",
            "task",
            "results",
            "num",
            "acc",
            "vqa",
            "contextgrounded"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T5\" title=\"Table 5 &#8227; 5.2.3 Context-grounded VQA &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the results of context-grounded VQA, with per-question accuracy listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T11\" title=\"Table 11 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. Average accuracy is computed either manually or using Gemini-2.5-Pro, and the two methods produced closely aligned results, indicating that Gemini offers a reliable approach for automatic evaluation. In this more natural interaction setting, ELLSA achieves an average accuracy of approximately 80%, demonstrating its ability to effectively integrate multiple modalities for both environmental interaction and understanding. Notably, although the speech expert was never trained on visual data, it can now interpret visual information and answer questions accurately, illustrating how SA-MoE effectively links experts to enable robust modality integration. These findings highlight ELLSA&#8217;s potential to advance AGI systems toward more natural, human-like interactive capabilities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel <span class=\"ltx_text ltx_font_bold\">SA-MoE</span> architecture (<span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.</p>\n\n",
                "matched_terms": [
                    "contextgrounded",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge this gap and advance toward more human-like AGI, we introduce <span class=\"ltx_text ltx_font_bold\">ELLSA</span> (<span class=\"ltx_text ltx_font_bold\">E</span>nd-to-end <span class=\"ltx_text ltx_font_bold\">L</span>isten, <span class=\"ltx_text ltx_font_bold\">L</span>ook, <span class=\"ltx_text ltx_font_bold\">S</span>peak and <span class=\"ltx_text ltx_font_bold\">A</span>ct), the first end-to-end model capable of simultaneous listening, looking, speaking, and acting. ELLSA adopts a full-duplex, streaming architecture for multimodal interaction, continuously processing visual and auditory inputs while generating speech and actions in parallel. This enables behaviors previously unattainable for AI agents, such as simultaneously answering questions in both text and speech while performing tasks (&#8220;speaking-while-acting&#8221;), particularly the question can be grounded in the context (&#8220;context-grounded VQA while acting&#8221;) or instantly stopping an action upon hearing an interruptive spoken command (&#8220;action barge-in&#8221;).</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To build ELLSA, we propose a novel architecture called <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">M</span>ixture-<span class=\"ltx_text ltx_font_bold\">o</span>f-<span class=\"ltx_text ltx_font_bold\">E</span>xperts (<span class=\"ltx_text ltx_font_bold\">SA-MoE</span>). SA-MoE enables full-duplex, streaming multimodal Multiple-Input-Multiple-Output (MIMO) interaction by processing multimodal data in an interleaved manner within each time block. To manage the distinct characteristics of different modalities, we employ an MoE framework where specialized modules handle specific data types: a Speech Expert specializes in speech and text processing for dialogue, while an Action Expert focuses on visual and action-related data for manipulation tasks. Crucially, these experts are not isolated. They are integrated through a unified self-attention mechanism, which allows each expert to maintain high performance on its primary task, thereby mitigating modality interference, while still attending to information from other modalities to understand complex cross-modal relationships. Experimental results demonstrate that ELLSA not only delivers competitive performance on a suite of basic tasks including spoken question answering and speech-conditioned robot manipulation, but also unlocks novel interaction capabilities made possible by its MIMO and full-duplex design, such as turn-taking, rejecting infeasible commands, speaking-while-acting and action barge-in. Together, these advancements push the frontier of embodied intelligence toward more natural human&#8211;AI interactions.</p>\n\n",
                "matched_terms": [
                    "results",
                    "question",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ELLSA is trained across a diverse spectrum of tasks, spanning a wide range of multimodal interaction scenarios. Basic tasks include ASR, spoken QA and speech-conditioned robot manipulation. More advanced tasks build upon these foundations, involving speaking-while-acting, context-grounded VQA, defective instruction rejection and action barge-ins. Full details of the training dataset are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2\" title=\"Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. Below are descriptions of advanced tasks and an illustrative example is presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Data and Task Specifications &#8227; 4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking-while-acting</span> In this task, ELLSA is required to generate speech and actions simultaneously, a capability achievable only by models endowed with multiple multimodal generative abilities. This skill is crucial for human-like AGI, as humans naturally engage in such behaviors (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, chatting while washing clothes). In our setup, ELLSA first receives a spoken action instruction and begins executing it. While executing the action, it may receive an additional spoken query. ELLSA must respond verbally to the query while continuing the instructed action without interruption. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Context-grounded VQA</span> This task is a variant of speaking-while-acting, where the query is grounded in the environment rather than being general. Questions are derived from LIBERO LONG <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib31\" title=\"\">2023</a>)</cite>, which requires the model to complete two sequential tasks. During execution, progress-related questions are asked to probe the current state. For example, if the instruction is &#8220;<span class=\"ltx_text ltx_font_italic\">put the black bowl in the bottom drawer of the cabinet and close it</span>&#8221;, a context-specific query could be &#8220;<span class=\"ltx_text ltx_font_italic\">Where is the black bowl now?</span>&#8221; The correct answer depends on the stage of execution and may be &#8220;<span class=\"ltx_text ltx_font_italic\">On the table</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">In my gripper</span>&#8221;, or &#8220;<span class=\"ltx_text ltx_font_italic\">Inside the drawer.</span>&#8221; This scenario requires the integration of all four modalities in ELLSA, highlighting how multimodal MIMO enables natural, human-like interaction. We construct 12 such context-related questions based on 9 LIBERO LONG tasks.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Defective instruction rejection</span> Existing robotic manipulation tasks generally assume that the given instructions are inherently reasonable and feasible. However, in real-world interactions, users sometimes, whether intentionally or inadvertently, issue defective instructions. The capacity to identify and reject such inappropriate commands underscores the necessity for embodied AI models to possess spoken interaction capabilities, while enhancing their robustness and safety in real-world environments. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib41\" title=\"\">2025</a>)</cite>, we consider defective instructions from four dimensions: <span class=\"ltx_text ltx_font_italic\">visual</span>, <span class=\"ltx_text ltx_font_italic\">semantic</span>, <span class=\"ltx_text ltx_font_italic\">motion</span> and <span class=\"ltx_text ltx_font_italic\">out-of-context</span>. This task further evaluates ELLSA&#8217;s capacity for cross-expert understanding. More details of the task are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Action barge-in</span> As another variant of speaking-while-acting, this task introduces interruptive commands such as &#8220;<span class=\"ltx_text ltx_font_italic\">Pause here</span>&#8221; or &#8220;<span class=\"ltx_text ltx_font_italic\">Hold it right there</span>&#8221;. Upon hearing such a command, ELLSA must immediately stop the ongoing action. Barge-in is a natural element of human conversation dynamics and can only be handled effectively by full-duplex models. We choose this task to showcase the full-duplex ability of ELLSA. In our setup, ELLSA explicitly responds with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221;, upon receiving an interruptive command, as an indicator to stop action. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate ELLSA across a broad range of widely used benchmarks, covering both basic capabilities inherited from its individual experts and advanced abilities unique to ELLSA, such as full-duplex interaction, multimodal joint perception and concurrent generation. We report speech-to-text (S2T) and speech-to-speech (S2S) performance on speech interaction tasks. For evaluation metrics, accuracy is used to assess general knowledge QA and context-grounded VQA, while GPTscore is employed for open-ended oral conversations. For robotic manipulation, we measure task success rate, which is also used to evaluate duplex abilities, which reflect ELLSA&#8217;s effectiveness in handling diverse conversational dynamics such as turn-taking and barge-ins. Further details on evaluation benchmarks and metrics are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A3\" title=\"Appendix C Evaluation Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficiency and effectiveness of SA-MoE by comparing it against a dense model trained on all tasks, with detailed results provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6.T7\" title=\"Table 7 &#8227; Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The results clearly show that SA-MoE substantially outperforms the dense baselines, highlighting the advantages of leveraging pretrained experts to reduce modality interference. Additional evidence of SA-MoE&#8217;s effectiveness is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A6\" title=\"Appendix F Further Discussion of SA-MoE &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Based on these observations, we now proceed with a comprehensive evaluation of ELLSA&#8217;s full capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also test ELLSA&#8217;s robot manipulation abilities on LIBERO benchmark. Results demonstrate that ELLSA achieves the highest average performance across all LIBERO task suites. This outcome underscores the effectiveness of SA-MoE in modality integration, since the action expert, previously unfamiliar with speech input, can now successfully execute actions based on spoken instructions. Note that ELLSA&#8217;s evaluation setting differs from that of conventional VLA policies, which are typically text-conditioned and turn-based. ELLSA is tested using speech instructions and needs to decide when to initiate actions by itself, presenting a more natural and challenging scenario.</p>\n\n",
                "matched_terms": [
                    "results",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The complexity increases in the speaking-while-acting scenario, where ELLSA must handle diverse speech inputs during ongoing action execution. When the input is a general question, ELLSA should continue the action while answering (dialogue turn-taking). If the input is an interruptive command, ELLSA is expected to respond with &#8220;<span class=\"ltx_text ltx_font_italic\">Action Cancelled</span>&#8221; and immediately stop the action (action barge-in). In contrast, when no speech is provided, ELLSA should simply proceed with the task while outputting only <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>, which serves as the control condition. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Full-duplex Ability &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c), ELLSA reliably distinguishes between these input types and responds appropriately, demonstrating its strong capacity for full-duplex interaction.</p>\n\n",
                "matched_terms": [
                    "question",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Speaking-while-acting &#8227; 5.2 Advanced Capabilities &#8227; 5 Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports ELLSA&#8217;s performance on its unique concurrent multimodal generation task, speaking-while-acting. The results are averaged over question-answering or manipulation datasets, with detailed outcomes provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A7.T10\" title=\"Table 10 &#8227; Appendix G Further Detailed Results &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Findings indicate that ELLSA is capable of managing this challenging task of producing speech while executing actions, though its performance exhibits a noticeable decline since ELLSA may be distracted when doing two things at once. This drop is particularly visible on more difficult benchmarks, such as LIBERO LONG and TriviaQA.</p>\n\n",
                "matched_terms": [
                    "results",
                    "task",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented ELLSA, the first end-to-end full-duplex model capable of simultaneously listening, looking, speaking, and acting, enabling more natural and human-like multimodal interaction. To build ELLSA, we proposed SA-MoE, a novel architecture that addresses modality interference while enabling fluid cross-modal communication by introducing attention-connected modality-specific experts. This design not only allows ELLSA to achieve competitive performance on standard benchmarks but also unlocks previously unattainable capabilities such as speaking-while-acting, context-grounded VQA, and action barge-ins. By demonstrating that an AI system can coordinate vision, speech, text and action in a real-time full-duplex nature, our work establishes a promising architectural paradigm for developing interactive agents that engage with humans and environments in fundamentally more natural ways, advancing the broader pursuit of truly intelligent embodied systems.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support reproducibility, we provide comprehensive details of the model architecture and specifications, training specifications, and datasets in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#S4\" title=\"4 Experimental Setup &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A1\" title=\"Appendix A Implementation Details of ELLSA &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. All models and datasets used in our work are publicly available. For our unique tasks, context-grounded VQA, defective instruction rejection and action barge-ins, we include additional details in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A4\" title=\"Appendix D Task Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Upon acceptance, we will release the code, model checkpoints, and our synthesized speech samples, ensuring that our work can be reliably reproduced and further explored by the community.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mamba streaming encoder is first pretrained on LibriHeavy <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib25\" title=\"\">2024</a>)</cite> and GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib6\" title=\"\">2021</a>)</cite> for 300k steps with a batch size of 512 before building the speech expert. In stage 1, The speech expert is trained on ASR and speech QA tasks for 40k steps, using a batch size of 512 and a learning rate of <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>. In stage 2, the SA-MoE backbone is trained on a diverse mixture of tasks, including ASR, speech QA, speech-conditioned robot manipulation, speaking-while-acting, context-grounded VQA, defective instruction rejection, and action barge-ins.\nThis stage uses a larger batch size of 1024, a learning rate of <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, and runs for 500 steps. In stage 3, training tasks remain largely the same as in Stage 2, except that speech-conditioned robot manipulation is omitted, since this task does not produce any textual output other than <span class=\"ltx_text ltx_font_typewriter\">&lt;silence&gt;</span>. Here, the batch size is reduced to 256, the learning rate is set to <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, and the training continues for 20k steps. Across all stages, the AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib32\" title=\"\">2019</a>)</cite> is used with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math> and a linear warmup over the first 1% of steps. Training is conducted in bfloat16 precision on A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "task",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Training Details &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For VoiceAssistant-400K and UltraChat, we only retain the first-round QA pairs and remove duplicate queries. The question speech is directly adopt from the dataset, whereas the answer speech is re-synthesized from the text responses using CosyVoice2-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib14\" title=\"\">2024</a>)</cite>. For other datasets, text responses are generated by Llama-3-8B-Instruct, with both the question and answer speech synthesized by CosyVoice2-0.5B. For LIBERO, text instructions are also converted into speech with CosyVoice2-0.5B. For action barge-in, each interruptive command is generated 150 times with CosyVoice2-0.5B for training and 20 times for testing. We additionally employ Whisper-medium-en <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#bib.bib39\" title=\"\">2023</a>)</cite> to filter samples with accurate ASR transcriptions. For defective instruction rejection and context-grounded VQA, annotations are created with Gemini-2.5-Pro. All speakers used for speech synthesis are sampled from LibriHeavy.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "question",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For full-duplex evaluation, turn-taking is considered successful if the model responds within 1 second after the question ends. For the speaking-while-acting evaluation, a speech query is introduced 2&#8211;8 seconds after the initial action instruction. Each test case consists of one action instruction paired with a speech query randomly sampled from the corresponding task suite or QA dataset. To ensure fair comparison with single-task performance, every action task is tested at least 50 times, and each speech query is presented at least once. In context-grounded VQA, this interval extends from 2&#8211;30 seconds. If the inserted speech is a general query, it is randomly drawn from the four spoken QA datasets; if it is an interruptive command, it is randomly selected from the corresponding test set. For full-duplex evaluation of speaking-while-acting, performance is reported as the average success rate across 100 episodes per task suite (10 episodes per task). For each task in every subset of the LIBERO benchmark, defective commands covering the four considered dimensions were generated for evaluation, resulting in a total test set of 160 samples. The model is expected to reject such instructions and provide a justification. For ease of evaluation, however, we did not formally assess the validity of these justifications. Our observations nonetheless suggest that most of the provided justifications are reasonable.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "question",
                    "task",
                    "contextgrounded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For context-grounded VQA, Gemini may occasionally misjudge accuracy because it processes videos at only 1 fps and therefore cannot take all frames into account, potentially missing essential contextual information.</p>\n\n",
                "matched_terms": [
                    "vqa",
                    "contextgrounded",
                    "gemini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For experiments, we use LLM to generate training data (Llama-3-Instruct and Gemini-2.5-Pro) and evaluate results for AlpacaEval (GPT-4.1-2025-04-14) and context-grounded VQA (Gemini-2.5-Pro), with prompts provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16756v1#A5\" title=\"Appendix E Prompts &#8227; End-to-end Listen, Look, Speak and Act\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "vqa",
                    "contextgrounded"
                ]
            }
        ]
    }
}