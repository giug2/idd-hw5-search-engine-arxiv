{
    "S2.T1": {
        "caption": "Table 1: Comparison of Speech Datasets in Terms of Fine-Grained Speech Style Control",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Spoken</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Dialogue</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "dialogue",
            "finegrained",
            "style",
            "terms",
            "control",
            "speech",
            "spoken",
            "datasets",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spoken",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "control",
                    "speech",
                    "spoken",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "control",
                    "speech",
                    "spoken",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "control",
                    "style",
                    "speech",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "control",
                    "style",
                    "speech",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "dialogue",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spoken",
                    "control",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spoken",
                    "control",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "dialogue",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "control",
                    "dialogue",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "control",
                    "dialogue",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "control",
                    "style",
                    "speech",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "control",
                    "comparison",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "style",
                    "finegrained",
                    "control",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The UltraVoice dataset is generated via a fully synthetic pipeline, employing GPT-4o for text creation and multiple Text-to-Speech (TTS) engines for audio synthesis. This approach ensures that the dataset contains no personally identifiable information or the voices of real individuals, thereby circumventing the privacy concerns and copyright issues often associated with human-derived data. The content is created and intended strictly for academic research on controllable spoken dialogue systems.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the configurations used for the Supervised Fine-tuning (SFT) of the Spoken Dialogue and Controllable TTS models, as mentioned in our experiments ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "dialogue"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: TTS model selections for different control dimensions.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Response</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">TTS Model</span></td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "tts",
            "model",
            "response",
            "dimensions",
            "control",
            "different",
            "selections"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model",
                    "tts",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "response",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "response",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "control",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model",
                    "tts",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "control",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "control",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "control",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "control",
                    "tts"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Detailed statistics of UltraVoice across different control dimensions. #Cnt. denotes the number of samples, Dur. is the total duration in hours, CER is the average character error rate, and UTMOS represents the averaged naturalness score. AU, CA, GB, IN, SG, and ZA correspond to accents from Australia, Canada, United Kingdom, India, Singapore, and South Africa, respectively.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dimension</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\"><span class=\"ltx_text ltx_font_bold\">Fine-grained Control Dimensions</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">#Cnt.</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dur.(h)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Emotion</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">Neutral, Happy, Sad, Angry, Surprised, Fearful, Disgusted</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">21,209</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">182.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">6.17</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">3.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Volume</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">Low Volume, High Volume, Normal Volume</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11,154</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">91.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">5.29</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">3.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Speed</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">Slow Speed, Fast Speed, Normal Speed</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">10,334</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">85.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">4.84</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Accent</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">AU, CA, GB, IN, SG, ZA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">26,839</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">253.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">6.69</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">4.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Language</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">Chinese, Korean, Japanese</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11,153</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">93.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">6.83</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">3.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Composite</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">Combinations of speed, volume, and emotion</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">4,143</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">33.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">5.02</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">3.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">General QA</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:229.8pt;\">English general question answering</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">15,938</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">93.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">6.69</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">4.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">Overall</span></td>\n<td class=\"ltx_td ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"/>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">100,770</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">832.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">5.93</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">4.00</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "dur",
            "cnt",
            "utmos",
            "singapore",
            "disgusted",
            "high",
            "naturalness",
            "denotes",
            "control",
            "answering",
            "overall",
            "accents",
            "general",
            "united",
            "error",
            "india",
            "angry",
            "hours",
            "australia",
            "rate",
            "kingdom",
            "average",
            "low",
            "statistics",
            "finegrained",
            "character",
            "slow",
            "durh",
            "fast",
            "accent",
            "from",
            "speed",
            "korean",
            "represents",
            "south",
            "across",
            "dimension",
            "fearful",
            "normal",
            "composite",
            "happy",
            "language",
            "samples",
            "english",
            "averaged",
            "number",
            "surprised",
            "africa",
            "emotion",
            "respectively",
            "score",
            "volume",
            "combinations",
            "total",
            "ultravoice",
            "neutral",
            "dimensions",
            "duration",
            "canada",
            "question",
            "cer",
            "correspond",
            "different",
            "japanese",
            "chinese",
            "detailed",
            "sad"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
            "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "score",
                    "volume",
                    "hours",
                    "composite",
                    "rate",
                    "language",
                    "average",
                    "ultravoice",
                    "finegrained",
                    "high",
                    "answering",
                    "accent",
                    "control",
                    "speed",
                    "dimensions",
                    "question",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "neutral",
                    "language",
                    "emotion",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "question",
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "across",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "composite",
                    "language",
                    "ultravoice",
                    "finegrained",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "general",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "finegrained",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "combinations",
                    "volume",
                    "composite",
                    "language",
                    "ultravoice",
                    "finegrained",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "general",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "high",
                    "control",
                    "ultravoice",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "question",
                    "from",
                    "ultravoice",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "finegrained",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "dimension",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "across",
                    "finegrained",
                    "high",
                    "control",
                    "from",
                    "dimension",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "samples",
                    "character",
                    "overall",
                    "high",
                    "duration",
                    "control",
                    "cer",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "total",
                    "ultravoice",
                    "statistics",
                    "duration",
                    "control",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "volume",
                    "composite",
                    "language",
                    "average",
                    "accent",
                    "duration",
                    "control",
                    "from",
                    "speed",
                    "cer",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "ultravoice",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "detailed",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "across",
                    "samples",
                    "ultravoice",
                    "finegrained",
                    "dimensions",
                    "control",
                    "from",
                    "dimension",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "high",
                    "control",
                    "rate",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "score",
                    "rate",
                    "overall",
                    "naturalness",
                    "emotion",
                    "from",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "score",
                    "ultravoice",
                    "finegrained",
                    "dimensions",
                    "control",
                    "from",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "score",
                    "volume",
                    "ultravoice",
                    "finegrained",
                    "overall",
                    "naturalness",
                    "high",
                    "control",
                    "from",
                    "dimensions",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "language",
                    "overall",
                    "control",
                    "from",
                    "dimension",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "across",
                    "average",
                    "ultravoice",
                    "control",
                    "from",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "ultravoice",
                    "finegrained",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "from",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "composite",
                    "rate",
                    "ultravoice",
                    "high",
                    "naturalness",
                    "accent",
                    "control",
                    "from",
                    "speed",
                    "dimensions",
                    "detailed",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "represents",
                    "ultravoice",
                    "finegrained",
                    "high",
                    "naturalness",
                    "overall",
                    "control",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "ultravoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Grammar and Language Refinement:</span> LLMs were employed to proofread the manuscript for grammatical errors, spelling mistakes, and awkward phrasing. This use was intended to improve the clarity, readability, and overall quality of the written text.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "language"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: MOS results across six fine-grained speech style control dimensions for each model. The third row of each group shows the relative gain (%) achieved by SFT.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Volume</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Composite</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">GPT-4o(Ground Truth)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SLAM-Omni-0.5B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SLAM-Omni-0.5B SFT</td>\n<td class=\"ltx_td ltx_align_center\">3.61</td>\n<td class=\"ltx_td ltx_align_center\">1.18</td>\n<td class=\"ltx_td ltx_align_center\">3.47</td>\n<td class=\"ltx_td ltx_align_center\">3.32</td>\n<td class=\"ltx_td ltx_align_center\">3.42</td>\n<td class=\"ltx_td ltx_align_center\">3.37</td>\n<td class=\"ltx_td ltx_align_center\">3.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+39.92%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+4.42%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+40.49%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+53.00%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+53.36%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+44.64%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+42.33%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-1B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-1B SFT</td>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n<td class=\"ltx_td ltx_align_center\">1.01</td>\n<td class=\"ltx_td ltx_align_center\">3.98</td>\n<td class=\"ltx_td ltx_align_center\">3.73</td>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_align_center\">3.95</td>\n<td class=\"ltx_td ltx_align_center\">3.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+24.06%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-14.41%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+28.39%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+54.13%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+37.59%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+39.58%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+31.68%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-7B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-7B SFT</td>\n<td class=\"ltx_td ltx_align_center\">4.25</td>\n<td class=\"ltx_td ltx_align_center\">2.19</td>\n<td class=\"ltx_td ltx_align_center\">3.95</td>\n<td class=\"ltx_td ltx_align_center\">3.79</td>\n<td class=\"ltx_td ltx_align_center\">3.88</td>\n<td class=\"ltx_td ltx_align_center\">3.51</td>\n<td class=\"ltx_td ltx_align_center\">3.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+13.33%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+33.54%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+41.07%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+56.61%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+23.96%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+34.48%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+31.50%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-8B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-8B SFT</td>\n<td class=\"ltx_td ltx_align_center\">4.52</td>\n<td class=\"ltx_td ltx_align_center\">1.02</td>\n<td class=\"ltx_td ltx_align_center\">4.21</td>\n<td class=\"ltx_td ltx_align_center\">4.10</td>\n<td class=\"ltx_td ltx_align_center\">4.19</td>\n<td class=\"ltx_td ltx_align_center\">4.07</td>\n<td class=\"ltx_td ltx_align_center\">3.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+26.61%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-12.82%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+34.94%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+41.38%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+20.75%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+42.31%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+29.12%</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "six",
            "style",
            "group",
            "control",
            "each",
            "row",
            "vocalnet1b",
            "mos",
            "vocalnet7b",
            "base",
            "finegrained",
            "accent",
            "third",
            "speed",
            "gain",
            "vocalnet8b",
            "truth",
            "across",
            "avg",
            "composite",
            "delta",
            "language",
            "gpt4oground",
            "sft",
            "results",
            "speech",
            "emotion",
            "relative",
            "volume",
            "achieved",
            "model",
            "dimensions",
            "shows",
            "slamomni05b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "across",
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control",
                    "results",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "control",
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "dimensions",
                    "control",
                    "third",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "sft",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "style",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "style",
                    "accent",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "finegrained",
                    "style",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "model",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "style",
                    "control",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "base",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "each",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "finegrained",
                    "style",
                    "control",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "composite",
                    "language",
                    "model",
                    "style",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finegrained",
                    "sft",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sft",
                    "control",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "across",
                    "six",
                    "finegrained",
                    "dimensions",
                    "control",
                    "each",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mos",
                    "control",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "vocalnet7b",
                    "finegrained",
                    "style",
                    "dimensions",
                    "control",
                    "slamomni05b",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "volume",
                    "vocalnet7b",
                    "language",
                    "control",
                    "slamomni05b",
                    "speech",
                    "vocalnet1b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "across",
                    "vocalnet7b",
                    "model",
                    "style",
                    "sft",
                    "control",
                    "slamomni05b",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "sft",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "across",
                    "volume",
                    "composite",
                    "achieved",
                    "model",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "control",
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "model"
                ]
            }
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Evaluation of our SFT models (upper part) and existing strong baselines (lower part) on URO-Bench (EN). Und.: Understanding. Conv.: Oral Conversation.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Basic</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Pro</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Und. &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Reasoning &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Conv. &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Avg. &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Und. &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Reasoning &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Conv. &#8593;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Avg. &#8593;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SLAM-Omni-0.5B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SLAM-Omni-0.5B SFT</td>\n<td class=\"ltx_td ltx_align_center\">31.51</td>\n<td class=\"ltx_td ltx_align_center\">24.58</td>\n<td class=\"ltx_td ltx_align_center\">50.14</td>\n<td class=\"ltx_td ltx_align_center\">35.41</td>\n<td class=\"ltx_td ltx_align_center\">26.30</td>\n<td class=\"ltx_td ltx_align_center\">20.07</td>\n<td class=\"ltx_td ltx_align_center\">35.57</td>\n<td class=\"ltx_td ltx_align_center\">27.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+18.46%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+5.22%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+5.47%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+8.95%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+1.98%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-18.81%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+18.84%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+1.87%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-1B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-1B SFT</td>\n<td class=\"ltx_td ltx_align_center\">70.41</td>\n<td class=\"ltx_td ltx_align_center\">45.19</td>\n<td class=\"ltx_td ltx_align_center\">70.81</td>\n<td class=\"ltx_td ltx_align_center\">62.14</td>\n<td class=\"ltx_td ltx_align_center\">36.06</td>\n<td class=\"ltx_td ltx_align_center\">51.42</td>\n<td class=\"ltx_td ltx_align_center\">41.08</td>\n<td class=\"ltx_td ltx_align_center\">42.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+20.69%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+8.40%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+5.94%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+11.73%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+3.38%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+9.73%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+5.44%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+6.51%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-7B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">81.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-7B SFT</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">71.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">84.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">81.56</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">46.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.52</span></td>\n<td class=\"ltx_td ltx_align_center\">47.20</td>\n<td class=\"ltx_td ltx_align_center\">52.70</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+8.85%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+12.13%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+7.28%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+9.24%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+22.40%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+9.60%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+4.33%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+11.32%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-8B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-8B SFT</td>\n<td class=\"ltx_td ltx_align_center\">72.37</td>\n<td class=\"ltx_td ltx_align_center\">61.52</td>\n<td class=\"ltx_td ltx_align_center\">80.87</td>\n<td class=\"ltx_td ltx_align_center\">71.59</td>\n<td class=\"ltx_td ltx_align_center\">40.57</td>\n<td class=\"ltx_td ltx_align_center\">62.07</td>\n<td class=\"ltx_td ltx_align_center\">48.50</td>\n<td class=\"ltx_td ltx_align_center\">50.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> (%)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+10.45%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+14.86%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+7.01%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+10.34%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+6.88%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+16.41%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+14.31%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+13.04%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Qwen2.5-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">66.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">69.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">76.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">70.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">44.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">63.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">49.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">52.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LLaMA-Omni-8B</td>\n<td class=\"ltx_td ltx_align_center\">47.45</td>\n<td class=\"ltx_td ltx_align_center\">36.03</td>\n<td class=\"ltx_td ltx_align_center\">64.98</td>\n<td class=\"ltx_td ltx_align_center\">49.49</td>\n<td class=\"ltx_td ltx_align_center\">28.85</td>\n<td class=\"ltx_td ltx_align_center\">47.62</td>\n<td class=\"ltx_td ltx_align_center\">34.47</td>\n<td class=\"ltx_td ltx_align_center\">36.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">GLM4-Voice-9B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">82.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">55.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">74.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">61.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">57.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">54.75</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "glm4voice9b",
            "baselines",
            "basic",
            "evaluation",
            "vocalnet1b",
            "pro",
            "our",
            "vocalnet7b",
            "urobench",
            "base",
            "strong",
            "understanding",
            "oral",
            "llamaomni8b",
            "vocalnet8b",
            "avg",
            "delta",
            "sft",
            "lower",
            "reasoning",
            "conversation",
            "conv",
            "existing",
            "und",
            "qwen25omni7b",
            "part",
            "upper",
            "slamomni05b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "urobench",
                    "basic",
                    "understanding",
                    "reasoning",
                    "pro",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "models",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "models",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "models",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "urobench",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sft",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "urobench",
                    "understanding",
                    "oral",
                    "evaluation",
                    "sft",
                    "reasoning",
                    "conversation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vocalnet7b",
                    "slamomni05b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vocalnet7b",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vocalnet1b",
                    "vocalnet7b",
                    "slamomni05b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "strong",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "urobench",
                    "strong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate potential misuse, we will release the UltraVoice dataset and our models under a research-only license. This license explicitly prohibits malicious applications, including but not limited to creating misinformation, engaging in fraudulent activities, or impersonating individuals without their explicit consent. The authors bear no responsibility for any misuse or harmful interpretations of the dataset or its derivatives.</p>\n\n",
                "matched_terms": [
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "sft",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the configurations used for the Supervised Fine-tuning (SFT) of the Spoken Dialogue and Controllable TTS models, as mentioned in our experiments ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "sft",
                    "our"
                ]
            }
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Performance of our UltraVoice-0.5B-SFT model on emotional TTS tasks. The evaluation is conducted on both an out-of-domain test set (EmoVoice-DB, top) and an in-domain test set (UltraVoice, bottom). Bold and underlined values denote the best and second-best results, respectively.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">UltraVoice</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "outofdomain",
            "evaluation",
            "our",
            "tts",
            "emotional",
            "tasks",
            "test",
            "performance",
            "indomain",
            "conducted",
            "top",
            "bottom",
            "bold",
            "denote",
            "both",
            "results",
            "ultravoice05bsft",
            "underlined",
            "values",
            "set",
            "respectively",
            "emovoicedb",
            "secondbest",
            "model",
            "ultravoice",
            "best"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "ultravoice",
                    "tasks",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "model",
                    "emovoicedb",
                    "both",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emovoicedb",
                    "emotional",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "both",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "ultravoice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our",
                    "tts",
                    "model",
                    "ultravoice",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "set",
                    "our",
                    "ultravoice",
                    "evaluation",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "ultravoice",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "emovoicedb",
                    "tts",
                    "model",
                    "ultravoice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "tasks",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The UltraVoice dataset is generated via a fully synthetic pipeline, employing GPT-4o for text creation and multiple Text-to-Speech (TTS) engines for audio synthesis. This approach ensures that the dataset contains no personally identifiable information or the voices of real individuals, thereby circumventing the privacy concerns and copyright issues often associated with human-derived data. The content is created and intended strictly for academic research on controllable spoken dialogue systems.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate potential misuse, we will release the UltraVoice dataset and our models under a research-only license. This license explicitly prohibits malicious applications, including but not limited to creating misinformation, engaging in fraudulent activities, or impersonating individuals without their explicit consent. The authors bear no responsibility for any misuse or harmful interpretations of the dataset or its derivatives.</p>\n\n",
                "matched_terms": [
                    "ultravoice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "ultravoice",
                    "evaluation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the configurations used for the Supervised Fine-tuning (SFT) of the Spoken Dialogue and Controllable TTS models, as mentioned in our experiments ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "tts",
                    "our"
                ]
            }
        ]
    },
    "S4.T7": {
        "caption": "Table 7: MOS and IFR results of UltraVoice-0.5B-SFT across five style dimensions.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Volume</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Composite</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IFR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IFR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IFR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IFR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IFR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">EmoVoice-0.5B-Pre&#8211;trained</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">UltraVoice-0.5B-SFT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">67.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">88.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">98.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">85.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.00</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "mos",
            "across",
            "volume",
            "composite",
            "model",
            "style",
            "five",
            "accent",
            "dimensions",
            "speed",
            "results",
            "ifr",
            "ultravoice05bsft",
            "emovoice05bpretrained",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "across",
                    "volume",
                    "composite",
                    "model",
                    "style",
                    "accent",
                    "dimensions",
                    "speed",
                    "ifr",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "composite",
                    "model",
                    "style",
                    "accent",
                    "dimensions",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speed",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "accent",
                    "speed",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "speed",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "style",
                    "accent",
                    "dimensions",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "style",
                    "accent",
                    "dimensions",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "composite",
                    "style",
                    "accent",
                    "dimensions",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "model",
                    "style",
                    "accent",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "model",
                    "ifr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "ifr",
                    "results",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "volume",
                    "dimensions",
                    "results",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "volume",
                    "ifr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "model",
                    "style",
                    "five",
                    "accent",
                    "dimensions",
                    "speed",
                    "emotion"
                ]
            }
        ]
    },
    "A2.T8": {
        "caption": "Table 8: Detailed statistics for each fine-grained control dimension, including sample count (#Cnt.), duration in hours (Dur.(h)), CER, and UTMOS.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dimension</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Sub</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Dimension</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">#Cnt.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dur.(h)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"7\">Emotion</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Angry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3097</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Disgusted</td>\n<td class=\"ltx_td ltx_align_center\">3032</td>\n<td class=\"ltx_td ltx_align_center\">26.03</td>\n<td class=\"ltx_td ltx_align_center\">5.83</td>\n<td class=\"ltx_td ltx_align_center\">3.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fearful</td>\n<td class=\"ltx_td ltx_align_center\">2590</td>\n<td class=\"ltx_td ltx_align_center\">23.49</td>\n<td class=\"ltx_td ltx_align_center\">6.74</td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Happy</td>\n<td class=\"ltx_td ltx_align_center\">3097</td>\n<td class=\"ltx_td ltx_align_center\">27.05</td>\n<td class=\"ltx_td ltx_align_center\">6.14</td>\n<td class=\"ltx_td ltx_align_center\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Neutral</td>\n<td class=\"ltx_td ltx_align_center\">3848</td>\n<td class=\"ltx_td ltx_align_center\">31.75</td>\n<td class=\"ltx_td ltx_align_center\">4.55</td>\n<td class=\"ltx_td ltx_align_center\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Sad</td>\n<td class=\"ltx_td ltx_align_center\">2147</td>\n<td class=\"ltx_td ltx_align_center\">19.40</td>\n<td class=\"ltx_td ltx_align_center\">5.01</td>\n<td class=\"ltx_td ltx_align_center\">3.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Surprised</td>\n<td class=\"ltx_td ltx_align_center\">3398</td>\n<td class=\"ltx_td ltx_align_center\">28.59</td>\n<td class=\"ltx_td ltx_align_center\">7.75</td>\n<td class=\"ltx_td ltx_align_center\">4.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Volume</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">High</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3575</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Low</td>\n<td class=\"ltx_td ltx_align_center\">3622</td>\n<td class=\"ltx_td ltx_align_center\">30.10</td>\n<td class=\"ltx_td ltx_align_center\">5.07</td>\n<td class=\"ltx_td ltx_align_center\">3.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Normal</td>\n<td class=\"ltx_td ltx_align_center\">3957</td>\n<td class=\"ltx_td ltx_align_center\">31.73</td>\n<td class=\"ltx_td ltx_align_center\">4.87</td>\n<td class=\"ltx_td ltx_align_center\">4.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Speed</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Fast</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4370</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Normal</td>\n<td class=\"ltx_td ltx_align_center\">3864</td>\n<td class=\"ltx_td ltx_align_center\">31.32</td>\n<td class=\"ltx_td ltx_align_center\">4.59</td>\n<td class=\"ltx_td ltx_align_center\">4.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Slow</td>\n<td class=\"ltx_td ltx_align_center\">2100</td>\n<td class=\"ltx_td ltx_align_center\">19.48</td>\n<td class=\"ltx_td ltx_align_center\">4.51</td>\n<td class=\"ltx_td ltx_align_center\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"6\">Accent</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">AU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4683</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CA</td>\n<td class=\"ltx_td ltx_align_center\">4844</td>\n<td class=\"ltx_td ltx_align_center\">45.41</td>\n<td class=\"ltx_td ltx_align_center\">6.63</td>\n<td class=\"ltx_td ltx_align_center\">4.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GB</td>\n<td class=\"ltx_td ltx_align_center\">4953</td>\n<td class=\"ltx_td ltx_align_center\">46.10</td>\n<td class=\"ltx_td ltx_align_center\">5.19</td>\n<td class=\"ltx_td ltx_align_center\">4.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IN</td>\n<td class=\"ltx_td ltx_align_center\">4128</td>\n<td class=\"ltx_td ltx_align_center\">39.62</td>\n<td class=\"ltx_td ltx_align_center\">5.75</td>\n<td class=\"ltx_td ltx_align_center\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SG</td>\n<td class=\"ltx_td ltx_align_center\">3702</td>\n<td class=\"ltx_td ltx_align_center\">35.37</td>\n<td class=\"ltx_td ltx_align_center\">9.38</td>\n<td class=\"ltx_td ltx_align_center\">4.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ZA</td>\n<td class=\"ltx_td ltx_align_center\">4529</td>\n<td class=\"ltx_td ltx_align_center\">42.92</td>\n<td class=\"ltx_td ltx_align_center\">6.45</td>\n<td class=\"ltx_td ltx_align_center\">4.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Language</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4388</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Japanese</td>\n<td class=\"ltx_td ltx_align_center\">2468</td>\n<td class=\"ltx_td ltx_align_center\">22.19</td>\n<td class=\"ltx_td ltx_align_center\">10.81</td>\n<td class=\"ltx_td ltx_align_center\">3.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Korean</td>\n<td class=\"ltx_td ltx_align_center\">4297</td>\n<td class=\"ltx_td ltx_align_center\">35.81</td>\n<td class=\"ltx_td ltx_align_center\">5.79</td>\n<td class=\"ltx_td ltx_align_center\">3.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Composite</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">General QA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">15938</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">93.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">6.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.15</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "cnt",
            "utmos",
            "disgusted",
            "high",
            "control",
            "each",
            "sub",
            "general",
            "angry",
            "hours",
            "low",
            "slow",
            "statistics",
            "finegrained",
            "korean",
            "accent",
            "durh",
            "fast",
            "speed",
            "count",
            "fearful",
            "normal",
            "composite",
            "happy",
            "language",
            "surprised",
            "emotion",
            "volume",
            "including",
            "neutral",
            "duration",
            "cer",
            "dimension",
            "japanese",
            "chinese",
            "detailed",
            "sad"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "hours",
                    "composite",
                    "language",
                    "finegrained",
                    "high",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "neutral",
                    "language",
                    "dimension",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "finegrained",
                    "including",
                    "accent",
                    "control",
                    "speed",
                    "general",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speed",
                    "including",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "finegrained",
                    "including",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "including",
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "general",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "high",
                    "control",
                    "each",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "each",
                    "dimension",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "high",
                    "control",
                    "each",
                    "dimension",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "high",
                    "duration",
                    "control",
                    "each",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "disgusted",
                    "high",
                    "control",
                    "general",
                    "angry",
                    "low",
                    "slow",
                    "korean",
                    "accent",
                    "fast",
                    "speed",
                    "fearful",
                    "normal",
                    "composite",
                    "happy",
                    "language",
                    "surprised",
                    "emotion",
                    "volume",
                    "neutral",
                    "dimension",
                    "japanese",
                    "chinese",
                    "sad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "count",
                    "hours",
                    "statistics",
                    "duration",
                    "control",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "high",
                    "duration",
                    "control",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "volume",
                    "composite",
                    "language",
                    "accent",
                    "duration",
                    "control",
                    "speed",
                    "cer",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "control",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "control",
                    "each",
                    "dimension",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "high",
                    "control",
                    "each",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "control",
                    "detailed",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "finegrained",
                    "high",
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "control",
                    "volume",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "control",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "high",
                    "accent",
                    "control",
                    "speed",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "high",
                    "control",
                    "general",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "including"
                ]
            }
        ]
    },
    "A2.T9": {
        "caption": "Table 9: Detailed statistics for each fine-grained control dimension, showing the mean duration (Dur.) and word count for the full dialogue (Dia.), instruction (Instr.), and response (Resp.).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Dimension</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Sub</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Dimension</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Mean Dur.(s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Mean Word Count</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Dia.</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Instr.</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Resp.</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Dia.</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Instr.</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Resp.</span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"7\">Emotion</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Angry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Disgusted</td>\n<td class=\"ltx_td ltx_align_center\">30.90</td>\n<td class=\"ltx_td ltx_align_center\">10.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">20.18</td>\n<td class=\"ltx_td ltx_align_center\">65.08</td>\n<td class=\"ltx_td ltx_align_center\">29.07</td>\n<td class=\"ltx_td ltx_align_center\">36.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Fearful</td>\n<td class=\"ltx_td ltx_align_center\">32.65</td>\n<td class=\"ltx_td ltx_align_center\">10.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.70</td>\n<td class=\"ltx_td ltx_align_center\">72.81</td>\n<td class=\"ltx_td ltx_align_center\">28.98</td>\n<td class=\"ltx_td ltx_align_center\">43.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Happy</td>\n<td class=\"ltx_td ltx_align_center\">31.44</td>\n<td class=\"ltx_td ltx_align_center\">11.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">20.40</td>\n<td class=\"ltx_td ltx_align_center\">72.81</td>\n<td class=\"ltx_td ltx_align_center\">30.20</td>\n<td class=\"ltx_td ltx_align_center\">42.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Neutral</td>\n<td class=\"ltx_td ltx_align_center\">29.70</td>\n<td class=\"ltx_td ltx_align_center\">11.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.38</td>\n<td class=\"ltx_td ltx_align_center\">66.55</td>\n<td class=\"ltx_td ltx_align_center\">29.45</td>\n<td class=\"ltx_td ltx_align_center\">37.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Sad</td>\n<td class=\"ltx_td ltx_align_center\">32.52</td>\n<td class=\"ltx_td ltx_align_center\">10.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.44</td>\n<td class=\"ltx_td ltx_align_center\">64.38</td>\n<td class=\"ltx_td ltx_align_center\">26.94</td>\n<td class=\"ltx_td ltx_align_center\">37.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Surprised</td>\n<td class=\"ltx_td ltx_align_center\">30.29</td>\n<td class=\"ltx_td ltx_align_center\">11.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.12</td>\n<td class=\"ltx_td ltx_align_center\">68.29</td>\n<td class=\"ltx_td ltx_align_center\">29.27</td>\n<td class=\"ltx_td ltx_align_center\">39.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Volume</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">High</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Low</td>\n<td class=\"ltx_td ltx_align_center\">29.91</td>\n<td class=\"ltx_td ltx_align_center\">11.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.44</td>\n<td class=\"ltx_td ltx_align_center\">64.95</td>\n<td class=\"ltx_td ltx_align_center\">30.90</td>\n<td class=\"ltx_td ltx_align_center\">34.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Normal</td>\n<td class=\"ltx_td ltx_align_center\">28.87</td>\n<td class=\"ltx_td ltx_align_center\">11.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.31</td>\n<td class=\"ltx_td ltx_align_center\">65.95</td>\n<td class=\"ltx_td ltx_align_center\">30.56</td>\n<td class=\"ltx_td ltx_align_center\">35.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Speed</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Fast</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Normal</td>\n<td class=\"ltx_td ltx_align_center\">29.18</td>\n<td class=\"ltx_td ltx_align_center\">11.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.80</td>\n<td class=\"ltx_td ltx_align_center\">67.66</td>\n<td class=\"ltx_td ltx_align_center\">30.37</td>\n<td class=\"ltx_td ltx_align_center\">37.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Slow</td>\n<td class=\"ltx_td ltx_align_center\">33.39</td>\n<td class=\"ltx_td ltx_align_center\">10.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.72</td>\n<td class=\"ltx_td ltx_align_center\">62.83</td>\n<td class=\"ltx_td ltx_align_center\">28.42</td>\n<td class=\"ltx_td ltx_align_center\">34.41</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"6\">Accent</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">AU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">83.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">CA</td>\n<td class=\"ltx_td ltx_align_center\">33.75</td>\n<td class=\"ltx_td ltx_align_center\">13.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.91</td>\n<td class=\"ltx_td ltx_align_center\">87.24</td>\n<td class=\"ltx_td ltx_align_center\">37.41</td>\n<td class=\"ltx_td ltx_align_center\">49.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">GB</td>\n<td class=\"ltx_td ltx_align_center\">33.51</td>\n<td class=\"ltx_td ltx_align_center\">14.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.11</td>\n<td class=\"ltx_td ltx_align_center\">84.47</td>\n<td class=\"ltx_td ltx_align_center\">38.11</td>\n<td class=\"ltx_td ltx_align_center\">46.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">IN</td>\n<td class=\"ltx_td ltx_align_center\">34.55</td>\n<td class=\"ltx_td ltx_align_center\">13.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.52</td>\n<td class=\"ltx_td ltx_align_center\">80.00</td>\n<td class=\"ltx_td ltx_align_center\">34.89</td>\n<td class=\"ltx_td ltx_align_center\">45.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">SG</td>\n<td class=\"ltx_td ltx_align_center\">34.39</td>\n<td class=\"ltx_td ltx_align_center\">13.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.22</td>\n<td class=\"ltx_td ltx_align_center\">80.20</td>\n<td class=\"ltx_td ltx_align_center\">35.28</td>\n<td class=\"ltx_td ltx_align_center\">44.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ZA</td>\n<td class=\"ltx_td ltx_align_center\">34.12</td>\n<td class=\"ltx_td ltx_align_center\">13.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">20.55</td>\n<td class=\"ltx_td ltx_align_center\">83.72</td>\n<td class=\"ltx_td ltx_align_center\">36.67</td>\n<td class=\"ltx_td ltx_align_center\">47.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">Language</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">99.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Japanese</td>\n<td class=\"ltx_td ltx_align_center\">32.37</td>\n<td class=\"ltx_td ltx_align_center\">12.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.85</td>\n<td class=\"ltx_td ltx_align_center\">70.95</td>\n<td class=\"ltx_td ltx_align_center\">31.17</td>\n<td class=\"ltx_td ltx_align_center\">39.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Korean</td>\n<td class=\"ltx_td ltx_align_center\">30.00</td>\n<td class=\"ltx_td ltx_align_center\">12.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.68</td>\n<td class=\"ltx_td ltx_align_center\">114.80</td>\n<td class=\"ltx_td ltx_align_center\">32.23</td>\n<td class=\"ltx_td ltx_align_center\">82.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Composite</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">General QA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">EN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">21.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">5.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">15.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">58.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">14.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">43.56</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "dur",
            "full",
            "word",
            "dialogue",
            "disgusted",
            "high",
            "control",
            "each",
            "sub",
            "instruction",
            "general",
            "angry",
            "instr",
            "low",
            "showing",
            "slow",
            "statistics",
            "finegrained",
            "korean",
            "accent",
            "fast",
            "speed",
            "count",
            "fearful",
            "normal",
            "composite",
            "happy",
            "language",
            "response",
            "mean",
            "durs",
            "surprised",
            "emotion",
            "volume",
            "neutral",
            "duration",
            "dia",
            "resp",
            "dimension",
            "japanese",
            "chinese",
            "detailed",
            "sad"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "dialogue",
                    "finegrained",
                    "high",
                    "accent",
                    "mean",
                    "control",
                    "speed",
                    "instruction",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "dialogue",
                    "neutral",
                    "dimension",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "response",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "dialogue",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "general",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "dialogue",
                    "finegrained",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "full",
                    "volume",
                    "composite",
                    "language",
                    "dialogue",
                    "response",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "general",
                    "instruction",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "high",
                    "control",
                    "each",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "language",
                    "response",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "each",
                    "dimension",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "response",
                    "finegrained",
                    "high",
                    "control",
                    "each",
                    "detailed",
                    "dimension",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "response",
                    "high",
                    "duration",
                    "control",
                    "each",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "disgusted",
                    "high",
                    "control",
                    "general",
                    "angry",
                    "low",
                    "slow",
                    "korean",
                    "accent",
                    "fast",
                    "speed",
                    "fearful",
                    "normal",
                    "composite",
                    "happy",
                    "language",
                    "surprised",
                    "emotion",
                    "volume",
                    "neutral",
                    "dimension",
                    "japanese",
                    "chinese",
                    "sad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "count",
                    "word",
                    "dialogue",
                    "statistics",
                    "duration",
                    "control",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "high",
                    "mean",
                    "duration",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "word",
                    "composite",
                    "language",
                    "showing",
                    "accent",
                    "mean",
                    "duration",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "response",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "finegrained",
                    "control",
                    "each",
                    "dimension",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "high",
                    "mean",
                    "control",
                    "each",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "control",
                    "dialogue",
                    "finegrained",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "showing",
                    "finegrained",
                    "high",
                    "control",
                    "instruction",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "full",
                    "volume",
                    "language",
                    "control",
                    "dimension",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "general",
                    "control",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "composite",
                    "finegrained",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "word",
                    "composite",
                    "high",
                    "accent",
                    "control",
                    "speed",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "full",
                    "dialogue",
                    "finegrained",
                    "high",
                    "control",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "full",
                    "dialogue",
                    "detailed"
                ]
            }
        ]
    },
    "A3.T10": {
        "caption": "Table 10: Detailed IFR (%) results across six fine-grained speech style control dimensions for each model.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Volume</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Accent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Description</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Overall</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">GPT-4o (Ground Truth)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">92.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">89.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SLAM-Omni-0.5B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SLAM-Omni-0.5B SFT</td>\n<td class=\"ltx_td ltx_align_center\">92.00</td>\n<td class=\"ltx_td ltx_align_center\">30.33</td>\n<td class=\"ltx_td ltx_align_center\">70.33</td>\n<td class=\"ltx_td ltx_align_center\">58.71</td>\n<td class=\"ltx_td ltx_align_center\">88.33</td>\n<td class=\"ltx_td ltx_align_center\">54.00</td>\n<td class=\"ltx_td ltx_align_center\">68.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+41.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+30.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+29.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+44.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+45.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+31.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+40.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-1B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-1B SFT</td>\n<td class=\"ltx_td ltx_align_center\">89.00</td>\n<td class=\"ltx_td ltx_align_center\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">65.00</td>\n<td class=\"ltx_td ltx_align_center\">50.00</td>\n<td class=\"ltx_td ltx_align_center\">68.67</td>\n<td class=\"ltx_td ltx_align_center\">61.00</td>\n<td class=\"ltx_td ltx_align_center\">55.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+17.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#808080;\">+0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+10.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+31.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+19.58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+34.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+19.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-7B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-7B SFT</td>\n<td class=\"ltx_td ltx_align_center\">90.67</td>\n<td class=\"ltx_td ltx_align_center\">34.00</td>\n<td class=\"ltx_td ltx_align_center\">63.00</td>\n<td class=\"ltx_td ltx_align_center\">46.43</td>\n<td class=\"ltx_td ltx_align_center\">59.00</td>\n<td class=\"ltx_td ltx_align_center\">45.00</td>\n<td class=\"ltx_td ltx_align_center\">55.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+15.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+10.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+12.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+24.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+6.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+18.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+14.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VocalNet-8B Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-8B SFT</td>\n<td class=\"ltx_td ltx_align_center\">90.33</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">69.00</td>\n<td class=\"ltx_td ltx_align_center\">56.43</td>\n<td class=\"ltx_td ltx_align_center\">70.67</td>\n<td class=\"ltx_td ltx_align_center\">65.00</td>\n<td class=\"ltx_td ltx_align_center\">59.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+18.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+0.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+14.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+28.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+37.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#008080;\">+14.61</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "six",
            "style",
            "overall",
            "control",
            "each",
            "vocalnet1b",
            "vocalnet7b",
            "base",
            "finegrained",
            "accent",
            "vocalnet8b",
            "speed",
            "truth",
            "across",
            "delta",
            "language",
            "sft",
            "results",
            "speech",
            "emotion",
            "volume",
            "gpt4o",
            "model",
            "description",
            "dimensions",
            "slamomni05b",
            "ifr",
            "detailed",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
            "<p class=\"ltx_p\">The detailed performance corresponding to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> is presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "six",
                    "language",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "ifr",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "gpt4o",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control",
                    "results",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "control",
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "dimensions",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "six",
                    "language",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "sft",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "style",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "style",
                    "accent",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "finegrained",
                    "style",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "model",
                    "finegrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "language",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a deeper understanding of our dataset construction pipeline, this section offers a comprehensive overview of the four key steps involved in building UltraVoice, as illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S0.F1\" title=\"In UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. We have designed a bottom-up, fine-grained data generation pipeline that spans text preparation, style instruction injection, speech synthesis, and data filtering. This pipeline integrates everyday conversational texts with a wide range of speech style control types, ensuring high consistency and diversity in content, vocal style, and audio quality. The following subsections will elaborate on the core tasks and implementation details of each step.</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "style",
                    "control",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "base",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "gpt4o",
                    "language",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "each",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "finegrained",
                    "style",
                    "control",
                    "each",
                    "speech",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "control",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "six",
                    "language",
                    "style",
                    "accent",
                    "description",
                    "dimensions",
                    "control",
                    "speed",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "control",
                    "model",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative metrics (average duration 29.35s, mean CER 5.93%, UTMOS 4.00), we provide visual analyses to further validate data quality. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F3\" title=\"In 3.2 Quality Assessment &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, six visualization types assess the effectiveness and clarity of our style control design. Emotion and accent are visualized using t-SNE plots from classification model features, showing clear category separability. Speed and volume are illustrated via word-per-minute (WPM) and root-mean-square (RMS) distributions, confirming consistent prosodic control. Language and composite styles are represented with word clouds, showcasing lexical diversity and expressive richness. These visualizations collectively demonstrate the robustness and interpretability of UltraVoice&#8217;s stylistic control.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "six",
                    "language",
                    "model",
                    "style",
                    "accent",
                    "control",
                    "speed",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finegrained",
                    "sft",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "style",
                    "sft",
                    "control",
                    "speech",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "across",
                    "six",
                    "finegrained",
                    "dimensions",
                    "control",
                    "each",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Audio-Language Model (ALM) based Metric.</span> Following the evaluation paradigm similar to methodologies proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Yan et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>); Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we employed Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib5\" title=\"\">2025</a>)</cite> as our automatic evaluator to automatically generate <span class=\"ltx_text ltx_font_bold\">Mean Opinion Scores (MOS)</span> and compute the <span class=\"ltx_text ltx_font_bold\">instruction-following rate (IFR)</span> for each control dimension. This choice is motivated by findings that advanced ALMs show high consistency with human judgments by  <cite class=\"ltx_cite ltx_citemacro_citet\">Chiang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib4\" title=\"\">2025</a>)</cite>. Details of prompts are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS11\" title=\"E.11 IFR Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E.11</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5.SS10\" title=\"E.10 MOS Evaluation Prompt &#8227; Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">E.10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control",
                    "each",
                    "ifr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "vocalnet7b",
                    "finegrained",
                    "overall",
                    "dimensions",
                    "control",
                    "results",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "vocalnet7b",
                    "language",
                    "overall",
                    "control",
                    "slamomni05b",
                    "ifr",
                    "speech",
                    "vocalnet1b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "across",
                    "vocalnet7b",
                    "model",
                    "style",
                    "sft",
                    "control",
                    "slamomni05b",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "model",
                    "finegrained",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "sft",
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "across",
                    "volume",
                    "model",
                    "style",
                    "accent",
                    "dimensions",
                    "control",
                    "speed",
                    "results",
                    "ifr",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "style",
                    "finegrained",
                    "overall",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "model",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Grammar and Language Refinement:</span> LLMs were employed to proofread the manuscript for grammatical errors, spelling mistakes, and awkward phrasing. This use was intended to improve the clarity, readability, and overall quality of the written text.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "language"
                ]
            }
        ]
    },
    "A4.T11": {
        "caption": "Table 11: SFT Training Configuration for SLAM-Omni-0.5B-SFT.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Batch Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gradient Accumulation Steps</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T11.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Training Epochs</td>\n<td class=\"ltx_td ltx_align_center\">5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Context Length</td>\n<td class=\"ltx_td ltx_align_center\">4,096</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hardware</td>\n<td class=\"ltx_td ltx_align_center\">4 NVIDIA A100-80G GPUs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate Scheduler</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Optimiser</td>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Warmup Steps</td>\n<td class=\"ltx_td ltx_align_center\">5,000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Weight Decay</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Use FP16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">True</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "size",
            "length",
            "accumulation",
            "parameter",
            "rate",
            "context",
            "optimiser",
            "nvidia",
            "batch",
            "learning",
            "decay",
            "training",
            "true",
            "scheduler",
            "value",
            "gpus",
            "gradient",
            "adamw",
            "warmup",
            "sft",
            "11051times",
            "hardware",
            "slamomni05bsft",
            "configuration",
            "weight",
            "use",
            "steps",
            "linear",
            "fp16",
            "epochs",
            "a10080g"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "length",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "value",
                    "sft"
                ]
            }
        ]
    },
    "A4.T12": {
        "caption": "Table 12: SFT Training Configuration for VocalNet1B/7B/8B-SFT.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Batch Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gradient Accumulation Steps</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T12.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Training Epochs</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Context Length</td>\n<td class=\"ltx_td ltx_align_center\">4,096</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hardware</td>\n<td class=\"ltx_td ltx_align_center\">4 NVIDIA A100-80G GPUs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate Scheduler</td>\n<td class=\"ltx_td ltx_align_center\">Cosine</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Optimiser</td>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Warmup Ratio</td>\n<td class=\"ltx_td ltx_align_center\">0.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Weight Decay</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Use BF16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">True</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "51055times",
            "size",
            "length",
            "accumulation",
            "parameter",
            "rate",
            "context",
            "optimiser",
            "nvidia",
            "batch",
            "learning",
            "decay",
            "training",
            "true",
            "scheduler",
            "value",
            "ratio",
            "gpus",
            "gradient",
            "bf16",
            "adamw",
            "warmup",
            "sft",
            "hardware",
            "configuration",
            "cosine",
            "weight",
            "use",
            "vocalnet1b7b8bsft",
            "steps",
            "epochs",
            "a10080g"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "length",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "value",
                    "sft"
                ]
            }
        ]
    },
    "A4.T13": {
        "caption": "Table 13: SFT Training Configuration for UltraVoice-0.5B-SFT",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Batch Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gradient Accumulation Steps</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T13.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Training Epochs</td>\n<td class=\"ltx_td ltx_align_center\">400</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Context Length</td>\n<td class=\"ltx_td ltx_align_center\">4,096</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hardware</td>\n<td class=\"ltx_td ltx_align_center\">4 NVIDIA A100-80G GPUs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate Scheduler</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Optimiser</td>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Warmup Steps</td>\n<td class=\"ltx_td ltx_align_center\">1,000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Weight Decay</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Use FP16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">True</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "length",
            "size",
            "accumulation",
            "parameter",
            "rate",
            "context",
            "optimiser",
            "nvidia",
            "batch",
            "learning",
            "decay",
            "training",
            "true",
            "scheduler",
            "value",
            "gpus",
            "gradient",
            "adamw",
            "warmup",
            "sft",
            "ultravoice05bsft",
            "11051times",
            "hardware",
            "configuration",
            "weight",
            "use",
            "steps",
            "linear",
            "fp16",
            "epochs",
            "a10080g"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "length",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "length",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "value",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our fine-tuned TTS model, <span class=\"ltx_text ltx_font_bold\">UltraVoice-0.5B-SFT</span>, demonstrates strong multi-dimensional style control. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T6\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, on emotional control tasks, our model achieves competitive performance against strong baselines such as PromptTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib15\" title=\"\">2023</a>)</cite>, CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>)</cite>, and EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on the out-of-domain EmoVoice-DB test set. Crucially, on our in-domain UltraVoice data, it substantially reduces the Word Error Rate (WER) to <span class=\"ltx_text ltx_font_bold\">3.97</span> from 19.82 achieved by EmoVoice-0.5B, while maintaining high emotional similarity and naturalness. Furthermore, as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T7\" title=\"In 4.4 Validating Data Quality via Controllable Text-To-Speech &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, the model consistently improves both MOS and IFR scores across all other tested dimensions (accent, speed, volume, and composite styles) compared to the pre-trained baseline. We omit the fully fine-tuned EmoVoice-0.5B from this broader comparison due to its poor robustness, already indicated by its high WER on our dataset. These results confirm that our instruction-style data effectively enhances controllable synthesis across a diverse range of styles.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "ultravoice05bsft"
                ]
            }
        ]
    },
    "A4.T14": {
        "caption": "Table 14: Spoken dialogue model configurations for SFT experiments.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model Name</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speech Encoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">LLM Backbone</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speech Decoder</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SLAM-Omni-0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Whisper-small-v3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Qwen2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CosyVoice1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-1B</td>\n<td class=\"ltx_td ltx_align_center\">Whisper-large-v3</td>\n<td class=\"ltx_td ltx_align_center\">LLaMA3.2</td>\n<td class=\"ltx_td ltx_align_center\">1B</td>\n<td class=\"ltx_td ltx_align_center\">CosyVoice2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VocalNet-7B</td>\n<td class=\"ltx_td ltx_align_center\">Whisper-large-v3</td>\n<td class=\"ltx_td ltx_align_center\">Qwen2.5</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">CosyVoice2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">VocalNet-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Whisper-large-v3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">LLaMA3.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">CosyVoice2</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "configurations",
            "dialogue",
            "size",
            "spoken",
            "experiments",
            "vocalnet1b",
            "vocalnet7b",
            "llm",
            "vocalnet8b",
            "encoder",
            "qwen2",
            "qwen25",
            "whispersmallv3",
            "backbone",
            "llama32",
            "cosyvoice2",
            "sft",
            "speech",
            "name",
            "llama31",
            "cosyvoice1",
            "model",
            "05b",
            "whisperlargev3",
            "slamomni05b",
            "decoder"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Settings. </span>Our experiments are based on four spoken dialogue models from the SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> and VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> series. These models span various sizes and utilize LLM backbones from the LLaMA and Qwen families. We applied SFT to these models to analyze their performance on speech style control. The detailed configurations of the spoken dialogue models ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>)and the training configurations for SFT ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>) are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4\" title=\"Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To ensure the full reproducibility of our work, we provide comprehensive details on our data, models, and experimental procedures. Our data generation pipeline for the UltraVoice dataset is thoroughly described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3\" title=\"3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. The selection criteria and configurations of the spoken dialogue models used for fine-tuning are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T14\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">14</span></a>. We provide the detailed Supervised Fine-Tuning (SFT) settings, including all hyperparameters, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T11\" title=\"In Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T12\" title=\"Table 12 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A4.T13\" title=\"Table 13 &#8227; Appendix D Supervised Fine-tuning Details &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. Finally, the evaluation metrics and protocols used to assess performance are detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>. All code, the dataset, and model checkpoints will be made publicly available to facilitate further research.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice.\nMoreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset&#8217;s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/bigai-nlco/UltraVoice\" title=\"\">https://github.com/bigai-nlco/UltraVoice</a>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "model",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The future of human-computer interaction is moving toward more natural, efficient, and expressive communication. With the rise of large language models (LLMs) and their integration with speech technologies, end-to-end spoken dialogue models such as GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, and Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> have emerged. These models enable real-time, low-latency speech interaction, greatly enhancing user experience. However, most current research has prioritized the functional aspects of conversation (what to say), while the expressive dimension (how to say it) remains largely underdeveloped. Current models can generate fluent responses, but often do so with a neutral or monotonous prosody, lacking the ability to convey nuanced intent, emotion, or personality <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib23\" title=\"\">2025</a>; Cui et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib6\" title=\"\">2024</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib14\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This lack of expressive control is a significant barrier to human-like interaction. For example, imagine a spoken dialogue model generating the response, &#8220;<span class=\"ltx_text ltx_font_typewriter\">That&#8217;s a fantastic idea.</span>&#8221; Without the ability to precisely control its delivery, the model cannot convey genuine excitement to celebrate a user&#8217;s suggestion or adopt a playfully sarcastic tone in a creative storytelling scenario. The speech it produces is functionally correct, but expressively sterile. This expressive gap stems from a fundamental flaw in existing training data. The common practice of simply applying Text-to-Speech (TTS) to text-based dialogue datasets fails to inject authentic paralinguistic information. This process results in speech that is linguistically correct but expressively impoverished, lacking the genuine variations in emotion, tone, and prosody that characterize human interaction. Consequently, models are trained on acoustically sterile data, learning what to say, but never learning how to say it with meaningful, human-like expression.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "model",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary goal is to significantly enhance the expressiveness of spoken dialogue models by enabling them to modulate their speech style on command. This objective motivates our core research question: <span class=\"ltx_text ltx_font_italic\">How can we construct a dataset that is sufficiently <span class=\"ltx_text ltx_font_bold\">large-scale</span>, <span class=\"ltx_text ltx_font_bold\">diverse</span>, and <span class=\"ltx_text ltx_font_bold\">instruction-rich</span> to effectively train spoken dialogue models for multi-dimensional, fine-grained speech style control?</span> We contend that this is achievable, but it requires overcoming the following key challenges.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">First</span>, existing spoken dialogue datasets are fundamentally inadequate. Many spoken dialogue datasets, such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, are created by simply converting text conversations to speech via TTS. This process yields a mere &#8220;spoken version&#8221; of text, stripped of the authentic, context-driven paralinguistic cues essential for human interaction. This necessitates a new approach beyond simple adaptation. <span class=\"ltx_text ltx_font_bold\">Second</span>, both collecting real data and repurposing existing resources present major obstacles. Acquiring large-scale, real-world spoken dialogues is prohibitively expensive and labor-intensive, while adapting controllable TTS datasets, such as EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite>, and InstructTTSEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib17\" title=\"\">2025b</a>)</cite>, for dialogue is also flawed; forcing them into a conversational format with prompts like, &#8220;Please read this in an excited tone,&#8221; fundamentally degenerates the interactive dialogue task into a non-interactive TTS task. <span class=\"ltx_text ltx_font_bold\">Third</span>, while data synthesis emerges as the most viable path, it presents its own complex hurdles. Its success hinges not only on selecting a sufficiently expressive TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib26\" title=\"\">2025a</a>; Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib36\" title=\"\">2025</a>)</cite> to avoid monotonous outputs but, more critically, on a sophisticated generation strategy. This strategy must ensure the authenticity of the generated speech while achieving broad diversity across instructions and control dimensions, ultimately creating data that enables models to learn the nuanced relationship between content (<span class=\"ltx_text ltx_font_italic\">what</span> to say) and delivery (<span class=\"ltx_text ltx_font_italic\">how</span> to say).</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "model",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the above challenges, this work makes three core contributions. <span class=\"ltx_text ltx_font_bold\">Firstly</span>, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control, filling a key gap in the field. It supports fine-grained control across six stylistic dimensions, including emotion, volume, speed, accent, language, and composite styles, providing a solid basis for training and evaluating expressive speech dialogue models. <span class=\"ltx_text ltx_font_bold\">Secondly</span>, we conduct comprehensive supervised fine-tuning (SFT) on mainstream spoken dialogue models on it, and we observe consistent gains in expressive style rendering and general conversational competence. <span class=\"ltx_text ltx_font_bold\">Thirdly</span>, we demonstrate the dataset&#8217;s generalizability beyond dialogue modeling by fine-tuning a pre-trained TTS model, which enables multidimensional controllable speech synthesis across diverse styles and highlights the dataset&#8217;s versatility and reliability for downstream speech generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialogue",
                    "sft",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end spoken dialogue models sought to integrate automatic speech recognition (ASR), text-based dialogue modeling, and text-to-speech synthesis (TTS) within a unified architecture to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib18\" title=\"\">2024</a>; An et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib1\" title=\"\">2024</a>)</cite>. Pioneering models such as Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite> and Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib7\" title=\"\">2024</a>)</cite> adopted shared decoders that jointly generate text and audio tokens, while later models, including LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib27\" title=\"\">2024</a>)</cite> employed modular multimodal pipelines with dedicated speech encoders and decoders built around a pre-trained LLM. Despite these architectural advances, style controllability remains a significant weakness. Since expressiveness is learned implicitly from training data, these models tend to produce homogeneous speaking styles and lack explicit control over paralinguistic features such as emotion and speed. This deficiency severely limits their use in personalized or emotionally expressive dialogue settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>; Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib16\" title=\"\">2025a</a>)</cite> have begun to address these limitations. For instance, SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite> introduces zero-shot timbre control, enabling real-time dialogue with dynamic speaker voices specified via audio prompts. On the efficiency front, VocalNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib28\" title=\"\">2025b</a>)</cite> enhances both generation speed and quality through multi-token prediction (MTP), producing multiple audio tokens per decoding step rather than one at a time. Nonetheless, none of the existing end-to-end spoken dialogue models provides explicit fine-grained speech style controls such as direct modulation of emotion, accent, or speed. In summary, while end-to-end dialogue models have made substantial progress in generating natural and low-latency speech, the ability to explicitly manipulate stylistic attributes remains entirely unaddressed in current approaches.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For general-purpose spoken dialogue tasks, existing datasets mainly prioritize functionality over expressiveness. Well-known corpora such as InstructS2S <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite> and VoiceAssistant <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>)</cite> have been widely adopted to train models, including LLaMA-Omni and Mini-Omni, supporting task-oriented interactions, voice assistants, and related applications. These datasets typically contain hundreds of thousands of speech dialogue pairs and enable direct speech interaction. Despite their scale and dialogue focus, they lack explicit fine-grained speech style annotations, such as speed, volume, or emotion. As a result, the generated speech is often homogeneous and lacks the fine-grained control required for emotionally rich or personalized interactions.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable TTS datasets, such as SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib21\" title=\"\">2024</a>)</cite> for description-based synthesis and EmoVoice-DB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> for emotional control, are designed to produce speech with specific styles. The recent success of state-of-the-art models <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib29\" title=\"\">2024</a>)</cite> trained on such data, including CosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib10\" title=\"\">b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib11\" title=\"\">2025</a>)</cite> and the audio model of GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite>, highlights the significant progress in fine-grained stylistic generation. However, a fundamental limitation of these datasets persists: they are overwhelmingly designed for non-interactive synthesis. Because these corpora lack the bidirectional dialogue structure and turn-taking context inherent to conversation, they are ultimately unsuitable for training end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "model",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the lack of explicit speech style control instructions in spoken dialogue datasets and the non-interactive limitation of TTS corpora, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>. As summarized in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S2.T1\" title=\"In 2.2 Spoken Dialogue and Controllable TTS Dataset &#8227; 2 Related Work &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UltraVoice covers six key stylistic dimensions: emotion, speed, volume, language, accent, and composite styles (e.g., combinations of speed, volume, and emotion). It also maintains full dialogue context along with instruction and response structure. This dataset fills a critical gap by supporting both general speech interaction and fine-grained speech style control, providing a unified and high-quality dataset for training and evaluating style-controllable end-to-end spoken dialogue models.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 1: Text Corpus Curation.</span>\nTo construct the UltraVoice dataset, we curated the foundational text corpus using UltraChat <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib8\" title=\"\">2023</a>)</cite>, a widely adopted English dialogue dataset frequently used for speech dialogue synthesis in models such as LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib13\" title=\"\">2025</a>)</cite>, Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib30\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib31\" title=\"\">b</a>)</cite>, and SLAM-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib3\" title=\"\">2024</a>)</cite>. We extracted dialogues primarily from the <span class=\"ltx_text ltx_font_italic\">Question About the World</span> and <span class=\"ltx_text ltx_font_italic\">Creation and Generation</span> categories due to their conciseness and independence from external references. To ensure high-quality input for downstream synthesis, we applied strict filtering rules to remove dialogues containing URLs, academic citations, or lengthy quoted texts. After filtering, we obtained approximately 200,000 clean and natural question-answer pairs, which served as the base for style-controlled speech generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Style Injection &amp; Response Generation.</span>\nTo enable fine-grained control over speaking styles, we predefined six stylistic dimensions: speed, volume, emotion, accent, language, and composite styles. For each dimension, we used GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> to generate diverse and natural style prompts (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A5\" title=\"Appendix E Prompts &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">E</span></a> for all prompt templates), leveraging semantically similar expressions (e.g., &#8220;respond in a joyful tone&#8221; vs. &#8220;reply with a cheerful voice&#8221;). Based on these prompts, GPT-4o was further invoked to generate stylized textual responses, ensuring alignment in semantics and tone. Additionally, we applied several practical adjustments to improve downstream TTS following <cite class=\"ltx_cite ltx_citemacro_citet\">Fang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib12\" title=\"\">2024</a>)</cite>. For example, we expanded numbers into spoken forms (e.g., &#8220;123&#8221; &#8594; &#8220;one two three&#8221;) and rephrased code-related queries to encourage natural spoken language responses. These refinements ensured better speech synthesis fluency and user interaction quality.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Stylized Speech Synthesizing.</span>\nIn this step, we performed speech synthesis for each instruction-response speech pair to simulate realistic conversations with fine-grained style control. For instruction speech, we randomly sampled speaker timbres from the seedtts_testset_en<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/BytedanceSpeech/seed-tts-eval</span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib2\" title=\"\">2024</a>)</cite> corpus. This corpus features diverse speakers and real-world background noise, allowing the instruction audio to better reflect realistic user conditions. In contrast, response speech was synthesized using a single fixed timbre to ensure consistency across all stylized outputs. We selected the TTS model for each style control dimension as detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T2\" title=\"In Figure 2 &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Most responses were synthesized using the GPT-4o-audio-preview <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib19\" title=\"\">2024</a>)</cite> model due to its expressiveness and high fidelity. For accent-specific responses, we used Edge TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/rany2/edge-tts</span></span></span>, which lacks support for custom speaker timbres. To address this, we applied voice conversion (VC) via CosyVoice-300M <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib9\" title=\"\">2024a</a>)</cite> to align the output with the designated fixed voice.\nTo ensure data balance, we further sampled 40,000 generic QA pairs without style instructions from the VoiceAssistant400k<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K</span></span></span> corpus. After removing templated phrases (e.g., &#8220;I&#8217;m mini omni&#8221;), we resynthesized them using CosyVoice-300M.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Quality Control &amp; Filtering.</span>\nTo ensure the overall quality and balanced stylistic coverage of the dataset, we applied an automated filtering process to all synthesized speech dialogue samples. Specifically, we utilized the Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite> to perform automatic speech recognition (ASR) on each instruction and response audio sample, and computed the character error rate (CER) based on the transcriptions. We applied a unified data filtering criterion to both instruction and response audio: only retaining samples with a CER below 20% and duration under 30 seconds. This filtering pipeline effectively removed samples with high ASR error or abnormal length, significantly improving the dataset&#8217;s consistency and usability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whisperlargev3",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the UltraVoice dataset comprises 100,770 speech dialogue samples, among which 84,832 are explicitly conditioned on six major control dimensions: emotion, volume, speed, accent, language, and composite styles description. The remaining 15,938 pairs are general English QA samples without style prompts, added to improve balance and generalization. The dataset includes 21,209 emotion-controlled samples across seven categories (<span class=\"ltx_text ltx_font_italic\">neutral, happy, sad, angry, surprised, fearful, disgusted</span>), 11,154 for volume (<span class=\"ltx_text ltx_font_italic\">low, normal, high</span>), and 10,334 for speed (<span class=\"ltx_text ltx_font_italic\">slow, normal, fast</span>). Accent control covers six English variants (<span class=\"ltx_text ltx_font_italic\">AU, CA, GB, IN, SG, ZA</span>) totaling 26,839 samples. Language-switching samples span Chinese, Japanese, and Korean, with 11,153 entries. The composite styles dimension includes 4,143 samples representing multidimensional control (<span class=\"ltx_text ltx_font_italic\">e.g., respond with a slow and loud air of thoughtful sadness</span>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, the dataset covers over 830 hours of speech, with duration distribution shown in  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.F2\" title=\"In 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. Alongside duration, we also report word count distributions to assess utterance complexity and length variation. The structured control space and balanced temporal characteristics make UltraVoice a valuable resource for training and evaluating stylistically controllable spoken dialogue systems. More detailed statistics are available in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A2\" title=\"Appendix B Detailed Dataset Statistics &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and consistency of the spoken dialogue data, we applied strict filtering criteria, retaining only samples with a duration under 30 seconds and a CER below 20%. This approach effectively eliminated samples with poor ASR quality or abnormal content, significantly improving dataset stability and usability. As reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S3.T3\" title=\"Table 3 &#8227; 3.1 Characteristics and Statistics &#8227; 3 The UltraVoice Dataset &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the final dataset achieves an average dialogue length of 29.35 seconds, a mean CER of 5.93%, and an overall UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite> score of 4.00, indicating high naturalness and stylistic fidelity of the generated speech. This automated quality control process lays a solid and reliable foundation for subsequent model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "model",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we systematically evaluate the performance of the end-to-end speech dialogue model trained via SFT on the UltraVoice. Firstly, we verify the model&#8217;s ability to control multi-dimensional speech styles on the UltraVoice internal test set after SFT. Next, we further examine the model&#8217;s generalization capability on the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>. Finally, we further validate the quality of our fine-grained controlled response speech by successfully training a controllable TTS model following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite> on a dataset constructed from the UltraVoice.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "sft",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and metrics. </span>To construct our evaluation benchmark, we randomly sampled 100 examples from each fine-grained dimension within the six major control dimensions defined in UltraVoice, resulting in a test set of 2,300 samples. The test set has no overlap with the training data. To further evaluate whether SFT on UltraVoice impacts general spoken dialogue capabilities such as natural conversation, comprehension, and reasoning, we utilized the URO-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib33\" title=\"\">2025</a>)</cite>, which assesses models across three dimensions: Oral Conversation, Understanding, and Reasoning. It allows us to analyze whether core dialogue competencies are preserved and whether expressive performance improves after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "sft",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic ltx_framed ltx_framed_underline\">Objective Numerical Metric.</span> Content consistency was measured by the <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, using transcriptions from the Whisper-large-v3 model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib24\" title=\"\">2023</a>)</cite>. For emotional expressiveness, we adopted metrics from <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, leveraging emotion2vec <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib22\" title=\"\">2023</a>)</cite> to compute both <span class=\"ltx_text ltx_font_bold\">Emotion Similarity</span> (cosine similarity between embeddings) and <span class=\"ltx_text ltx_font_bold\">Recall Rate</span> (from emotion classification). Finally, the overall naturalness and perceptual audio quality were evaluated using the <span class=\"ltx_text ltx_font_bold\">UTMOS</span> score <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib25\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "whisperlargev3",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements of Multi-Dimensional Instruction-Following Capabilities.</span> As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.F4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> and detailed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3.T10\" title=\"In Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a> in the <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#A3\" title=\"Appendix C The Detailed Performance Comparison &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">C</span></a>, fine-tuning with UltraVoice significantly boosts the spoken dialogue models&#8217; instruction-following capability for fine-grained speech style control, with IFR gains ranging from 14.61 to 40.09 points. This improvement is particularly pronounced for smaller models with weaker baseline performance. For instance, the IFR of SLAM-Omni-0.5B surged from 28.30% to 68.39%, while VocalNet-1B&#8217;s score increased from 36.28% to 55.91%. These results demonstrate that even resource-constrained models can achieve substantial gains in responsiveness to control instructions through UltraVoice. Concurrently, larger models such as VocalNet-7B and 8B also exhibited consistent improvements, indicating an enhanced ability to precisely control various dimensions of fine-grained speech styles.</p>\n\n",
                "matched_terms": [
                    "vocalnet7b",
                    "dialogue",
                    "speech",
                    "slamomni05b",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Enhancements in the Subjective Naturalness of Fine-Grained Speech Styles Control. </span>As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T4\" title=\"In 4.2 Performance on Fine-grained Speech Style Control &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, all models exhibit significant improvements in MOS after being fine-tuned with UltraVoice. The relative gains range from 29.12% to 42.33%, with the Emotion and Volume dimensions showing particularly remarkable improvements. For instance, the overall MOS for VocalNet-7B increased from 2.73 to 3.59, while VocalNet-8B&#8217;s score rose from 2.85 to 3.68. These results indicate that our fine-tuning process enhances the models&#8217; ability to render the specified styles with high naturalness, demonstrating that improved instruction control does not come at the cost of audio quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vocalnet7b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Consistency and Limitations. </span>Overall, MOS and IFR trends are strongly aligned, suggesting that stronger instruction adherence typically yields more natural speech. However, the Language control dimension presents a notable exception. Models based on the LLaMA backbone (e.g., VocalNet-1B and 8B) exhibit a slight MOS decline and stagnant IFR, while Qwen-based models (e.g., SLAM-Omni-0.5B and VocalNet-7B) achieve clear gains. This discrepancy likely stems from differences in multilingual pretraining exposure. Language control requires full responses in a different language, introducing unique generalization challenges. The current fine-tuning data lacks sufficient multilingual diversity and volume, limiting the models&#8217; ability to generalize. Future work should explore targeted augmentation of multilingual instruction data to address this limitation.</p>\n\n",
                "matched_terms": [
                    "vocalnet7b",
                    "backbone",
                    "slamomni05b",
                    "speech",
                    "vocalnet1b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results on the URO-Bench (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4.T5\" title=\"In 4.3 General Conversational Ability &#8227; 4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>) confirm that fine-tuning spoken dialogue models on UltraVoice enhances, rather than compromises, general conversational skills. All models showed substantial gains across <span class=\"ltx_text ltx_font_italic\">Understanding</span>, <span class=\"ltx_text ltx_font_italic\">Reasoning</span>, and <span class=\"ltx_text ltx_font_italic\">Oral Conversation</span>, with average improvements of <span class=\"ltx_text ltx_font_bold\">+10.84%</span> on the Basic setting and <span class=\"ltx_text ltx_font_bold\">+7.87%</span> on the Pro setting. Notably, the VocalNet-7B SFT model establishes a new state-of-the-art, outperforming strong baselines like Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib32\" title=\"\">2025</a>)</cite> and GLM4-Voice-9B <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib35\" title=\"\">2024</a>)</cite>, highlighting practical value beyond style control. The only exception to this positive trend is a performance drop in Pro Reasoning (from 24.72 to 20.07) for the smallest model, SLAM-Omni-0.5B. We attribute this to the current dataset&#8217;s focus on single-turn interactions, which may not sufficiently support complex, multi-turn reasoning. Future work could address this by incorporating multi-turn dialogue examples during SFT.</p>\n\n",
                "matched_terms": [
                    "vocalnet7b",
                    "model",
                    "dialogue",
                    "sft",
                    "slamomni05b",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the quality and utility of our data synthesised using fine-grained speech style control, we repurposed it into a controllable TTS dataset. This new dataset, derived from five stylistic dimensions in UltraVoice (speed, volume, emotion, accent, and composite styles), consists of explicit instruction-speech pairs. Following the pipeline of EmoVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#bib.bib34\" title=\"\">2025</a>)</cite>, we performed supervised fine-tuning (SFT) on a pre-trained EmoVoice-0.5B model, using its checkpoint before it was trained on the EmoVoice-DB to ensure a clean baseline.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sft",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_bold\">UltraVoice</span>, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. By fine-tuning mainstream spoken dialogue models on UltraVoice, we significantly enhanced their controllability over diverse fine-grained speech styles, while also improving their overall speech naturalness and general conversational competence. The dataset&#8217;s high quality and generalization were further validated through strong performance on the URO-Bench benchmark and in controllable TTS tasks, establishing a solid foundation for expressive spoken dialogue modeling. While this work represents a significant step forward, the full scope of human-like expressive speech presents formidable long-term challenges. The framework and data we provide can be extended to explore these frontiers, such as modeling the dynamic evolution of styles within multi-turn conversations or capturing the nuanced paralinguistic features in massively multilingual contexts. Addressing these complex scenarios, though beyond the scope of this paper, will be critical for developing the next generation of truly context-aware and intelligent speech interaction systems.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "speech",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The UltraVoice dataset is generated via a fully synthetic pipeline, employing GPT-4o for text creation and multiple Text-to-Speech (TTS) engines for audio synthesis. This approach ensures that the dataset contains no personally identifiable information or the voices of real individuals, thereby circumventing the privacy concerns and copyright issues often associated with human-derived data. The content is created and intended strictly for academic research on controllable spoken dialogue systems.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the configurations used for the Supervised Fine-tuning (SFT) of the Spoken Dialogue and Controllable TTS models, as mentioned in our experiments ( <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22588v1#S4\" title=\"4 Experiment &#8227; UltraVoice: Scaling Fine-GrainedStyle-Controlled Speech Conversationsfor Spoken Dialogue Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "dialogue",
                    "sft",
                    "spoken",
                    "experiments"
                ]
            }
        ]
    }
}