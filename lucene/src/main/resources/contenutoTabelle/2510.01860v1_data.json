{
    "S3.T1": {
        "source_file": "SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision",
        "caption": "Table 1: Dataset statistics and metadata availability. Checkmarks indicate available annotation types. Dem.: Demographics (age, sex), Voc.: Voice quality ratings, Med.: Medical/clinical assessments. (CN: Chinese, EN: English, FR: French, IT: Italian, SK: Slovak, SP: Spanish)",
        "body": "#Spk\nDur.(h)\nDimension\n\n\nDataset\n\n\nDem.\nVoc.\nMed.\n\n\nPretraining\n\n\nVoxceleb1 [17]\n\n1251\n351\n✓\n\n\n\n\nVoxceleb2 [18]\n\n6112\n2440\n✓\n\n\n\n\nVOCES [19] (SP)\n90\n12\n✓\n\n✓\n\n\nCN-Celeb1 [20] (CN)\n997\n273\n✓\n\n\n\n\nPopGen [21] (FR)\n1667\n135\n✓\n\n✓\n\n\nCLAC [22] (EN)\n1832\n105\n✓\n\n\n\n\nEWA-DB [16] (SK)\n997\n92\n✓\n\n✓\n\n\nAndroids [23] (IT)\n110\n7.5\n\n\n✓\n\n\nTotal\n13056\n3415.5\n\n\n\n\n\nOOD test datasets\n\n\nDreamVoiceDB [24] (EN)\n899\n221.5\n\n✓\n\n\n\nPVQD [25] (EN)\n296\n2\n✓\n✓\n✓\n\n\nLanzhou [26] (CN)\n52\n7\n✓\n\n✓\n\n\nSAP [27] (EN)\n424\n304\n✓\n✓\n\n\n\nNeurovoz [28] (SP)\n105\n3.5\n\n\n✓\n\n\nNaples Voiced [29] (IT)\n208\n0.3\n✓\n✓\n✓\n\n\nVOC-ALS [30] (IT)\n153\n5\n\n\n✓\n\n\nTotal\n2137\n543.3",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">#Spk</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Dur.(h)</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Dimension</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Dataset</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Dem.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Voc.</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Med.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Pretraining</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Voxceleb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib17\" title=\"\">17</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">1251</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">351</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Voxceleb2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">6112</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">2440</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite> (SP)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">90</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">12</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CN-Celeb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib20\" title=\"\">20</a>]</cite> (CN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">997</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">273</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite> (FR)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">1667</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">135</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib22\" title=\"\">22</a>]</cite> (EN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">1832</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">105</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite> (SK)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">997</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">92</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Androids <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib23\" title=\"\">23</a>]</cite> (IT)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">110</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">7.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Total</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">13056</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">3415.5</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">OOD test datasets</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">DreamVoiceDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib24\" title=\"\">24</a>]</cite> (EN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">899</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">221.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">PVQD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib25\" title=\"\">25</a>]</cite> (EN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">296</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Lanzhou <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib26\" title=\"\">26</a>]</cite> (CN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">52</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib27\" title=\"\">27</a>]</cite> (EN)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">424</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">304</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Neurovoz <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib28\" title=\"\">28</a>]</cite> (SP)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">105</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">3.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Naples Voiced <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib29\" title=\"\">29</a>]</cite> (IT)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">208</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">0.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">VOC-ALS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib30\" title=\"\">30</a>]</cite> (IT)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">153</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Total</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">2137</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">543.3</td>\n<td class=\"ltx_td ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "checkmarks",
            "dreamvoicedb",
            "cnceleb1",
            "slovak",
            "assessments",
            "french",
            "datasets",
            "voiced",
            "naples",
            "demographics",
            "chinese",
            "italian",
            "pretraining",
            "ratings",
            "voxceleb2",
            "med",
            "lanzhou",
            "types",
            "dem",
            "pvqd",
            "metadata",
            "annotation",
            "total",
            "availability",
            "statistics",
            "durh",
            "ood",
            "voice",
            "voxceleb1",
            "spanish",
            "voces",
            "dimension",
            "androids",
            "dataset",
            "test",
            "available",
            "english",
            "neurovoz",
            "indicate",
            "sex",
            "spk",
            "sap",
            "ewadb",
            "quality",
            "popgen",
            "age",
            "voc",
            "clac",
            "vocals",
            "medicalclinical"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We constructed our pretraining dataset from 9 diverse speech corpora, comprising over 3.4k hours of audio from more than 10k speakers, along with corresponding metadata and descriptions. The dataset comprises audio-text pairs extracted from Androids <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib23\" title=\"\">23</a>]</cite>, VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>, EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>, SAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib27\" title=\"\">27</a>]</cite>, PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>, CLAC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib22\" title=\"\">22</a>]</cite>, VoxCeleb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib17\" title=\"\">17</a>]</cite>, VoxCeleb2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib18\" title=\"\">18</a>]</cite>, and CN-Celeb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib20\" title=\"\">20</a>]</cite>. These corpora provide rich speaker-level annotations including demographic information (age, sex), linguistic characteristics (e.g.: language, accent), health status (e.g.: Parkinson&#8217;s, Alzheimer&#8217;s, depression), and voice attributes (e.g.: roughness, intensity) that serve as textual supervision signals for our contrastive learning framework. Detailed statistics for each dataset, including the number of speakers, and available metadata fields, are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Pretraining Datasets and Speaker Descriptions &#8227; 3 Experiments &#8227; SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nTo address data imbalance during pretraining, we resampled subsets of large datasets (VoxCeleb1, VoxCeleb2, CN-Celeb1, and CLAC) every 2000 steps.\nBased on preliminary experiments, we generated descriptions with Gemma-3-27b-it <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib31\" title=\"\">31</a>]</cite> and we used Qwen3-Embedding-8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib32\" title=\"\">32</a>]</cite> to embed speaker textual descriptions to match.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech encodes paralinguistic information such as demographics, voice quality, and health. Yet no audio foundation model supports zero-shot or out-of-distribution (OOD) generalization to these tasks. We introduce SLAP (Speaker contrastive Language-Audio Pretraining), the first model aligning speech with natural language descriptions of speaker and health metadata through contrastive learning. SLAP combines a Vision Transformer audio encoder with text encoders, trained on more than 3400 hours across 9 datasets with diverse speaker annotations. We evaluated on 38 binary classification tasks spanning demographics, voice characteristics, and clinical assessments across 14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shot evaluation, a 48% relative improvement over CLAP (42.4%), while demonstrating strong OOD generalization to unseen languages and clinical populations. When fine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achieves best-in-class performance on health tasks (57.9% F1), surpassing larger foundation models.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "assessments",
                    "datasets",
                    "metadata",
                    "demographics",
                    "quality",
                    "pretraining",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech conveys rich paralinguistic information beyond lexical content, revealing who is speaking and how they sound through acoustic cues. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib1\" title=\"\">1</a>]</cite>. These cues encode an important number of speaker attributes including age, sex, emotional state, language background most importantly health status, making speech a non-invasive modality into human physiology, mental health and cognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib2\" title=\"\">2</a>]</cite>. The capability to extract these attributes at scale could greatly help the healthcare delivery system, enabling easy remote monitoring, early detection of relapses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib3\" title=\"\">3</a>]</cite>, and personalized interventions across diverse vulnerable populations and languages.</p>\n\n",
                "matched_terms": [
                    "age",
                    "sex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While foundation models like Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite> allowed great progress for speech recognition and semantic understanding, they did not integrate any mechanisms to leverage speaker metadata for representation learning or improved inference <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib5\" title=\"\">5</a>]</cite>. Although recent models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib6\" title=\"\">6</a>]</cite> have made progress toward modeling certain speaker characteristics, current audio foundation models remain limited when evaluated on speaker-centric tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib5\" title=\"\">5</a>]</cite>. For instance, SALMONN freezes its audio encoders, which may constrain the richness of speaker representations and limit their usefulness for tasks such as age, sex, or language recognition, as well as health-related applications.</p>\n\n",
                "matched_terms": [
                    "age",
                    "sex",
                    "metadata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current approaches to speech representation learning face fundamental limitations for clinical deployment and wider distribution. Self-supervised methods such as Wav2Vec2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib7\" title=\"\">7</a>]</cite> or MAE-style pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib8\" title=\"\">8</a>]</cite> learn powerful acoustic representations by predicting masked segments or reconstructing spectrograms. Yet, without any finetuning data they offer little capacity to infer attributes such as age, sex, language, or health-related indicators, especially in low-data regimes.</p>\n\n",
                "matched_terms": [
                    "age",
                    "sex",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These diverse efforts show the potential of language supervision for audio, but they have so far been explored only in some specific contexts such as recognition, verification, or emotion. This leaves open the opportunity to build a more general foundation model that captures demographics, voice quality, and clinical aspects, and can be queried directly through natural-language prompts.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "quality",
                    "demographics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SLAP (Speaker contrastive Language-Audio Pretraining), the first framework designed for zero-shot inference of speaker and health attributes. SLAP converts heterogeneous annotations into natural language prompts and aligns them with audio through a contrastive objective, enabling out-of-distribution (OOD) generalization to unseen languages and clinical populations. Our contributions are threefold: <span class=\"ltx_text ltx_font_bold\">(i)</span> we designed a CLAP-style audio-language pretraining tailored for speaker, voice and health tasks, <span class=\"ltx_text ltx_font_bold\">(ii)</span> we conducted a large comprehensive evaluation of its kind, 38 tasks across 14 corpora in 7 languages, and <span class=\"ltx_text ltx_font_bold\">(iii)</span> we demonstrated, for the first time, zero-shot and out-of-domain generalization, especially for health-related speech analysis compared to speech foundation models, in addition to achieving state-of-the-art zero-shot and competitive linear probing results.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "ood",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated SLAP on a broad spectrum of speaker-centric downstream tasks, organized into three categories: demographics, voice characteristics, and health. All tasks are framed as binary classification.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "demographics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Demographic tasks (11 in total) target attributes such as age group and sex. Voice characteristics tasks (9) involve perceptual judgments such as strength, nasality, and standardized rating scales (GRBAS, CAPE-V). Health-related tasks (18) address clinical screening and diagnosis, including mental health (depression, anxiety, insomnia, fatigue, suicidal ideation) and neurological conditions (Parkinson&#8217;s disease, Alzheimer&#8217;s disease, ALS). In total, our evaluation covers 38 tasks across 14 datasets and 7 languages, providing a comprehensive benchmark for both in-domain, OOD performances and zero-shot generalization.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "sex",
                    "datasets",
                    "age",
                    "total",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We instantiated these 3 task categories using 14 corpora covering diverse populations and languages. We evaluated SLAP in both in-domain and OOD (out-of-distribution) conditions, reporting F1 scores on held-out test sets for zero-shot and supervised baselines.</p>\n\n",
                "matched_terms": [
                    "test",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Demographics.</span> Sex and age classification tasks are evaluated across Naples Voiced <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib29\" title=\"\">29</a>]</cite>, VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>, PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>, PVQD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib25\" title=\"\">25</a>]</cite>, and EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "sex",
                    "ewadb",
                    "voces",
                    "voiced",
                    "pvqd",
                    "age",
                    "demographics",
                    "naples",
                    "popgen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice characteristics.</span> Perceptual assessments of vocal strength, nasality, and GRBAS ratings are drawn from DreamVoiceDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib24\" title=\"\">24</a>]</cite>, PVQD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib25\" title=\"\">25</a>]</cite>, and SAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dreamvoicedb",
                    "voice",
                    "sap",
                    "assessments",
                    "pvqd",
                    "ratings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Health.</span> Tasks include mental health screening of depression, anxiety, insomnia, and fatigue in French general population (PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>), depression detection in Italian (Androids <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib23\" title=\"\">23</a>]</cite>) and Chinese (Lanzhou <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib26\" title=\"\">26</a>]</cite>), depression and suicidality detection in Spanish psychiatric patients (VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>), and neurological disease detection: Parkinson&#8217;s and Alzheimer&#8217;s in Slovak (EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>), Parkinson&#8217;s in Spanish (Neurovoz <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib28\" title=\"\">28</a>]</cite>) and ALS in Italian (VOC-ALS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib30\" title=\"\">30</a>]</cite>).</p>\n\n",
                "matched_terms": [
                    "lanzhou",
                    "neurovoz",
                    "slovak",
                    "spanish",
                    "voces",
                    "ewadb",
                    "french",
                    "popgen",
                    "chinese",
                    "italian",
                    "vocals",
                    "androids"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared SLAP with several baselines representing different paradigms in speaker tasks: OpenSMILE with eGeMAPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib33\" title=\"\">33</a>]</cite>, AudioMAE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib8\" title=\"\">8</a>]</cite> and the generalist foundation models WhisperM and WhisperL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>.\nWe also pretrained our AudioMAE model, called SLAP-MAE, on the same pretraining datasets as SLAP to compare to pure self-supervised learning (<math alttext=\"\\lambda=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0</annotation></semantics></math>). Both SLAP and SLAP-MAE was initialized with the pretrained weights of AudioMAE. For zero-shot approaches, we compared SLAP to two open-source CLAP models from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/clap-htsat-fused\" title=\"\">https://huggingface.co/laion/clap-htsat-fused</a></span></span></span>, the LAION CLAP-Fused model and an optimized version for music and speech referred as CLAP-MS <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/larger_clap_music_and_speech\" title=\"\">https://huggingface.co/laion/larger_clap_music_and_speech</a></span></span></span>. All supervised baselines were trained via linear probing on the train set of each dataset and evaluated on the test split, whereas zero-shot baselines are evaluated directly on the test sets with natural language prompts.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "test",
                    "datasets",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#S4.T2\" title=\"Table 2 &#8227; 4 Results and discussions &#8227; SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the classification results for supervised and zero-shot approaches, considering both OOD and overall (in- and OOD) performance. The results are averaged separately for each task category (demographics, vocal and health) as well as for all tasks. SLAP set state-of-the-art zero-shot results (90.0 F1 on demographics, 51.2 on health), approaching supervised performance. As mentioned in the introduction, the original CLAP model computes mel filter banks with hop sizes of 320ms and window sizes of 1024ms, which is not optimal to capture the fine-grained temporal structure essential for speech. In linear probing, WhisperM obtained the best overall results (F1 70.1), except on medical tasks where SLAP performed best (F1 57.9). Interestingly, SLAP-MAE surpassed SLAP in linear probing, highlighting the robustness of self-supervised pretraining. However, it cannot be evaluated in zero-shot setting. Combining contrastive and self-supervised pretraining enables SLAP to deliver state-of-the-art zero-shot results while remaining strong in linear probing.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "ood",
                    "demographics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SLAP demonstrated that combining contrastive language-audio pretraining with self-supervised learning enables both strong zero-shot performance and competitive supervised results on speaker and health-related tasks. By learning from heterogeneous speaker metadata with natural language supervision, the model achieved generalization across languages and clinical populations without task-specific training.\nThis capability addresses a critical challenge in clinical deployment: the need for models that can adapt to diverse populations and novel health conditions where labeled data is scarce or unavailable.</p>\n\n",
                "matched_terms": [
                    "metadata",
                    "pretraining"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision",
        "caption": "Table 2: Classification results comparing supervised and zero-shot approaches across different speaker attribute prediction tasks. All results are reported on the test sets with F1 scores. Best results are bold and second best results are underlined. The Naive Classifier predicts the majority class.",
        "body": "Dimension\nDemog.\nVoc.\nHealth\nTotal\n\n\nNaive Classifier\n50.8\n22.3\n36.9\n34.3\n\n\nOverall (In domain + OOD evaluation)\n\n\nSupervised Baselines\n\n\nOpenSMILE [33]\n\n87.8\n60.3\n39.7\n58.3\n\n\nWhisperM [4]\n\n95.1\n67.7\n56.1\n70.1\n\n\nWhisperL [4]\n\n92.8\n62.4\n55.5\n67.9\n\n\nAudioMAE [8]\n\n88.7\n64.6\n51.5\n64.6\n\n\nSLAP-MAE (ours, λ=0\\lambda=0)\n91.8\n66.7\n51.8\n69.9\n\n\nCLAP-Fused [11]\n\n84.3\n54.5\n32.2\n52.5\n\n\nCLAP-MS [11]\n\n76.4\n60.8\n31.7\n52.5\n\n\nSLAP (ours, λ=1\\lambda=1)\n91.4\n65.2\n57.9\n69.3\n\n\nZero-shot Approaches\n\n\nCLAP-Fused [11]\n\n38.4\n30.4\n29.5\n32.3\n\n\nCLAP-MS [11]\n\n52.3\n35.0\n40.0\n42.4\n\n\nSLAP (ours, λ=1\\lambda=1)\n90.0\n53.3\n51.2\n62.9\n\n\nOOD evaluation\n\n\nSupervised Baselines\n\n\nOpenSMILE [33]\n\n91.4\n60.3\n53.8\n64.8\n\n\nWhisperM [4]\n\n96.4\n67.7\n75.6\n76.2\n\n\nWhisperL [4]\n\n89.9\n62.4\n77.1\n72.9\n\n\nAudioMAE [8]\n\n89.6\n64.6\n71.3\n72.0\n\n\nSLAP-MAE (ours, λ=0\\lambda=0)\n91.6\n66.7\n82.7\n77.0\n\n\nCLAP-Fused [11]\n\n82.9\n54.5\n42.0\n56.5\n\n\nCLAP-MS [11]\n\n77.0\n60.8\n41.8\n58.2\n\n\nSLAP (ours, λ=1\\lambda=1)\n90.7\n65.2\n79.0\n74.9\n\n\nZero-shot Approaches\n\n\nCLAP-Fused [11]\n\n33.3\n30.4\n23.2\n28.8\n\n\nCLAP-MS [11]\n\n52.8\n35.0\n47.1\n42.5\n\n\nSLAP (ours, λ=1\\lambda=1)\n89.6\n53.3\n58.5\n62.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">Dimension</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">Demog.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">Voc.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">Health</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Naive Classifier</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">50.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">22.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">36.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">34.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">Overall (In domain + OOD evaluation)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Supervised Baselines</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">OpenSMILE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib33\" title=\"\">33</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">87.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">60.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">39.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">58.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">WhisperM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">95.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">67.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">56.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">70.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">WhisperL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">92.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">62.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">55.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">67.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">AudioMAE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib8\" title=\"\">8</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">88.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">64.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">51.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">64.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP-MAE (ours, <math alttext=\"\\lambda=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">91.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">66.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">51.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">69.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-Fused <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">84.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">54.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">32.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">52.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-MS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">76.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">60.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">31.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">52.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP (ours, <math alttext=\"\\lambda=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">91.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">65.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">57.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">69.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Zero-shot Approaches</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-Fused <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">38.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">30.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">29.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">32.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-MS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">52.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">35.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">40.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">42.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP (ours, <math alttext=\"\\lambda=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">90.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">53.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">51.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">62.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">OOD evaluation</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Supervised Baselines</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">OpenSMILE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib33\" title=\"\">33</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">91.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">60.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">53.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">64.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">WhisperM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">96.4</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">67.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">75.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">76.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">WhisperL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">89.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">62.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">77.1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">72.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">AudioMAE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib8\" title=\"\">8</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">89.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">64.6</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">71.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">72.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP-MAE (ours, <math alttext=\"\\lambda=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">91.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">66.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">82.7</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">77.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-Fused <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">82.9</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">54.5</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">42.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">56.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-MS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">77.0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">60.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">41.8</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">58.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP (ours, <math alttext=\"\\lambda=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">90.7</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">65.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">79.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">74.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"5\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Zero-shot Approaches</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-Fused <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">33.3</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">30.4</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">23.2</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">28.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">CLAP-MS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">52.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">35.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">47.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">42.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-left:1.6pt;padding-right:1.6pt;\">SLAP (ours, <math alttext=\"\\lambda=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">89.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">53.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">58.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span class=\"ltx_text ltx_font_bold\">62.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "approaches",
            "opensmile",
            "underlined",
            "reported",
            "classification",
            "majority",
            "whisperm",
            "slap",
            "evaluation",
            "prediction",
            "attribute",
            "best",
            "domain",
            "clapms",
            "different",
            "results",
            "slapmae",
            "total",
            "λ0lambda0",
            "demog",
            "bold",
            "scores",
            "overall",
            "ood",
            "sets",
            "whisperl",
            "dimension",
            "clapfused",
            "zeroshot",
            "test",
            "second",
            "λ1lambda1",
            "health",
            "predicts",
            "ours",
            "across",
            "naive",
            "all",
            "comparing",
            "classifier",
            "speaker",
            "class",
            "audiomae",
            "tasks",
            "supervised",
            "voc"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#S4.T2\" title=\"Table 2 &#8227; 4 Results and discussions &#8227; SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the classification results for supervised and zero-shot approaches, considering both OOD and overall (in- and OOD) performance. The results are averaged separately for each task category (demographics, vocal and health) as well as for all tasks. SLAP set state-of-the-art zero-shot results (90.0 F1 on demographics, 51.2 on health), approaching supervised performance. As mentioned in the introduction, the original CLAP model computes mel filter banks with hop sizes of 320ms and window sizes of 1024ms, which is not optimal to capture the fine-grained temporal structure essential for speech. In linear probing, WhisperM obtained the best overall results (F1 70.1), except on medical tasks where SLAP performed best (F1 57.9). Interestingly, SLAP-MAE surpassed SLAP in linear probing, highlighting the robustness of self-supervised pretraining. However, it cannot be evaluated in zero-shot setting. Combining contrastive and self-supervised pretraining enables SLAP to deliver state-of-the-art zero-shot results while remaining strong in linear probing.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech encodes paralinguistic information such as demographics, voice quality, and health. Yet no audio foundation model supports zero-shot or out-of-distribution (OOD) generalization to these tasks. We introduce SLAP (Speaker contrastive Language-Audio Pretraining), the first model aligning speech with natural language descriptions of speaker and health metadata through contrastive learning. SLAP combines a Vision Transformer audio encoder with text encoders, trained on more than 3400 hours across 9 datasets with diverse speaker annotations. We evaluated on 38 binary classification tasks spanning demographics, voice characteristics, and clinical assessments across 14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shot evaluation, a 48% relative improvement over CLAP (42.4%), while demonstrating strong OOD generalization to unseen languages and clinical populations. When fine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achieves best-in-class performance on health tasks (57.9% F1), surpassing larger foundation models.</p>\n\n",
                "matched_terms": [
                    "health",
                    "across",
                    "evaluation",
                    "speaker",
                    "zeroshot",
                    "tasks",
                    "classification",
                    "overall",
                    "slap",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nContrastive learning, Audio-language representation Learning, Speaker modeling, Zero-shot</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech conveys rich paralinguistic information beyond lexical content, revealing who is speaking and how they sound through acoustic cues. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib1\" title=\"\">1</a>]</cite>. These cues encode an important number of speaker attributes including age, sex, emotional state, language background most importantly health status, making speech a non-invasive modality into human physiology, mental health and cognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib2\" title=\"\">2</a>]</cite>. The capability to extract these attributes at scale could greatly help the healthcare delivery system, enabling easy remote monitoring, early detection of relapses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib3\" title=\"\">3</a>]</cite>, and personalized interventions across diverse vulnerable populations and languages.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "health",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While foundation models like Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite> allowed great progress for speech recognition and semantic understanding, they did not integrate any mechanisms to leverage speaker metadata for representation learning or improved inference <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib5\" title=\"\">5</a>]</cite>. Although recent models such as SALMONN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib6\" title=\"\">6</a>]</cite> have made progress toward modeling certain speaker characteristics, current audio foundation models remain limited when evaluated on speaker-centric tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib5\" title=\"\">5</a>]</cite>. For instance, SALMONN freezes its audio encoders, which may constrain the richness of speaker representations and limit their usefulness for tasks such as age, sex, or language recognition, as well as health-related applications.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent variants such as ParaCLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib12\" title=\"\">12</a>]</cite> and EmotionRankCLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib13\" title=\"\">13</a>]</cite> have begun to adapt CLAP for paralinguistic tasks, mainly targeting emotion recognition, and <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib14\" title=\"\">14</a>]</cite> developed speechCLIP for scene retrieval. While these studies illustrate the potential to combine more adapted speech encoders with language supervision, they remained limited to emotional state and do not address the broader range of speaker and health attributes.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "health",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose SLAP (Speaker contrastive Language-Audio Pretraining), the first framework designed for zero-shot inference of speaker and health attributes. SLAP converts heterogeneous annotations into natural language prompts and aligns them with audio through a contrastive objective, enabling out-of-distribution (OOD) generalization to unseen languages and clinical populations. Our contributions are threefold: <span class=\"ltx_text ltx_font_bold\">(i)</span> we designed a CLAP-style audio-language pretraining tailored for speaker, voice and health tasks, <span class=\"ltx_text ltx_font_bold\">(ii)</span> we conducted a large comprehensive evaluation of its kind, 38 tasks across 14 corpora in 7 languages, and <span class=\"ltx_text ltx_font_bold\">(iii)</span> we demonstrated, for the first time, zero-shot and out-of-domain generalization, especially for health-related speech analysis compared to speech foundation models, in addition to achieving state-of-the-art zero-shot and competitive linear probing results.</p>\n\n",
                "matched_terms": [
                    "health",
                    "across",
                    "evaluation",
                    "speaker",
                    "results",
                    "tasks",
                    "zeroshot",
                    "slap",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated SLAP on a broad spectrum of speaker-centric downstream tasks, organized into three categories: demographics, voice characteristics, and health. All tasks are framed as binary classification.</p>\n\n",
                "matched_terms": [
                    "health",
                    "all",
                    "tasks",
                    "classification",
                    "slap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Demographic tasks (11 in total) target attributes such as age group and sex. Voice characteristics tasks (9) involve perceptual judgments such as strength, nasality, and standardized rating scales (GRBAS, CAPE-V). Health-related tasks (18) address clinical screening and diagnosis, including mental health (depression, anxiety, insomnia, fatigue, suicidal ideation) and neurological conditions (Parkinson&#8217;s disease, Alzheimer&#8217;s disease, ALS). In total, our evaluation covers 38 tasks across 14 datasets and 7 languages, providing a comprehensive benchmark for both in-domain, OOD performances and zero-shot generalization.</p>\n\n",
                "matched_terms": [
                    "health",
                    "across",
                    "evaluation",
                    "total",
                    "tasks",
                    "zeroshot",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_italic\">supervised evaluation</span>, we extracted fixed-size representations with each approach and trained a Logistic Regression classifier on the training splits of each dataset.\nFor <span class=\"ltx_text ltx_font_italic\">zero-shot evaluation</span>, we followed the CLAP paradigm by comparing audio embeddings with text embeddings of task-specific prompts. For each binary classification task, we constructed positive and negative prompt pairs. Initial experiments revealed that negation (e.g., &#8220;not depressed&#8221;) led to suboptimal performance at inference time. Instead, we used antonym pairs: <span class=\"ltx_text ltx_font_italic\">&#8221;The speaker is depressed.&#8221;</span> vs. <span class=\"ltx_text ltx_font_italic\">&#8221;The speaker is healthy.&#8221;</span>. Classification is performed by computing cosine similarities between the audio embeddings and the text embeddings, assigning the label with the higher similarity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "classifier",
                    "comparing",
                    "speaker",
                    "classification",
                    "supervised",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We trained SLAP using the Adam optimizer with <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math> and <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math> on 4 NVIDIA 40GB A100 GPUs for 50k steps. We employed an exponential learning rate schedule with 2500 warmup steps, starting from <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> with a learning rate decay factor of 0.99 applied after every 2000 step. The temperature parameter <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> was initialized at <math alttext=\"0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>0.07</mn><annotation encoding=\"application/x-tex\">0.07</annotation></semantics></math>. All audio are resampled to 16 kHz and transformed into log-mel spectrograms with 128 filterbanks, using a 25ms window size and 10ms hop size. There was no data augmentation.</p>\n\n",
                "matched_terms": [
                    "slap",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed our pretraining dataset from 9 diverse speech corpora, comprising over 3.4k hours of audio from more than 10k speakers, along with corresponding metadata and descriptions. The dataset comprises audio-text pairs extracted from Androids <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib23\" title=\"\">23</a>]</cite>, VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>, EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>, SAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib27\" title=\"\">27</a>]</cite>, PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>, CLAC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib22\" title=\"\">22</a>]</cite>, VoxCeleb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib17\" title=\"\">17</a>]</cite>, VoxCeleb2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib18\" title=\"\">18</a>]</cite>, and CN-Celeb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib20\" title=\"\">20</a>]</cite>. These corpora provide rich speaker-level annotations including demographic information (age, sex), linguistic characteristics (e.g.: language, accent), health status (e.g.: Parkinson&#8217;s, Alzheimer&#8217;s, depression), and voice attributes (e.g.: roughness, intensity) that serve as textual supervision signals for our contrastive learning framework. Detailed statistics for each dataset, including the number of speakers, and available metadata fields, are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Pretraining Datasets and Speaker Descriptions &#8227; 3 Experiments &#8227; SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nTo address data imbalance during pretraining, we resampled subsets of large datasets (VoxCeleb1, VoxCeleb2, CN-Celeb1, and CLAC) every 2000 steps.\nBased on preliminary experiments, we generated descriptions with Gemma-3-27b-it <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib31\" title=\"\">31</a>]</cite> and we used Qwen3-Embedding-8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib32\" title=\"\">32</a>]</cite> to embed speaker textual descriptions to match.</p>\n\n",
                "matched_terms": [
                    "health",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We instantiated these 3 task categories using 14 corpora covering diverse populations and languages. We evaluated SLAP in both in-domain and OOD (out-of-distribution) conditions, reporting F1 scores on held-out test sets for zero-shot and supervised baselines.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "slap",
                    "sets",
                    "supervised",
                    "test",
                    "zeroshot",
                    "scores",
                    "ood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Demographics.</span> Sex and age classification tasks are evaluated across Naples Voiced <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib29\" title=\"\">29</a>]</cite>, VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>, PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>, PVQD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib25\" title=\"\">25</a>]</cite>, and EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "across",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Health.</span> Tasks include mental health screening of depression, anxiety, insomnia, and fatigue in French general population (PopGen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib21\" title=\"\">21</a>]</cite>), depression detection in Italian (Androids <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib23\" title=\"\">23</a>]</cite>) and Chinese (Lanzhou <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib26\" title=\"\">26</a>]</cite>), depression and suicidality detection in Spanish psychiatric patients (VOCES <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib19\" title=\"\">19</a>]</cite>), and neurological disease detection: Parkinson&#8217;s and Alzheimer&#8217;s in Slovak (EWA-DB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib16\" title=\"\">16</a>]</cite>), Parkinson&#8217;s in Spanish (Neurovoz <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib28\" title=\"\">28</a>]</cite>) and ALS in Italian (VOC-ALS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib30\" title=\"\">30</a>]</cite>).</p>\n\n",
                "matched_terms": [
                    "health",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared SLAP with several baselines representing different paradigms in speaker tasks: OpenSMILE with eGeMAPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib33\" title=\"\">33</a>]</cite>, AudioMAE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib8\" title=\"\">8</a>]</cite> and the generalist foundation models WhisperM and WhisperL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib4\" title=\"\">4</a>]</cite>.\nWe also pretrained our AudioMAE model, called SLAP-MAE, on the same pretraining datasets as SLAP to compare to pure self-supervised learning (<math alttext=\"\\lambda=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0</annotation></semantics></math>). Both SLAP and SLAP-MAE was initialized with the pretrained weights of AudioMAE. For zero-shot approaches, we compared SLAP to two open-source CLAP models from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#bib.bib11\" title=\"\">11</a>]</cite> <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/clap-htsat-fused\" title=\"\">https://huggingface.co/laion/clap-htsat-fused</a></span></span></span>, the LAION CLAP-Fused model and an optimized version for music and speech referred as CLAP-MS <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/larger_clap_music_and_speech\" title=\"\">https://huggingface.co/laion/larger_clap_music_and_speech</a></span></span></span>. All supervised baselines were trained via linear probing on the train set of each dataset and evaluated on the test split, whereas zero-shot baselines are evaluated directly on the test sets with natural language prompts.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "approaches",
                    "opensmile",
                    "whisperm",
                    "slap",
                    "clapms",
                    "different",
                    "slapmae",
                    "λ0lambda0",
                    "sets",
                    "whisperl",
                    "clapfused",
                    "zeroshot",
                    "test",
                    "all",
                    "speaker",
                    "audiomae",
                    "tasks",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.01860v1#S4.F2\" title=\"Figure 2 &#8227; 4 Results and discussions &#8227; SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we compared performances of SLAP, OpenSmile and WhisperM on all psychiatry, neurology, and vocal dysfunction tasks of open-source clinical datasets. SLAP outperformed WhisperM for the majority of tasks, and while WhisperM showed great performance on depression and vocal tasks overall, performances dropped on neurology tasks.</p>\n\n",
                "matched_terms": [
                    "all",
                    "opensmile",
                    "tasks",
                    "whisperm",
                    "majority",
                    "overall",
                    "slap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SLAP demonstrated that combining contrastive language-audio pretraining with self-supervised learning enables both strong zero-shot performance and competitive supervised results on speaker and health-related tasks. By learning from heterogeneous speaker metadata with natural language supervision, the model achieved generalization across languages and clinical populations without task-specific training.\nThis capability addresses a critical challenge in clinical deployment: the need for models that can adapt to diverse populations and novel health conditions where labeled data is scarce or unavailable.</p>\n\n",
                "matched_terms": [
                    "health",
                    "across",
                    "speaker",
                    "results",
                    "tasks",
                    "supervised",
                    "zeroshot",
                    "slap"
                ]
            }
        ]
    }
}