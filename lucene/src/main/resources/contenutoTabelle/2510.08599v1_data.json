{
    "S4.T1": {
        "caption": "Table 1: Main results. The effect of the two compression stages (Layer Merging and Embedding Decomposition) on the accuracy and efficiency. The Speedup was measured on an M1 MacBook Air, as the time to decode 256 tokens for each utterance in a batch of 10. The high WER may be explained by the fact that Bambara orthography is not as standardized as in English.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params (M)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Decoder layers</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speed (t/s)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speedup (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Whisper-73M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.00x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math> Layer Merging</td>\n<td class=\"ltx_td ltx_align_center\">60</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">102.24</td>\n<td class=\"ltx_td ltx_align_center\">1.54x</td>\n<td class=\"ltx_td ltx_align_center\">34.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math> Embedding Decomposition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">142.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.15x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.49</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "embedding",
            "wer",
            "high",
            "speedup",
            "each",
            "100x",
            "↓downarrow",
            "not",
            "two",
            "time",
            "compression",
            "effect",
            "params",
            "batch",
            "fact",
            "speed",
            "efficiency",
            "explained",
            "accuracy",
            "215x",
            "154x",
            "english",
            "air",
            "main",
            "whisper73m",
            "utterance",
            "standardized",
            "results",
            "measured",
            "decomposition",
            "macbook",
            "tokens",
            "layers",
            "orthography",
            "bambara",
            "↑uparrow",
            "model",
            "stages",
            "layer",
            "merging",
            "decoder",
            "decode"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance.</span> The main results for both compression stages are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 Experiments &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Stage&#160;1 (Layer Merging) only represents 18% parameter reduction compared to Whisper-base. However, after compressing the embedding, the model is <span class=\"ltx_text ltx_font_bold\">48% smaller and retains up to 90% of the base performance</span> while being compressed using only 32h of ASR training data.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Speedup.</span> We benchmarked the inference speed (tokens/s) of the models on a single Macbook Air M1 by recording the wall-clock time of decoding 256 tokens for each utterance in a batch of 10. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 Experiments &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that the compressed model with layer merging is already 1.54x faster compared to the base model, while being only 18% smaller. This is because the decoder of the compressed model has fewer decoder layers, which overall accelerates inference. When additionally compressing the embedding, the inference is <span class=\"ltx_text ltx_font_bold\">2.15x faster.</span>\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Comparison to Whisper-tiny.</span> After compressing Whisper-base using the proposed approach, the resulting model has 38M parameters, the same as Whisper-tiny. While the performance of the two models are comparable, our approach produces a much faster model. Whisper-tiny has 4 decoder layers and a huge embedding matrix, while our compressed Whisper-base has 3 decoder layers and a low-rank embedding matrix. The compressed model achieves <span class=\"ltx_text ltx_font_bold\">142.82</span> tokens per second while Whisper-tiny generates at a speed of <span class=\"ltx_text ltx_font_bold\">116.24</span> tokens per second.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Pruning large pre-trained transformers for low-resource languages is challenging, as it often requires massive retraining data to recover performance. For instance, Distill-Whisper prunes Whisper by 40% and retrains on 21,000 hours of speech, far beyond what is available for most languages. Can Whisper be made lighter and faster for edge devices in data-scarce settings? Focusing on Bambara with only 32h of speech-to-text data, we propose a new pruning recipe. Instead of vocabulary pruning, which is unsuitable due to frequent code-switching by Bambara speakers, we compress the embeddings with low-rank decomposition and feature distillation. Rather than removing layers, we merge them to limit performance loss. The final model preserves 90% of the original performance while being 48% smaller and 2.15x faster on a MacBook Air M1.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "air",
                    "model",
                    "decomposition",
                    "215x",
                    "macbook",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One approach for local edge-device inference for speech recognition is to prune a large, pre-trained, high-performing model into a small model. However, traditional pruning methods often require substantial amounts of retraining data. For example, Distill-Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib1\" title=\"\">1</a>]</cite> prunes the English-only version of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib2\" title=\"\">2</a>]</cite> and further retrained on 21k hours of ASR data, a quantity not available for many languages. This leads us to our central research question:</p>\n\n",
                "matched_terms": [
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We are interested in edge-device inference of Whisper, a multilingual encoder-decoder speech-text model, for Bambara, a low-resource language spoken mainly in Mali. For pruning approaches to be effective in a low-resource scenario, they must preserve the performance of the base model as much as possible. To address this, we propose a two-stage pruning approach. First, we used layer merging as an alternative to simple pruning, aiming to preserve model performance. Second, because the model is intended for a single language, many features in the Whisper multilingual vocabulary matrix are not relevant. This matrix represents over 50% of the decoder parameters in small Whisper models, so we shrink it using activation-aware low-rank decomposition. The resulting compressed model is 48% smaller and 2.14x faster than the original Whisper when deployed offline on a single M1 chip, while still maintaining over 90% of the base performance.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "decoder",
                    "model",
                    "layer",
                    "decomposition",
                    "merging",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deep Encoder, Shallow Decoder</span> We take inspiration from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib3\" title=\"\">3</a>]</cite> who show that the decoder part of encoder-decoder architectures can be very short while still achieving good accuracy and being faster. To make the model lighter and faster, we mainly compress the decoder part of Whisper. However, contrary to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib3\" title=\"\">3</a>]</cite>, which trains encoder-decoder models from scratch, we use a pre-trained Whisper and compress the decoder.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Distill-Whisper</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib1\" title=\"\">1</a>]</cite> compress Whisper-large by removing 30 layers (out of 32) of the decoder. The pruned model further undergoes a retraining on 21k hours with sequence-level knowledge distillation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib4\" title=\"\">4</a>]</cite>. While the resulting model performs as well as the base non-pruned model, the training data is large (2&#771;1k hours of speech) and unavailable for many languages. Instead of removing layers, we merge them to limit the performance drop.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Model Merging</span> We merge the decoder layers to make the model lighter and faster, which is also related to model merging approaches, used for combining models of different tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib6\" title=\"\">6</a>]</cite> or reducing the size of the model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib7\" title=\"\">7</a>]</cite>. However, the approach <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib7\" title=\"\">7</a>]</cite> uses manifold learning to align activations before layer merging. Our approach is simpler as, motivated by the observation that adjacent layers produce similar activations, we simply compress the decoder by applying a weighted average of adjacent layers. We also study the effect of different weighting values and show that a high weight for the top layers produces better results.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Vocabulary pruning.</span> In small multilingual models, the embedding matrix accounts for a large share of the parameters due to the large multilingual vocabulary. For instance, in Whisper-tiny, the embedding matrix represents 51% of the total parameters. A common approach to address this is vocabulary pruning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib9\" title=\"\">9</a>]</cite>, which removes all tokens not used by the target language. However, this method is risky in the presence of out-of-vocabulary words and in code-switching scenarios, where foreign-language tokens must be generated. For example, Bambara speakers frequently mix in French in francophone regions such as Mali, or English in anglophone regions such as Gambia. To address this, we replace vocabulary pruning with a safer alternative: low-rank decomposition combined with feature distillation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Low-Rank Decomposition</span> is a common technique for dimensionality reduction. Recent work has shown that the activations of pre-trained transformers are low-rank, motivating activation-aware low-rank decomposition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib11\" title=\"\">11</a>]</cite>. In Whisper, the embedding matrix is particularly large due to its multilingual vocabulary, and we compress it using activation information rather than applying a standard SVD. However, unlike prior approaches, we train the low-rank embeddings using gradient-based feature distillation.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "embedding",
                    "english",
                    "decoder",
                    "model",
                    "effect",
                    "high",
                    "layer",
                    "results",
                    "decomposition",
                    "merging",
                    "accuracy",
                    "not",
                    "tokens",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Motivation.</span> We are interested in local offline deployment, preferably on mobile devices. To achieve efficient inference on edge devices, our method reduces model size and improves the generation speed using a two-stage compression approach. Both stages are designed to be data-efficient, since Bambara is a low-resource language with only 32 hours of supervised ASR training data available. Our approach compresses only the decoder layer, because in encoder&#8211;decoder model, inference time is tied to the size of the decoder, with smaller decoders being faster. The encoder forwards the speech only once, while the decoder is forwarded for each generated token, which is costly. Starting from a fine-tuned Whisper model, the first stage compresses the decoder by merging consecutive layers rather than pruning them, which helps preserve performance. This already yields a faster model, but more than 50% of the decoder parameters lie in the shared input and output embeddings. In the second stage, we therefore compress these embeddings using a combination of SVD and feature distillation.</p>\n\n",
                "matched_terms": [
                    "time",
                    "bambara",
                    "compression",
                    "model",
                    "stages",
                    "layer",
                    "speed",
                    "each",
                    "merging",
                    "decoder",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To accelerate inference with minimal accuracy drop, we reduce the decoder size by merging layers rather than removing them, thereby mitigating performance degradation. Let <math alttext=\"\\theta^{(l)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\theta^{(l)}</annotation></semantics></math> denote the parameters of the <math alttext=\"l^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi>l</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">l^{th}</annotation></semantics></math> layer. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates this stage. The decoder of Whisper-base is composed of 6 layers, then the pairs of consecutive layers are: <math alttext=\"\\{(\\theta^{(1)},\\theta^{(2)}),(\\theta^{(3)},\\theta^{(4)}),(\\theta^{(5)},\\theta^{(6)})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>4</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>5</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>&#952;</mi><mrow><mo stretchy=\"false\">(</mo><mn>6</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{(\\theta^{(1)},\\theta^{(2)}),(\\theta^{(3)},\\theta^{(4)}),(\\theta^{(5)},\\theta^{(6)})\\}</annotation></semantics></math>; our approach merges each pair of layers through a weighted average:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "each",
                    "merging",
                    "accuracy",
                    "decoder",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> is the <math alttext=\"(i+1)^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">(i+1)^{\\text{th}}</annotation></semantics></math> layer, and <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> are hyperparameters that control the contribution of each layer. Compressing as such Whisper-base results in a decoder with only 3 layers.\nAfter merging, the studen model, which is composed of 6 non-compressed encoder layer and 3 merged layers, undergoes a retraining on an ASR task with a Cross Entropy loss <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math>. We also distill the compressed model (student) using the non-compressed model as a teacher, since distillation has proven to be effective for speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib1\" title=\"\">1</a>]</cite>. Note that only the student is trained (both the encoder and decoder) not the teacher, which is frozen.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "model",
                    "layer",
                    "each",
                    "results",
                    "merging",
                    "not",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For small Whisper models, the input/output embedding matrix <math alttext=\"E\\in\\mathbb{R}^{V\\times h}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>V</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E\\in\\mathbb{R}^{V\\times h}</annotation></semantics></math>, where <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> is the vocabulary size and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> the hidden size, can account for a large amount of the total parameters. Whisper, being a multilingual model, this matrix is large due to the multilingual vocabulary. However, when specializing the model to a single language, as in our case, many features of <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> become redundant, making the embeddings compressible. This means that it is possible to further speed up the decoding by compressing the embeddings. We propose reducing the parameters in this matrix through low-rank decomposition followed by feature distillation. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S3.F2\" title=\"Figure 2 &#8227; 3 BaldWhisper Approach &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates this compression stage. We compress the shared input/output embeddings <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> using singular value decomposition (SVD) to factorize <math alttext=\"E\\approx E_{1}E_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo>&#8776;</mo><mrow><msub><mi>E</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">E\\approx E_{1}E_{2}</annotation></semantics></math>, where <math alttext=\"E_{1}\\in\\mathbb{R}^{V\\times r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><mn>1</mn></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>V</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>r</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E_{1}\\in\\mathbb{R}^{V\\times r}</annotation></semantics></math> and <math alttext=\"E_{2}\\in\\mathbb{R}^{r\\times h}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><mn>2</mn></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>r</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">E_{2}\\in\\mathbb{R}^{r\\times h}</annotation></semantics></math> with <math alttext=\"r\\ll\\min(V,h)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8810;</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">r\\ll\\min(V,h)</annotation></semantics></math>. To ensure that the low-rank embedding maximally preserves the features of the base embedding, we combine Cross Entropy loss with a Feature Distillation objective:</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "compression",
                    "model",
                    "speed",
                    "decomposition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> is the input sequence, <math alttext=\"o\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\"><semantics><mi>o</mi><annotation encoding=\"application/x-tex\">o</annotation></semantics></math> the output from the decoder layer, and <math alttext=\"d(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d(\\cdot,\\cdot)</annotation></semantics></math> is the Feature Distillation loss, which is defined as:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data.</span> We use 32 hours of Bambara speech data from an openly available resource<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/RobotsMali/jeli-asr\" title=\"\">https://huggingface.co/datasets/RobotsMali/jeli-asr</a></span></span></span>. We use 50mn for development, 1h20 for testing and the remaining for training.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Implementation Details.</span> We first fine-tuned the Whisper-73M parameters on the Bambara dataset for 20 epochs, on a single A100-80GB GPU, with a learning rate of <math alttext=\"5e{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">5e{-5}</annotation></semantics></math>. After fine-tuning, we compressed the model using the proposed method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S3\" title=\"3 BaldWhisper Approach &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For the first stage, we used Bayesian optimization to find the merging hyperparameters <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> that minimize the WER on the dev set. We used the implementation provided by <em class=\"ltx_emph ltx_font_italic\">Ax</em><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ax.dev/\" title=\"\">https://ax.dev/</a></span></span></span> for hyperparameter search and optimized for 30 <span class=\"ltx_text ltx_font_italic\">iterations</span>. Then, the merged model undergoes another fine-tuning but using the objective in Eq&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S3.E2\" title=\"In 3.1 Stage 1: Layer Merging &#8227; 3 BaldWhisper Approach &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We also searched, independently of the merging, for the best hyperparameters <math alttext=\"\\lambda,\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>,</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda,\\gamma</annotation></semantics></math> and used a learnable temperature for the Kullback-Leibler Divergence loss. For the second stage, we compressed the input/output embeddings with a rank of <math alttext=\"r=96\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>96</mn></mrow><annotation encoding=\"application/x-tex\">r=96</annotation></semantics></math>, which represents 4x rank reduction. The model with its low-rank input/output embeddings undergoes again another fine-tuning but with the loss in Eq&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S3.Ex1\" title=\"3.2 Stage 2: Activation-Aware Embedding Decomposition &#8227; 3 BaldWhisper Approach &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "bambara",
                    "wer",
                    "whisper73m",
                    "model",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">How to choose the merging parameter <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> &amp; <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>?</span> To choose the best merging weights <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (importance of layer 1) and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m4\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (importance of layer 2), we performed a hyperparameter search using Bayesian optimization as implemented in <span class=\"ltx_text ltx_font_italic\">Ax</span> library. We searched for 30 iterations by training at each time on 30% of the training set and testing on 60% of the development set. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S5.F3\" title=\"Figure 3 &#8227; 5 Analysis &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that the best WER scores tend to concentrate on the zones where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is low.</p>\n\n",
                "matched_terms": [
                    "time",
                    "wer",
                    "layer",
                    "each",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why merging adjacent layers?</span> In the proposed approach, we merge pairs of adjacent layers, but any subset of layers may technically be merged. Exploring the optimal merging strategy certainly deserves its own set of experiments, however, because of limited computing resources and time, we rather relied on the following assumption: merging layers that share similar activations shall limit the impact on performance.\nTherefore, we compare the activation similarities between all possible pair of layers of the decoder. The activations of the decoder are computed by forwarding the supervised validation corpus into Whisper-base. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#S5.F4\" title=\"Figure 4 &#8227; 5 Analysis &#8227; BaldWhisper: Faster Whisper with Head Shearing and Layer Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the results, and we can see that adjacent layers tend to have similar activations, motivating merging pairs of adjacent layers together.</p>\n\n",
                "matched_terms": [
                    "time",
                    "results",
                    "merging",
                    "decoder",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a new pruning approach to work in low-resource scenario and applied it to Whisper for Bambara. We show that merging adjacent layers instead of pruning limits the performance drop. The compressed model, with half of the layers and low-rank embedding, is 2.15x faster and 48% smaller while maintaining over 90% of the performance of the base non-pruned model. Several levers remain to further improve speed and performance, such as specializing <math alttext=\"\\alpha,\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>,</mo><mi>&#946;</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha,\\beta</annotation></semantics></math> per layer and searching for their optimal values. Also, the merging function we used is a simple weighted average, but many new merging methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08599v1#bib.bib13\" title=\"\">13</a>]</cite> may be beneficial.</p>\n\n",
                "matched_terms": [
                    "merging",
                    "bambara",
                    "embedding",
                    "model",
                    "layer",
                    "speed",
                    "215x",
                    "layers"
                ]
            }
        ]
    }
}