{
    "S3.T1": {
        "caption": "Table 1: A concise comparison of different alignment strategies for multimodal speech understanding models. Train part: E = encoder, P = projector, L = LLM (parentheses denote optional part).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Train Part</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Zero-shot Multitask</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Audio, Text)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">P + (L)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 5.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LegoSLM</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Audio, Text)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(P) + L</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AlignFormer</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Audio, Text)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">E + P</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-only</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "alignformer",
            "optional",
            "slam",
            "comparison",
            "zeroshot",
            "strategies",
            "audio",
            "llm",
            "understanding",
            "concise",
            "encoder",
            "training",
            "system",
            "text",
            "alignment",
            "Ã—times",
            "denote",
            "legoslm",
            "parentheses",
            "speech",
            "multimodal",
            "train",
            "multitask",
            "tasu",
            "part",
            "data",
            "projector",
            "textonly",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">By combining these three operations, CPS transforms clean symbolic labels into noisy multi-frame pseudo-posteriors that closely approximate the distributional properties of real audio. To provide a more intuitive understanding of how TASU differs from other alignment paradigms, Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 CTC Posterior Simulation (CPS) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides a concise comparison of alignment paradigms for speech LLMs. In particular, only TASU, trained solely on text, achieves zero-shot performance across multiple tasks with projector parameters trainable only. It is worth noting that LSD achieves an average downsampling ratio of nearly 6 on the experimental data, leading to substantial speedups in both training and inference.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in Speech Large Language Models (Speech LLMs) have paved the way for unified architectures across diverse speech understanding tasks. However, prevailing alignment paradigms rely heavily on large-scale audio-text paired data and computationally intensive training, yet often exhibit limited generalization to unseen domains or tasks. To address these limitations, we propose TASU (Text-only Alignment for Speech Understanding), a novel alignment paradigm that can leverage only unpaired text data to guide cross-modal alignment. Experiments show that TASU achieves competitive zero-shot speech recognition. Leveraging this property, it can further function as a pre-training stage in curriculum learning, enhancing domain generalization in speech recognition. Ultimately, TASU can extend its zero-shot generalization to a wide range of speech understanding tasks and notably outperforms prominent Speech LLMs including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark, establishing TASU as an efficient and scalable alignment paradigm for Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "alignment",
                    "tasu",
                    "understanding",
                    "data",
                    "textonly",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAutomatic speech recognition, Speech large language model, Speech understanding</span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have demonstrated remarkable capability in contextual reasoning and multitask learning, and have been increasingly applied to speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional cascaded systems that rely on automatic speech recognition (ASR) to provide textual input, modern Speech LLMs align speech and text modalities directly through mechanisms such as continuous feature projection or discrete token augmentation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These approaches have enabled state-of-the-art (SOTA) performance in both single-task settings and broad multi-task speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "multitask",
                    "understanding",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, existing alignment paradigms face two major limitations.\nFirst, continuous feature projection, though capable of preserving detailed audio information, often introduces substantial redundancy.\nSuch redundancy not only increases computational cost during training and inference but also raises the risk of overfitting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Second, mitigating these issues typically requires massive amounts of paired audio&#8211;text data and complex training pipelines in order to achieve competitive multitask performance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "multitask",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To alleviate the issue of redundancy in continuous audio features, earlier studies explored alternative representation refinement techniques. The </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC lattice</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, first introduced in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, organizes frame-level CTC posterior distributions into a compact structure that represents all possible alignment paths. Building on this lattice, Chen </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed the Phoneme Synchronous Decoding (PSD) and Label Synchronous Decoding (LSD) methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which exploit CTC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> posteriors to perform efficient variable frame rate search and effectively reduce redundant acoustic frames.\nLiu </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further proposed PSD joint training within end-to-end ASR models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, verifying that the extracted audio semantic representations accelerate model training with almost no loss of semantic information. This representation also enables the speech and text modalities to be aligned at a comparable level of information flow.\nMoreover, compared with raw audio hidden embeddings, CTC posteriors exhibit stronger structural similarity to text, which makes it possible to approximate them using one-hot vectors derived from transcripts.\nThis insight suggests that training can rely on minimal, or even no, real speech data, substantially mitigating the two limitations discussed earlier.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment",
                    "audio",
                    "data",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Motivated by these, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TASU (<span class=\"ltx_text ltx_font_bold\">T</span>ext-only <span class=\"ltx_text ltx_font_bold\">A</span>lignment for <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel alignment paradigm that achieves robust cross-modal alignment without relying on audio supervision. We similarly use LSD to extract audio CTC posteriors into compact &#8220;codebook&#8221;-like features, preserving semantic content while removing redundancy. From the text side, we introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CTC posterior simulation (CPS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which mimics real CTC distributions, including frame dropping and repetition, to generate pseudo-&#8220;codebooks&#8221; from text-only data. This dual design allows TASU to bridge modalities efficiently while keeping the LLM backbone frozen, thus retaining its inherent multitask capability. In this work, we focus on semantic speech understanding tasks, which are representative of core challenges in spoken language processing and well-suited for evaluating multitask performance in Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "multitask",
                    "tasu",
                    "llm",
                    "understanding",
                    "data",
                    "textonly",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Zero-shot Speech Recognition and Domain Generalization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We show that TASU alone delivers zero-shot ASR with small accuracy degradation relative to audio-text supervision in in-domain evaluation; when leveraged as a curriculum pre-training stage, it further enhances domain generalization while preserving source-domain accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasu",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multitask Generalization in Speech Understanding:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TASU enables Speech LLMs to achieve strong zero-shot generalization on speech understanding tasks using limited task-specific text data. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU surpasses mainstream alignment paradigms such as SLAM at the same data scale, and further outperforms large-scale speech models including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SALMONN-13B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "alignment",
                    "multitask",
                    "tasu",
                    "understanding",
                    "slam",
                    "data",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU: Audio-Efficient and Generalizable Speech&#8211;Text Alignment:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LSD achieves nearly 6</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> downsampling, greatly accelerating while enhancing semantic extraction and also alleviating overfitting; meanwhile, CPS markedly reduces the reliance on audio data and helps domain generalization in recognition and multitask generalization in speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "multitask",
                    "tasu",
                    "understanding",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">There are already methods that leverage CTC posterior probabilities to address the two issues outlined in the Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. First, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses CTC signals to downsample acoustic features more effectively and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to some extent, demonstrates strong instruction-following ability. However, when relying only on a small amount of paired audio-text data, its multitask performance degrades markedly, with accuracy on multiple-choice tasks approaching chance. In contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs CTC posteriors as weights to reweight LLM word embeddings for acoustic representation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding more structured representations; yet it sacrifices multitask capability and requires large-scale data to re-align the LLM&#8217;s vocabulary.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignformer",
                    "multitask",
                    "llm",
                    "legoslm",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional alignment strategies for Speech LLMs often rely on encoder hidden states with heuristic subsampling, which either produce redundant and noisy representations or risk discarding critical information. In addition, acoustic features exhibit high temporal variability that mismatches the structured nature of text embeddings, making cross-modal alignment challenging. To address these issues, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech and text directly at the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC posterior level</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The key idea is to establish a unified posterior interface for both training and inference:</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "strategies",
                    "tasu",
                    "encoder",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcriptions are tokenized into one-hot vectors and transformed into pseudo-posteriors by the CPS module, which supervise the trainable projector.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "projector",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> raw speech is encoded into real CTC posteriors, refined by LSD, and mapped by the pretrained projector into the frozen LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "projector",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this way, TASU enables text-only training while ensuring that both modalities share compact, structured, and semantically aligned posterior representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "textonly",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CTC decoding typically involves large portions of blank symbols and consecutive repetitions of the same token&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDirectly feeding such posteriors into a Speech LLM would propagate redundancy and obscure semantics.\nLSD is designed to compact the sequence while preserving semantic fidelity through two operations.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable training with text-only data, we propose CPS, which converts each ground-truth token into a pseudo-posterior sequence </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CPS consists of three stochastic stages that mimic the variability of real CTC outputs, as detailed in Algorithm&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#algorithm1\" style=\"font-size:90%;\" title=\"In 3.1 Label-Synchronous Decoding (LSD) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(2) Random Deletions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nEach element of </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is removed independently with probability </span>\n  <math alttext=\"p_{\\text{del}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">del</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{del}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, simulating token drops commonly observed in CTC alignments.\nThis operation models the fact that non-blank tokens can occasionally disappear due to alignment errors, forcing the system to be robust to missing evidence.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the two core processes described in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3\" style=\"font-size:90%;\" title=\"3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU can enable zero-shot transfer from text-only training to speech inference.\nTo validate its rationality and effectiveness, we conduct a series of controlled experiments with step-by-step verification.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tasu",
                    "textonly",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nSince TASU relies on reliable CTC posterior probabilities, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SenseVoice-Small</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the speech encoder and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Qwen2.5-1.5B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the language model backbone.\nThe projector is instantiated as a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Linear&#8211;SiLU&#8211;Linear</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> module, with only its parameters being trainable. Bottleneck is typically set to 1024. For broader speech understanding tasks, it is set to 2048, as in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "understanding",
                    "projector",
                    "encoder",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Data.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor ASR, the datasets include </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor speech-to-text translation (ST), we use </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and for spoken instruction understanding, we adopt </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dataset and Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our model on both ASR and Speech Understanding tasks. For ASR, we report Word Error Rate (WER) on the standard in-domain test sets. To further assess generalization, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TED-LIUM 3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, testing robustness to a distinct topical and acoustic domain (lectures). For speech understanding task, we assess performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. WER is computed using the official </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Wenet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> toolkit&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present experimental results in two parts. First, we show that TASU enables zero-shot speech recognition and, when used as a curriculum pre-training stage, allows models fine-tuned only on source-domain audio data to generalize effectively to new domains.\nSecond, we evaluate TASU on multitask speech understanding, where it achieves zero-shot generalization from limited text and delivers strong performance on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "audio",
                    "multitask",
                    "tasu",
                    "understanding",
                    "data",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of TASU in speech recognition, we conduct a series of experiments as summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To enable a controlled comparison, we implement the SLAM alignment strategy proposed in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without performing downsampling to avoid potential performance degradation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean and test-other sets, TASU shows only less than 1.5% WER gap compared to the baseline, demonstrating that it can achieve reasonable semantic alignment without paired audio&#8211;text training. Furthermore, when </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transcripts are incorporated into TASU training, we observe consistent improvements on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> itself, and even surpass the baseline on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in new domain.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "tasu",
                    "slam",
                    "training",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further explore scalability, we extend TASU as the pre-training stage of Curriculum Learning. In this stage, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcripts are used to simulate CTC posteriors for training, followed by fine-tuning with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio&#8211;text pairs. Results show that TASU not only maintains performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Librispeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> but also yields substantial gains in both </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These findings highlight the scalability of TASU in leveraging large-scale text-only resources for domain generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "textonly",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Model architecture and training setup kept unchanged in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. SLAM refers to the alignment paradigm adopted in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Considering the differences in training configurations and convergence issues of the alignment paradigms, results for </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are not reported. We find that LSD can almost fully preserve the semantic information of speech and also alleviates model overfitting, while playing an indispensable role within TASU.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignformer",
                    "alignment",
                    "tasu",
                    "legoslm",
                    "slam",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further investigate the performance of TASU on multi-task speech semantic understanding, we conduct the experiments summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We still consider SLAM method as the baseline: using hidden states as projection features without downsampling, which reflects the prevalent alignment paradigm in most existing Speech LLMs. Given that the SLAM architecture fails to develop multitask capabilities when trained on limited task-specific data, we expanded the training data to ensure a fair comparison: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ASR, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ST, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for instruction understanding. TASU only uses text, while TASU (+SFT) uses half of audio-text pairs for the second-stage SFT. In addition, to provide a more intuitive assessment of TASU, we further compare it with the results of other Speech LLMs in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "multitask",
                    "tasu",
                    "understanding",
                    "slam",
                    "data",
                    "training",
                    "speech",
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As we can see, TASU demonstrates strong zero-shot multitask generalization for speech understanding: without any audio&#8211;text pairs, it achieves better result on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than SLAM. When half of audio-text data is incorporated for SFT, the model shows rapid improvement on the ASR and ST tasks. Notably, TASU even surpasses several large-scale Speech LLMs, underscoring its efficiency as a lightweight yet effective paradigm for speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "multitask",
                    "tasu",
                    "understanding",
                    "slam",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TASU, a novel alignment paradigm for Speech LLMs trained solely on text data. On the one hand, TASU enables zero-shot speech recognition with only a minor accuracy drop. It can further serve as the first stage of curriculum learning in ASR, improving performance on new target domains while preserving recognition accuracy on the source domain. On the other hand, TASU delivers strong zero-shot multitask speech understanding with limited text data, highlighting its potential as a simple yet effective paradigm for scalable and generalizable Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "alignment",
                    "multitask",
                    "tasu",
                    "understanding",
                    "data",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the future, we aim to further refine the CPS approach to narrow the gap between real CTC posteriors derived from audio and pseudo-posteriors generated from text. This will enable a more accurate audio-free alignment paradigm. Moreover, by incorporating large-scale text data, we plan to explore the scalability and performance of this alignment method on a greater scale.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data",
                    "text",
                    "alignment"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Comparison of different alignment paradigms. All systems share the same components and training setup with only projector trainable. Libri = LibriSpeech, Ted-3 = TedLium-3, Slide = SlideSpeech. Results are WER%. TASU (+SFT) denotes a two-stage curriculum learning process.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Train Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\">Libri</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text ltx_font_italic\">clean/other</span></span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Slide</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ted-3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Audio, Text)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.72 / 8.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.57 / 9.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri + Slide</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.21 / 10.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU (+SFT)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.55 / </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri + Slide</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Libri</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.06</span><span class=\"ltx_text\" style=\"font-size:90%;\"> / 8.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.35pt;padding-bottom:0.35pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.40</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "paradigms",
            "libri",
            "wer",
            "setup",
            "librispeech",
            "twostage",
            "denotes",
            "slam",
            "slidespeech",
            "curriculum",
            "comparison",
            "systems",
            "audio",
            "cleanother",
            "learning",
            "training",
            "slide",
            "system",
            "trainable",
            "text",
            "alignment",
            "ted3",
            "tedlium3",
            "sft",
            "results",
            "only",
            "same",
            "components",
            "process",
            "share",
            "tasu",
            "train",
            "all",
            "data",
            "projector",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of TASU in speech recognition, we conduct a series of experiments as summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To enable a controlled comparison, we implement the SLAM alignment strategy proposed in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without performing downsampling to avoid potential performance degradation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean and test-other sets, TASU shows only less than 1.5% WER gap compared to the baseline, demonstrating that it can achieve reasonable semantic alignment without paired audio&#8211;text training. Furthermore, when </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transcripts are incorporated into TASU training, we observe consistent improvements on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> itself, and even surpass the baseline on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in new domain.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ablation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To further justify the rationality of the baseline presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LSD, ablation studies were conducted to compare recognition performance under the current alignment paradigm.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in Speech Large Language Models (Speech LLMs) have paved the way for unified architectures across diverse speech understanding tasks. However, prevailing alignment paradigms rely heavily on large-scale audio-text paired data and computationally intensive training, yet often exhibit limited generalization to unseen domains or tasks. To address these limitations, we propose TASU (Text-only Alignment for Speech Understanding), a novel alignment paradigm that can leverage only unpaired text data to guide cross-modal alignment. Experiments show that TASU achieves competitive zero-shot speech recognition. Leveraging this property, it can further function as a pre-training stage in curriculum learning, enhancing domain generalization in speech recognition. Ultimately, TASU can extend its zero-shot generalization to a wide range of speech understanding tasks and notably outperforms prominent Speech LLMs including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark, establishing TASU as an efficient and scalable alignment paradigm for Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigms",
                    "alignment",
                    "tasu",
                    "learning",
                    "data",
                    "training",
                    "curriculum",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have demonstrated remarkable capability in contextual reasoning and multitask learning, and have been increasingly applied to speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional cascaded systems that rely on automatic speech recognition (ASR) to provide textual input, modern Speech LLMs align speech and text modalities directly through mechanisms such as continuous feature projection or discrete token augmentation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These approaches have enabled state-of-the-art (SOTA) performance in both single-task settings and broad multi-task speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "learning",
                    "text",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, existing alignment paradigms face two major limitations.\nFirst, continuous feature projection, though capable of preserving detailed audio information, often introduces substantial redundancy.\nSuch redundancy not only increases computational cost during training and inference but also raises the risk of overfitting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Second, mitigating these issues typically requires massive amounts of paired audio&#8211;text data and complex training pipelines in order to achieve competitive multitask performance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigms",
                    "alignment",
                    "audio",
                    "data",
                    "training",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To alleviate the issue of redundancy in continuous audio features, earlier studies explored alternative representation refinement techniques. The </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC lattice</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, first introduced in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, organizes frame-level CTC posterior distributions into a compact structure that represents all possible alignment paths. Building on this lattice, Chen </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed the Phoneme Synchronous Decoding (PSD) and Label Synchronous Decoding (LSD) methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which exploit CTC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> posteriors to perform efficient variable frame rate search and effectively reduce redundant acoustic frames.\nLiu </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further proposed PSD joint training within end-to-end ASR models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, verifying that the extracted audio semantic representations accelerate model training with almost no loss of semantic information. This representation also enables the speech and text modalities to be aligned at a comparable level of information flow.\nMoreover, compared with raw audio hidden embeddings, CTC posteriors exhibit stronger structural similarity to text, which makes it possible to approximate them using one-hot vectors derived from transcripts.\nThis insight suggests that training can rely on minimal, or even no, real speech data, substantially mitigating the two limitations discussed earlier.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "all",
                    "data",
                    "training",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Motivated by these, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TASU (<span class=\"ltx_text ltx_font_bold\">T</span>ext-only <span class=\"ltx_text ltx_font_bold\">A</span>lignment for <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel alignment paradigm that achieves robust cross-modal alignment without relying on audio supervision. We similarly use LSD to extract audio CTC posteriors into compact &#8220;codebook&#8221;-like features, preserving semantic content while removing redundancy. From the text side, we introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CTC posterior simulation (CPS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which mimics real CTC distributions, including frame dropping and repetition, to generate pseudo-&#8220;codebooks&#8221; from text-only data. This dual design allows TASU to bridge modalities efficiently while keeping the LLM backbone frozen, thus retaining its inherent multitask capability. In this work, we focus on semantic speech understanding tasks, which are representative of core challenges in spoken language processing and well-suited for evaluating multitask performance in Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "tasu",
                    "data",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Zero-shot Speech Recognition and Domain Generalization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We show that TASU alone delivers zero-shot ASR with small accuracy degradation relative to audio-text supervision in in-domain evaluation; when leveraged as a curriculum pre-training stage, it further enhances domain generalization while preserving source-domain accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "curriculum"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multitask Generalization in Speech Understanding:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TASU enables Speech LLMs to achieve strong zero-shot generalization on speech understanding tasks using limited task-specific text data. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU surpasses mainstream alignment paradigms such as SLAM at the same data scale, and further outperforms large-scale speech models including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SALMONN-13B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "same",
                    "paradigms",
                    "alignment",
                    "tasu",
                    "slam",
                    "data",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU: Audio-Efficient and Generalizable Speech&#8211;Text Alignment:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LSD achieves nearly 6</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> downsampling, greatly accelerating while enhancing semantic extraction and also alleviating overfitting; meanwhile, CPS markedly reduces the reliance on audio data and helps domain generalization in recognition and multitask generalization in speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "audio",
                    "tasu",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">There are already methods that leverage CTC posterior probabilities to address the two issues outlined in the Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. First, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses CTC signals to downsample acoustic features more effectively and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to some extent, demonstrates strong instruction-following ability. However, when relying only on a small amount of paired audio-text data, its multitask performance degrades markedly, with accuracy on multiple-choice tasks approaching chance. In contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs CTC posteriors as weights to reweight LLM word embeddings for acoustic representation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding more structured representations; yet it sacrifices multitask capability and requires large-scale data to re-align the LLM&#8217;s vocabulary.</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional alignment strategies for Speech LLMs often rely on encoder hidden states with heuristic subsampling, which either produce redundant and noisy representations or risk discarding critical information. In addition, acoustic features exhibit high temporal variability that mismatches the structured nature of text embeddings, making cross-modal alignment challenging. To address these issues, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech and text directly at the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC posterior level</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The key idea is to establish a unified posterior interface for both training and inference:</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "tasu",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcriptions are tokenized into one-hot vectors and transformed into pseudo-posteriors by the CPS module, which supervise the trainable projector.</span>\n</p>\n\n",
                "matched_terms": [
                    "trainable",
                    "text",
                    "projector",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this way, TASU enables text-only training while ensuring that both modalities share compact, structured, and semantically aligned posterior representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "share",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">J</mi>\n      <annotation encoding=\"application/x-tex\">J</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the number of frames retained after Eq.&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.E1\" style=\"font-size:90%;\" title=\"In 3.1 Label-Synchronous Decoding (LSD) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThis process eliminates blank-dominated frames and collapses redundant repetitions, yielding temporally compact posteriors that retain essential information for alignment. The proposed method achieves significant compression of acoustic feature sequences without sacrificing semantic completeness.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable training with text-only data, we propose CPS, which converts each ground-truth token into a pseudo-posterior sequence </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CPS consists of three stochastic stages that mimic the variability of real CTC outputs, as detailed in Algorithm&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#algorithm1\" style=\"font-size:90%;\" title=\"In 3.1 Label-Synchronous Decoding (LSD) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(2) Random Deletions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nEach element of </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is removed independently with probability </span>\n  <math alttext=\"p_{\\text{del}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">del</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{del}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, simulating token drops commonly observed in CTC alignments.\nThis operation models the fact that non-blank tokens can occasionally disappear due to alignment errors, forcing the system to be robust to missing evidence.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">By combining these three operations, CPS transforms clean symbolic labels into noisy multi-frame pseudo-posteriors that closely approximate the distributional properties of real audio. To provide a more intuitive understanding of how TASU differs from other alignment paradigms, Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 CTC Posterior Simulation (CPS) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides a concise comparison of alignment paradigms for speech LLMs. In particular, only TASU, trained solely on text, achieves zero-shot performance across multiple tasks with projector parameters trainable only. It is worth noting that LSD achieves an average downsampling ratio of nearly 6 on the experimental data, leading to substantial speedups in both training and inference.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigms",
                    "alignment",
                    "audio",
                    "tasu",
                    "data",
                    "projector",
                    "training",
                    "trainable",
                    "only",
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the two core processes described in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3\" style=\"font-size:90%;\" title=\"3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU can enable zero-shot transfer from text-only training to speech inference.\nTo validate its rationality and effectiveness, we conduct a series of controlled experiments with step-by-step verification.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nSince TASU relies on reliable CTC posterior probabilities, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SenseVoice-Small</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the speech encoder and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Qwen2.5-1.5B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the language model backbone.\nThe projector is instantiated as a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Linear&#8211;SiLU&#8211;Linear</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> module, with only its parameters being trainable. Bottleneck is typically set to 1024. For broader speech understanding tasks, it is set to 2048, as in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "trainable",
                    "only",
                    "projector",
                    "tasu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Data.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor ASR, the datasets include </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor speech-to-text translation (ST), we use </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and for spoken instruction understanding, we adopt </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "slidespeech",
                    "data",
                    "librispeech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor LSD, parameter </span>\n  <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#964;</mi>\n      <annotation encoding=\"application/x-tex\">\\tau</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is set to </span>\n  <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.9</mn>\n      <annotation encoding=\"application/x-tex\">0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For CPS, we set the label smoothing range </span>\n  <math alttext=\"(\\lambda_{\\text{low}},\\lambda_{\\text{high}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#955;</mi>\n          <mtext mathsize=\"0.900em\">low</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#955;</mi>\n          <mtext mathsize=\"0.900em\">high</mtext>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\lambda_{\\text{low}},\\lambda_{\\text{high}})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"(0.8,1.0)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <mn mathsize=\"0.900em\">0.8</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1.0</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(0.8,1.0)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the deletion and duplication probabilities, </span>\n  <math alttext=\"p_{\\text{del}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">del</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{del}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"p_{\\text{dup}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">dup</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{dup}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, are both set to </span>\n  <math alttext=\"0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.05</mn>\n      <annotation encoding=\"application/x-tex\">0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The learning rate is fixed at </span>\n  <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with 5 training epochs.\nCheckpoints are selected when the evaluation loss stops decreasing.</span>\n</p>\n\n",
                "matched_terms": [
                    "learning",
                    "setup",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dataset and Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our model on both ASR and Speech Understanding tasks. For ASR, we report Word Error Rate (WER) on the standard in-domain test sets. To further assess generalization, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TED-LIUM 3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, testing robustness to a distinct topical and acoustic domain (lectures). For speech understanding task, we assess performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. WER is computed using the official </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Wenet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> toolkit&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "setup",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present experimental results in two parts. First, we show that TASU enables zero-shot speech recognition and, when used as a curriculum pre-training stage, allows models fine-tuned only on source-domain audio data to generalize effectively to new domains.\nSecond, we evaluate TASU on multitask speech understanding, where it achieves zero-shot generalization from limited text and delivers strong performance on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tasu",
                    "data",
                    "results",
                    "curriculum",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further explore scalability, we extend TASU as the pre-training stage of Curriculum Learning. In this stage, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcripts are used to simulate CTC posteriors for training, followed by fine-tuning with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio&#8211;text pairs. Results show that TASU not only maintains performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Librispeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> but also yields substantial gains in both </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These findings highlight the scalability of TASU in leveraging large-scale text-only resources for domain generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "tedlium3",
                    "tasu",
                    "librispeech",
                    "learning",
                    "slidespeech",
                    "training",
                    "curriculum",
                    "results",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Model architecture and training setup kept unchanged in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. SLAM refers to the alignment paradigm adopted in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Considering the differences in training configurations and convergence issues of the alignment paradigms, results for </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are not reported. We find that LSD can almost fully preserve the semantic information of speech and also alleviates model overfitting, while playing an indispensable role within TASU.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigms",
                    "alignment",
                    "tasu",
                    "setup",
                    "slam",
                    "training",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further investigate the performance of TASU on multi-task speech semantic understanding, we conduct the experiments summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We still consider SLAM method as the baseline: using hidden states as projection features without downsampling, which reflects the prevalent alignment paradigm in most existing Speech LLMs. Given that the SLAM architecture fails to develop multitask capabilities when trained on limited task-specific data, we expanded the training data to ensure a fair comparison: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ASR, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ST, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for instruction understanding. TASU only uses text, while TASU (+SFT) uses half of audio-text pairs for the second-stage SFT. In addition, to provide a more intuitive assessment of TASU, we further compare it with the results of other Speech LLMs in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "tasu",
                    "librispeech",
                    "slam",
                    "sft",
                    "data",
                    "training",
                    "results",
                    "only",
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As we can see, TASU demonstrates strong zero-shot multitask generalization for speech understanding: without any audio&#8211;text pairs, it achieves better result on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than SLAM. When half of audio-text data is incorporated for SFT, the model shows rapid improvement on the ASR and ST tasks. Notably, TASU even surpasses several large-scale Speech LLMs, underscoring its efficiency as a lightweight yet effective paradigm for speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "tasu",
                    "slam",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TASU, a novel alignment paradigm for Speech LLMs trained solely on text data. On the one hand, TASU enables zero-shot speech recognition with only a minor accuracy drop. It can further serve as the first stage of curriculum learning in ASR, improving performance on new target domains while preserving recognition accuracy on the source domain. On the other hand, TASU delivers strong zero-shot multitask speech understanding with limited text data, highlighting its potential as a simple yet effective paradigm for scalable and generalizable Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "tasu",
                    "learning",
                    "data",
                    "curriculum",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the future, we aim to further refine the CPS approach to narrow the gap between real CTC posteriors derived from audio and pseudo-posteriors generated from text. This will enable a more accurate audio-free alignment paradigm. Moreover, by incorporating large-scale text data, we plan to explore the scalability and performance of this alignment method on a greater scale.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data",
                    "text",
                    "alignment"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Ablation study on LSD (WER%). All models are only trained on Librispeech with the same structure. CTC refers to CTC posterior. Note that TASU without LSD fails to work, resulting in unusable WER scores.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Projection Feature</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">Libri</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_italic\">clean/ other</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Slide</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ted-3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Hidden</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.72 / 8.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM-CTC</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.79 / 8.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pseudo CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM-CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.13</span><span class=\"ltx_text\" style=\"font-size:90%;\"> / 8.59</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.61</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pseudo CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.57 / 9.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU (+SFT)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">(Pseudo) CTC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.55 / </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.38</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "lsd",
            "models",
            "libri",
            "hidden",
            "wer",
            "librispeech",
            "ctc",
            "slam",
            "feature",
            "projection",
            "study",
            "ablation",
            "slide",
            "system",
            "trained",
            "resulting",
            "scores",
            "ted3",
            "structure",
            "posterior",
            "without",
            "Ã—times",
            "note",
            "sft",
            "unusable",
            "slamctc",
            "only",
            "pseudo",
            "fails",
            "same",
            "clean",
            "tasu",
            "work",
            "all",
            "other",
            "refers"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Model architecture and training setup kept unchanged in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. SLAM refers to the alignment paradigm adopted in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Considering the differences in training configurations and convergence issues of the alignment paradigms, results for </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are not reported. We find that LSD can almost fully preserve the semantic information of speech and also alleviates model overfitting, while playing an indispensable role within TASU.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in Speech Large Language Models (Speech LLMs) have paved the way for unified architectures across diverse speech understanding tasks. However, prevailing alignment paradigms rely heavily on large-scale audio-text paired data and computationally intensive training, yet often exhibit limited generalization to unseen domains or tasks. To address these limitations, we propose TASU (Text-only Alignment for Speech Understanding), a novel alignment paradigm that can leverage only unpaired text data to guide cross-modal alignment. Experiments show that TASU achieves competitive zero-shot speech recognition. Leveraging this property, it can further function as a pre-training stage in curriculum learning, enhancing domain generalization in speech recognition. Ultimately, TASU can extend its zero-shot generalization to a wide range of speech understanding tasks and notably outperforms prominent Speech LLMs including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark, establishing TASU as an efficient and scalable alignment paradigm for Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "only",
                    "tasu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have demonstrated remarkable capability in contextual reasoning and multitask learning, and have been increasingly applied to speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional cascaded systems that rely on automatic speech recognition (ASR) to provide textual input, modern Speech LLMs align speech and text modalities directly through mechanisms such as continuous feature projection or discrete token augmentation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These approaches have enabled state-of-the-art (SOTA) performance in both single-task settings and broad multi-task speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "projection",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, existing alignment paradigms face two major limitations.\nFirst, continuous feature projection, though capable of preserving detailed audio information, often introduces substantial redundancy.\nSuch redundancy not only increases computational cost during training and inference but also raises the risk of overfitting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Second, mitigating these issues typically requires massive amounts of paired audio&#8211;text data and complex training pipelines in order to achieve competitive multitask performance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "only",
                    "projection",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To alleviate the issue of redundancy in continuous audio features, earlier studies explored alternative representation refinement techniques. The </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC lattice</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, first introduced in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, organizes frame-level CTC posterior distributions into a compact structure that represents all possible alignment paths. Building on this lattice, Chen </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed the Phoneme Synchronous Decoding (PSD) and Label Synchronous Decoding (LSD) methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which exploit CTC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> posteriors to perform efficient variable frame rate search and effectively reduce redundant acoustic frames.\nLiu </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further proposed PSD joint training within end-to-end ASR models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, verifying that the extracted audio semantic representations accelerate model training with almost no loss of semantic information. This representation also enables the speech and text modalities to be aligned at a comparable level of information flow.\nMoreover, compared with raw audio hidden embeddings, CTC posteriors exhibit stronger structural similarity to text, which makes it possible to approximate them using one-hot vectors derived from transcripts.\nThis insight suggests that training can rely on minimal, or even no, real speech data, substantially mitigating the two limitations discussed earlier.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "models",
                    "hidden",
                    "structure",
                    "posterior",
                    "ctc",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Motivated by these, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TASU (<span class=\"ltx_text ltx_font_bold\">T</span>ext-only <span class=\"ltx_text ltx_font_bold\">A</span>lignment for <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel alignment paradigm that achieves robust cross-modal alignment without relying on audio supervision. We similarly use LSD to extract audio CTC posteriors into compact &#8220;codebook&#8221;-like features, preserving semantic content while removing redundancy. From the text side, we introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CTC posterior simulation (CPS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which mimics real CTC distributions, including frame dropping and repetition, to generate pseudo-&#8220;codebooks&#8221; from text-only data. This dual design allows TASU to bridge modalities efficiently while keeping the LLM backbone frozen, thus retaining its inherent multitask capability. In this work, we focus on semantic speech understanding tasks, which are representative of core challenges in spoken language processing and well-suited for evaluating multitask performance in Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "posterior",
                    "without",
                    "tasu",
                    "ctc",
                    "work"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multitask Generalization in Speech Understanding:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TASU enables Speech LLMs to achieve strong zero-shot generalization on speech understanding tasks using limited task-specific text data. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU surpasses mainstream alignment paradigms such as SLAM at the same data scale, and further outperforms large-scale speech models including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SALMONN-13B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "slam",
                    "tasu",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU: Audio-Efficient and Generalizable Speech&#8211;Text Alignment:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LSD achieves nearly 6</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> downsampling, greatly accelerating while enhancing semantic extraction and also alleviating overfitting; meanwhile, CPS markedly reduces the reliance on audio data and helps domain generalization in recognition and multitask generalization in speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "tasu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Connectionist Temporal Classification (CTC) introduces an explicit </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">blank</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> symbol and marginalizes over all possible alignments, thereby mapping unsegmented acoustic sequences into variable-length label sequences. Its &#8220;collapse&#8221; operation, which removes blanks and merges repetitions, projects frame-level posteriors into a compact representation that can be directly exploited for decoding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "ctc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">There are already methods that leverage CTC posterior probabilities to address the two issues outlined in the Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. First, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses CTC signals to downsample acoustic features more effectively and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to some extent, demonstrates strong instruction-following ability. However, when relying only on a small amount of paired audio-text data, its multitask performance degrades markedly, with accuracy on multiple-choice tasks approaching chance. In contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs CTC posteriors as weights to reweight LLM word embeddings for acoustic representation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding more structured representations; yet it sacrifices multitask capability and requires large-scale data to re-align the LLM&#8217;s vocabulary.</span>\n</p>\n\n",
                "matched_terms": [
                    "only",
                    "ctc",
                    "posterior"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional alignment strategies for Speech LLMs often rely on encoder hidden states with heuristic subsampling, which either produce redundant and noisy representations or risk discarding critical information. In addition, acoustic features exhibit high temporal variability that mismatches the structured nature of text embeddings, making cross-modal alignment challenging. To address these issues, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech and text directly at the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC posterior level</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The key idea is to establish a unified posterior interface for both training and inference:</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "ctc",
                    "hidden",
                    "posterior"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> raw speech is encoded into real CTC posteriors, refined by LSD, and mapped by the pretrained projector into the frozen LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "ctc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this way, TASU enables text-only training while ensuring that both modalities share compact, structured, and semantically aligned posterior representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "posterior"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CTC decoding typically involves large portions of blank symbols and consecutive repetitions of the same token&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDirectly feeding such posteriors into a Speech LLM would propagate redundancy and obscure semantics.\nLSD is designed to compact the sequence while preserving semantic fidelity through two operations.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "same",
                    "ctc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">J</mi>\n      <annotation encoding=\"application/x-tex\">J</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the number of frames retained after Eq.&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.E1\" style=\"font-size:90%;\" title=\"In 3.1 Label-Synchronous Decoding (LSD) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThis process eliminates blank-dominated frames and collapses redundant repetitions, yielding temporally compact posteriors that retain essential information for alignment. The proposed method achieves significant compression of acoustic feature sequences without sacrificing semantic completeness.</span>\n</p>\n\n",
                "matched_terms": [
                    "without",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(2) Random Deletions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nEach element of </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is removed independently with probability </span>\n  <math alttext=\"p_{\\text{del}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">del</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{del}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, simulating token drops commonly observed in CTC alignments.\nThis operation models the fact that non-blank tokens can occasionally disappear due to alignment errors, forcing the system to be robust to missing evidence.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "ctc",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"p_{\\text{ins}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">ins</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{ins}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls the insertion rate. For each insertion, a position </span>\n  <math alttext=\"pos\\in{0,\\dots,|\\tilde{\\mathbf{S}}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">&#119826;</mi>\n              <mo mathsize=\"0.900em\">~</mo>\n            </mover>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">pos\\in{0,\\dots,|\\tilde{\\mathbf{S}}|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is sampled, and either:\n(i) a duplicate of </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}[\\max(0,pos-1)]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119826;</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mrow>\n            <mi mathsize=\"0.900em\">max</mi>\n            <mo>&#8289;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mn mathsize=\"0.900em\">0</mn>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mrow>\n                <mrow>\n                  <mi mathsize=\"0.900em\">p</mi>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mi mathsize=\"0.900em\">o</mi>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mi mathsize=\"0.900em\">s</mi>\n                </mrow>\n                <mo mathsize=\"0.900em\">&#8722;</mo>\n                <mn mathsize=\"0.900em\">1</mn>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}[\\max(0,pos-1)]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or\n(ii) a blank one-hot vector </span>\n  <math alttext=\"\\mathbf{e}_{\\text{blank}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119838;</mi>\n        <mtext mathsize=\"0.900em\">blank</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{e}_{\\text{blank}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nis inserted with equal probability.\nThis step introduces alignment jitter to capture CTC-specific repetitions and blank separations, mitigating CTC imprecision and enhancing robustness, without which performance degrades notably based on experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "without",
                    "ctc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">By combining these three operations, CPS transforms clean symbolic labels into noisy multi-frame pseudo-posteriors that closely approximate the distributional properties of real audio. To provide a more intuitive understanding of how TASU differs from other alignment paradigms, Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 CTC Posterior Simulation (CPS) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides a concise comparison of alignment paradigms for speech LLMs. In particular, only TASU, trained solely on text, achieves zero-shot performance across multiple tasks with projector parameters trainable only. It is worth noting that LSD achieves an average downsampling ratio of nearly 6 on the experimental data, leading to substantial speedups in both training and inference.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "clean",
                    "tasu",
                    "other",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nSince TASU relies on reliable CTC posterior probabilities, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SenseVoice-Small</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the speech encoder and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Qwen2.5-1.5B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the language model backbone.\nThe projector is instantiated as a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Linear&#8211;SiLU&#8211;Linear</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> module, with only its parameters being trainable. Bottleneck is typically set to 1024. For broader speech understanding tasks, it is set to 2048, as in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "only",
                    "ctc",
                    "posterior"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present experimental results in two parts. First, we show that TASU enables zero-shot speech recognition and, when used as a curriculum pre-training stage, allows models fine-tuned only on source-domain audio data to generalize effectively to new domains.\nSecond, we evaluate TASU on multitask speech understanding, where it achieves zero-shot generalization from limited text and delivers strong performance on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "only",
                    "tasu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of TASU in speech recognition, we conduct a series of experiments as summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To enable a controlled comparison, we implement the SLAM alignment strategy proposed in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without performing downsampling to avoid potential performance degradation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean and test-other sets, TASU shows only less than 1.5% WER gap compared to the baseline, demonstrating that it can achieve reasonable semantic alignment without paired audio&#8211;text training. Furthermore, when </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transcripts are incorporated into TASU training, we observe consistent improvements on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> itself, and even surpass the baseline on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in new domain.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "without",
                    "tasu",
                    "librispeech",
                    "slam",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further explore scalability, we extend TASU as the pre-training stage of Curriculum Learning. In this stage, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcripts are used to simulate CTC posteriors for training, followed by fine-tuning with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio&#8211;text pairs. Results show that TASU not only maintains performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Librispeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> but also yields substantial gains in both </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These findings highlight the scalability of TASU in leveraging large-scale text-only resources for domain generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "only",
                    "librispeech",
                    "ctc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ablation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To further justify the rationality of the baseline presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LSD, ablation studies were conducted to compare recognition performance under the current alignment paradigm.</span>\n</p>\n\n",
                "matched_terms": [
                    "lsd",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further investigate the performance of TASU on multi-task speech semantic understanding, we conduct the experiments summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We still consider SLAM method as the baseline: using hidden states as projection features without downsampling, which reflects the prevalent alignment paradigm in most existing Speech LLMs. Given that the SLAM architecture fails to develop multitask capabilities when trained on limited task-specific data, we expanded the training data to ensure a fair comparison: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ASR, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ST, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for instruction understanding. TASU only uses text, while TASU (+SFT) uses half of audio-text pairs for the second-stage SFT. In addition, to provide a more intuitive assessment of TASU, we further compare it with the results of other Speech LLMs in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "without",
                    "tasu",
                    "librispeech",
                    "slam",
                    "sft",
                    "other",
                    "only",
                    "projection",
                    "trained",
                    "fails"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As we can see, TASU demonstrates strong zero-shot multitask generalization for speech understanding: without any audio&#8211;text pairs, it achieves better result on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than SLAM. When half of audio-text data is incorporated for SFT, the model shows rapid improvement on the ASR and ST tasks. Notably, TASU even surpasses several large-scale Speech LLMs, underscoring its efficiency as a lightweight yet effective paradigm for speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "without",
                    "tasu",
                    "slam",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TASU, a novel alignment paradigm for Speech LLMs trained solely on text data. On the one hand, TASU enables zero-shot speech recognition with only a minor accuracy drop. It can further serve as the first stage of curriculum learning in ASR, improving performance on new target domains while preserving recognition accuracy on the source domain. On the other hand, TASU delivers strong zero-shot multitask speech understanding with limited text data, highlighting its potential as a simple yet effective paradigm for scalable and generalizable Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "work",
                    "other",
                    "only",
                    "trained"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Speech understanding multitask generalization with TASU.\nThe models in the upper block are built upon the same components and training setup, and share the same multitask data, while TASU only uses text.\nResults are reported as WER%, BLEU and Accuracy.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">Train Audio</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">Duration (h)</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">LibriSpeech</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text ltx_font_italic\">clean / other</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">(WER%<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">CoVoST2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.25pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">MMSU</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.25pt 3.0pt;\">(ACC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.47 / 10.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TASU (+SFT)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.9k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.28 / 6.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-2.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SLAM</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.30 / 7.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.34</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:-1.0pt;padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.70</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SALMONN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.25pt 3.0pt;\">\n<math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">k</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.10 / 4.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GLM-4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.25pt 3.0pt;\">\n<math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">k</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.82 / 7.66</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.51</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Step-Audio</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">130B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.25pt 3.0pt;\">\n<math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m7\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">k</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.36 / 6.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.25pt 3.0pt;\">\n<math alttext=\"&gt;100\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m8\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&gt;</mo><mn mathsize=\"0.900em\">100</mn></mrow><annotation encoding=\"application/x-tex\">&gt;100</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">k</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.37 / 4.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.25pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.57</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "werâ†“downarrow",
            "15b",
            "while",
            "wer",
            "setup",
            "librispeech",
            "size",
            "130b",
            "slam",
            "glm4voice",
            "stepaudio",
            "100100k",
            "audio",
            "covost2",
            "13b",
            "salmonn",
            "understanding",
            "reported",
            "enâ†’rightarrowzh",
            "training",
            "mmsu",
            "accuracy",
            "text",
            "upon",
            "bleuâ†‘uparrow",
            "built",
            "09k",
            "18k",
            "block",
            "sft",
            "results",
            "speech",
            "only",
            "qwen25omni",
            "same",
            "components",
            "clean",
            "multitask",
            "tasu",
            "share",
            "model",
            "train",
            "accâ†‘uparrow",
            "duration",
            "data",
            "upper",
            "other",
            "generalization",
            "uses",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nSince TASU relies on reliable CTC posterior probabilities, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SenseVoice-Small</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the speech encoder and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Qwen2.5-1.5B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the language model backbone.\nThe projector is instantiated as a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Linear&#8211;SiLU&#8211;Linear</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> module, with only its parameters being trainable. Bottleneck is typically set to 1024. For broader speech understanding tasks, it is set to 2048, as in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further investigate the performance of TASU on multi-task speech semantic understanding, we conduct the experiments summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We still consider SLAM method as the baseline: using hidden states as projection features without downsampling, which reflects the prevalent alignment paradigm in most existing Speech LLMs. Given that the SLAM architecture fails to develop multitask capabilities when trained on limited task-specific data, we expanded the training data to ensure a fair comparison: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ASR, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for ST, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for instruction understanding. TASU only uses text, while TASU (+SFT) uses half of audio-text pairs for the second-stage SFT. In addition, to provide a more intuitive assessment of TASU, we further compare it with the results of other Speech LLMs in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in Speech Large Language Models (Speech LLMs) have paved the way for unified architectures across diverse speech understanding tasks. However, prevailing alignment paradigms rely heavily on large-scale audio-text paired data and computationally intensive training, yet often exhibit limited generalization to unseen domains or tasks. To address these limitations, we propose TASU (Text-only Alignment for Speech Understanding), a novel alignment paradigm that can leverage only unpaired text data to guide cross-modal alignment. Experiments show that TASU achieves competitive zero-shot speech recognition. Leveraging this property, it can further function as a pre-training stage in curriculum learning, enhancing domain generalization in speech recognition. Ultimately, TASU can extend its zero-shot generalization to a wide range of speech understanding tasks and notably outperforms prominent Speech LLMs including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark, establishing TASU as an efficient and scalable alignment paradigm for Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasu",
                    "understanding",
                    "data",
                    "glm4voice",
                    "training",
                    "mmsu",
                    "speech",
                    "stepaudio",
                    "generalization",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAutomatic speech recognition, Speech large language model, Speech understanding</span>\n</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have demonstrated remarkable capability in contextual reasoning and multitask learning, and have been increasingly applied to speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional cascaded systems that rely on automatic speech recognition (ASR) to provide textual input, modern Speech LLMs align speech and text modalities directly through mechanisms such as continuous feature projection or discrete token augmentation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These approaches have enabled state-of-the-art (SOTA) performance in both single-task settings and broad multi-task speech understanding&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "multitask",
                    "understanding",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, existing alignment paradigms face two major limitations.\nFirst, continuous feature projection, though capable of preserving detailed audio information, often introduces substantial redundancy.\nSuch redundancy not only increases computational cost during training and inference but also raises the risk of overfitting&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Second, mitigating these issues typically requires massive amounts of paired audio&#8211;text data and complex training pipelines in order to achieve competitive multitask performance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "multitask",
                    "data",
                    "training",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To alleviate the issue of redundancy in continuous audio features, earlier studies explored alternative representation refinement techniques. The </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC lattice</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, first introduced in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, organizes frame-level CTC posterior distributions into a compact structure that represents all possible alignment paths. Building on this lattice, Chen </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed the Phoneme Synchronous Decoding (PSD) and Label Synchronous Decoding (LSD) methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which exploit CTC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> posteriors to perform efficient variable frame rate search and effectively reduce redundant acoustic frames.\nLiu </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further proposed PSD joint training within end-to-end ASR models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, verifying that the extracted audio semantic representations accelerate model training with almost no loss of semantic information. This representation also enables the speech and text modalities to be aligned at a comparable level of information flow.\nMoreover, compared with raw audio hidden embeddings, CTC posteriors exhibit stronger structural similarity to text, which makes it possible to approximate them using one-hot vectors derived from transcripts.\nThis insight suggests that training can rely on minimal, or even no, real speech data, substantially mitigating the two limitations discussed earlier.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "data",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Motivated by these, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TASU (<span class=\"ltx_text ltx_font_bold\">T</span>ext-only <span class=\"ltx_text ltx_font_bold\">A</span>lignment for <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">U</span>nderstanding)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel alignment paradigm that achieves robust cross-modal alignment without relying on audio supervision. We similarly use LSD to extract audio CTC posteriors into compact &#8220;codebook&#8221;-like features, preserving semantic content while removing redundancy. From the text side, we introduce </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CTC posterior simulation (CPS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which mimics real CTC distributions, including frame dropping and repetition, to generate pseudo-&#8220;codebooks&#8221; from text-only data. This dual design allows TASU to bridge modalities efficiently while keeping the LLM backbone frozen, thus retaining its inherent multitask capability. In this work, we focus on semantic speech understanding tasks, which are representative of core challenges in spoken language processing and well-suited for evaluating multitask performance in Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "while",
                    "audio",
                    "multitask",
                    "tasu",
                    "understanding",
                    "data",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Zero-shot Speech Recognition and Domain Generalization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We show that TASU alone delivers zero-shot ASR with small accuracy degradation relative to audio-text supervision in in-domain evaluation; when leveraged as a curriculum pre-training stage, it further enhances domain generalization while preserving source-domain accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "while",
                    "tasu",
                    "speech",
                    "accuracy",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multitask Generalization in Speech Understanding:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TASU enables Speech LLMs to achieve strong zero-shot generalization on speech understanding tasks using limited task-specific text data. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU surpasses mainstream alignment paradigms such as SLAM at the same data scale, and further outperforms large-scale speech models including </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SALMONN-13B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">GLM-4-Voice</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Step-Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "same",
                    "multitask",
                    "tasu",
                    "understanding",
                    "slam",
                    "data",
                    "glm4voice",
                    "mmsu",
                    "speech",
                    "stepaudio",
                    "generalization",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU: Audio-Efficient and Generalizable Speech&#8211;Text Alignment:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LSD achieves nearly 6</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> downsampling, greatly accelerating while enhancing semantic extraction and also alleviating overfitting; meanwhile, CPS markedly reduces the reliance on audio data and helps domain generalization in recognition and multitask generalization in speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "while",
                    "audio",
                    "tasu",
                    "multitask",
                    "understanding",
                    "data",
                    "speech",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">There are already methods that leverage CTC posterior probabilities to address the two issues outlined in the Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. First, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> uses CTC signals to downsample acoustic features more effectively and&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to some extent, demonstrates strong instruction-following ability. However, when relying only on a small amount of paired audio-text data, its multitask performance degrades markedly, with accuracy on multiple-choice tasks approaching chance. In contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> employs CTC posteriors as weights to reweight LLM word embeddings for acoustic representation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding more structured representations; yet it sacrifices multitask capability and requires large-scale data to re-align the LLM&#8217;s vocabulary.</span>\n</p>\n\n",
                "matched_terms": [
                    "multitask",
                    "uses",
                    "data",
                    "accuracy",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional alignment strategies for Speech LLMs often rely on encoder hidden states with heuristic subsampling, which either produce redundant and noisy representations or risk discarding critical information. In addition, acoustic features exhibit high temporal variability that mismatches the structured nature of text embeddings, making cross-modal alignment challenging. To address these issues, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech and text directly at the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">CTC posterior level</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The key idea is to establish a unified posterior interface for both training and inference:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasu",
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcriptions are tokenized into one-hot vectors and transformed into pseudo-posteriors by the CPS module, which supervise the trainable projector.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this way, TASU enables text-only training while ensuring that both modalities share compact, structured, and semantically aligned posterior representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "while",
                    "share",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CTC decoding typically involves large portions of blank symbols and consecutive repetitions of the same token&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDirectly feeding such posteriors into a Speech LLM would propagate redundancy and obscure semantics.\nLSD is designed to compact the sequence while preserving semantic fidelity through two operations.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "same",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To enable training with text-only data, we propose CPS, which converts each ground-truth token into a pseudo-posterior sequence </span>\n  <math alttext=\"\\tilde{\\mathbf{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119826;</mi>\n        <mo mathsize=\"0.900em\">~</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CPS consists of three stochastic stages that mimic the variability of real CTC outputs, as detailed in Algorithm&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#algorithm1\" style=\"font-size:90%;\" title=\"In 3.1 Label-Synchronous Decoding (LSD) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">By combining these three operations, CPS transforms clean symbolic labels into noisy multi-frame pseudo-posteriors that closely approximate the distributional properties of real audio. To provide a more intuitive understanding of how TASU differs from other alignment paradigms, Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.2 CTC Posterior Simulation (CPS) &#8227; 3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides a concise comparison of alignment paradigms for speech LLMs. In particular, only TASU, trained solely on text, achieves zero-shot performance across multiple tasks with projector parameters trainable only. It is worth noting that LSD achieves an average downsampling ratio of nearly 6 on the experimental data, leading to substantial speedups in both training and inference.</span>\n</p>\n\n",
                "matched_terms": [
                    "clean",
                    "audio",
                    "tasu",
                    "understanding",
                    "data",
                    "other",
                    "training",
                    "speech",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the two core processes described in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S3\" style=\"font-size:90%;\" title=\"3 TEXT-ONLY ALIGNMENT FOR SPEECH UNDERSTANDING &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TASU can enable zero-shot transfer from text-only training to speech inference.\nTo validate its rationality and effectiveness, we conduct a series of controlled experiments with step-by-step verification.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasu",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Data.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor ASR, the datasets include </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CommonVoice4</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor speech-to-text translation (ST), we use </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CoVoST2 En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Zh</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and for spoken instruction understanding, we adopt </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLURP</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "covost2",
                    "librispeech",
                    "enâ†’rightarrowzh",
                    "understanding",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor LSD, parameter </span>\n  <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#964;</mi>\n      <annotation encoding=\"application/x-tex\">\\tau</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is set to </span>\n  <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.9</mn>\n      <annotation encoding=\"application/x-tex\">0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For CPS, we set the label smoothing range </span>\n  <math alttext=\"(\\lambda_{\\text{low}},\\lambda_{\\text{high}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#955;</mi>\n          <mtext mathsize=\"0.900em\">low</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#955;</mi>\n          <mtext mathsize=\"0.900em\">high</mtext>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\lambda_{\\text{low}},\\lambda_{\\text{high}})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"(0.8,1.0)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <mn mathsize=\"0.900em\">0.8</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1.0</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(0.8,1.0)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the deletion and duplication probabilities, </span>\n  <math alttext=\"p_{\\text{del}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">del</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{del}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"p_{\\text{dup}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mtext mathsize=\"0.900em\">dup</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">p_{\\text{dup}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, are both set to </span>\n  <math alttext=\"0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.05</mn>\n      <annotation encoding=\"application/x-tex\">0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The learning rate is fixed at </span>\n  <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with 5 training epochs.\nCheckpoints are selected when the evaluation loss stops decreasing.</span>\n</p>\n\n",
                "matched_terms": [
                    "setup",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Dataset and Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our model on both ASR and Speech Understanding tasks. For ASR, we report Word Error Rate (WER) on the standard in-domain test sets. To further assess generalization, we employ </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TED-LIUM 3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, testing robustness to a distinct topical and acoustic domain (lectures). For speech understanding task, we assess performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. WER is computed using the official </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Wenet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> toolkit&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "setup",
                    "model",
                    "understanding",
                    "mmsu",
                    "speech",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present experimental results in two parts. First, we show that TASU enables zero-shot speech recognition and, when used as a curriculum pre-training stage, allows models fine-tuned only on source-domain audio data to generalize effectively to new domains.\nSecond, we evaluate TASU on multitask speech understanding, where it achieves zero-shot generalization from limited text and delivers strong performance on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "tasu",
                    "multitask",
                    "understanding",
                    "data",
                    "results",
                    "mmsu",
                    "speech",
                    "generalization",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of TASU in speech recognition, we conduct a series of experiments as summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Experimental Details &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To enable a controlled comparison, we implement the SLAM alignment strategy proposed in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without performing downsampling to avoid potential performance degradation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean and test-other sets, TASU shows only less than 1.5% WER gap compared to the baseline, demonstrating that it can achieve reasonable semantic alignment without paired audio&#8211;text training. Furthermore, when </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transcripts are incorporated into TASU training, we observe consistent improvements on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SlideSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> itself, and even surpass the baseline on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in new domain.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "tasu",
                    "librispeech",
                    "slam",
                    "training",
                    "speech",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further explore scalability, we extend TASU as the pre-training stage of Curriculum Learning. In this stage, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text transcripts are used to simulate CTC posteriors for training, followed by fine-tuning with </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio&#8211;text pairs. Results show that TASU not only maintains performance on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Librispeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> but also yields substantial gains in both </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TedLium-3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Slidespeech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These findings highlight the scalability of TASU in leveraging large-scale text-only resources for domain generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "librispeech",
                    "training",
                    "results",
                    "generalization",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Model architecture and training setup kept unchanged in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03310v1#S5.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 5.1 Zero-Shot Recognition and Domain Generalization via TASU Curriculum Pre-training &#8227; 5 Results and Evaluation &#8227; TASU: Text-Only Alignment for Speech Understanding\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. SLAM refers to the alignment paradigm adopted in </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SLAM-LLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Considering the differences in training configurations and convergence issues of the alignment paradigms, results for </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LegoSLM</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">AlignFormer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are not reported. We find that LSD can almost fully preserve the semantic information of speech and also alleviates model overfitting, while playing an indispensable role within TASU.</span>\n</p>\n\n",
                "matched_terms": [
                    "while",
                    "tasu",
                    "setup",
                    "model",
                    "reported",
                    "slam",
                    "training",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As we can see, TASU demonstrates strong zero-shot multitask generalization for speech understanding: without any audio&#8211;text pairs, it achieves better result on </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">MMSU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than SLAM. When half of audio-text data is incorporated for SFT, the model shows rapid improvement on the ASR and ST tasks. Notably, TASU even surpasses several large-scale Speech LLMs, underscoring its efficiency as a lightweight yet effective paradigm for speech understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasu",
                    "multitask",
                    "model",
                    "understanding",
                    "slam",
                    "sft",
                    "data",
                    "mmsu",
                    "speech",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TASU, a novel alignment paradigm for Speech LLMs trained solely on text data. On the one hand, TASU enables zero-shot speech recognition with only a minor accuracy drop. It can further serve as the first stage of curriculum learning in ASR, improving performance on new target domains while preserving recognition accuracy on the source domain. On the other hand, TASU delivers strong zero-shot multitask speech understanding with limited text data, highlighting its potential as a simple yet effective paradigm for scalable and generalizable Speech LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "while",
                    "tasu",
                    "multitask",
                    "understanding",
                    "data",
                    "other",
                    "speech",
                    "accuracy",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the future, we aim to further refine the CPS approach to narrow the gap between real CTC posteriors derived from audio and pseudo-posteriors generated from text. This will enable a more accurate audio-free alignment paradigm. Moreover, by incorporating large-scale text data, we plan to explore the scalability and performance of this alignment method on a greater scale.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data",
                    "text"
                ]
            }
        ]
    }
}