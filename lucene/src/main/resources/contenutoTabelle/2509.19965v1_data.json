{
    "S3.T1": {
        "source_file": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "caption": "Table 1: Comparison with SOTA approaches on the HDTF [41] (top) and MEAD [31] (bottom) datasets.",
        "body": "Method\nPSNR ↑\\uparrow\n\nSSIM ↑\\uparrow\n\nLPIPS ↓\\downarrow\n\nFID ↓\\downarrow\n\nFVD ↓\\downarrow\n\nE-FID ↓\\downarrow\n\nF1 ↑\\uparrow\n\nSync ↑\\uparrow\n\nC​C​CV↑CCC_{V}\\uparrow\nC​C​CA↑CCC_{A}\\uparrow\n\n\nHallo\n30.63\n0.71\n0.18\n28.91\n156.32\n1.38\n0.68\n7.58\n0.53\n0.56\n\n\nEchoMimic\n28.90\n0.51\n0.44\n71.33\n153.84\n1.32\n0.63\n6.20\n0.51\n0.49\n\n\nVExpress\n29.41\n0.61\n0.36\n58.63\n350.18\n1.85\n0.46\n8.07\n0.32\n0.30\n\n\nAniportrait\n30.64\n0.66\n0.23\n42.38\n449.51\n1.97\n0.41\n3.15\n0.11\n0.12\n\n\nOurs\n32.97\n0.73\n0.17\n27.67\n149.67\n1.24\n0.71\n7.03\n0.56\n0.58\n\n\nHallo\n31.69\n0.86\n0.11\n31.30\n147.27\n1.41\n0.65\n6.25\n0.54\n0.51\n\n\nEchoMimic\n29.07\n0.68\n0.42\n63.25\n164.75\n1.50\n0.61\n5.98\n0.53\n0.50\n\n\nVExpress\n29.34\n0.65\n0.39\n61.48\n443.96\n2.31\n0.42\n7.25\n0.34\n0.25\n\n\nAniportrait\n30.36\n0.80\n0.16\n51.40\n508.36\n2.87\n0.31\n2.06\n0.13\n0.11\n\n\nOurs\n32.21\n0.86\n0.09\n28.47\n147.87\n1.36\n0.67\n6.84\n0.57\n0.54",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">PSNR <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SSIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LPIPS <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">FVD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E-FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">F1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Sync <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><math alttext=\"CCC_{V}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">CCC_{V}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><math alttext=\"CCC_{A}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">CCC_{A}\\uparrow</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Hallo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">28.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">156.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EchoMimic</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">28.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">71.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">153.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">6.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">VExpress</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">350.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">8.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Aniportrait</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">449.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">32.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">27.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">149.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">1.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Hallo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">31.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">147.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">6.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EchoMimic</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">164.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">5.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">VExpress</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">61.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">443.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">7.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Aniportrait</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">508.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">32.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">147.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">1.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">6.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "approaches",
            "ssim",
            "datasets",
            "hallo",
            "↑uparrow",
            "echomimic",
            "c​c​cv↑cccvuparrow",
            "bottom",
            "psnr",
            "top",
            "fid",
            "sync",
            "method",
            "aniportrait",
            "↓downarrow",
            "comparison",
            "sota",
            "fvd",
            "lpips",
            "efid",
            "ours",
            "c​c​ca↑cccauparrow",
            "hdtf",
            "mead",
            "vexpress"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Comparison.</span>\nWe perform quantitative comparison with several SOTA, Hallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, Aniportrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>, Echomimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and VExpress <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite> on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, <math alttext=\"CCC_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{V}</annotation></semantics></math> and <math alttext=\"CCC_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{A}</annotation></semantics></math>, our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appropriate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo.\nNotably, despite using less training data than the other methods, our approach achieves better results across various metrics.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">User Study.</span>\nWe conduct a user study to further evaluate the quality of videos generated by our method and by other SOTA methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>. All videos in the study include a balanced representation of genders, with varied ages, poses, and expressions. The study contained 20 participants, all of whom are Master&#8217;s or PhD students with a background in Computer Science. Each participant is shown videos generated from the same image and audio inputs across all methods. They are then asked to rate each video on a scale from 1 to 5 based on lip sync, motion diversity, video smoothness, and overall naturalness. We collect the ratings and compute the average percentage score for each method. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.T2\" title=\"Table 2 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Participants give higher ratings to our approach in terms of video quality, naturalness and motion consistency. Our performance in lip sync is comparable to that of other methods, which is consistent with the findings presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Diffusion models have recently gained significant attention in talking face generation.\nHallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> leverages the Stable Diffusion Model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> with the ReferenceNet to maintain appearance consistency and introduces a hierarchical audio-visual cross-attention mechanism to align audio and visual features.\nVASA-1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> operates in a disentangled latent space to enable precise and expressive facial animations. AniTalker <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> use universal motion representation to capture a wide range of facial dynamics including subtle expressions and head movements. X-Portrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> employs a conditional diffusion model enhanced with a hierarchical patch-based local control module for accurate and coherent motion transfer. Diff2Lip <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> and DiffTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> use Latent Diffusion Models (LDMs) conditioned on audio features, reference images, masked ground-truth images, and facial landmarks. GAIA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite> disentangles motion and appearance to preserve identity while enabling speech-driven motion synthesis.\nEchoMimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and AniPortrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> employ concurrent training, conditioning simultaneously on audio signals and facial landmarks to generate realistic portrait animations.\nVividTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> leverages a 3D hybrid prior to decompose facial expressions and head movements, using a learnable codebook for natural motion and a dual-branch Motion VAE to generate dense motion fields.\nEMO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> utilizes Audio2Video diffusion model, integrating weak condition constraints such as face locators and motion guidance, bypassing the need for intermediate 3D models.\nMODA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite> employs a mapping-once network with dual attention mechanisms to convert audio into motion representations, where one attention captures accurate lip-sync and the other models natural head and eye movements.</p>\n\n",
                "matched_terms": [
                    "hallo",
                    "aniportrait",
                    "echomimic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe train our model using VFHQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, HDTF <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>, and a selection of in-the-wild scene clips shared by Hallo3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>. To enhance training data quality: all videos are resized to 512x512 resolution; we filter out extreme side-profile views by detecting facial landmarks with MediaPipe <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> (ensuring clarity in the lip region); videos containing multiple speakers are excluded (maintaining speaker identity consistency). In the end, we obtain around 80 hours of video data, with individual clips ranging from 3 to 20 seconds in length. All videos are standardized to 25 fps to ensure temporal consistency across samples. Audio tracks are resampled to 16 kHz and normalized. When present, background music and noise are removed, as they can negatively impact emotion recognition and audio-to-motion alignment. For evaluation, we use <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> videos from each of HDTF <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> and emotion-aware MEAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "hdtf",
                    "mead",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nWe employ several metrics to evaluate the performance of our model. To assess the similarity between the generated and ground truth images, we compute PSNR, SSIM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, and LPIPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite>. Additionally, we use FID and FVD to measure how closely the generated data matches that of the actual data. To evaluate the accuracy of facial expressions, we compute: (i) Expression-FID (E-FID) for AUs (following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> - expression parameters are extracted using face reconstruction model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite>); (ii) F1 score for AUs; (iii) Concordance Correlation Coefficient (CCC) for Valence <math alttext=\"(CCC_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{V})</annotation></semantics></math> and Arousal <math alttext=\"(CCC_{A})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{A})</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, we adopt the confidence score from SyncNet (Sync) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> to measure audio-visual synchronization.</p>\n\n",
                "matched_terms": [
                    "ssim",
                    "psnr",
                    "efid",
                    "fvd",
                    "lpips",
                    "fid",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Comparison.</span>\nWe perform a qualitative comparison of our proposed method with SOTA approaches <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> on both the HDTF and MEAD datasets. To evaluate the comparison, we extract frames from the original videos and use the first frame as the reference image. The corresponding audio is also extracted and use it as the driving audio.\nHallo, Echomimic, and VExpress achieve accurate lip sync on both datasets. However, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Hallo produces artifacts in some frames (e.g., a black region around the teeth and unrealistic teeth), while Echomimic exhibits inconsistent motion between frames and unnatural lip (e.g., teeth appearing on the lips). VExpress often fail to generate the correct pose (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), does not maintain identity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and introduces excessive motion. Aniportrait struggles with lip sync accuracy (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), showing minimal or indistinct lip movements across frames. In contrast, our approach generates talking face videos with good overall quality, natural lip movements, and consistent identity. Furthermore, by incorporating emotional awareness in addition to lip synchronization, our model generates videos that are both realistic and emotionally expressive. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method generates emotions that appear more realistic and better aligned with the ground truth than those generated by SOTA approaches.</p>\n\n",
                "matched_terms": [
                    "method",
                    "aniportrait",
                    "approaches",
                    "comparison",
                    "datasets",
                    "echomimic",
                    "sota",
                    "hdtf",
                    "mead",
                    "vexpress",
                    "hallo",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Motion (A2M) Module.</span>\nWhen we omit the A2M module and train our model using only the original video frames, the generated video fails to provide correct synchronization and lacks the necessary subject movements (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-middle). By adding the motion frames driven by audio and training our model on these audio-aware frames, we ensure accurate lip movements. Based on FVD and Sync scores (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T3\" title=\"Table 3 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), our approach provides better video quality while maintaining the accurate lip synchronization.</p>\n\n",
                "matched_terms": [
                    "sync",
                    "fvd"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "caption": "Table 2: Results (in %) of the user study for comparison of our method with SOTA methods.",
        "body": "Method\n\n\nLip Sync.\n\n\n\n\nMotion Diversity\n\n\n\n\nVideo Smoothness\n\n\n\n\nOverall Naturalness\n\n\n\n\nHallo\n\n\n25.73\n\n\n\n\n23.61\n\n\n\n\n28.53\n\n\n\n\n27.35\n\n\n\n\nEchoMimic\n\n\n21.11\n\n\n\n\n19.02\n\n\n\n\n10.82\n\n\n\n\n20.30\n\n\n\n\nVExpress\n\n\n26.10\n\n\n\n\n7.25\n\n\n\n\n5.23\n\n\n\n\n7.58\n\n\n\n\nAniportrait\n\n\n2.05\n\n\n\n\n5.37\n\n\n\n\n2.55\n\n\n\n\n2.00\n\n\n\n\nOurs\n\n\n25.01\n\n\n\n\n44.75\n\n\n\n\n52.87\n\n\n\n\n42.77",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Method</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Lip Sync.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Motion Diversity</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Video Smoothness</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Overall Naturalness</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Hallo</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">25.73</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">23.61</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">28.53</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">27.35</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EchoMimic</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">21.11</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">19.02</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">10.82</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">20.30</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">VExpress</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">26.10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.25</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.23</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">7.58</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Aniportrait</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.05</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5.37</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.55</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">2.00</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">25.01</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">44.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">52.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">42.77</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "study",
            "lip",
            "hallo",
            "smoothness",
            "echomimic",
            "video",
            "methods",
            "results",
            "overall",
            "sync",
            "method",
            "aniportrait",
            "comparison",
            "sota",
            "naturalness",
            "ours",
            "motion",
            "diversity",
            "user",
            "our",
            "vexpress"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">User Study.</span>\nWe conduct a user study to further evaluate the quality of videos generated by our method and by other SOTA methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>. All videos in the study include a balanced representation of genders, with varied ages, poses, and expressions. The study contained 20 participants, all of whom are Master&#8217;s or PhD students with a background in Computer Science. Each participant is shown videos generated from the same image and audio inputs across all methods. They are then asked to rate each video on a scale from 1 to 5 based on lip sync, motion diversity, video smoothness, and overall naturalness. We collect the ratings and compute the average percentage score for each method. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.T2\" title=\"Table 2 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Participants give higher ratings to our approach in terms of video quality, naturalness and motion consistency. Our performance in lip sync is comparable to that of other methods, which is consistent with the findings presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction.\nHowever, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues.\nAdditionally, most methods condition on a single reference image, restricting the model&#8217;s ability to represent dynamic changes in actions or attributes across time.\nTo address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity.\nTo ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio.\nFinally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism.\nQuantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at\n<a class=\"ltx_ref ltx_href\" href=\"https://novicemm.github.io/synchrorama/index.html\" style=\"--ltx-fg-color:#005FAF;\" title=\"\">https://novicemm.github.io/synchrorama</a>.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "study",
                    "motion",
                    "diversity",
                    "user",
                    "lip",
                    "our",
                    "naturalness",
                    "overall",
                    "smoothness",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Talking face generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> aims to animate a portrait image by integrating audio. It has gained popularity in various domains such as video games, film industries, social media, digital marketing and education sectors. Existing approaches to talking face generation primarily utilize either GANs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> or diffusion models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>. GAN-based methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite> use a combination of audio and visual encoders to extract features from speech and video frames, which are then processed by a generator network to produce synchronized lip movements.\nIn contrast, diffusion-based models generate results through an iterative refinement process, leading to higher-quality and more temporally coherent outputs. Despite these improvements, generating a realistic talking face remains challenging, as it requires precise lip synchronization, and head poses with the given speech. In addition to lip synchronization, maintaining the visual coherence and capturing the richness of expression and emotion remain open challenges in talking face generation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "methods",
                    "video",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion-based talking face generation approaches typically use a reference network and denoising UNet as their backbone. Visual appearance information is fed into Reference network, which is then integrates with the Denoising UNet during the denoising process. However, providing only visual information is insufficient, as existing methods mostly use a single frame extracted from the input video as the reference image during training. Relying on a single frame to represent the entire video may fail to capture potential changes in scenes, subject&#8217;s actions, and attributes in subsequent frames. As a result, the generated video may lack detailed appearance consistency. To overcome this, we incorporate textual descriptions that contain changes in scenes (temporal) information as additional input. Recent advancements in visual large language models such as VideoLLaMA2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>, can generate comprehensive textual descriptions of the entire video, capturing changes in scenes, actions and attributes. This textual information complements the detailed visual cues of the extracted reference image, enhancing the visual quality of the generated video.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another important aspect in talking face generation is motion consistency. Generated video should exhibit realistic head movement and lip should be synchronized with the speech. To ensure this, we also introduce an audio-to-motion module, producing motion frames driven by the audio. Training the model with these audio-driven motion frames guarantees realistic head movements and accurate lip synchronization.\nFurthermore, we introduce several loss functions in our work, including syncloss, emo loss, facial action units (AU) loss, and attr-action loss.</p>\n\n",
                "matched_terms": [
                    "our",
                    "motion",
                    "video",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">main contributions</span> of our paper can be summarized as: 1) We propose a novel talking face generation framework, which leverages multi-modal information, including visual, textual and audio data. 2) We introduce a multi-modal emotion embedding module to enrich emotional expressiveness in the generated videos. 3) We embed LLM generated scene description to make the generation better aligned to context. 4) We present an Audio-to-Motion (A2M) module, designed to generate realistic motion frames synchronized with the audio. Our quantitative and qualitative experiments demonstrate that our approach achieves superior performance compared to the state-of-the-art (SOTA) methods.</p>\n\n",
                "matched_terms": [
                    "our",
                    "methods",
                    "motion",
                    "sota"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion models have recently gained significant attention in talking face generation.\nHallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> leverages the Stable Diffusion Model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> with the ReferenceNet to maintain appearance consistency and introduces a hierarchical audio-visual cross-attention mechanism to align audio and visual features.\nVASA-1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> operates in a disentangled latent space to enable precise and expressive facial animations. AniTalker <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> use universal motion representation to capture a wide range of facial dynamics including subtle expressions and head movements. X-Portrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> employs a conditional diffusion model enhanced with a hierarchical patch-based local control module for accurate and coherent motion transfer. Diff2Lip <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> and DiffTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> use Latent Diffusion Models (LDMs) conditioned on audio features, reference images, masked ground-truth images, and facial landmarks. GAIA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite> disentangles motion and appearance to preserve identity while enabling speech-driven motion synthesis.\nEchoMimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and AniPortrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> employ concurrent training, conditioning simultaneously on audio signals and facial landmarks to generate realistic portrait animations.\nVividTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> leverages a 3D hybrid prior to decompose facial expressions and head movements, using a learnable codebook for natural motion and a dual-branch Motion VAE to generate dense motion fields.\nEMO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> utilizes Audio2Video diffusion model, integrating weak condition constraints such as face locators and motion guidance, bypassing the need for intermediate 3D models.\nMODA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite> employs a mapping-once network with dual attention mechanisms to convert audio into motion representations, where one attention captures accurate lip-sync and the other models natural head and eye movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "hallo",
                    "aniportrait",
                    "echomimic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference image, textual description, and input audio, our model generates a lip-synchronized talking face video while preserving natural head movements, facial expressions, emotions, and overall appearance consistency. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our framework integrates several critical components: Denoising UNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> as the backbone network, ReferenceNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> to encode the reference image, proposed Audio-to-Motion (A2M) module to generate the motion frames synchronized with the input audio, and proposed multi-modal emotion embedding module to align the generated video with the emotional content of the audio. The careful incorporation of these components ensures the generation of realistic and contextually coherent talking face videos. The following sections present a detailed explanation of each component.</p>\n\n",
                "matched_terms": [
                    "our",
                    "overall",
                    "motion",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our framework, we adopt the Denoising UNet architecture from the Stable Diffusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> 1.5 as the backbone network by incorporating 4 distinct attention mechanisms within each Transformer block (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)). Specifically, Spatial Attention layer focuses on the important spatial regions, ensuring the model captures the relevant facial features such as mouth, eyes, and expressions while preserving the identity consistency. Audio Attention layer integrates the audio features into the generation process. Cross Attention layer bridges the multiple modalities such as reference image, audio, and textual description by dynamically aligning them throughout the denoising process. Finally, Temporal Attention layer ensures smooth and coherent motion transitions between the consecutive video frames. These integrated attention mechanisms enhance the model&#8217;s ability to capture and integrate spatial and temporal relationships effectively within the generation process.</p>\n\n",
                "matched_terms": [
                    "our",
                    "motion",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main challenge in audio-driven motion generation is to generate temporally consistent and expressive motion sequences that align with the input audio. To address this, we propose a VAE-based Audio-to-Motion module, which takes both a reference image and audio features as input and generates motion frames (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b)). We first extract audio features using wav2vec 2.0, and use them as conditional inputs to the VAE encoder.\nIn VAE, the encoder typically outputs parameters <math alttext=\"(\\mu,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>&#956;</mi><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mu,\\sigma)</annotation></semantics></math> that define a Gaussian distribution, from which a latent variable <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> is sampled as <math alttext=\"z_{q}\\sim\\mathcal{N}(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>q</mi></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{q}\\sim\\mathcal{N}(0,1)</annotation></semantics></math>. However, this Gaussian prior tends to force the posterior toward the mean, which limits output diversity and generative power. To overcome this, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite> and introduce a Volume-Preserving Normalizing Flow (VP-Flow) that transforms the simple latent variable <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> into a more expressive latent representation <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math> (i.e, <math alttext=\"z_{p}=f_{VP-Flow}(z_{q})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>p</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>&#8722;</mo><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{p}=f_{VP-Flow}(z_{q})</annotation></semantics></math>) (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c)). The VP-Flow is composed of a stack of residual affine coupling layers and channel-wise flip operations, which ensure invertibility and maintain the volume of the latent space during transformation. We then pass the transformed latent variable <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>, together with the audio features to the VAE decoder, which then synthesizes a sequence of visual motion frames that are aligned with the input audio. Therefore, VP-Flow-based conditioning in our A2M module enhances the diversity and realism of generated motion, while the audio features ensure accurate lip sync and smooth transitions across frames.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "motion",
                    "lip",
                    "our",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This combined embedding is fed into the denoising UNet via a cross-attention layer. By employing multiple modalities, our method captures a broader range of emotional nuances, resulting in a more expressive talking face that closely aligns with the speaker&#8217;s intended emotion.</p>\n\n",
                "matched_terms": [
                    "our",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sync Loss.</span>\nThis loss enforces precise temporal alignment between generated lip movements and input audio, which is critical for perceptual realism in talking face. We evaluate synchronization using <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, which estimates the temporal offset between audio and video, and detects the location of misalignment. The loss compares the global temporal alignment signals, which are more robust to the local jitter and better reflect the human perception.\nIt is defined as:</p>\n\n",
                "matched_terms": [
                    "sync",
                    "video",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Facial Action Unit (AU) Loss.</span>\nThis loss provides fine-grained supervision to enhance the expressiveness and realism of the generated videos. It enables the model to accurately capture subtle facial muscle movements, such as eyebrow raises or lip stretches, which are often overlooked by global emotion descriptors such as valence-arousal values. This targeted supervision helps improve the semantic accuracy and coherence of facial expressions, ensuring that localized facial actions align with the intended emotion. Furthermore, AU loss promotes the temporal and spatial consistency across video frames. It also complements the multi-modal emotion embedding by refining the local expression details, while the embedding captures the overall emotional tone.\nAU loss is computed as the squared L2 distance between the predicted AUs of the generated video and the ground truth AUs from the original video. Formally, it is defined as:</p>\n\n",
                "matched_terms": [
                    "overall",
                    "video",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details.</span>\nWe conduct all experiments on NVIDIA A100 GPUs. The model is trained in two stages, each for 30k steps, with batch size 4 and resolution 512x512. In the first stage, we train the model to encode appearance information using a reference image and its corresponding textual description as inputs to the ReferenceNet. Each training sample consists of a 14-frame video clip, from which one frame is randomly selected as the reference frame and another as the target frame. Additionally, motion frames are added to ensure temporal smoothness and coherence across frames. During this stage, the VAE encoder/decoder and the CLIP image/text encoders are kept frozen, while only the ReferenceNet and the Denoising UNet are optimized. Both networks are initialized with weights from the original Stable Diffusion model. In the second stage, the model is trained on full video sequences with audio injection for audio-visual alignment and emotion embedding to capture emotional cues. In both stages, a learning rate of <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> is used. During inference, the model takes a reference image, its textual prompt, and driving audio as inputs, and generates a video sequence by animating the reference image in sync with the audio. We use DDIM sampling with 40 steps to generate each output video clip.</p>\n\n",
                "matched_terms": [
                    "sync",
                    "motion",
                    "smoothness",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe train our model using VFHQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib34\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, HDTF <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite>, and a selection of in-the-wild scene clips shared by Hallo3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>. To enhance training data quality: all videos are resized to 512x512 resolution; we filter out extreme side-profile views by detecting facial landmarks with MediaPipe <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> (ensuring clarity in the lip region); videos containing multiple speakers are excluded (maintaining speaker identity consistency). In the end, we obtain around 80 hours of video data, with individual clips ranging from 3 to 20 seconds in length. All videos are standardized to 25 fps to ensure temporal consistency across samples. Audio tracks are resampled to 16 kHz and normalized. When present, background music and noise are removed, as they can negatively impact emotion recognition and audio-to-motion alignment. For evaluation, we use <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> videos from each of HDTF <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib41\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> and emotion-aware MEAD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib31\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nWe employ several metrics to evaluate the performance of our model. To assess the similarity between the generated and ground truth images, we compute PSNR, SSIM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, and LPIPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite>. Additionally, we use FID and FVD to measure how closely the generated data matches that of the actual data. To evaluate the accuracy of facial expressions, we compute: (i) Expression-FID (E-FID) for AUs (following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> - expression parameters are extracted using face reconstruction model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite>); (ii) F1 score for AUs; (iii) Concordance Correlation Coefficient (CCC) for Valence <math alttext=\"(CCC_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{V})</annotation></semantics></math> and Arousal <math alttext=\"(CCC_{A})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{A})</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, we adopt the confidence score from SyncNet (Sync) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> to measure audio-visual synchronization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Comparison.</span>\nWe perform quantitative comparison with several SOTA, Hallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, Aniportrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>, Echomimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and VExpress <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite> on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, <math alttext=\"CCC_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{V}</annotation></semantics></math> and <math alttext=\"CCC_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{A}</annotation></semantics></math>, our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appropriate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo.\nNotably, despite using less training data than the other methods, our approach achieves better results across various metrics.</p>\n\n",
                "matched_terms": [
                    "method",
                    "aniportrait",
                    "methods",
                    "comparison",
                    "results",
                    "sync",
                    "sota",
                    "our",
                    "vexpress",
                    "hallo",
                    "echomimic",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Comparison.</span>\nWe perform a qualitative comparison of our proposed method with SOTA approaches <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> on both the HDTF and MEAD datasets. To evaluate the comparison, we extract frames from the original videos and use the first frame as the reference image. The corresponding audio is also extracted and use it as the driving audio.\nHallo, Echomimic, and VExpress achieve accurate lip sync on both datasets. However, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Hallo produces artifacts in some frames (e.g., a black region around the teeth and unrealistic teeth), while Echomimic exhibits inconsistent motion between frames and unnatural lip (e.g., teeth appearing on the lips). VExpress often fail to generate the correct pose (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), does not maintain identity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and introduces excessive motion. Aniportrait struggles with lip sync accuracy (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), showing minimal or indistinct lip movements across frames. In contrast, our approach generates talking face videos with good overall quality, natural lip movements, and consistent identity. Furthermore, by incorporating emotional awareness in addition to lip synchronization, our model generates videos that are both realistic and emotionally expressive. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method generates emotions that appear more realistic and better aligned with the ground truth than those generated by SOTA approaches.</p>\n\n",
                "matched_terms": [
                    "method",
                    "overall",
                    "aniportrait",
                    "motion",
                    "comparison",
                    "echomimic",
                    "sota",
                    "lip",
                    "our",
                    "vexpress",
                    "hallo",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform the following ablation studies to evaluate contribution of different components of our method:</p>\n\n",
                "matched_terms": [
                    "our",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Motion (A2M) Module.</span>\nWhen we omit the A2M module and train our model using only the original video frames, the generated video fails to provide correct synchronization and lacks the necessary subject movements (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-middle). By adding the motion frames driven by audio and training our model on these audio-aware frames, we ensure accurate lip movements. Based on FVD and Sync scores (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T3\" title=\"Table 3 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), our approach provides better video quality while maintaining the accurate lip synchronization.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "lip",
                    "our",
                    "sync",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM based Textual Integration.</span>\nWe evaluate our model without integrating the textual input, relying mainly on appearance cues from the reference image. Without additional semantic guidance from text, the visual quality of the generated video degrades, although lip sync remains unaffected. The model generates inconsistent facial attributes, such as noticeable artifacts in makeup near the eyes and distortions in lip shape (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-right). Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T4\" title=\"Table 4 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the quantitative results with and without textual integration.</p>\n\n",
                "matched_terms": [
                    "results",
                    "lip",
                    "our",
                    "sync",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emo Loss.</span>\nAlthough the multi-modal emotion embedding module enables the model to generate emotional expressions, we found that training the model without emo loss results in less expressive and less accurate facial emotion (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, top-left). By including the emo loss, which aligns the model&#8217;s output with valence and arousal cues derived from audio, the generated expressions become more expressive and realistic (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, bottom-left). The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T5\" title=\"Table 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> validate the effectiveness of the emo loss in our model.</p>\n\n",
                "matched_terms": [
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AU Loss.</span>\nWhen we exclude the AU loss, the generated faces exhibit limited facial actions; for example, lip region appears flat and expressionless, and eyes remain closed despite the emotional context (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, top-right). With AU loss, the model is able to generate fine-grained facial movements, such as the appearance of expression lines near the mouth and properly opened eyes, even when the reference image shows closed eyes (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, bottom-right). The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T5\" title=\"Table 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that AU loss enhances the realism and expressiveness of the generated videos.</p>\n\n",
                "matched_terms": [
                    "results",
                    "lip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel framework that effectively integrates multi-modal emotional nuances with audio-driven motion modules to generate high-quality, lip-synchronized talking face video. By conditioning the model on visual and textual info, we provide better visual details. Comprehensive experiments, ablation studies &amp; user evaluations demonstrate our model outperforms SOTA. Results show that SynchroRaMa is an effective tool for creating high-quality, emotionally rich, and lip-synchronized talking face videos.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "user",
                    "sota",
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our proposed approach achieves promising results, it has some limitations which we will address in future work. First, since our model is trained mainly on portrait images, it is currently unable to generate full-body talking videos. Additionally, because the model was trained only on English language data, its performance on other languages needs to be evaluated.</p>\n\n",
                "matched_terms": [
                    "results",
                    "our"
                ]
            }
        ]
    },
    "S6.T3": {
        "source_file": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "caption": "Table 3: Effect of addition of Audio-to-Motion (A2M) Module.",
        "body": "Method\nSync ↑\\uparrow\n\nFVD ↓\\downarrow\n\n\n\nw/o A2M module\n4.33\n182.25\n\n\nw/ A2M module\n6.84\n147.87",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Sync <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">FVD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/o A2M module</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">182.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">w/ A2M module</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">6.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">147.87</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "method",
            "↓downarrow",
            "addition",
            "a2m",
            "effect",
            "module",
            "fvd",
            "↑uparrow",
            "sync",
            "audiotomotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Motion (A2M) Module.</span>\nWhen we omit the A2M module and train our model using only the original video frames, the generated video fails to provide correct synchronization and lacks the necessary subject movements (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-middle). By adding the motion frames driven by audio and training our model on these audio-aware frames, we ensure accurate lip movements. Based on FVD and Sync scores (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T3\" title=\"Table 3 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), our approach provides better video quality while maintaining the accurate lip synchronization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction.\nHowever, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues.\nAdditionally, most methods condition on a single reference image, restricting the model&#8217;s ability to represent dynamic changes in actions or attributes across time.\nTo address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity.\nTo ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio.\nFinally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism.\nQuantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at\n<a class=\"ltx_ref ltx_href\" href=\"https://novicemm.github.io/synchrorama/index.html\" style=\"--ltx-fg-color:#005FAF;\" title=\"\">https://novicemm.github.io/synchrorama</a>.</p>\n\n",
                "matched_terms": [
                    "module",
                    "a2m",
                    "audiotomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another important aspect in talking face generation is motion consistency. Generated video should exhibit realistic head movement and lip should be synchronized with the speech. To ensure this, we also introduce an audio-to-motion module, producing motion frames driven by the audio. Training the model with these audio-driven motion frames guarantees realistic head movements and accurate lip synchronization.\nFurthermore, we introduce several loss functions in our work, including syncloss, emo loss, facial action units (AU) loss, and attr-action loss.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audiotomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">main contributions</span> of our paper can be summarized as: 1) We propose a novel talking face generation framework, which leverages multi-modal information, including visual, textual and audio data. 2) We introduce a multi-modal emotion embedding module to enrich emotional expressiveness in the generated videos. 3) We embed LLM generated scene description to make the generation better aligned to context. 4) We present an Audio-to-Motion (A2M) module, designed to generate realistic motion frames synchronized with the audio. Our quantitative and qualitative experiments demonstrate that our approach achieves superior performance compared to the state-of-the-art (SOTA) methods.</p>\n\n",
                "matched_terms": [
                    "module",
                    "a2m",
                    "audiotomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference image, textual description, and input audio, our model generates a lip-synchronized talking face video while preserving natural head movements, facial expressions, emotions, and overall appearance consistency. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our framework integrates several critical components: Denoising UNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> as the backbone network, ReferenceNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> to encode the reference image, proposed Audio-to-Motion (A2M) module to generate the motion frames synchronized with the input audio, and proposed multi-modal emotion embedding module to align the generated video with the emotional content of the audio. The careful incorporation of these components ensures the generation of realistic and contextually coherent talking face videos. The following sections present a detailed explanation of each component.</p>\n\n",
                "matched_terms": [
                    "module",
                    "a2m",
                    "audiotomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main challenge in audio-driven motion generation is to generate temporally consistent and expressive motion sequences that align with the input audio. To address this, we propose a VAE-based Audio-to-Motion module, which takes both a reference image and audio features as input and generates motion frames (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b)). We first extract audio features using wav2vec 2.0, and use them as conditional inputs to the VAE encoder.\nIn VAE, the encoder typically outputs parameters <math alttext=\"(\\mu,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>&#956;</mi><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mu,\\sigma)</annotation></semantics></math> that define a Gaussian distribution, from which a latent variable <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> is sampled as <math alttext=\"z_{q}\\sim\\mathcal{N}(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>q</mi></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{q}\\sim\\mathcal{N}(0,1)</annotation></semantics></math>. However, this Gaussian prior tends to force the posterior toward the mean, which limits output diversity and generative power. To overcome this, we follow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib39\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite> and introduce a Volume-Preserving Normalizing Flow (VP-Flow) that transforms the simple latent variable <math alttext=\"z_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">z_{q}</annotation></semantics></math> into a more expressive latent representation <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math> (i.e, <math alttext=\"z_{p}=f_{VP-Flow}(z_{q})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>p</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>&#8722;</mo><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{p}=f_{VP-Flow}(z_{q})</annotation></semantics></math>) (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c)). The VP-Flow is composed of a stack of residual affine coupling layers and channel-wise flip operations, which ensure invertibility and maintain the volume of the latent space during transformation. We then pass the transformed latent variable <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m7\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>, together with the audio features to the VAE decoder, which then synthesizes a sequence of visual motion frames that are aligned with the input audio. Therefore, VP-Flow-based conditioning in our A2M module enhances the diversity and realism of generated motion, while the audio features ensure accurate lip sync and smooth transitions across frames.</p>\n\n",
                "matched_terms": [
                    "module",
                    "a2m",
                    "sync",
                    "audiotomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nWe employ several metrics to evaluate the performance of our model. To assess the similarity between the generated and ground truth images, we compute PSNR, SSIM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, and LPIPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite>. Additionally, we use FID and FVD to measure how closely the generated data matches that of the actual data. To evaluate the accuracy of facial expressions, we compute: (i) Expression-FID (E-FID) for AUs (following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> - expression parameters are extracted using face reconstruction model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite>); (ii) F1 score for AUs; (iii) Concordance Correlation Coefficient (CCC) for Valence <math alttext=\"(CCC_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{V})</annotation></semantics></math> and Arousal <math alttext=\"(CCC_{A})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{A})</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, we adopt the confidence score from SyncNet (Sync) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> to measure audio-visual synchronization.</p>\n\n",
                "matched_terms": [
                    "sync",
                    "fvd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Comparison.</span>\nWe perform quantitative comparison with several SOTA, Hallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, Aniportrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>, Echomimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and VExpress <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite> on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, <math alttext=\"CCC_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{V}</annotation></semantics></math> and <math alttext=\"CCC_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{A}</annotation></semantics></math>, our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appropriate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo.\nNotably, despite using less training data than the other methods, our approach achieves better results across various metrics.</p>\n\n",
                "matched_terms": [
                    "method",
                    "sync",
                    "fvd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Comparison.</span>\nWe perform a qualitative comparison of our proposed method with SOTA approaches <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> on both the HDTF and MEAD datasets. To evaluate the comparison, we extract frames from the original videos and use the first frame as the reference image. The corresponding audio is also extracted and use it as the driving audio.\nHallo, Echomimic, and VExpress achieve accurate lip sync on both datasets. However, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Hallo produces artifacts in some frames (e.g., a black region around the teeth and unrealistic teeth), while Echomimic exhibits inconsistent motion between frames and unnatural lip (e.g., teeth appearing on the lips). VExpress often fail to generate the correct pose (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), does not maintain identity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and introduces excessive motion. Aniportrait struggles with lip sync accuracy (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), showing minimal or indistinct lip movements across frames. In contrast, our approach generates talking face videos with good overall quality, natural lip movements, and consistent identity. Furthermore, by incorporating emotional awareness in addition to lip synchronization, our model generates videos that are both realistic and emotionally expressive. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method generates emotions that appear more realistic and better aligned with the ground truth than those generated by SOTA approaches.</p>\n\n",
                "matched_terms": [
                    "addition",
                    "method",
                    "sync"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">User Study.</span>\nWe conduct a user study to further evaluate the quality of videos generated by our method and by other SOTA methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>. All videos in the study include a balanced representation of genders, with varied ages, poses, and expressions. The study contained 20 participants, all of whom are Master&#8217;s or PhD students with a background in Computer Science. Each participant is shown videos generated from the same image and audio inputs across all methods. They are then asked to rate each video on a scale from 1 to 5 based on lip sync, motion diversity, video smoothness, and overall naturalness. We collect the ratings and compute the average percentage score for each method. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.T2\" title=\"Table 2 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Participants give higher ratings to our approach in terms of video quality, naturalness and motion consistency. Our performance in lip sync is comparable to that of other methods, which is consistent with the findings presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "sync"
                ]
            }
        ]
    },
    "S6.T4": {
        "source_file": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "caption": "Table 4: Effect of inclduing LLM generated textual description.",
        "body": "Method\nPSNR ↑\\uparrow\n\nSSIM ↑\\uparrow\n\nLPIPS ↓\\downarrow\n\n\n\n\n\nw/o textual integration\n\n\n31.30\n0.62\n0.15\n\n\n\n\nw/ textual integration\n\n\n32.21\n0.86\n0.09",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">PSNR <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">SSIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">LPIPS <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">w/o textual integration</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">w/ textual integration</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">32.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.09</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "integration",
            "method",
            "textual",
            "↓downarrow",
            "generated",
            "ssim",
            "effect",
            "psnr",
            "inclduing",
            "llm",
            "description",
            "lpips",
            "↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM based Textual Integration.</span>\nWe evaluate our model without integrating the textual input, relying mainly on appearance cues from the reference image. Without additional semantic guidance from text, the visual quality of the generated video degrades, although lip sync remains unaffected. The model generates inconsistent facial attributes, such as noticeable artifacts in makeup near the eyes and distortions in lip shape (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-right). Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T4\" title=\"Table 4 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the quantitative results with and without textual integration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction.\nHowever, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues.\nAdditionally, most methods condition on a single reference image, restricting the model&#8217;s ability to represent dynamic changes in actions or attributes across time.\nTo address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity.\nTo ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio.\nFinally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism.\nQuantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at\n<a class=\"ltx_ref ltx_href\" href=\"https://novicemm.github.io/synchrorama/index.html\" style=\"--ltx-fg-color:#005FAF;\" title=\"\">https://novicemm.github.io/synchrorama</a>.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "generated",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we introduce SynchroRaMa, a novel framework designed to generate high-quality, emotionally expressive, and lip-synchronized talking faces from the audio input, textual description and a reference image. Most previous works on emotion-aware talking face generation primarily focus on a single modality, such as text, audio or visual cues for emotion embedding. However, relying on a single modality limits model performance, as each modality contains its specific constraints, and individually, none can fulfill all requirements perfectly. Therefore, our work leverages multi-modal emotion embedding by combining textual sentiment analysis, speech-based emotion recognition, and valence-arousal (VA) emotion embedding derived from audio signals.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion-based talking face generation approaches typically use a reference network and denoising UNet as their backbone. Visual appearance information is fed into Reference network, which is then integrates with the Denoising UNet during the denoising process. However, providing only visual information is insufficient, as existing methods mostly use a single frame extracted from the input video as the reference image during training. Relying on a single frame to represent the entire video may fail to capture potential changes in scenes, subject&#8217;s actions, and attributes in subsequent frames. As a result, the generated video may lack detailed appearance consistency. To overcome this, we incorporate textual descriptions that contain changes in scenes (temporal) information as additional input. Recent advancements in visual large language models such as VideoLLaMA2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>, can generate comprehensive textual descriptions of the entire video, capturing changes in scenes, actions and attributes. This textual information complements the detailed visual cues of the extracted reference image, enhancing the visual quality of the generated video.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">main contributions</span> of our paper can be summarized as: 1) We propose a novel talking face generation framework, which leverages multi-modal information, including visual, textual and audio data. 2) We introduce a multi-modal emotion embedding module to enrich emotional expressiveness in the generated videos. 3) We embed LLM generated scene description to make the generation better aligned to context. 4) We present an Audio-to-Motion (A2M) module, designed to generate realistic motion frames synchronized with the audio. Our quantitative and qualitative experiments demonstrate that our approach achieves superior performance compared to the state-of-the-art (SOTA) methods.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "generated",
                    "llm",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference image, textual description, and input audio, our model generates a lip-synchronized talking face video while preserving natural head movements, facial expressions, emotions, and overall appearance consistency. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our framework integrates several critical components: Denoising UNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> as the backbone network, ReferenceNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> to encode the reference image, proposed Audio-to-Motion (A2M) module to generate the motion frames synchronized with the input audio, and proposed multi-modal emotion embedding module to align the generated video with the emotional content of the audio. The careful incorporation of these components ensures the generation of realistic and contextually coherent talking face videos. The following sections present a detailed explanation of each component.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "generated",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our framework, we adopt the Denoising UNet architecture from the Stable Diffusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> 1.5 as the backbone network by incorporating 4 distinct attention mechanisms within each Transformer block (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)). Specifically, Spatial Attention layer focuses on the important spatial regions, ensuring the model captures the relevant facial features such as mouth, eyes, and expressions while preserving the identity consistency. Audio Attention layer integrates the audio features into the generation process. Cross Attention layer bridges the multiple modalities such as reference image, audio, and textual description by dynamically aligning them throughout the denoising process. Finally, Temporal Attention layer ensures smooth and coherent motion transitions between the consecutive video frames. These integrated attention mechanisms enhance the model&#8217;s ability to capture and integrate spatial and temporal relationships effectively within the generation process.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ReferenceNet employs the same architecture as the Denoising UNet and guides the generation process by embedding both reference image and the textual input (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)). It ensures the preservation of facial identity and background consistency. Within the ReferenceNet, each Transformer block employs a Spatial Attention mechanism to extract features from the reference image. These features are then passed to the Denoising UNet via the corresponding Spatial Attention layers. Additionally, Cross Attention layer integrates the textual description that contains attributes and actions of the corresponding video as additional conditioning input. This provides semantic guidance to further enhance appearance consistency. This integration of visual and textual conditioning ensures identity and background consistency, as well as high-quality video generation.</p>\n\n",
                "matched_terms": [
                    "integration",
                    "description",
                    "textual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous works use one extracted frame from the original video as a reference image during training to provide appearance information (e.g., identity and background details). However, relying on a single image may fail to capture dynamic changes in actions and attributes that occur over time, e.g., a person sitting in the first frame might change position in subsequent frames - a single image cannot reflect these changes. To address this limitation, we incorporate a textual description that represents the entire video alongside the reference image. Specifically, we employ VideoLLaMA2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite> to generate a video description that captures fine-grained details such as attributes (e.g., <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;wearing earrings&#8221;</span>) and actions (e.g., <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;turning head&#8221;</span>). This textual description is encoded using a CLIP Text Encoder and is incorporated into the model through cross-attention layers in the ReferenceNet and the Denoising UNet (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)). During training, ReferenceNet processes the reference image and textual description to extract visual-textual appearance features, which are integrated into the Denoising UNet, then fused with emotion and audio features. This conditioning strategy ensures that high-level semantic cues from text influence both attributes and actions throughout the video.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attr-Action Loss</span>\nThis loss ensures that the generated video preserves the high-level semantic attributes and actions described in the original video, e.g., head movements, or appearance cues.\nWe generate the textual descriptions for both predicted and ground truth videos using VideoLLaMA2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>, and compute the loss as the cosine distance between their textual embeddings:</p>\n\n",
                "matched_terms": [
                    "textual",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details.</span>\nWe conduct all experiments on NVIDIA A100 GPUs. The model is trained in two stages, each for 30k steps, with batch size 4 and resolution 512x512. In the first stage, we train the model to encode appearance information using a reference image and its corresponding textual description as inputs to the ReferenceNet. Each training sample consists of a 14-frame video clip, from which one frame is randomly selected as the reference frame and another as the target frame. Additionally, motion frames are added to ensure temporal smoothness and coherence across frames. During this stage, the VAE encoder/decoder and the CLIP image/text encoders are kept frozen, while only the ReferenceNet and the Denoising UNet are optimized. Both networks are initialized with weights from the original Stable Diffusion model. In the second stage, the model is trained on full video sequences with audio injection for audio-visual alignment and emotion embedding to capture emotional cues. In both stages, a learning rate of <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> is used. During inference, the model takes a reference image, its textual prompt, and driving audio as inputs, and generates a video sequence by animating the reference image in sync with the audio. We use DDIM sampling with 40 steps to generate each output video clip.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nWe employ several metrics to evaluate the performance of our model. To assess the similarity between the generated and ground truth images, we compute PSNR, SSIM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib32\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite>, and LPIPS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib40\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite>. Additionally, we use FID and FVD to measure how closely the generated data matches that of the actual data. To evaluate the accuracy of facial expressions, we compute: (i) Expression-FID (E-FID) for AUs (following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> - expression parameters are extracted using face reconstruction model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite>); (ii) F1 score for AUs; (iii) Concordance Correlation Coefficient (CCC) for Valence <math alttext=\"(CCC_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{V})</annotation></semantics></math> and Arousal <math alttext=\"(CCC_{A})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(CCC_{A})</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>, we adopt the confidence score from SyncNet (Sync) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> to measure audio-visual synchronization.</p>\n\n",
                "matched_terms": [
                    "psnr",
                    "generated",
                    "lpips",
                    "ssim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Comparison.</span>\nWe perform quantitative comparison with several SOTA, Hallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, Aniportrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>, Echomimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and VExpress <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite> on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, <math alttext=\"CCC_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{V}</annotation></semantics></math> and <math alttext=\"CCC_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{A}</annotation></semantics></math>, our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appropriate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo.\nNotably, despite using less training data than the other methods, our approach achieves better results across various metrics.</p>\n\n",
                "matched_terms": [
                    "method",
                    "generated",
                    "ssim",
                    "psnr",
                    "lpips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Comparison.</span>\nWe perform a qualitative comparison of our proposed method with SOTA approaches <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> on both the HDTF and MEAD datasets. To evaluate the comparison, we extract frames from the original videos and use the first frame as the reference image. The corresponding audio is also extracted and use it as the driving audio.\nHallo, Echomimic, and VExpress achieve accurate lip sync on both datasets. However, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Hallo produces artifacts in some frames (e.g., a black region around the teeth and unrealistic teeth), while Echomimic exhibits inconsistent motion between frames and unnatural lip (e.g., teeth appearing on the lips). VExpress often fail to generate the correct pose (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), does not maintain identity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) and introduces excessive motion. Aniportrait struggles with lip sync accuracy (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S4.F3\" title=\"Figure 3 &#8227; 4 Experiments &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), showing minimal or indistinct lip movements across frames. In contrast, our approach generates talking face videos with good overall quality, natural lip movements, and consistent identity. Furthermore, by incorporating emotional awareness in addition to lip synchronization, our model generates videos that are both realistic and emotionally expressive. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.F4\" title=\"Figure 4 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method generates emotions that appear more realistic and better aligned with the ground truth than those generated by SOTA approaches.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">User Study.</span>\nWe conduct a user study to further evaluate the quality of videos generated by our method and by other SOTA methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>. All videos in the study include a balanced representation of genders, with varied ages, poses, and expressions. The study contained 20 participants, all of whom are Master&#8217;s or PhD students with a background in Computer Science. Each participant is shown videos generated from the same image and audio inputs across all methods. They are then asked to rate each video on a scale from 1 to 5 based on lip sync, motion diversity, video smoothness, and overall naturalness. We collect the ratings and compute the average percentage score for each method. The results are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S5.T2\" title=\"Table 2 &#8227; 5 Comparison with state-of-the-art Methods &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Participants give higher ratings to our approach in terms of video quality, naturalness and motion consistency. Our performance in lip sync is comparable to that of other methods, which is consistent with the findings presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "method"
                ]
            }
        ]
    },
    "S6.T5": {
        "source_file": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding",
        "caption": "Table 5: Effect of adding the multi-modal emotion embedding module and loss functions.",
        "body": "Method\nEFID ↓\\downarrow\n\nF1 ↑\\uparrow\n\nC​C​CV↑CCC_{V}\\uparrow\nC​C​CA↑CCC_{A}\\uparrow\n\n\n\n\nw/o emo embedding\n\n\n1.43\n0.58\n0.48\n0.50\n\n\nw/o emo loss\n1.41\n0.63\n0.51\n0.51\n\n\nw/o AU loss\n1.38\n0.63\n0.53\n0.48\n\n\nOurs\n1.36\n0.67\n0.57\n0.54",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EFID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">F1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><math alttext=\"CCC_{V}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">CCC_{V}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><math alttext=\"CCC_{A}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><mo stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">CCC_{A}\\uparrow</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">w/o emo embedding</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">w/o emo loss</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">w/o AU loss</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">1.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "method",
            "c​c​cv↑cccvuparrow",
            "adding",
            "↓downarrow",
            "c​c​ca↑cccauparrow",
            "embedding",
            "loss",
            "emo",
            "effect",
            "module",
            "emotion",
            "multimodal",
            "functions",
            "efid",
            "↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-modal Emotion Embedding Module.</span>\nTo evaluate the effectiveness of multi-modal emotion embedding module, we perform an experiment (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T5\" title=\"Table 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) by excluding this component. Without the module, the model fails to generate appropriate emotional facial expressions that correspond to audio cues (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-left). In contrast, when it is included, the model accurately captures and generates the intended emotional expressions based on the driving audio (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, bottom-left). This is because our multi-modal emotion embedding extracts emotional cues from multiple perspectives, including sentiment analysis, speech emotion recognition, and valence-arousal of the audio.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emo Loss.</span>\nAlthough the multi-modal emotion embedding module enables the model to generate emotional expressions, we found that training the model without emo loss results in less expressive and less accurate facial emotion (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, top-left). By including the emo loss, which aligns the model&#8217;s output with valence and arousal cues derived from audio, the generated expressions become more expressive and realistic (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, bottom-left). The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T5\" title=\"Table 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> validate the effectiveness of the emo loss in our model.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AU Loss.</span>\nWhen we exclude the AU loss, the generated faces exhibit limited facial actions; for example, lip region appears flat and expressionless, and eyes remain closed despite the emotional context (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, top-right). With AU loss, the model is able to generate fine-grained facial movements, such as the appearance of expression lines near the mouth and properly opened eyes, even when the reference image shows closed eyes (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F6\" title=\"Figure 6 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, bottom-right). The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T5\" title=\"Table 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that AU loss enhances the realism and expressiveness of the generated videos.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction.\nHowever, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues.\nAdditionally, most methods condition on a single reference image, restricting the model&#8217;s ability to represent dynamic changes in actions or attributes across time.\nTo address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity.\nTo ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio.\nFinally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism.\nQuantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at\n<a class=\"ltx_ref ltx_href\" href=\"https://novicemm.github.io/synchrorama/index.html\" style=\"--ltx-fg-color:#005FAF;\" title=\"\">https://novicemm.github.io/synchrorama</a>.</p>\n\n",
                "matched_terms": [
                    "module",
                    "embedding",
                    "emotion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, we introduce SynchroRaMa, a novel framework designed to generate high-quality, emotionally expressive, and lip-synchronized talking faces from the audio input, textual description and a reference image. Most previous works on emotion-aware talking face generation primarily focus on a single modality, such as text, audio or visual cues for emotion embedding. However, relying on a single modality limits model performance, as each modality contains its specific constraints, and individually, none can fulfill all requirements perfectly. Therefore, our work leverages multi-modal emotion embedding by combining textual sentiment analysis, speech-based emotion recognition, and valence-arousal (VA) emotion embedding derived from audio signals.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "emotion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another important aspect in talking face generation is motion consistency. Generated video should exhibit realistic head movement and lip should be synchronized with the speech. To ensure this, we also introduce an audio-to-motion module, producing motion frames driven by the audio. Training the model with these audio-driven motion frames guarantees realistic head movements and accurate lip synchronization.\nFurthermore, we introduce several loss functions in our work, including syncloss, emo loss, facial action units (AU) loss, and attr-action loss.</p>\n\n",
                "matched_terms": [
                    "module",
                    "emo",
                    "loss",
                    "functions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">main contributions</span> of our paper can be summarized as: 1) We propose a novel talking face generation framework, which leverages multi-modal information, including visual, textual and audio data. 2) We introduce a multi-modal emotion embedding module to enrich emotional expressiveness in the generated videos. 3) We embed LLM generated scene description to make the generation better aligned to context. 4) We present an Audio-to-Motion (A2M) module, designed to generate realistic motion frames synchronized with the audio. Our quantitative and qualitative experiments demonstrate that our approach achieves superior performance compared to the state-of-the-art (SOTA) methods.</p>\n\n",
                "matched_terms": [
                    "module",
                    "embedding",
                    "emotion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion models have recently gained significant attention in talking face generation.\nHallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> leverages the Stable Diffusion Model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> with the ReferenceNet to maintain appearance consistency and introduces a hierarchical audio-visual cross-attention mechanism to align audio and visual features.\nVASA-1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib37\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>]</cite> operates in a disentangled latent space to enable precise and expressive facial animations. AniTalker <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> use universal motion representation to capture a wide range of facial dynamics including subtle expressions and head movements. X-Portrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib35\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> employs a conditional diffusion model enhanced with a hierarchical patch-based local control module for accurate and coherent motion transfer. Diff2Lip <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> and DiffTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite> use Latent Diffusion Models (LDMs) conditioned on audio features, reference images, masked ground-truth images, and facial landmarks. GAIA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite> disentangles motion and appearance to preserve identity while enabling speech-driven motion synthesis.\nEchoMimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and AniPortrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> employ concurrent training, conditioning simultaneously on audio signals and facial landmarks to generate realistic portrait animations.\nVividTalk <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> leverages a 3D hybrid prior to decompose facial expressions and head movements, using a learnable codebook for natural motion and a dual-branch Motion VAE to generate dense motion fields.\nEMO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> utilizes Audio2Video diffusion model, integrating weak condition constraints such as face locators and motion guidance, bypassing the need for intermediate 3D models.\nMODA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite> employs a mapping-once network with dual attention mechanisms to convert audio into motion representations, where one attention captures accurate lip-sync and the other models natural head and eye movements.</p>\n\n",
                "matched_terms": [
                    "module",
                    "emo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference image, textual description, and input audio, our model generates a lip-synchronized talking face video while preserving natural head movements, facial expressions, emotions, and overall appearance consistency. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our framework integrates several critical components: Denoising UNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> as the backbone network, ReferenceNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> to encode the reference image, proposed Audio-to-Motion (A2M) module to generate the motion frames synchronized with the input audio, and proposed multi-modal emotion embedding module to align the generated video with the emotional content of the audio. The careful incorporation of these components ensures the generation of realistic and contextually coherent talking face videos. The following sections present a detailed explanation of each component.</p>\n\n",
                "matched_terms": [
                    "module",
                    "embedding",
                    "emotion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion embedding is an important aspect in talking face generation to provide more realistic and expressive facial movements aligned with the driven audio. To achieve this, we introduce a multi-modal emotion embedding mechanism that integrates emotional cues from three modalities - text, audio, and valence-arousal (VA) features (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Diffusion-based Talking Face Generation &#8227; 2 Related Work &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)), providing a richer perspective than any single modality alone. First, we transcribe the input audio using Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> and perform sentiment analysis on the resulting text using emotion-english-distilroberta <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite>.\nSecond, we perform speech emotion recognition (SER) on the audio using <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>. Third, we extract valence and arousal features from the audio using a fine-tuned wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>]</cite>. To ensure accurate extraction of valence-arousal features, we remove background music and divide the audio into <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m1\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> overlapping segments to capture temporal variations. The features from each segment are then concatenated to form the final representation.\nLet <math alttext=\"E_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">E_{t}</annotation></semantics></math>, <math alttext=\"E_{ser}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m3\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{ser}</annotation></semantics></math>, and <math alttext=\"E_{va}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m4\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{va}</annotation></semantics></math> denote emotion embeddings from textual sentiment analysis, SER, and VA features, respectively, computed as <math alttext=\"E_{t}=f_{\\text{Transcription}}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>Transcription</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">E_{t}=f_{\\text{Transcription}}(a)</annotation></semantics></math>, <math alttext=\"E_{ser}=f_{\\text{SER}}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>SER</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">E_{ser}=f_{\\text{SER}}(a)</annotation></semantics></math>, <math alttext=\"E_{va}=f_{\\text{VA}}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>VA</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">E_{va}=f_{\\text{VA}}(a)</annotation></semantics></math>.\nThese embeddings are concatenated to form the final emotion embedding:\n</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "emotion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This combined embedding is fed into the denoising UNet via a cross-attention layer. By employing multiple modalities, our method captures a broader range of emotional nuances, resulting in a more expressive talking face that closely aligns with the speaker&#8217;s intended emotion.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "method",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emo Loss.</span>\nThis loss enhances emotionally consistent facial expressions by aligning the temporal emotional dynamics of the generated video with those of the ground truth. Specifically, it encourages the output to reflect similar variations in valence and arousal over time. To compute it, we first remove the background music and divide the audio (from both generated and ground truth videos) into 50% overlapping segments. Employing overlapping segments ensures smoother emotional transitions and reduces sudden shifts or potential artifacts that may occur from non-overlapping segments. For each segment, we extract valence and arousal values using a transformer-based model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib29\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>]</cite>, a fine-tuned variant of wav2vec 2.0 that predicts continuous emotional dimensions (valence and arousal) directly from the audio. This yields a sequence of valence and arousal pairs that captures the evolving emotional state throughout the video. The loss provides fine-grained, temporally-aware supervision, enhancing the emotional expressiveness and coherence of the generated facial behavior. The loss is defined as:</p>\n\n",
                "matched_terms": [
                    "emo",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Facial Action Unit (AU) Loss.</span>\nThis loss provides fine-grained supervision to enhance the expressiveness and realism of the generated videos. It enables the model to accurately capture subtle facial muscle movements, such as eyebrow raises or lip stretches, which are often overlooked by global emotion descriptors such as valence-arousal values. This targeted supervision helps improve the semantic accuracy and coherence of facial expressions, ensuring that localized facial actions align with the intended emotion. Furthermore, AU loss promotes the temporal and spatial consistency across video frames. It also complements the multi-modal emotion embedding by refining the local expression details, while the embedding captures the overall emotional tone.\nAU loss is computed as the squared L2 distance between the predicted AUs of the generated video and the ground truth AUs from the original video. Formally, it is defined as:</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "loss",
                    "multimodal",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details.</span>\nWe conduct all experiments on NVIDIA A100 GPUs. The model is trained in two stages, each for 30k steps, with batch size 4 and resolution 512x512. In the first stage, we train the model to encode appearance information using a reference image and its corresponding textual description as inputs to the ReferenceNet. Each training sample consists of a 14-frame video clip, from which one frame is randomly selected as the reference frame and another as the target frame. Additionally, motion frames are added to ensure temporal smoothness and coherence across frames. During this stage, the VAE encoder/decoder and the CLIP image/text encoders are kept frozen, while only the ReferenceNet and the Denoising UNet are optimized. Both networks are initialized with weights from the original Stable Diffusion model. In the second stage, the model is trained on full video sequences with audio injection for audio-visual alignment and emotion embedding to capture emotional cues. In both stages, a learning rate of <math alttext=\"1e^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1e^{-5}</annotation></semantics></math> is used. During inference, the model takes a reference image, its textual prompt, and driving audio as inputs, and generates a video sequence by animating the reference image in sync with the audio. We use DDIM sampling with 40 steps to generate each output video clip.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Comparison.</span>\nWe perform quantitative comparison with several SOTA, Hallo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib36\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite>, Aniportrait <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite>, Echomimic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> and VExpress <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#bib.bib30\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite> on the HDTF and MEAD datasets. We evaluate all methods using their publicly available checkpoints. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Loss Functions &#8227; 3 Method &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our approach outperforms existing methods across all metrics, except for sync confidence on the HDTF and MEAD datasets, and FVD on MEAD. In terms of image-level quality, our method achieves better FID, PSNR, SSIM, and LPIPS scores compared to the SOTA approaches. According to E-FID, F1 scores of AUs, <math alttext=\"CCC_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>V</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{V}</annotation></semantics></math> and <math alttext=\"CCC_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>C</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CCC_{A}</annotation></semantics></math>, our method outperforms competing methods in both expression and emotion preservation. Better performance across these metrics indicates that the generated face videos contain appropriate expressions and emotions, which are reflected in the input audio and reference image. The lower FVD score demonstrates that our approach achieves better video quality. Additionally, our lip-synchronization performance is comparable to that of VExpress and Hallo.\nNotably, despite using less training data than the other methods, our approach achieves better results across various metrics.</p>\n\n",
                "matched_terms": [
                    "efid",
                    "method",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-to-Motion (A2M) Module.</span>\nWhen we omit the A2M module and train our model using only the original video frames, the generated video fails to provide correct synchronization and lacks the necessary subject movements (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.F5\" title=\"Figure 5 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, top-middle). By adding the motion frames driven by audio and training our model on these audio-aware frames, we ensure accurate lip movements. Based on FVD and Sync scores (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19965v1#S6.T3\" title=\"Table 3 &#8227; 6 Ablation Studies &#8227; SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), our approach provides better video quality while maintaining the accurate lip synchronization.</p>\n\n",
                "matched_terms": [
                    "module",
                    "adding"
                ]
            }
        ]
    }
}